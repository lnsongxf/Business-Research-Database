{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import collections\n",
    "import json\n",
    "import io\n",
    "import fix,parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(html,publisher):\n",
    "    global s,s_body\n",
    "    if publisher=='sae':\n",
    "        s = bs(html, 'lxml').find('head')\n",
    "        s_body=bs(html, 'lxml').find('body')\n",
    "        \n",
    "        try:\n",
    "            vol_iss_year=s_body.find('div',{\"class\":\"articleJournalNavTitle\"}).text.replace('\\n','').replace('\\t','')\n",
    "            year=vol_iss_year[-4:]\n",
    "            iss= vol_iss_year.split('Issue ')[1].split(', ')[0]         \n",
    "            vol= vol_iss_year.split('Vol ')[1].split(', ')[0]            \n",
    "        except Exception:\n",
    "            year=s_body.find('div',{\"class\":\"published-dates\"}).text.split('\\n')[1][-4:]\n",
    "            vol='';iss=''\n",
    "            pass\n",
    "        \n",
    "        source= s.find(\"meta\",{\"name\":\"citation_journal_title\"})['content']\n",
    "        title=s.find(\"meta\",{\"name\":\"dc.Title\"})['content']\n",
    "        creators=s.findAll(\"meta\",{\"name\":\"dc.Creator\"});names=[c['content'] for c in creators]         \n",
    "        abstract=s.find(\"meta\",{\"name\":\"dc.Description\"}) or ''\n",
    "        keywords=s.find(\"meta\",{\"name\":\"keywords\"}) or ''\n",
    "        url=s.find(\"meta\",{\"name\":\"dc.Source\"})['content']\n",
    "\n",
    "        \n",
    "        if abstract is not '':\n",
    "            abstract= abstract['content']\n",
    "        if keywords is not '' : \n",
    "            keywords= keywords['content']\n",
    "    \n",
    "    if publisher=='taf':\n",
    "        s = bs(html, 'lxml').find('head')\n",
    "        source= s.find(\"meta\",{\"name\":\"citation_journal_title\"})['content']\n",
    "        year=s.find(\"meta\",{\"name\":\"og:description\"})['content'][1:5]\n",
    "        vol=s.find(\"meta\",{\"name\":\"pbContext\"})['content'].split('.i')[1].split(';')[0].lstrip('0')\n",
    "        iss=s.find(\"meta\",{\"name\":\"pbContext\"})['content'].split('.v')[1].split('.i')[0].lstrip('0')\n",
    "        title=s.find(\"meta\",{\"name\":\"dc.Title\"})['content']\n",
    "        creators=s.findAll(\"meta\",{\"name\":\"dc.Creator\"});names=[c['content'] for c in creators]\n",
    "        abstract=s.find(\"meta\",{\"name\":\"dc.Description\"}) or ''\n",
    "        keywords=s.find(\"meta\",{\"name\":\"keywords\"}) or ''\n",
    "        url=s.find(\"meta\",{\"name\":\"pbContext\"})['content'].split('article:article:')[1].split(';')[0] or ''\n",
    "        \n",
    "        \n",
    "        names=[' '.join(name.strip().split()) for name in names]\n",
    "        if url is not '':\n",
    "            url='http://www.tandfonline.com/doi/'+url\n",
    "        if keywords is not '':\n",
    "            keywords=keywords['content']\n",
    "        if abstract is not '':\n",
    "            abstract=abstract['content']\n",
    "\n",
    "\n",
    "    if publisher=='bla':\n",
    "        s = bs(html, 'lxml').find('head')\n",
    "        s_body=bs(html, 'lxml').find('body')\n",
    "        source=s.find('meta',{'name':'citation_journal_title'})['content']\n",
    "        year=s.find('meta',{'name':'citation_publication_date'})['content'][:4]\n",
    "        vol= s.find('meta',{'name':'citation_volume'})['content']\n",
    "        iss= s.find('meta',{'name':'citation_issue'})['content']\n",
    "        title=s.find('meta',{'property':'og:title'})['content']\n",
    "        creators=s.findAll('meta',{'name':'citation_author'}); names=[c['content'] for c in creators]\n",
    "        abstract=s_body.find('div',{'class':'article-section__content mainAbstract'}) or ''\n",
    "        keywords_html=s.findAll('meta',{'name':'citation_keywords'})\n",
    "        keywords=', '.join([k['content'] for k in keywords_html])\n",
    "        url=s.find('meta',{'name':'citation_abstract_html_url'})['content']\n",
    "\n",
    "        if abstract is not '':\n",
    "            abstract=abstract.find('p').text\n",
    "\n",
    "    article={\n",
    "        'source'   : source,\n",
    "        'date'     : year,\n",
    "        'volume'   : vol,\n",
    "        'issue'    : iss,\n",
    "        'title'    : title,\n",
    "        'creator'  : names,\n",
    "        'abstract' : abstract,        \n",
    "        'keywords'  : keywords,\n",
    "        'url'      : url        \n",
    "    }\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def src2redif(publisher, journal_name,base_folder,redif_folder):\n",
    "    global article,f\n",
    "    folder=base_folder+\"/htmls\"\n",
    "    files=os.listdir(folder);print len(files)\n",
    "    l=[]\n",
    "    i=0\n",
    "    redif_file=redif_folder+(base_folder+'.redif').replace('/','_')\n",
    "    try:\n",
    "        os.remove(redif_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "    for f in files:\n",
    "        i=i+1\n",
    "        if 'html' in f:\n",
    "            if i % 100 ==0:\n",
    "                print i,\n",
    "            html=open(folder+'/'+f).read()\n",
    "            article=parse(html,publisher)\n",
    "            with io.open(redif_file, 'a', encoding=\"utf-8\") as f:\n",
    "                f.write(unicode('Template-Type: ReDIF-Article 1.0\\n'))\n",
    "                f.write(unicode('Title: '+article['title']+'\\n'))\n",
    "                for creator in article['creator']:\n",
    "                    f.write(unicode('Author-Name: '+creator+'\\n'))\n",
    "                f.write(unicode('Abstract: '+article['abstract']+'\\n'))\n",
    "                f.write(unicode('Year: '+article['date']+'\\n'))\n",
    "                f.write(unicode('Volume: '+article['volume']+'\\n'))\n",
    "                f.write(unicode('Issue: '+article['issue']+'\\n'))\n",
    "                f.write(unicode('Keywords: '+article['keywords']+'\\n'))\n",
    "                f.write(unicode('File-url: '+article['url']+'\\n'))\n",
    "                f.write(unicode('Journal: '+article['source']+'\\n'))            \n",
    "                f.write(unicode('\\n'))            \n",
    "    return (redif_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redif_folder='redif/src/parsed/'\n",
    "if not os.path.exists(redif_folder):\n",
    "    os.makedirs(redif_folder)\n",
    "    \n",
    "with open('../OpenScience/subject_journals.json') as data_file:    \n",
    "    subject_journals = json.load(data_file)\n",
    "\n",
    "journals=[]\n",
    "for subject in subject_journals:\n",
    "    journals=journals+(subject['journals'])\n",
    "print len(journals)\n",
    "    \n",
    "for journal in journals:    \n",
    "    source=journal['jname']\n",
    "    journal_name=journal['journal']\n",
    "    data=journal['data']\n",
    "    for type_pub in data:   \n",
    "        file_type=type_pub['type']\n",
    "        publisher=type_pub['publisher']\n",
    "        if file_type=='src' and publisher in ['taf','bla','sae']:\n",
    "            print publisher,source,journal_name,\n",
    "            base_folder=file_type+'/'+publisher+'/'+source\n",
    "            src2redif(publisher,journal_name,base_folder,redif_folder)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
