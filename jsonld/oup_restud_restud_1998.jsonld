[{"ex:issue": "1", "ex:abstract": "We develop a methodology for analyzing the revenue and efficiency performance of auctions when buyers have private information about their willingness to pay and ability to pay. We then apply the framework to scenarios involving standard auction mechanisms. In the simplest case, where bidders face absolute spending limits, first-price auctions yield higher expected revenue and social surplus than second-price auctions. The revenue dominance of first-price auctions over second-price auctions carries over to the case where bidders have access to credit. These rankings are explained by differences in the extent to which financial constraints bind in different auction formats.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Standard Auctions with Financially Constrained Bidders", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00033", "ex:pages": "1-21", "@context": {"dc": "http://schema.org/"}, "ex:template": "﻿Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Yeon-Koo Che"}, {"ex:name": "Ian Gale"}]}, {"ex:issue": "1", "ex:abstract": "Drawing upon recent contributions in the statistical literature, we present new results on the convergence of recursive, stochastic algorithms which can be applied to economic models with learning and which generalize previous results. The formal results provide probability bounds for convergence which can be used to describe the local stability under learning of rational expectations equilibria in stochastic models. Economic examples include local stability in a multivariate linear model with multiple equilibria and global convergence in a model with a unique equilibrium.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Economic Dynamics with Learning: New Stability Results", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00034", "ex:pages": "23-44", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "George W. Evans"}, {"ex:name": "Seppo Honkapohja"}]}, {"ex:issue": "1", "ex:abstract": "This paper develops and implements a semiparametric estimator for investigating, with panel data, the importance of human capital and time nonseparable preferences to females when aggregate shocks are present. It provides a set of conditions for making statistical inferences about agents' expectations of their correlated future choices, from a short panel. Under the assumption that observed allocations are Pareto optimal, a dynamic model of female labour supply and participation is estimated, in which experience on the job raises future wages, and time spent off the job in the past directly affects current utility (or, indirectly through productivity in the nonmarket sector).", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "The Effect of Work Experience on Female Wages and Labour Supply", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00035", "ex:pages": "45-85", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Sumru Altuğ"}, {"ex:name": "Robert A. Miller"}]}, {"ex:issue": "1", "ex:abstract": "In many theoretical models, it is assumed that there are costs associated with adjusting prices. In this paper, the existence, type, and magnitude of such costs are investigated empirically. A discrete-choice dynamic-programming model that nests fixed and variable costs is developed, and econometric estimates of the adjustment-cost function are obtained. The data used to implement the model consist of weekly retail prices and sales of three brands of saltine crackers sold by four chains of grocery stores in a small U.S. town. The study has several distinguishing characteristics. First, the data are highly disaggregate (at the level of brand and store). Second, variables are sampled at weekly intervals. Third, the model nests fixed- and variable-adjustment costs and can therefore determine their relative importance. Finally, the discrete-choice model is both structural and dynamic.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Optimal Pricing with Costly Adjustment: Evidence from Retail-Grocery Prices", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00036", "ex:pages": "87-107", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Margaret E. Slade"}, {"ex:name": "G.R.E.Q.A.M."}]}, {"ex:issue": "1", "ex:abstract": "An econometric methodology is proposed for reconciling inaccurate measures of latent data which are subject to accounting constraints. The method deals with the case in which the measurement errors are serially correlated, generalizing previous contributions. A class of efficient estimators are derived for the latent data. Consistent estimators for the weight matrices applied to the observed information based on a linear regression procedure are obtained together with confidence interval estimators for these weight matrices. Approximate confidence intervals are suggested for the latent data themselves together with specification tests for the assumptions underlying the procedure. An application of the proposed method is made to U.K. Gross Domestic Product in constant prices for 1958Q1–1989Q4.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Measurement Error with Accounting Constraints: Point and Interval Estimation for Latent Data with an Application to U.K. Gross Domestic Product", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00037", "ex:pages": "109-134", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Richard J. Smith"}, {"ex:name": "Martin R. Weale"}, {"ex:name": "Steven E. Satchell"}]}, {"ex:issue": "1", "ex:abstract": "This paper analyses the sustainability of inter-generational transfers in Samuelson's consumption-loan model when agents are imperfectly informed about past events. We find that with mild informational constraints, transfers cannot be supported by pure-strategy equilibria. Mixed strategies allow transfers to be sustained even if agents have little information, so that a version of the Folk theorem holds. However, these equilibria are not robust. If each agent's utility function is subjected to a small random perturbation as in Harsanyi (1973), these mixed strategy equilibria unravel, and only the zero-transfer allocation survives as the unique rationalizable outcome. This result is an example of mixed strategy equilibrium of an extensive form game which cannot be purified.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Informational Constraints and the Overlapping Generations Model: Folk and Anti-Folk Theorems", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00038", "ex:pages": "135-149", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "V. Bhaskar"}]}, {"ex:issue": "1", "ex:abstract": "Because of the inherent uncertainty, promotion of innovation critically depends on screening mechanisms to select projects. This paper studies the relationship between bureaucracy and financial constraints as two such mechanisms. The lack of commitment to hard financial constraints interferes with its ex post screening capability; ex ante bureaucratic screening is optimally chosen as a substitute. However, bureaucracy makes mistakes by rejecting promising projects and delays innovation, and the efficiency loss due to soft financial constraints increases as prior knowledge becomes worse and as research stage investment requirements become lower. In a centralized economy, bureaucracy may reduce the number of parallel projects, particularly for projects with higher uncertainties and less research stage requirements. This theory fits much of the evidence and in particular it explains why the computer industry, but not the nuclear or aerospace industries, has fared so poorly in centralized economies.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Innovation and Bureaucracy Under Soft and Hard Budget Constraints", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00039", "ex:pages": "151-164", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Yingyi Qian"}, {"ex:name": "Chenggang Xu"}]}, {"ex:issue": "1", "ex:abstract": "This paper develops a general approach to characterizing optimal income tax and enforcement schemes. Our analysis clarifies the nature of the interplay between tax rates, audit probabilities and penalties for misreporting. In particular, it is shown that for a variety of objective functions for the principal the optimal tax schedule is in general concave (at least weakly) and monotonic; the marginal tax rates determine the audit probabilities; and less harsh penalties lead to higher enforcement costs. Our results imply that there exists a tradeoff between equity and efficiency considerations in the enforcement context which is similar to that in the moral hazard context for tax policy.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "A General Characterization of Optimal Income Tax Enforcement", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00040", "ex:pages": "165-183", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Parkash Chander"}, {"ex:name": "Louis L. Wilde"}]}, {"ex:issue": "2", "ex:abstract": "This paper solves for equilibria of sequential bid (or English) auctions with affiliated values when jump bidding strategies may be employed to intimidate one's opponents. In these equilibria, jump bids serve as correlating devices which select asymmetric bidding functions to be played subsequently. Each possibility of jump bidding provides a Pareto improvement for the bidders from the symmetric equilibrium of a sealed bid, second-price auction. The expanded set of equilibria can approximate either first- or second-price outcomes and produce exactly the set of expected prices between those two bounds. These results contrast with standard conclusions that equate English and second-price auctions.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Strategic Jump Bidding in English Auctions", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00041", "ex:pages": "185-210", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Christopher Avery"}]}, {"ex:issue": "2", "ex:abstract": "Most policy debates on regional policies implicitly assume that there is too much concentration. In our two-region model of migration, desirable concentration fails to occur under some conditions, and undesirable concentration occurs in others. In the latter case, even though the individuals collectively prefer to be distributed evenly across the two regions, they end up concentrating into one region in their pursuit of a better life. Hence, the freedom to move can be self-defeating. We characterize the conditions for such self-defeating concentration to occur. The coordination failures between the entry decision of service firms and the migration decision of individuals are caused by the incompleteness of markets due to the endogeneity of the range of services available, which deprive the agents of the opportunity to signal demand and supply for potential services. The argument does not rely on price distortions, the nonconvexities implied by increasing returns and nontradedness, congestion externalities, nor myopia in migration decisions.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Self-Defeating Regional Concentration", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00042", "ex:pages": "211-234", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Kiminori Matsuyama"}, {"ex:name": "Takaaki Takahashi"}]}, {"ex:issue": "2", "ex:abstract": "This paper extends the bargaining and matching literature, such as Rubinstein and Wolinsky (1985), by considering a new matching process. We assume that a central information agency exists, such as real estate agencies in the housing market and employment agencies (or newspapers) in the labour market, which puts traders into direct contact with each other. With heterogeneity of trader preferences, equilibrium trade is characterized by existing traders on each side of the market trying to match with the flow of new traders on the other side (since existing traders have already sampled and rejected each other). Two procedures of trade co-exist, namely a strategic bilateral bargaining process and a competitive bidding process, depending on the number of potential matches a new trader obtains. We characterize the unique symmetric Markov perfect equilibrium to this stochastic trading game.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Strategic Bargaining and Competitive Bidding in a Dynamic Market Equilibrium", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00043", "ex:pages": "235-260", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Melvyn G. Coles"}, {"ex:name": "Abhinay Muthoo"}]}, {"ex:issue": "2", "ex:abstract": "This paper develops the method of matching as an econometric evaluation estimator. A rigorous distribution theory for kernel-based matching is presented. The method of matching is extended to more general conditions than the ones assumed in the statistical literature on the topic. We focus on the method of propensity score matching and show that it is not necessarily better, in the sense of reducing the variance of the resulting estimator, to use the propensity score method even if propensity score is known. We extend the statistical literature on the propensity score by considering the case when it is estimated both parametrically and nonparametrically. We examine the benefits of separability and exclusion restrictions in improving the efficiency of the estimator. Our methods also apply to the econometric selection bias estimator.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Matching As An Econometric Evaluation Estimator", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00044", "ex:pages": "261-294", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "James J. Heckman"}, {"ex:name": "Hidehiko Ichimura"}, {"ex:name": "Petra Todd"}]}, {"ex:issue": "2", "ex:abstract": "We investigate an infinite horizon bargaining problem in which a firm and a worker bargain over two dimensions, quality and wage. The worker has private information about his type. Only the uninformed firm makes an offer and it can offer a menu of quality-wage contracts instead of single one. We show that for all discount factors, the unique sequential equilibrium outcome is separating without delay; the firm separates the types of worker with a menu of contracts in the first period. Our result shows that in multi-dimensional bargaining, the \"Coase Conjecture\" holds in the sense that the game ends in the first period. But it fails in the sense that the uninformed party can preserve the entire bargaining power.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Bargaining over a Menu of Wage Contracts", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00045", "ex:pages": "295-305", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Gyu Ho Wang"}]}, {"ex:issue": "2", "ex:abstract": "Speculators buy an asset hoping to sell it later to investors with higher private valuations. If agents are uncertain about the distribution of private valuations and about the beliefs of others about this distribution, a beauty contest with an infinite hierarchy of beliefs arises. Under Harsanyi's assumption of a common prior the infinite beliefs hierarchy is readily solved using Bayes' law. This paper shows that common knowledge of the \"beliefs formation rule,\" mapping the private valuation of each agent into his first-order belief, also simplifies the beliefs hierarchy while allowing for disagreement among agents. We analyse the resulting speculation in a stylized asset market. Several statistics, computed only from readily observable quote, return and volume data, are evaluated in terms of their power to discriminate between genuine disagreement and the Harsanyian case. Only statistics that relate volume and volatility, or volume and changes in best offers, have the necessary discriminatory power.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Asset Prices and Trading Volume in a Beauty Contest", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00046", "ex:pages": "307-340", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Bruno Biais"}, {"ex:name": "Peter Bossaerts"}]}, {"ex:issue": "2", "ex:abstract": "This paper introduces the concept of a factor subspace in competitive equilibrium asset pricing. A factor subspace contains the market portfolio and is such that every marketed contingent claim is second-order stochastically dominated by a claim from the factor subspace. Conditions are given for the existence of equilibrium, and it is shown how APT and CAPM can be interpreted in the framework of the paper. If sufficiently many call options on the market portfolio are traded, then the space spanned by these options can be used as the factor subspace.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Stochastic Dominance, Pareto Optimality, and Equilibrium Asset Pricing", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00047", "ex:pages": "341-356", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Chongmin Kim"}]}, {"ex:issue": "2", "ex:abstract": "", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Delay and Cycles: Erratum", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00049", "ex:pages": "357-359", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Douglas Gale"}, {"ex:name": "Leslie J. Reinhorn"}]}, {"ex:issue": "3", "ex:abstract": "In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a unified, practical likelihood-based framework for the analysis of stochastic volatility models. A highly effective method is developed that samples all the unobserved volatilities at once using an approximating offset mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The paper also develops simulation-based methods for filtering, likelihood evaluation and model failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes factors is also investigated. These methods are used to compare the fit of stochastic volatility and GARCH models. All the procedures are illustrated in detail.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Stochastic Volatility: Likelihood Inference and Comparison with ARCH Models", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00050", "ex:pages": "361-393", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Sangjoon Kim"}, {"ex:name": "Neil Shephard"}, {"ex:name": "Siddhartha Chib"}]}, {"ex:issue": "3", "ex:abstract": "This paper develops methods for constructing asymptotically valid confidence intervals for the date of a single break in multivariate time series, including I(0), I(1), and deterministically trending regressors. Although the width of the asymptotic confidence interval does not decrease as the sample size increases, it is inversely related to the number of series which have a common break date, so there are substantial gains to multivariate inference about break dates. These methods are applied to two empirical examples: the mean growth rate of output in three European countries, and the mean growth rate of U.S. consumption, investment, and output.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Testing For and Dating Common Breaks in Multivariate Time Series", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00051", "ex:pages": "395-432", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Jushan Bai"}, {"ex:name": "Robin L. Lumsdaine"}, {"ex:name": "James H. Stock"}]}, {"ex:issue": "3", "ex:abstract": "We propose a constructive, multivariate framework for assessing agreement between (generally misspecified) dynamic equilibrium models and data, which enables a complete second-order comparison of the dynamic properties of models and data. We use bootstrap algorithms to evaluate the significance of deviations between models and data, and we use goodness-of-fit criteria to produce estimators that optimize economically-relevant loss functions. We provide a detailed illustrative application to modelling the U.S. cattle cycle.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Dynamic Equilibrium Economies: A Framework for Comparing Models and Data", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00052", "ex:pages": "433-451", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Francis X. Diebold"}, {"ex:name": "Lee E. Ohanian"}, {"ex:name": "Jeremy Berkowitz"}]}, {"ex:issue": "3", "ex:abstract": "This paper develops a method for analysing the dynamics of large cross-sections based on a factor analytic model. We use \"law of large numbers\" arguments to show that the number of common factors can be determined by a principal components method, the economy-wide shocks can be identified by means of simple structural VAR techniques and that the parameters of the unobserved factor model can be estimated consistently by applying OLS equation by equation. We distinguish between a technological and a non-technological shock. Identification is obtained by minimizing the negative realizations of the technology shock. Empirical results on 4-digit industrial output and productivity for the U.S. economy from 1958 to 1986 show that: (1) at least two economy-wide shocks, both having a long-run effect on sectoral output, are needed to explain the common dynamics; (2) although the technological shock accounts for at least 50% of the aggregate dynamics of output, it cannot by itself explain dynamics at business cycle frequencies; (3) sector-specific shocks explain the main bulk of total variance but generate mainly high frequency dynamics; (4) both the technological and the non-technological component of output show a peak for positive sectoral comovements of output at business cycle frequencies; (5) technological shocks are strongly correlated with the growth rates of the investment in machinery and equipment sectors and their inputs.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Let's Get Real: A Factor Analytical Approach to Disaggregated Business Cycle Dynamics", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00053", "ex:pages": "453-473", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Mario Forni"}, {"ex:name": "Lucrezia Reichlin"}]}, {"ex:issue": "3", "ex:abstract": "There is frequently interest in testing that a scalar or vector time series is I(0), possibly after first-differencing or other detrending, while the I(0) assumption is also taken for granted in autocorrelation-consistent variance estimation. We propose a test for I(0) against fractional alternatives. The test is nonparametric, and indeed makes no assumptions on spectral behaviour away from zero frequency. It seems likely to have good efficiency against fractional alternatives, relative to other nonparametric tests. The test is given large sample justification, subjected to a Monte Carlo analysis of finite sample behaviour, and applied to various empirical data series.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "A Nonparametric Test for I(0)", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00054", "ex:pages": "475-495", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Ignacio N. Lobato"}, {"ex:name": "Peter M. Robinson"}]}, {"ex:issue": "3", "ex:abstract": "This paper provides a consistent and asymptotically normal estimator for the intercept of a semiparametrically estimated sample selection model. The estimator uses a decreasingly small fraction of all observations as the sample size goes to infinity, as in Heckman (1990). In the semiparametrics literature, estimation of the intercept has typically been subsumed in the nonparametric sample selection bias correction term. The estimation of the intercept, however, is important from an economic perspective. For instance, it permits one to determine the \"wage gap\" between unionized and nonunionized workers, decompose the wage differential between different socioeconomic groups (e.g. male-female and black-white), and evaluate the net benefits of a social programme.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Semiparametric Estimation of the Intercept of a Sample Selection Model", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00055", "ex:pages": "497-517", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Donald W. K. Andrews"}, {"ex:name": "Marcia M. A. Schafgans"}]}, {"ex:issue": "3", "ex:abstract": "This article presents a monetary growth model where spatial separation and limited communication create a role for banks. Monetary policy interacts with the financial system's liquidity provision to affect the existence, multiplicity, and dynamical properties of equilibria. Moderate levels of risk aversion and tight monetary policy can lead to multiple steady states. Dynamical equilibria can be indeterminate, with oscillatory paths. Thus financial market frictions are a source of indeterminacies and endogenous volatility. Under plausible conditions, tight monetary policy raises the nominal interest rate and inflation rate and reduces long run output. Thus, a central bank's liquidity provision can promote growth.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "The Effects of Open Market Operations in a Model of Intermediation and Growth", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00056", "ex:pages": "519-550", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Stacey L. Schreft"}, {"ex:name": "Bruce D. Smith"}]}, {"ex:issue": "3", "ex:abstract": "We examine the characteristics of optimal monetary policies in a general equilibrium model with incomplete markets. Markets are incomplete because of uninsured preference uncertainty, and because productive capital is traded infrequently. Rational individuals are willing to hold a liquid asset—\"money\"—at a premium. Monetary policy interacts with existing financial institutions to determine this premium, as well as the level of precautionary holdings. We show that inflation is expansionary, and that the optimal inflation rate is positive if there is no operative banking system (the Tobin effect). Otherwise, efficiency requires that money be undominated in its rate of return (the Friedman Rule).", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Liquidity Preference and Financial Intermediation", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00057", "ex:pages": "551-572", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Jayasri Dutta"}, {"ex:name": "Sandeep Kapur"}]}, {"ex:issue": "3", "ex:abstract": "I study alternating-offer bargaining games with two-sided incomplete information about the players' discount rates. For both perfect Bayesian equilibrium and a rationalizability-style notion, I characterize the set of expected payoffs which may arise in the game. I also construct bounds on agreements that may be made. The set of expected payoffs is easy to compute and incorporate into applied models. My main result is a full characterization of the set of perfect Bayesian equilibrium payoffs for games in which the distribution over the players' discount rates is of wide support, yet is in a weak sense close to a point mass distribution. I prove a lopsided convergence result: each player cannot gain from a slight chance that she is a strong type, but the player can suffer greatly if there is a slight chance that she is a weak type.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Alternating-Offer Bargaining with Two-Sided Incomplete Information", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00058", "ex:pages": "573-594", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Joel Watson"}]}, {"ex:issue": "3", "ex:abstract": "When payoffs from different actions are unknown, agents use their own past experience as well as the experience of their neighbours to guide their decision making. In this paper, we develop a general framework to study the relationship between the structure of these neighbourhoods and the process of social learning. We show that, in a connected society, local learning ensures that all agents obtain the same payoffs in the long run. Thus, if actions have different payoffs, then all agents choose the same action, and social conformism obtains. We develop conditions on the distribution of prior beliefs, the structure of neighbourhoods and the informativeness of actions under which this action is optimal. In particular, we identify a property of neighbourhood structures—local independence—which greatly facilitates social learning. Simulations of the model generate spatial and temporal patterns of adoption that are consistent with empirical work.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Learning from Neighbours", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00059", "ex:pages": "595-621", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Venkatesh Bala"}, {"ex:name": "Sanjeev Goyal"}]}, {"ex:issue": "3", "ex:abstract": "", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "A Note on \"Strategic Trade Policy Design with Asymmetric Information and Public Contracts\"", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00060", "ex:pages": "623-625", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Giovanni Maggi"}]}, {"ex:issue": "3", "ex:abstract": "", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Strategic Trade Policy Design with Asymmetric Information and Public Contracts Corrigendum", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00061", "ex:pages": "627-630", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "S. Lael Brainard"}, {"ex:name": "David Martimort"}]}, {"ex:issue": "4", "ex:abstract": "We examine the interactions between different institutional arrangements in a general equilibrium model of a modernizing economy. There is a modern sector, where productivity is high but information asymmetries are large, and a traditional sector where productivity is low but information asymmetries are small. Consequently, agency costs in the modern sector make consumption lending difficult, while such lending is readily done in the traditional sector. The resulting trade-off between credit availability and productivity implies that not everyone will move to the modern sector. In fact, the laissez-faire level of modernization may fail to maximize net social surplus. This situation may also hold in the long run: in a dynamic version of the model, a \"trickle-down\" effect links the process of modernization with reduction in modern sector agency costs. This effect may be too weak and the economy may get stuck in a trap and never fully modernize. The two-sector structure also yields a natural theoretical testing ground for the Kuznets inverted-U hypothesis: we show that even within the \"sectoral shifting\" class of models, this phenomenon is not robust to small changes in model specification.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Information, the Dual Economy, and Development", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00062", "ex:pages": "631-653", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Abhijit V. Banerjee"}, {"ex:name": "Andrew F. Newman"}]}, {"ex:issue": "4", "ex:abstract": "The marketing literature refers to the concept of brand capital and provides empirical evidence that firms with a large stock of well-established brands have an advantage in introducing new products. This paper develops a theory of brand extension as a mechanism for informational leverage in which a firm leverages off a good's reputation in one market to alleviate the problem of informational asymmetry encountered in other markets. It is shown that brand extension helps a multi-product monopolist introduce a new experience good with less price distortion. Thus, the paper provides a theoretical foundation for the concept of brand capital.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Brand Extension as Informational Leverage", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00063", "ex:pages": "655-669", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Jay Pil Choi"}]}, {"ex:issue": "4", "ex:abstract": "Empirical estimates of the private value of patent protection are derived for four technology areas—computers, textiles, combustion engines, and pharmaceuticals—using new patent data for West Germany, 1953–1988. Patentees must pay renewal fees to keep their patents in force as well as legal expenses in order to enforce them. A dynamic stochastic discrete choice model of optimal renewal decisions is developed incorporating both learning and depreciation as well as the potential need to prosecute infringement. The evolution of the distribution of returns over the life of a group of patents is calculated for each technology using a minimum distance simulation estimator. Results indicate that the aggregate value of protection generated per year is on the order of 10% of related R&amp;D expenditure.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Patent Protection in the Shadow of Infringement: Simulation Estimations of Patent Value", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00064", "ex:pages": "671-710", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Jean Olson Lanjouw"}]}, {"ex:issue": "4", "ex:abstract": "This paper examines the extent to which swings in stock prices can be related to variations in the discounted value of expected future dividends when investors face uncertainty about their future behaviour. I develop an econometric model that accounts for the instability of U.S. dividend growth and discount rates during the past 120 years. Estimates of the model reveal that changing forecasts of future dividend growth account for more than 90% of the predictable variations in dividend-prices. The estimates also imply that instability in the dividend and discount rate processes contribute significantly to the predictability of long-horizon stock returns.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Dividend Variability and Stock Market Swings", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00065", "ex:pages": "711-740", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Martin D. D. Evans"}]}, {"ex:issue": "4", "ex:abstract": "This paper analyses a security market with transaction costs and a sequential trading structure. Transaction costs may prevent many traders from revealing their private information if they trade in a sequential fashion. Due to the information aggregation failure, hidden information gets accumulated in the market which may be revealed by a small trigger, yielding a high volatility in the absence of an accompanying event. The paper first characterizes the optimal trading strategy of the agent which constitute the unique equilibrium. Further properties of the price sequence are obtained using the concepts of informational cascade and informational avalanche. The results are applied to the explanation of market crashes. In particular, the dynamics of market crashes are illustrated as evolving through the following four phases: (1) boom; (2) euphoria; (3) trigger; and (4) panic; where the euphoria corresponds to the informational cascade and the panic corresponds to the informational avalanche.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Market Crashes and Informational Avalanches", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00066", "ex:pages": "741-759", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "In Ho Lee"}]}, {"ex:issue": "4", "ex:abstract": "The paper presents a model in which a population of agents repeatedly play games against nature; the rules of behaviour followed are revised over time through a process of imitation. For binary decisions, imitation selects rules consistent with a preference relation of the kind proposed by SSB utility theory and regret theory. In general, this preference relation need not satisfy either independence or transitivity; we state conditions on imitation necessary for it to do so. For decisions over three or more options, the long-run tendency is for options that are maximally preferred in terms of SSB preferences to be chosen. If no maximally preferred option exists, the process of imitation may not converge.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "The Selection of Preferences Through Imitation", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00067", "ex:pages": "761-771", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Robin P. Cubitt"}, {"ex:name": "Robert Sugden"}]}, {"ex:issue": "4", "ex:abstract": "A \"conventional\" contract is a contract that each side of a bargain expects the other side to insist on, because it is standard and customary under the circumstances. We consider a process of convention formation in which agents' expectations evolve through repeated interactions in a large-population setting. Agents choose best replies given their knowledge of the precedents, subject to some inertia and random error in their choice behaviour. Over the long run, this adaptive learning process tends to select contracts that are efficient, and egalitarian in the sense that the payoffs are centrally located on the efficiency frontier of the payoff possibility set. When the payoffs form a convex, comprehensive bargaining set, the process selects the Kalai-Smorodinsky solution.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Conventional Contracts", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00068", "ex:pages": "773-792", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "H. Peyton Young"}]}, {"ex:issue": "4", "ex:abstract": "We study noncooperative multilateral bargaining games, based on underlying TU games, in which coalitions can renegotiate their agreements. We distinguish between models in which players continue to bargain after implementing agreements (\"reversible actions\") and models in which players who implement agreements must leave the game (\"irreversible actions\"). We show that renegotiation always results in formation of the grand coalition if actions are reversible, but that the process may otherwise end with smaller coalitions. On the other hand, we show that the grand coalition cannot form in one step if the core of the game is empty, irrespective of the reversibility of actions.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "A Theory of Gradual Coalition Formation", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00069", "ex:pages": "793-815", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Daniel J. Seidmann"}, {"ex:name": "Eyal Winter"}]}, {"ex:issue": "4", "ex:abstract": "A reasonable model of the labour market over the business cycle should predict, among other things, that (a) in very low states of product demand there may be too little employment from an efficiency perspective, but as the state improves employment will increase until ultimately it is efficiently deployed, and (b) in low states of demand, a worker's welfare level will be \"low\" and as the state of the world improves so will the worker's welfare, except, possibly, at high levels of demand where the worker's utility may start to fall. Surprisingly, there does not exist a labour contract based model that is consistent with predictions (a) and (b). In fact, the standard results in the literature are if leisure is a normal good then there will be too much employment in essentially all states of the world and the welfare of the worker declines as the state of the world improves. In this paper a labour contracting model is constructed that is consistent with the above mentioned predictions. Two necessary ingredients in the model are the possibility of financial distress in low demand states and \"partial provability\" in contracting. Financial distress can be viewed as frustrating renegotiation and, thus, inefficient outcomes are possible in equilibrium. Partial provability—the ability of an informed player to make verifiable claims or statements to an uninformed player—eliminates certain kinds of inefficient outcomes. In particular, it eliminates the possibility that, in equilibrium, there is too much employment. This last result is interesting in itself because it is commonly believed that normality of leisure necessarily implies that labour contracting models will generate employment levels that are too high from an efficiency perspective.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Financial Distress and Underemployment", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00070", "ex:pages": "817-845", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Ed Nosal"}]}, {"ex:issue": "4", "ex:abstract": "Arrangements for achieving efficient risk-sharing vary depending on the information available to agents in the economy. The usual Euler equation restricts efficient allocations in an economy which obeys the permanent income hypothesis, while efficient allocations in an economy with private information and long-term contracts satisfy a symmetric restriction, but not the Euler equation. Full insurance arrangements are unique in that they satisfy both restrictions. We look at an environment in which it seems likely that long-term contracts play a role in mitigating the effects of private information: three village economies in South India. The evidence that consumption allocations satisfy the private information restriction is quite strong for households in two of the three villages; the evidence for the third village suggests that while consumption for some households satisfies the private information restrictions, other households' consumption obey the permanent income hypothesis.", "ex:volume": "65", "ex:source": "Review of Economic Studies", "ex:title": "Risk Sharing and Information in Village Economies", "ex:url": "http://hdl.handle.net/10.1111/1467-937X.00071", "ex:pages": "847-864", "@context": {"dc": "http://schema.org/"}, "ex:template": "Template-Type: ReDIF-Article 1.0", "ex:date": "1998", "ex:keyword": [], "ex:creator": [{"ex:name": "Ethan Ligon"}]}]