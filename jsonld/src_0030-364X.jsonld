[{"@id": "http://dx.doi.org/10.1287/opre.1030.0061", "e:abstract": "We analyze how sharing of future demand information (FDI) can help companies to lower cost. FDI is imperfect information on the customer demands of the upcoming period. We consider a supply chain with a single retailer and multiple customers, where customer demands are normally distributed and correlated. The retailer faces two decisions: With which customers should information be shared and how much should be ordered? We model the problem as a two-stage dynamic program, develop an optimal solution approach, and provide structural insights into the optimal extent of FDI sharing. We show that information cost and demand correlation are important factors for determining the optimal extent of FDI sharing. For a simplified version of the problem where only a single customer is contacted, we analyze how the optimal solution is affected by nonidentically distributed or nonuniformly correlated demands.", "e:keyword": ["Forecasting", "Inventory/production", "Information systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0065", "e:abstract": "A robust approach to solving linear optimization", "e:keyword": ["Programming", "Stochastic: robust approach for solving LP/MIP with data uncertainties"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0070", "e:abstract": "We develop a new method for pricing American options. The main practical contribution of this paper is a general algorithm for constructing upper and lower bounds on the true price of the option using any approximation to the option price. We show that our bounds are tight, so that if the initial approximation is close to the true price of the option, the bounds are also guaranteed to be close. We also explicitly characterize the worst-case performance of the pricing bounds. The computation of the lower bound is straightforward and relies on simulating the suboptimal exercise strategy implied by the approximate option price. The upper bound is also computed using Monte Carlo simulation. This is made feasible by the representation of the American option price as a solution of a properly defined dual minimization problem, which is the main theoretical result of this paper. Our algorithm proves to be accurate on a set of sample problems where we price call options on the maximum and the geometric mean of a collection of stocks. These numerical results suggest that our pricing method can be successfully applied to problems of practical interest.", "e:keyword": ["Finance: asset pricing: American options", "Duality", "Monte Carlo simulation", "Dynamic programming: approximate dynamic programming", "Optimal stopping"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0071", "e:abstract": "The problem of estimating and predicting Origin-Destination (OD) tables is known to be important and difficult. In the specific context of Intelligent Transportation Systems (ITS), the dynamic nature of the problem and the real-time requirements make it even more intricate.We consider here a least-square modeling approach for solving the OD estimation and prediction problem, which seems to offer convenient and flexible algorithms. The dynamic nature of the problem is represented by an autoregressive process, capturing the serial correlations of the state variables. Our formulation is inspired from Cascetta et al. (1993) and Ashok and Ben-Akiva (1993). We compare the Kalman filter algorithm to LSQR, an iterative algorithm proposed by Paige and Saunders (1982) for the solution of large-scale least-squares problems. LSQR explicitly exploits matrix sparsity, allowing to consider larger problems likely to occur in real applications.We show that the LSQR algorithm significantly decreases the computation effort needed by the Kalman filter approach for large-scale problems. We also provide a theoretical number of flops for both algorithms to predict which algorithm will perform better on a specific instance of the problem.", "e:keyword": ["Transportation: models. Programming: nonlinear algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0072", "e:abstract": "Data Envelopment Analysis (DEA) requires that the data for all inputs and outputs are known exactly. When some outputs and inputs are unknown decision variables, such as bounded and ordinal data, the DEA model becomes a nonlinear programming problem and is called imprecise DEA (IDEA). The nonlinear IDEA program can be converted into a linear program by an algorithm based upon scale transformations and variable alterations. Such an algorithm requires a set of special computational codes for each evaluation, because a different objective function and a different constraint with a set of new variables are present for each unit under evaluation. The current paper revisits a published Korean telecommunication analysis, and, by so doing, presents a new and simple approach to execute the IDEA through the standard linear DEA models. This greatly enhances the applicability of IDEA in applications, and the IDEA analysis is no longer limited to obtaining the efficiency scores. The key to the new approach lies in the finding that imprecise data can be easily converted into exact data. Based upon the exact data, models can be developed to determine all possible multiple optimal solutions in imprecise data, and to perform efficiency sensitivity analysis in IDEA.", "e:keyword": ["Organization: employee performance evaluation", "Telecommunications: performance evaluation", "Effectiveness/performance: DEA"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0073", "e:abstract": "A robotic cell---a manufacturing system widely used in industry---contains two or more robot-served machines, repetitively producing a number of part types. In this paper, we consider scheduling of operations in a bufferless dual-gripper robotic cell processing multiple part types. The processing constraints specify the cell to be a flowshop. The objective is to determine the robot move sequence and the sequence in which parts are to be processed so as to maximize the long-run average throughput rate for repetitive production of parts. We provide a framework to study the problem, and address the issues of problem complexity and solvability. Focusing on a particular class of robot move sequences, we identify all potentially optimal robot move sequences for the part-sequencing problem in a two-machine dual-gripper robot cell. In the case when the gripper switching time is sufficiently small, we specify the best robot move sequence in the class. We prove the problem of finding an optimal part sequence to be strongly NP-hard, even when the robot move sequence is specified. We provide a heuristic approach to solve the general two-machine problem and evaluate its performance on the set of randomly generated problem instances. We perform computations to estimate the productivity gain of using a dual-gripper robot in place of a single-gripper robot. Finally, we extend our results for the two-machine cell to solve an <i>m</i>-machine problem.", "e:keyword": ["Production/scheduling: sequencing-deterministic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0076", "e:abstract": "In this paper, we study the class scheduling problem at the training center of Continental Airlines. When pilots get new assignments, they must be retrained for up to eight consecutive weeks. During that time, they are removed from the roster, and thus impose a significant cost on the airlines. We formulate the problem with the objective of minimizing the total weighted length of all classes. Solutions are obtained with a branch-and-bound algorithm and a family of heuristics based on the idea of a rolling horizon. A series of computational experiments is performed to evaluate the algorithms. The results indicate that it is possible to obtain near-optimal solutions within acceptable time limits. The algorithms have been implemented and are now in use at Continental.", "e:keyword": ["Transportation/scheduling/personnel: class scheduling", "Programming/algorithms: branch-and-bound", "Heuristics", "Rolling horizon"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0077", "e:abstract": "In this paper we propose to modify the big square small square global optimization search in the plane with a big triangle small triangle approach. The triangulation of the feasible region is obtained by using Voronoi diagrams. The resulting algorithm was tested on the obnoxious facility location and the attraction-repulsion Weber problems with excellent results.", "e:keyword": ["Facilities/equipment planning: location", "Continuous", "Programming: nonlinear", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0078", "e:abstract": "We outline the development of a model for predicting the outcome of a yacht match race between two competing designs. The model is a fixed-time-increment simulation that accounts for the dynamic performance of each yacht. The wind speed and direction are modelled using hidden Markov chain models. Each yacht is assumed to follow a fixed sailing strategy determined by a set of simple decision rules. The simulation models both yachts simultaneously and accounts for interactions between them---for example, when they cross. The model is illustrated by applying it to International America's Cup Class designs.", "e:keyword": ["Simulation applications. Recreation and sports: yacht performance analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0079", "e:abstract": "This paper considers an overbooking problem with multiple reservation and inventory classes, in which the multiple inventory classes may be used as substitutes to satisfy the demand of a given reservation class (perhaps at a cost). The problem is to jointly determine overbooking levels for the reservation classes, taking into account the substitution options. Such problems arise in a variety of revenue management contexts, including multicabin aircraft, back-to-back scheduled flights on the same leg, hotels with multiple room types, and mixed-vehicle car rental fleets. We model this problem as a two-period optimization problem. In the first period, reservations are accepted given only probabilistic knowledge of cancellations. In the second period, cancellations are realized and surviving customers are assigned to the various inventory classes to maximize the net benefit of assignments (e.g., minimize penalties). For this formulation, we show that the expected revenue function is submodular in the overbooking levels, which implies the natural property that the optimal overbooking level in one reservation class decreases with the number of reservations held in the other reservation classes. We then propose a stochastic gradient algorithm to find the joint optimal overbooking levels. We compare the decisions of the model to those produced by more naive heuristics on some examples motivated by airline applications. The results show that accounting for substitution when setting overbooking levels has a small, but still significant, impact on revenues and costs.", "e:keyword": ["Transportation: overbooking", "Yield management", "Inventory/production: perishable inventory", "Stochastic", "Probability: stochastic optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0080", "e:abstract": "We consider infinite horizon production scheduling under stochastic demand. All problem data are allowed to vary across periods, including demand distributions, costs, and revenues. A forecast horizon, when it exists, is a finite problem horizon with the property that the corresponding first-period optimal production decision remains optimal regardless of demand and cost projections beyond this horizon. Thus, a forecast horizon allows us to reduce the amount of future data we need to forecast to solve for an optimal first decision for the infinite horizon problem. In this paper, we establish the existence of a forecast horizon under the assumptions that (1) costs and revenues are time-varying linear, and (2) demand is never eventually zero. A key result for establishing the existence and computation of forecast horizons is the monotonicity, and hence convergence, of optimal first-period policies as the horizon increases of finite horizon versions of the infinite horizon problem. A closed-form formula is provided for computing a forecast horizon that depends <i>only</i> on the discount factor and uniform upper and lower bounds on demand and unit production and inventory holding costs. In particular, its value is independent of, and determined in advance of, forecasting the demand distribution. We show that the effect of uncertainty in demand is to increase the forecast horizon associated with a deterministic problem by a constant plus a factor equal to one plus the ratio of these upper to lower bounds on per-period demand. The associated forecast horizon can be surprisingly short, even a few days, when the inventory costs are high.", "e:keyword": ["Dynamic programming: applications. Production scheduling: applications", "Approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0081", "e:abstract": "We develop a framework for asymptotic optimization of a queueing system. The motivation is the staffing problem of large call centers, which we have modeled as M/M/N queues with <i>N</i>, the number of agents, being large. Within our framework, we determine the asymptotically optimal staffing level <i>N</i>* that trades off agents' costs with service quality: the higher the latter, the more expensive is the former. As an alternative to this optimization, we also develop a constraint satisfaction approach where one chooses the least <i>N</i>* that adheres to a given constraint on waiting cost. Either way, the analysis gives rise to three regimes of operation: quality-driven, where the focus is on service quality; efficiency-driven, which emphasizes agents' costs; and a rationalized regime that balances, and in fact unifies, the other two. Numerical experiments reveal remarkable accuracy of our asymptotic approximations: over a wide range of parameters, from the very small to the extremely large, <i>N</i>* is <i>exactly</i> optimal, or it is accurate to within a single agent. We demonstrate the utility of our approach by revisiting the square-root safety staffing principle, which is a long-existing rule of thumb for staffing the M/M/N queue. In its simplest form, our rule is as follows: if <i>c</i> is the hourly cost of an agent, and <i>a</i> is the hourly cost of customers' delay, then <i>N</i>* = <i>R + y* (a/c)(sqrt)R</i>, where <i>R</i> is the offered load, and <i>y</i>*(·) is a function that is easily computable.", "e:keyword": ["Queues", "Optimization: choosing optimal number of servers", "Queues", "Limit theorems: many server queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0082", "e:abstract": "A center is a function that associates with every finite connected and undirected graph a nonempty subset of its vertices. These functions play an important role in networks such as social or interorganizational networks. Centers capture notions like: being a focal point of communication, being strategically located, ability and willingness to participate in strategic alliances, and the like. We focus on the conceptual issue of what makes a position in a graph a central one and investigate some possible concepts of centrality in relation to various properties. Characterizations of the uncovered center, the median, and degree center are presented, where each of these centers is defined for arbitrary connected undirected simple, and possibly cyclic, graphs.", "e:keyword": ["Networks/graphs: centrality", "Social networks", "Axiomatic approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0083", "e:abstract": "Television networks sell advertising slots to clients by the shows on which the commercials air. The networks determine the exact location in the show that a commercial will air at a later stage, usually close to the airdate of the show. There are several criteria the networks must meet in scheduling commercials in a show. The schedule should be such that no two commercials promoting competing products from different clients air in the same break. The audience ratings tend to be higher at the start and end of a commercial break than during the middle of the break. Therefore, advertisers generally prefer the first and last positions in a commercial segment, to those in the middle. TV networks normally promise their clients an equitable rotation of commercials among the positions within a commercial break. The scheduling of commercials on shows is traditionally done manually and is a cumbersome, time-intensive, and error-prone process. We formulate the commercial scheduling problem as an integer program and develop near-optimal heuristics for automatically scheduling the commercials to meet all the requirements. We implemented our algorithm at the National Broadcasting Company (NBC). In addition to reducing sales personnel costs by automating the scheduling of commercials, our work has increased customer satisfaction by minimizing errors in meeting customer requirements.", "e:keyword": ["Programming", "Multiple criteria", "Integer", "Applications", "Production/scheduling", "Approximations/heuristic", "Information systems", "Decision support systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0084", "e:abstract": "We consider a Markovian model of a multiclass queueing system in which a single large pool of servers attends to the various customer classes. Customers waiting to be served may abandon the queue, and there is a cost penalty associated with such abandonments. Service rates, abandonment rates, and abandonment penalties are generally different for the different classes. The problem studied is that of dynamically scheduling the various classes. We consider the Halfin-Whitt heavy traffic regime, where the total arrival rate and the number of servers both become large in such a way that the system's traffic intensity parameter approaches one. An approximating diffusion control problem is described and justified as a purely formal (that is, nonrigorous) heavy traffic limit. The Hamilton-Jacobi-Bellman equation associated with the limiting diffusion control problem is shown to have a smooth (classical) solution, and optimal controls are shown to have an extremal or “bang-bang” character. Several useful qualitative insights are derived from the mathematical analysis, including a “square-root rule” for sizing large systems and a sharp contrast between system behavior in the Halfin-Whitt regime versus that observed in the “conventional” heavy traffic regime. The latter phenomenon is illustrated by means of a numerical example having two customer classes.", "e:keyword": ["Scheduling", "Queueing", "Diffusion approximations", "Many server limits", "Halfin-Whitt regime", "Stochastic control", "Numerical methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0085", "e:abstract": "The rendezvous search problem is the problem of finding optimal search strategies for two people who are placed randomly on a known search region and want to meet each other in minimal expected time. We focus on initial location distributions that are centrally symmetric and nonincreasing as one moves away from the center, including the discretized and/or truncated Gaussian densities. When the search region is a discrete or a continuous interval, and the interval is labeled so that the searchers know their own location at all times, we prove that the optimal strategy for both searchers is to go directly to the center and wait there. The same result also holds for rendezvous search on the infinite line.", "e:keyword": ["Military", "Search/surveillance: rendezvous search on the discrete and continuous intervals"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0086", "e:abstract": "We consider the optimal use of information in shooting at a collection of targets, generally with the object of maximizing the average number (or value) of targets killed. The shooting problem is viewed as a Markov decision process, and the modal solution technique is stochastic dynamic programming. Information obtained about target status may or may not be perfect, and there may or may not be constraints on the number of shots. Previous results are reviewed, and some new results are obtained.", "e:keyword": ["Decision analysis", "Sequential", "Dynamic programming/optimal control", "Markov finite state", "Markov infinite state", "Military", "Targeting", "Probability", "Markov processes", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0087", "e:abstract": "This paper studies the stochastic two-item, periodic review, single facility, flexible manufacturing systems, where stochasticity comes from random demands or/and unreliable production process. Based on a notion called <i>(mu)</i>-difference monotone introduced in this paper, we prove that the hedging point policy is optimal to the systems in general for both finite and infinite horizon cases of the problems. This result is (demand) distribution free and does not require strict convexity or even differentiability of the one-period expected cost function. A general characterization of the hedging point policy, together with monotone switching curves, is provided.", "e:keyword": ["Inventory/production: multi-item", "Stochastic", "Policies", "Planning horizon", "Production/scheduling: flexible manufacturing", "Stochastic", "Dynamic program/optimal control: application", "Models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0088", "e:abstract": "Organizations worldwide use contact centers as an important channel of communication and transaction with their customers. This paper describes a contact center with two channels, one for real-time telephone service, and another for a postponed call-back service offered with a guarantee on the maximum delay until a reply is received. Customers are sensitive to both real-time and call-back delay and their behavior is captured through a probabilistic choice model. The dynamics of the system are modeled as an <i>M/M/N</i> multiclass system. We rigorously justify that as the number of agents increases, the system's load approaches its maximum processing capacity. Based on this observation, we perform an asymptotic analysis in the many-server, heavy traffic regime to find an asymptotically optimal routing rule, characterize the unique equilibrium regime of the system, approximate the system performance, and finally, propose a staffing rule that picks the minimum number of agents that satisfies a set of operational constraints on the performance of the system.", "e:keyword": ["Service networks", "Call centers", "Heavy traffic", "Service level guarantees", "Choice models", "Nash equilibrium", "Halfin-Whitt regime"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0089", "e:abstract": "This study illustrates how a manufacturer can use leadtime differentiation--selling the same product to different customers at different prices based on delivery leadtime--to simultaneously increase revenue and reduce capacity requirements. The manufacturer’s production facility is modeled as an exponential single-server queue with two classes of customers that differ in price sensitivity and delay sensitivity. The manufacturer chooses the service rate and a static price for each class of customer, and then dynamically quotes leadtimes to potential customers and decides the order in which customers are processed. The arrival rate for each class decreases linearly with price and leadtime. The manufacturer’s objective is to maximize profit, subject to the constraint that each customer must be processed within the promised leadtime. Assuming that some customers will tolerate a long delivery leadtime, we show that this problem has a simple near-optimal solution. Under our proposed policy, capacity utilization is near 100%. Impatient customers pay a premium for immediate delivery and receive priority in scheduling, whereas patient customers are quoted a leadtime proportional to the current queue length. Queue length and leadtime can be closely approximated by a reflected Ornstein-Uhlenbeck diffusion process. Hence, we have a closed form expression for profit, and choose prices and capacity to optimize this. In case customers may choose either the class 1 deal or the class 2 deal, the proposed policy is made incentive compatible by quoting a leadtime for the class 2 (patient) customers that is longer than the actual queueing delay.", "e:keyword": ["Pricing", "Capacity", "Dynamic leadtime quotation and sequencing in production/scheduling", "Diffusion approximations for priority queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0090", "e:abstract": "We study a periodic-review production/inventory control problem where both the supply of raw material and demand for the finished product are exogenous and random, the raw material can be stored for future use, can be purchased from or sold to an outside market. We study both the lost sales and backlogging cases under both strict convex and linear raw material purchasing/selling costs. Convexity of the purchasing/selling cost implies that the more the firm purchases from or sells to the outside market, the more expensive or less valuable the raw material becomes. For all cases, we find the partial characterizations for the optimal policies, which all turn out to be very intuitively appealing. In particular, for each of the linear-cost cases, the optimal policy degenerates into a combination of two base-stock policies: one for the raw material inventory and the other for the finished product inventory.", "e:keyword": ["Inventory and production: uncertainty", "Dynamic programming and optimal control", "Models", "Mathematics: convexity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0091", "e:abstract": "This work pleads for the use of the concept of strategies, and their network-theoretic representation as hyperpaths, for modeling network assignment problems. While this concept describes adequately the behavior of users in transit systems, we show that it can apply as well to networks where arc capacities are rigid. This opens up a whole new field of research and raises several questions, from both the theoretical and computational points of view. These are investigated in the paper.", "e:keyword": ["Equilibrium", "Traffic assignment", "Hyperpath", "Capacities", "Variational inequalities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0092", "e:abstract": "We consider the scheduling problems arising when two agents, each with a set of nonpreemptive jobs, compete to perform their respective jobs on a common processing resource. Each agent wants to minimize a certain objective function, which depends on the completion times of its jobs only. The objective functions we consider in this paper are maximum of regular functions (associated with each job), number of late jobs, and total weighted completion times. We obtain different scenarios, depending on the objective function of each agent, and on the structure of the processing system (single machine or shop). For each scenario, we address the complexity of various problems, namely, finding the optimal solution for one agent with a constraint on the other agent's cost function, finding single nondominated schedules (i.e., such that a better schedule for one of the two agents necessarily results in a worse schedule for the other agent), and generating all nondominated schedules.", "e:keyword": ["Production/scheduling: multiagent deterministic sequencing", "Games/group decisions: cooperative sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0093", "e:abstract": "This paper develops an approach based on performance targets to assess a preference function for a multiobjective decision under uncertainty. This approach yields preference functions that are strategically equivalent to conventional multiattribute utility functions, but the target-oriented approach is more natural for some classes of decisions. In some situations, the target-oriented preference conditions are analogous to reliability theory conditions for series or parallel failure modes in a system. In such cases, reinterpreting the conditions using reliability concepts can be useful in assessing the preference function. The target-oriented approach is also a generalization of common forms of goal programming. The approach has particular applicability for resource allocation decisions where the outcome of the decision is significantly determined by the actions of other stakeholders to the decision, such as new product development or decision making in a controversial regulated environment.", "e:keyword": ["Decision analysis", "Utility function assessment", "Utility/preference", "Multiattribute", "Target-oriented preferences", "Mathematical programming", "Goal programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0095", "e:abstract": "In April of 2003, <i>Science</i> (2003) and <i>Nature</i> (2003) published special issues marking two significant achievements in the history of science: the 50th anniversary of discovering the double helical structure of the DNA, and the completion of the Human Genome Project. The first discovery led to a new age in genetics, and the second event marked the beginning of a new era that uses the genome in medicine. The international efforts to determine the human DNA sequence and assess its ethical, legal, and social implications started in 1990. Since then, the data from the project has been available in public databases for researchers and scientists around the world. The vast increase in biological data led to increasing interest in computational biology and an emerging multidisciplinary research area known as bioinformatics. Most people working in this area have mathematics, biology, biochemistry, or computer science backgrounds and have learned about the field by using tools from another discipline to answer questions in biology. The current challenge is to utilize the genome data to its full extent and to develop tools that improve our understanding of biological pathways and accelerate drug discovery. Many of the algorithms needed to solve these problems have management science and operations research aspects. This paper introduces some of the fundamental problems in bioinformatics to an operations research audience and demonstrates the application of management science tools in their formulation and solution.", "e:keyword": ["Analysis of algorithms: computational complexity", "Dynamic programming: Markov", "Health care", "Diagnosis: pharmaceutical", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0096", "e:abstract": "In this paper, we study the distribution network design problem integrating transportation and infinite horizon multiechelon inventory cost function. We consider the trade-off between inventory cost, direct shipment cost, and facility location cost in such a system. The problem is to determine how many warehouses to set up, where to locate them, how to serve the retailers using these warehouses, and to determine the optimal inventory policies for the warehouses and retailers. The objective is to minimize the total multiechelon inventory, transportation, and facility location costs. To the best of our knowledge, none of the papers in the area of distribution network design has explicitly addressed the issues of the 2-echelon inventory cost function arising from coordination of replenishment activities between the warehouses and the retailers. We structure this problem as a set-partitioning integer-programming model and solve it using column generation. The pricing subproblem that arises from the column generation algorithm gives rise to a new class of the submodular function minimization problem. We show that this pricing subproblem can be solved in <i>O</i>(<i>n</i>log <i>n</i>) time, where <i>n</i> is the number of retailers. Computational results show that the moderate size distribution network design problem can be solved efficiently via this approach.", "e:keyword": ["Transportation", "Models", "Programming", "Integer", "Facilities/equipment planning", "Location"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0097", "e:abstract": "In this paper we provide necessary and sufficient conditions for the distribution of demand in the future to be stochastically increasing in the demand that has been observed in the past. We base our analysis on the multiperiod inventory model examined by Eppen and Iyer (1997). In the process of establishing the necessary and sufficient conditions we develop a new property called the sequential monotone likelihood ratio property.", "e:keyword": ["Inventory/production", "Policies", "Probability", "Distribution comparisons", "Stochastic model applications", "Forecasting", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0098", "e:abstract": "We consider contests with a fixed proportion of winners based on relative performance. Special attention is paid to winner-take-all contests, which we define as contests with relatively few winners receiving relatively large awards, but we consider the full range of values of the proportion of winners. If a contestant has the opportunity to modify the distribution of her performance, what strategy is advantageous? When the proportion of winners is less than one-half, a riskier performance distribution is preferred; when this proportion is greater than one-half, it is better to choose a less risky distribution. Using a multinormal model, we consider modifications in the variability of the distribution and in correlations with the performance of other contestants. Increasing variability and decreasing correlations lead to improved chances of winning when the proportion of winners is less than one-half, and the opposite directions should be taken for proportions greater than one-half. Thus, it is better to take chances and to attempt to distance oneself from the other contestants (i.e., to break away from the herd) when there are few winners; a more conservative, herding strategy makes sense when there are many winners. Our analytical and numerical results indicate that the probability of winning can change substantially as variability and/or correlations are modified. Furthermore, in a game-theoretic setting in which all contestants can make modifications, choosing a riskier (less risky) performance distribution when the proportion of winners is low (high) is the dominant best-response strategy. We briefly consider some practical issues related to the recommended strategies and some possible extensions.", "e:keyword": ["Decision analysis", "Strategies in contests", "Modifying variability", "Modifying correlations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0099", "e:abstract": "We show that superadditive lifting functions lead to sequence independent lifting of inequalities for general mixed-integer programming. As an application, we note that mixed-integer rounding (MIR) may be viewed as sequence independent lifting. Consequently, we obtain facet conditions for MIR inequalities for mixed-integer knapsacks.", "e:keyword": ["Integer programming", "Theory", "Superadditive functions", "Lifting", "Facets"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0100", "e:abstract": "We study a system with multiple components and preventive maintenance. At predetermined times, some of the components are replaced with new ones. We permit the set of components that are chosen for replacement to be random and study the effect of dependency in this selection. For example, we show that it is often better to have simultaneous replacements of components, rather than replacing each component independently of the others. We also show that preventive maintenance only makes sense for components whose lifetimes are new-better-than-used (NBU).", "e:keyword": ["Reliability", "Maintenance and repair", "Replacement and renewal", "Probability", "Distribution comparisons", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0101", "e:abstract": "This paper considers scheduling problems where a set of original jobs has already been scheduled to minimize some cost objective, when a new set of jobs arrives and creates a disruption. The decision maker needs to insert the new jobs into the existing schedule without excessively disrupting it. Two classes of models are considered. First, we minimize the scheduling cost of all the jobs, subject to a limit on the disruption caused to the original schedule, where this disruption is measured in various ways. In the second class, a total cost objective, which includes both the original cost measure and the cost of disruption, is minimized. For both classes and various costs based on classical scheduling objectives, and for almost all problems, we provide either an efficient algorithm or a proof that such an algorithm is unlikely to exist. We also show how to extend both classes of models to deal with multiple disruptions in the form of repeated arrivals of new jobs. Our work refocuses the extensive literature on scheduling problems towards issues of rescheduling, which are important because of the frequency with which disruptions occur in manufacturing practice.", "e:keyword": ["Production/scheduling", "Sequencing", "Deterministic", "Single machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0102", "e:abstract": "Nonsmoothness and nonconvexity in optimization problems often arise because a combinatorial structure is imposed on smooth or convex data. The combinatorial aspect can be explicit, e.g., through the use of “max,” “min,” or “if” statements in a model; or implicit, as in the case of bilevel optimization, where the combinatorial structure arises from the possible choices of active constraints in the lower-level problem. In analyzing such problems, it is desirable to decouple the combinatorial aspect from the nonlinear aspect and deal with them separately. This paper suggests a problem formulation that explicitly decouples the two aspects. A suitable generalization of the traditional Lagrangian framework allows an extension of the popular sequential quadratic programming (SQP) methodology to such structurally nonconvex nonlinear programs. We show that the favorable local convergence properties of SQP are retained in this setting and illustrate the potential of the approach in the context of optimization problems with max-min constraints that arise, for example, in robust optimization.", "e:keyword": ["Programming", "Nonlinear", "Nondifferentiable", "Complementarity", "Mathematics", "Piecewise linear", "Combinatorics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0103", "e:abstract": "We consider periodic review inventory control problems in directed networks, primary examples of which are distribution systems and assembly systems. External demand could occur at each node. When inventory is insufficient to meet requirements at a node, a portion of this demand is backordered and the remaining is lost. External demands, as well as lead times for inventory purchase, assembly, and transportation, are stochastic. In each period, linear sales revenues and the following costs, all linear, are charged at each node: (a) inventory purchase/assembly/transfer cost to receive inventory, (b) holding cost, (c) backorder cost, and (d) lost sales cost. When the objective function of interest is a discounted sum of profits over a finite planning horizon, it is shown that the sales prices and the inventory purchase/assembly/transfer cost parameters can be assumed to be zero without loss of generality. The result is proved for every realization of demands and lead times. Some extensions to these results are discussed. During this process, we also generalize the concept of echelon inventories to directed networks.", "e:keyword": ["Inventory/production", "Multi-item/echelon/stage", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0104", "e:abstract": "The one-item, periodic review production and inventory system has been extensively studied in literature. Theories have been established for various basic constructs of the system of either finite or infinite horizon, except for the case where production capacity is finite and production cost contains a fixed (as well as a variable) component. It was conjectured in earlier research papers that the modified (<i>s, S</i>) policy would be optimal to the finite-capacity, fixed-cost model in infinite horizon. This paper studies the long-run limiting behavior of such systems. It proves that the limiting cost function exists, and there exist stationary policies that are optimal in the long run. The optimal policy, however, is not of the modified (<i>s, S</i>) type in general, but continues to exhibit the <i>X</i>-<i>Y</i> band structure: Whenever the inventory level drops below <i>X</i>, order up to capacity; when the inventory level is above <i>Y</i>, do nothing. When the inventory level is between <i>X</i> and <i>Y</i>, however, the ordering pattern seems to be changing from problem to problem. Nevertheless, based on a concept called (<i>C, K</i>)-convexity, introduced in this paper, the <i>X</i>-<i>Y</i> band is shown to be no more than one capacity of width. One calculation for the bounds on such <i>X</i> and <i>Y</i> boundaries that are tight in some cases is also provided. By exploring the <i>X</i>-<i>Y</i> band structure, a linear program model is proposed to find the optimal policy completely. Finally, an attempt is made to compare “the best modified (<i>s, S</i>) policy” with the optimal one, and a numerical example indicates that the deviation may be more than 11% in cost performance.", "e:keyword": ["Inventory/production", "Planning horizons", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0106", "e:abstract": "Car pooling is a transportation service organized by a large company which encourages its employees to pick up colleagues while driving to/from work to minimize the number of private cars travelling to/from the company site. The car pooling problem consists of defining the subsets of employees that will share each car and the paths the drivers should follow, so that sharing is maximized and the sum of the path costs is minimized. The special case of the car pooling problem where all cars are identical can be modeled as a Dial-a-Ride Problem. In this paper, we propose both an exact and a heuristic method for the car pooling problem, based on two integer programming formulations of the problem. The exact method is based on a bounding procedure that combines three lower bounds derived from different relaxations of the problem. A valid upper bound is obtained by the heuristic method, which transforms the solution of a Lagrangean lower bound into a feasible solution. The computational results show the effectiveness of the proposed methods.", "e:keyword": ["Programming", "Integer", "Set partitioning", "Relaxation/subgradient", "Transportation", "Car pooling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1030.0107", "e:abstract": "Singular stochastic control has found diverse applications in operations management, economics, and finance. However, in all but the simplest of cases, singular stochastic control problems cannot be solved analytically. In this paper, we propose a method for numerically solving a class of singular stochastic control problems. We combine finite element methods that numerically solve partial differential equations with a policy update procedure based on the principle of smooth pasting to iteratively solve Hamilton-Jacobi-Bellman equations associated with the stochastic control problem. A key feature of our method is that the presence of singular controls simplifies the procedure. We illustrate the method on two examples of singular stochastic control problems, one drawn from economics and the other from queueing systems.", "e:keyword": ["Dynamic programming/optimal control", "Singular stochastic control", "HJB equations", "Numerical methods", "Probability", "Diffusions", "Queueing", "Scheduling", "Brownian approximations", "Economics", "Investments under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0105", "e:abstract": "We consider a joint inventory-pricing problem in which buyers act strategically and bid for units of a firm's product over an infinite horizon. The number of bidders in each period as well as the individual bidders' valuations are random but stationary over time. There is a holding cost for inventory and a unit cost for ordering more stock from an outside supplier. Backordering is not allowed. The firm must decide how to conduct its auctions and how to replenish its stock over time to maximize its profits. We show that the optimal auction and replenishment policy for this problem is quite simple, consisting of running a standard first-price or second-price auction with a fixed reserve price in each period and following an order-up-to (basestock) policy for replenishing inventory at the end of each period. Moreover, the optimal basestock level can be easily computed. We then compare this optimal basestock, reserve-price-auction policy to a traditional basestock, list-price policy. We prove that in the limiting case of one buyer per period and in the limiting case of a large number of buyers per period and linear holding cost, list pricing is optimal. List pricing also becomes optimal as the holding cost tends to zero. Numerical comparisons confirm these theoretical results and show that auctions provide significant benefits when: (1) the number of buyers is moderate, (2) holding costs are high, or (3) there is high variability in the number of buyers per period.", "e:keyword": ["Optimal auction", "Pricing", "Dynamic programming", "Infinite horizon", "Inventory control", "Basestock policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0108", "e:abstract": "In some industries, a certain part can be needed in a very large number of different configurations. This is the case, e.g., for the electrical wirings in European car factories. A given configuration can be replaced by a more complete, therefore more expensive, one. The diversity management problem consists of choosing an optimal set of some given number <i>k</i> of configurations that will be produced, any nonproduced configuration being replaced by the cheapest produced one that is compatible with it. We model the problem as an integer linear program. Our aim is to solve those problems to optimality. The large-scale instances we are interested in lead to difficult LP relaxations, which seem to be intractable by the best direct methods currently available. Most of this paper deals with the use of Lagrangean relaxation to reduce the size of the problem in order to be able, subsequently, to solve it to optimality via classical integer optimization.", "e:keyword": ["Large-scale integer programming and relaxations", "Lagrangean relaxation of a large linear integer program arising from an application", "Production planning", "Choice of production"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0109", "e:abstract": "This paper presents a new best-fit heuristic for the two-dimensional rectangular stock-cutting problem and demonstrates its effectiveness by comparing it against other published approaches. A placement algorithm usually takes a list of shapes, sorted by some property such as increasing height or decreasing area, and then applies a placement rule to each of these shapes in turn. The proposed method is not restricted to the first shape encountered but may dynamically search the list for better candidate shapes for placement. We suggest an efficient implementation of our heuristic and show that it compares favourably to other heuristic and metaheuristic approaches from the literature in terms of both solution quality and execution time. We also present data for new problem instances to encourage further research and greater comparison between this and future methods.", "e:keyword": ["Production/scheduling", "Cutting stock/trim", "Production/scheduling", "Approximations/heuristic", "Computers/computer science", "Artificial intelligence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0110", "e:abstract": "The problem of finding a work assignment for airline crew members in a given time horizon is addressed. In the literature this problem is usually referred to as the <i>airline crew rostering problem</i>. It consists of constructing monthly schedules for crew members by assigning them pairings, rest periods, annual and sick leave, training periods, union activities, and so forth, so as to satisfy the collective agreements and security rules. We formulate the airline crew rostering problem as a 0--1 <i>multicommodity flow problem</i> where each employee corresponds to a commodity; determining a monthly schedule for an employee is the same as computing a path on a suitably defined graph while still satisfying union conventions. A preprocessing phase is performed that reduces the dimension of the graph. To tighten the linear programming formulation of our model, we propose some families of valid inequalities that have proved to be computationally effective. Some of them can be treated implicitly when constructing the graph. Computational results obtained with a commercial integer programming solver (CPLEX) are analyzed.", "e:keyword": ["Transportation", "Scheduling", "Personnel", "Crew rostering", "Programming", "Integer", "Cutting plane/facet", "Valid inequalities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0111", "e:abstract": "The capacitated vehicle routing problem (CVRP) is the problem in which a set of identical vehicles located at a central depot is to be optimally routed to supply customers with known demands subject to vehicle capacity constraints. In this paper, we describe a new integer programming formulation for the CVRP based on a two-commodity network flow approach. We present a lower bound derived from the linear programming (LP) relaxation of the new formulation which is improved by adding valid inequalities in a cutting-plane fashion. Moreover, we present a comparison between the new lower bound and lower bounds derived from the LP relaxations of different CVRP formulations proposed in the literature. A new branch-and-cut algorithm for the optimal solution of the CVRP is described. Computational results are reported for a set of test problems derived from the literature and for new randomly generated problems.", "e:keyword": ["Programming", "Integer", "Two-commodity formulation", "Programming", "Integer", "Cutting plane", "Branch-and-cut algorithm", "Transportation", "Capacitated vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0112", "e:abstract": "The co-printing problem is a new variant of the bin-packing problem. It finds its origin in the printing of Tetra-bricks in the beverage industry. Combining different types of bricks in one printing pattern reduces the stock. With each brick, a number of colors are associated, and the total number of colors for the whole pattern cannot exceed a given limit. We develop a branch-and-price algorithm to obtain proven optimal solutions. After introducing a Dantzig-Wolfe reformulation for the problem, we derive cutting planes to tighten the LP relaxation. We present heuristics and develop a branching scheme, avoiding complex pricing problem modifications. We present some further algorithmic enhancements, such as the implementation of dominance rules and a lower bound based on a combinatorial relaxation. Finally, we discuss computational results for real-life data sets. In addition to the introduction of a new bin-packing problem, this paper illustrates the complex balance in branch-and-price algorithms among using cutting planes, the branching scheme, and the tractability of the pricing problem. It also shows how dominance rules can be implemented in a branch-and-price framework, resulting in a substantial reduction in computation time.", "e:keyword": ["Programming", "Integer", "Algorithms", "Decomposition", "Branch-and-price", "Inventory/production", "Applications", "Packing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0113", "e:abstract": "Arithmetic Asian or average price options deliver payoffs based on the average underlying price over a prespecified time period. Asian options are an important family of derivative contracts with a wide variety of applications in currency, equity, interest rate, commodity, energy, and insurance markets. We derive two analytical formulas for the value of the continuously sampled arithmetic Asian option when the underlying asset price follows geometric Brownian motion. We use an identity in law between the integral of geometric Brownian motion over a finite time interval [0, <i>t</i>] and the state at time <i>t</i> of a one-dimensional diffusion process with affine drift and linear diffusion and express Asian option values in terms of spectral expansions associated with the diffusion infinitesimal generator. The first formula is an infinite series of terms involving Whittaker functions <i>M</i> and <i>W</i>. The second formula is a single real integral of an expression involving Whittaker function <i>W</i> plus (for some parameter values) a finite number of additional terms involving incomplete gamma functions and Laguerre polynomials. The two formulas allow accurate computation of continuously sampled arithmetic Asian option prices.", "e:keyword": ["Finance", "Asset pricing", "Option pricing", "Finance", "Securities", "Asian  options", "Probability", "Diffusion", "Average of geometric Brownian motion", "Spectral theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0114", "e:abstract": "We consider a new approach to stochastic inventory/routing that approximates the future costs of current actions using optimal dual prices of a linear program. We obtain two such linear programs by formulating the control problem as a Markov decision process and then replacing the optimal value function with the sum of single-customer inventory value functions. The resulting approximation yields statewise lower bounds on optimal infinite-horizon discounted costs. We present a linear program that takes into account inventory dynamics and economics in allocating transportation costs for stochastic inventory routing. On test instances we find that these allocations do not introduce any error in the value function approximations relative to the best approximations that can be achieved without them. Also, unlike other approaches, we do not restrict the set of allowable vehicle itineraries in any way. Instead, we develop an efficient algorithm to both generate and eliminate itineraries during solution of the linear programs and control policy. In simulation experiments, the price-directed policy outperforms other policies from the literature.", "e:keyword": ["Dynamic programming/optimal control", "Discounted infinite-horizon", "Separable functional approximations", "Transportation", "Inventory routing", "Stochastic", "Inventory/production", "Multi-item", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0115", "e:abstract": "We present a practical algorithm for computing the minimum-volume <i>n</i>-dimensional ellipsoid that must contain <i>m</i> given points <i>a</i><sub>1</sub>,…,<i>a<sub>m</sub></i> ∈ ℝ<i><sup>n</sup></i>. This convex constrained problem arises in a variety of applied computational settings, particularly in data mining and robust statistics. Its structure makes it particularly amenable to solution by interior-point methods, and it has been the subject of much theoretical complexity analysis. Here we focus on computation. We present a combined interior-point and active-set method for solving this problem. Our computational results demonstrate that our method solves very large problem instances (<i>m</i> = 30,000 and <i>n</i> = 30) to a high degree of accuracy in under 30 seconds on a personal computer.", "e:keyword": ["Programming", "Nonlinear", "Algorithms", "Large-scale systems", "Statistics", "Cluster analysis", "Data analysis", "Mathematics", "Convexity", "Matrices"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0116", "e:abstract": "For a broad class of discrete- and continuous-time queueing systems, we show that the stationary number of customers in <i>system</i> (queue plus servers) is the sum of two independent random variables, one of which is the stationary number of customers in <i>queue</i> and the other is the number of customers that arrive during the time a customer spends in service. We call this relation an <i>invariance relation</i> in the sense that it does not change for a variety of single-sever queues (with batch arrivals and batch services) and some multiserver queues (with batch arrivals and deterministic service times) that satisfy a certain set of assumptions. Making use of this relation, we also present a simple method of deriving the probability generating functions (PGFs) of the stationary numbers in queue and in system, as well as some of their properties. This is illustrated by several examples, which show that new simple derivations of old results as well as new results can be obtained in a unified manner. Furthermore, we show that the invariance relation and the method we are presenting are easily generalized to analyze queues with batch Markovian arrival process (BMAP) arrivals. Most of the results are presented under the discrete-time setting. The corresponding continuous-time results, however, are covered as well because deriving the results for continuous-time queues runs exactly parallel to that for their discrete-time counterparts.", "e:keyword": ["Queues", "Invariance relation", "Unified method", "Discrete-time queue", "Queues", "Batch/bulk", "Batch arrivals", "Batch services", "Multiple servers", "BMAP arrivals"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0117", "e:abstract": "We present an algorithm for the valuation and optimal operation of hydroelectric and thermal power generators in deregulated electricity markets. Real options theory is used to derive nonlinear partial-integro-differential equations (PIDEs) for the valuation and optimal operating strategies of both types of facilities. The equations are designed to incorporate a wide class of spot price models that can exhibit the same time-dependent, mean-reverting dynamics and price spikes as those observed in most electricity markets. Particular attention is paid to the operational characteristics of real power generators. For thermal power plants, these characteristics include variable start-up times and costs, control response time lags, minimum generating levels, nonlinear output functions, and structural limitations on ramp rates. For hydroelectric units, head effects and environmental constraints are addressed. We illustrate the models with numerical examples of a pump storage facility and a thermal power plant. This PIDE framework can achieve high levels of computational speed and accuracy while incorporating a wide range of spot price dynamics and operational characteristics.", "e:keyword": ["Natural resources/energy", "Deregulated electricity markets", "Finance/asset pricing", "Pricing power plants as real options", "Dynamic programming/optimal control", "Application"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0119", "e:abstract": "This paper, motivated by the experiences of a major U.S.-based broadcast television network, presents algorithms and heuristics to schedule commercial videotapes. Major advertisers purchase several slots to air commercials during a given time period on a broadcast network. We study the problem of scheduling advertiser's commercials in the slots it purchased when the same commercial is to be aired multiple times. Under such a situation, the advertisers typically want the airings of a commercial to be as evenly spaced as possible. Thus, our objective is to schedule a set of commercials in a set of available slots such that multiple airings of the same commercial are as evenly spaced as possible. A natural formulation of this problem is a mixed-integer program that can be solved using third-party solvers. We also develop a branch-and-bound algorithm based on a problem-specific bounding scheme. Both approaches fail to solve larger problem instances within a reasonable time frame. We present an alternative mixed-integer program that lends itself to an efficient solution. For solving even larger problems, we present multiple heuristics.", "e:keyword": ["Scheduling", "Mixed-integer programming", "Heuristics", "Broadcast television"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0120", "e:abstract": "<i>Dedicated to our doctor father and grandfather, Prof. Dr. E. Rainer Burkard, on the occasion of his 60th birthday</i>Given a public transportation system represented by its stops and direct connections between stops, we present OR models for two problems dealing with the prices for the customers. In the first, the <i>fare problem</i>, subsets of stops are already aggregated to zones and “good” tariffs have to be found in the existing zone system. Closed form solutions for the fare problem are presented for three objective functions. The second problem, the <i>zone problem</i> includes the design of the zones. In an exemplary way we study this problem for one of the objectives. It is NP hard and we therefore propose three heuristics which prove to be very successful in the redesign of one of Germany's transportation systems.", "e:keyword": ["Integer programming", "Algorithms", "Heuristics", "Applications", "Transportation", "Models", "Traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0121", "e:abstract": "This paper studies two important variants of the dynamic economic lot-sizing problem that are applicable to a wide range of real-world situations. In the first model, production in each time period is restricted to a multiple of a constant batch size, where backlogging is allowed and all cost parameters are time varying. Several properties of the optimal solution are discussed. Based on these properties, an efficient dynamic programming algorithm is developed. The efficiency of the dynamic program is further improved through the use of Monge matrices. Using the results developed for the first model, an <i>O</i>(<i>n</i><sup>3</sup>log <i>n</i>) algorithm is developed to solve the second model, which has a general form of product acquisition cost structure, including a fixed charge for each acquisition, a variable unit production cost, and a freight cost with a truckload discount. This algorithm can also be used to solve a more general problem with concave cost functions.", "e:keyword": ["Inventory/production", "Dynamic lot sizing", "Quantity discount", "Dynamic programming", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0122", "e:abstract": "We consider problems in which a defender is attempting to protect a channel from infiltration by laying static underwater devices across the channel. These devices can detect infiltrators that come within a given distance of them, and it is assumed that an infiltrator so detected can be apprehended before he can fulfill his mission. Previous work has concentrated on cases in which there is just one infiltrator and the infiltrator knows both the number of devices and their detection radii, but the emphasis in this paper is on situations in which the defender does not know the number of infiltrators and the infiltrators have only partial information about the devices. It is shown that the defender has a strategy that is optimal against any number of infiltrators when the detection radii satisfy certain conditions and, in particular, when the detection radii all lie in specific intervals. In the latter case, the infiltrators can also act optimally with only partial information concerning the detection devices. In addition, we obtain results giving the number of infiltrators that will ensure that at least one gets through undetected for various types of partial information available to the infiltrators.", "e:keyword": ["Games/group decisions", "Noncooperative", "Ambush game", "Military", "Search/surveillance", "Detecting infiltration down a channel"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0123", "e:abstract": "Motivated by practices in customer contact centers, we consider a system that offers two modes of service: real-time and postponed with a delay guarantee. Customers are informed of anticipated delays and select their preferred option of service. The resulting system is a multiclass, multiserver queueing system with state-dependent arrival rates. We propose an estimation scheme for the anticipated real-time delay that is asymptotically correct, and a routing policy that is asymptotically optimal in the sense that it minimizes real-time delay subject to the deadline of the postponed service mode. We also show that our proposed state-dependent scheme performs better than a system in which customers make decisions based on steady-state waiting-time information. Our results are derived using an asymptotic analysis based on “many-server” limits for systems with state-dependent parameters.", "e:keyword": ["Service networks", "Service level guarantees", "Multiclass queueing systems", "Call-back option", "Call centers", "Halfin-Whitt regime", "Real-time delay notification"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0124", "e:abstract": "The multiple vehicle routing problem with time windows (VRPTW) is a hard and extensively studied combinatorial optimization problem. This paper considers a dynamic VRPTW with stochastic customers, where the goal is to maximize the number of serviced customers. It presents a multiple scenario approach (MSA) that continuously generates routing plans for scenarios including known and future requests. Decisions during execution use a distinguished plan chosen, at each decision, by a consensus function. The approach was evaluated on vehicle routing problems adapted from the Solomon benchmarks with a degree of dynamism varying between 30% and 80%. They indicate that MSA exhibits dramatic improvements over approaches not exploiting stochastic information, that the use of consensus function improves the quality of the solutions significantly, and that the benefits of MSA increase with the (effective) degree of dynamism.", "e:keyword": ["Vehicle routing", "Stochastic model applications", "Sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0125", "e:abstract": "This research addresses two common tools for reducing product development lead times: overlapping of development stages and crashing of development times. For the first time in the product development literature, a formal model addresses both tools concurrently, thus facilitating analysis of the interdependencies between overlapping and crashing.The results exhibit the necessity of addressing overlapping and crashing concurrently, and exhibit general characteristics of optimal overlapping/crashing policies. The impact of different evolution/sensitivity constellations on optimal policies is investigated, and comprehensive guidelines for structuring development processes are provided.For the special case of linear costs, an efficient procedure is presented that generates the efficient time-cost trade-off curves and determines the corresponding optimal overlapping/crashing policies. The impact of key parameters and the robustness regarding their estimates is illustrated with a simple two-stage example.", "e:keyword": ["Product development processes", "Overlapping", "Crashing", "Development leadtimes", "Development costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0126", "e:abstract": "Manufacturers make production decisions and carry inventory to satisfy uncertain demand. When holding and shortage costs are high, carrying inventory could be even more expensive for a <i>capacitated</i> production system. Recent developments in information technology and sales strategies enabled firms to acquire, collect, or induce advance demand information. We address a periodic-review, stochastic, capacitated, finite and infinite horizon production system faced by a manufacturer who has the ability to obtain advance demand information. We establish optimal policies and characterize their behavior with respect to capacity, fixed costs, advance demand information, and the planning horizon. With a numerical study, we quantify the value of advance demand information and additional capacity for specific problem instances. We illustrate how advance demand information can be a substitute for capacity and inventory.", "e:keyword": ["Inventory/production", "Stochastic", "Optimal policies", "Nonstationary", "Fixed cost", "Capacity", "Dynamic programming", "Markov", "Forecasting", "Advance demand information"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0127", "e:abstract": "We analyze a finite horizon, single product, periodic review model in which pricing and production/inventory decisions are made simultaneously. Demands in different periods are random variables that are independent of each other and their distributions depend on the product price. Pricing and ordering decisions are made at the beginning of each period and all shortages are backlogged. Ordering cost includes both a fixed cost and a variable cost proportional to the amount ordered. The objective is to find an inventory policy and a pricing strategy maximizing expected profit over the finite horizon. We show that when the demand model is additive, the profit-to-go functions are <i>k</i>-concave and hence an (<i>s, S, p</i>) policy is optimal. In such a policy, the period inventory is managed based on the classical (<i>s, S</i>) policy and price is determined based on the inventory position at the beginning of each period. For more general demand functions, i.e., multiplicative plus additive functions, we demonstrate that the profit-to-go function is not necessarily <i>k</i>-concave and an (<i>s, S, p</i>) policy is not necessarily optimal. We introduce a new concept, the symmetric <i>k</i>-concave functions, and apply it to provide a characterization of the optimal policy.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Planning horizons", "Operating characteristics", "Marketing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0130", "e:abstract": "We consider a single-location inventory system with periodic review and stochastic demand. It places replenishment orders to raise the inventory position—that is, inventory on hand plus inventory in transit—to exactly <i>S</i> at the beginning of every period. The lead time associated with each of these orders is random. However, the lead-time process is such that these orders do not cross. Demand that cannot be met with inventory available on hand is lost permanently. We state and prove some sample-path properties of lost sales, inventory on hand at the end of a period, and inventory position at the end of a period as functions of <i>S</i>. The main result is the convexity of the expected discounted sum of holding and lost-sales costs as a function of <i>S</i>. This result justifies the use of common search procedures or linear programming methods to determine optimal base-stock levels for inventory systems with lost sales and stochastic lead times. It should be noted that the class of base-stock policies is suboptimal for such systems, and we are primarily interested in them because of their widespread use.", "e:keyword": ["Inventory/production", "Periodic review", "Lost sales", "Base-stock policies", "Convexity", "Sample-path properties"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0131", "e:abstract": "This paper demonstrates optimal policies for capacitated serial multiechelon production/inventory systems. Extending the Clark and Scarf (1960) model to include installations with production capacity limits, we demonstrate that a modified echelon base-stock policy is optimal in a two-stage system when there is a smaller capacity at the downstream facility. This is shown by decomposing the dynamic programming value function into value functions dependent upon individual echelon stock variables. We show that the optimal structure holds for both stationary and nonstationary stochastic customer demand. Finite-horizon and infinite-horizon results are included under discounted-cost and average-cost criteria.", "e:keyword": ["Inventory/production", "Multiechelon", "Stochastic uncertainty", "Policies", "Capacity expansion"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0132", "e:abstract": "The bullwhip effect (amplification of order variance from a downstream stage in a supply chain to an upstream stage) is widely observed in practice, and is generally considered a major cause of supply chain inefficiencies. But are supply chains always better off with strategies that are designed to dampen the bullwhip effect? This paper considers a model where a single product is sold through multiple retail outlets. The retailers replenish their inventories from a factory, which in turn replenishes its own finished-goods inventory through production. The factory's production capacity is finite, and there are transportation economies of scale in replenishing the retailer inventories. We study two types of replenishment strategies that are widely used in practice, and show that a replenishment strategy that reduces the volatility of orders received by the factory does not necessarily reduce the total costs in the supply chain.", "e:keyword": ["Inventory/production", "Multiechelon", "Operating characteristics", "Stochastic", "Approximations/heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0133", "e:abstract": "How should a manager make replacement decisions for a chain of machines over time if each is maintained by an optimal control model addressing uncertainty of machine breakdowns? A network representation of the problem involves arcs with interdependent costs. A solution algorithm is presented and replacement considerations under technological change are incorporated into a well-known optimal control model for maintenance under uncertainty (that of Kamien and Schwartz 1971). The method is illustrated by an example.", "e:keyword": ["Dynamic programming/optimal control", "Models", "Facilities/equipment planning", "Maintenance/replacement", "Inventory/production policies", "Maintenance/replacement", "Reliability", "Replacement/renewal"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0134", "e:abstract": "This note discusses the relationships among three assumptions that appear frequently in the pricing/revenue management literature. These assumptions are mostly needed for analytical tractability, and they have the common property of ensuring a well-behaved “revenue function.” The three assumptions are decreasing marginal revenue with respect to demand, decreasing marginal revenue with respect to price, and increasing price elasticity of demand. We provide proofs and examples to show that none of these conditions implies any other. However, they can be ordered from strongest to weakest over restricted regions, and the ordering depends upon the region.", "e:keyword": ["Marketing", "Pricing", "Economics", "Marginal revenue", "Price elasticity of demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0135", "e:abstract": "This paper considers a production-inventory problem where a manufacturer fulfills stochastic, stationary demand for a single product from a finished-goods inventory. The inventory can be replenished by two production resources, in-house production and a subcontractor, which both have finite capacity. We construct a Brownian approximation of the optimal control problem, assuming that the manufacturer uses a “dual base-stock” policy to control replenishment from the two sources and that her objective is to minimize average cost. A closed-form expression is obtained for one optimal base-stock policy and an analytical expression is derived from which the other optimal base stock can be computed numerically. We show conditions under which the objective is convex in capacity, and the unique globally optimal capacity can be computed numerically. We thus provide a tractable approximation to the two-source problem, which is generally intractable. We demonstrate the accuracy of this approximation for an <i>M</i>/<i>M</i>/1 model. We also draw managerial insight from the Brownian optimal base-stock results into how the optimal base-stock policies control the inventory distribution and under what conditions the contingent source is used to build inventory or to resolve backorders.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Policies", "Probability", "Diffusion", "Stochastic model applications", "Production/scheduling", "Approximations/heuristics", "Queues", "Applications", "Approximations", "Diffusion models", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0136", "e:abstract": "We develop a diffusion approximation for the queue-length stochastic process in the <i>G/GI/n/m</i> queueing model (having a general arrival process, independent and identically distributed service times with a general distribution, <i>n</i> servers, and <i>m</i> extra waiting spaces). We use the steady-state distribution of that diffusion process to obtain approximations for steady-state performance measures of the queueing model, focusing especially upon the steady-state delay probability. The approximations are based on heavy-traffic limits in which <i>n</i> tends to infinity as the traffic intensity increases. Thus, the approximations are intended for large <i>n</i>.For the <i>GI/M/n/</i>∞ special case, Halfin and Whitt (1981) showed that scaled versions of the queue-length process converge to a diffusion process when the traffic intensity <i>ρ<sub>n</sub></i> approaches 1 with (1 – <i>ρ<sub>n</sub></i>)√<i>n → β</i> for 0 < <i>β</i> < ∞. A companion paper, Whitt (2005), extends that limit to a special class of <i>G/GI/n/m<sub>n</sub></i> models in which the number of waiting places depends on <i>n</i> and the service-time distribution is a mixture of an exponential distribution with probability <i>p</i> and a unit point mass at 0 with probability 1 – <i>p</i>. Finite waiting rooms are treated by incorporating the additional limit <i>m<sub>n</sub>/√n → κ</i> for 0 < <i>κ</i> ≤ ∞. The approximation for the more general <i>G/GI/n/m</i> model developed here is consistent with those heavy-traffic limits. Heavy-traffic limits for the <i>GI/PH/n/</i>∞ model with phase-type service-time distributions established by Puhalskii and Reiman (2000) imply that our approximating process is not asymptotically correct for nonexponential phase-type service-time distributions, but nevertheless, the heuristic diffusion approximation developed here yields useful approximations for key performance measures such as the steady-state delay probability. The accuracy is confirmed by making comparisons with exact numerical results and simulations.", "e:keyword": ["Queues", "Approximations", "Multiserver queues", "Queues", "Multichannel", "Diffusion approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0137", "e:abstract": "We give the first <i>exact</i> algorithmic study of facility location problems that deal with finding a median for a <i>continuum</i> of demand points. In particular, we consider versions of the “continuous <i>k</i>-median (Fermat-Weber) problem” where the goal is to select one or more center points that minimize the average distance to a set of points in a demand <i>region</i>. In such problems, the average is computed as an integral over the relevant region, versus the usual discrete sum of distances. The resulting facility location problems are inherently geometric, requiring analysis techniques of computational geometry. We provide polynomial-time algorithms for various versions of the <i>L</i><sub>1</sub> 1-median (Fermat-Weber) problem. We also consider the multiple-center version of the <i>L</i><sub>1</sub> <i>k</i>-median problem, which we prove is NP-hard for large <i>k</i>.", "e:keyword": ["Facilities/equipment planning", "Continuous location:Fermat-Weber problem", "Continuous demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0138", "e:abstract": "We study the optimal resource investment decision faced by a two-product, price-setting firm that operates in a monopolistic setting and employs a postponed pricing scheme. The firm has the option to invest in dedicated resources as well as a more expensive, flexible resource that can satisfy both products. While the resource investment decision is made under demand uncertainty, pricing and resource allocation decisions are postponed to the time when demand curves are realized. Our analysis provides the structure of the firm's optimal resource investment strategy as a function of demand parameters and investment costs, and shows that the flexible resource investment decision follows a <i>threshold</i> policy. We also show that it can be optimal for the firm to invest in the flexible resource even when demand patterns are perfectly positively correlated. The reason for flexible capacity investment in this case is financial rather than risk pooling. On the other hand, we show that it can be optimal for the firm not to invest in the flexible resource even when demand patterns are perfectly negatively correlated. The flexible resource investment decision in this case depends on the profitability of the two products. Based on our analysis, we provide principles on the firm's optimal resource investment decision.", "e:keyword": ["Facilities/equipment planning", "Resource flexibility", "Pricing", "Uncertainty", "Inventory/production", "Substitution and demand correlation", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0139", "e:abstract": "This paper studies stability of network models that capture macroscopic features of data communication networks, including the Internet. The network model consists of a set of links and a set of possible routes that are fixed subsets of links. A connection is dynamically established along one of the routes to transmit data as requested and is terminated after the transmission is over. The transmission bandwidth of a link is dynamically allocated, according to specific bandwidth allocation policy, to ongoing connections that traverse the link. A network model is said to be stable under a given bandwidth allocation policy if, roughly, the number of ongoing connections in the network will not blow up over time. We consider a stationary and a bursty network model; the former assumes stochastically stationary arrival processes of connections as did many theoretical studies, while the latter allows more realistic bursty and correlated arrival processes. For both models under a necessary stability condition (i.e., the average offered transmission load on each link is within its bandwidth capacity), we show that the proportionally fair, the minimum potential delay, the max-min fair, and a class of utility-maximizing bandwidth allocation policies ensure network model stability, while some priority-oriented and maximum throughput policies do not. Interestingly, the bandwidth allocation policy that maximizes the arctan(·) utility ensures the stability of the stationary model but <i>not</i> the bursty model. This raises a serious concern about the current practice in the Internet protocol design, since such a policy is thought of as a good approximation of one of the most widely used TCP in the Internet.", "e:keyword": ["Data network", "Internet", "Rate control", "Bandwidth allocation", "Burstiness", "Stability", "Fluid network model", "Lyapunov function"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0140", "e:abstract": "We study the stochastic transportation-inventory network design problem involving one supplier and multiple retailers. Each retailer faces some uncertain demand, and safety stock must be maintained to achieve suitable service levels. However, risk-pooling benefits may be achieved by allowing some retailers to serve as distribution centers for other retailers. The problem is to determine which retailers should serve as distribution centers and how to allocate the other retailers to the distribution centers. Shen et al. (2003) formulated this problem as a set-covering integer-programming model. The pricing problem that arises from the column generation algorithm gives rise to a new class of the submodular function minimization problem. In this paper, we show that by exploiting certain special structures, we can solve the general pricing problem in Shen et al. efficiently. Our approach utilizes the fact that the set of all lines in a two-dimension plane has low VC-dimension. We present computational results on several instances of sizes ranging from 40 to 500 retailers. Our solution technique can be applied to a wide range of other concave cost-minimization problems.", "e:keyword": ["Facilities/equipment planning:stochastic", "Inventory/production:uncertainty", "Stochastic", "Programming:nonlinear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0143", "e:abstract": "The World Health Organization (WHO) recommends which strains of influenza to include in each year’s vaccine to countries around the globe. The current WHO strategy attempts to match the vaccine strains with the expected upcoming epidemic strains, a strategy we refer to as the <i>follow</i> policy. The recently proposed antigenic distance hypothesis suggests that vaccine efficacy can be enhanced by taking into account the antigenic histories of vaccinees. To assess the potential benefit of history-based vaccination, we formulate the annual vaccine-strains selection problem as a stochastic dynamic program using the theory of shape space, which maps each vaccine and epidemic strain into a point in multidimensional space. Computational results show that a near-optimal policy can be derived by approximating the entire antigenic history by a single reduced historical strain, and then solving the multiperiod problem myopically, as a series of single-period problems. The modest suboptimality of the follow policy, together with our current inability to quantitatively link the model’s objective function (a measure of cross-reactivity) with actual vaccine efficacy, leads us to recommend the continued use of the follow policy.", "e:keyword": ["Dynamic programming", "Health care"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0144", "e:abstract": "In the manufacture of circuit boards, panels are immersed sequentially in a series of tanks, with upper and lower bounds on the processing time within each tank. The panels are mounted on carriers that are lowered into and raised from the tanks, and transported from tank to tank by programmable hoists. The sequence of hoist moves does not have to follow the sequence of processing stages for the circuit boards. By optimising the sequence of hoist moves, we can maximise the production throughput.We consider simple cyclic schedules, where the hoist move sequence repeats every cycle and one panel is completed per cycle. Phillips and Unger (1976) developed the first mixed integer programming model for finding the hoist move schedule to minimise the cycle time for lines with only one hoist. We discuss how their formulation can be tightened, and introduce new valid inequalities. We present the first mixed integer programming formulation for finding the minimum-time cycle for lines with multiple hoists and present valid inequalities for this problem. Some preliminary computational results are also presented.", "e:keyword": ["Mixed integer programming", "Applications", "Formulation", "Production/scheduling", "Applications", "Cyclic scheduling", "Industries", "Chemical", "Electric", "Hoist scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0145", "e:abstract": "Based on recent results for multiarmed bandit problems, we propose an adaptive sampling algorithm that approximates the optimal value of a finite-horizon Markov decision process (MDP) with finite state and action spaces. The algorithm adaptively chooses which action to sample as the sampling process proceeds and generates an asymptotically unbiased estimator, whose bias is bounded by a quantity that converges to zero at rate (ln <i>N</i>)/<i>N</i>, where <i>N</i> is the total number of samples that are used per state sampled in each stage. The worst-case running-time complexity of the algorithm is <i>O</i>((<i>|A|N</i>)<i><sup>H</sup></i>), independent of the size of the state space, where |<i>A</i>| is the size of the action space and <i>H</i> is the horizon length. The algorithm can be used to create an approximate receding horizon control to solve infinite-horizon MDPs. To illustrate the algorithm, computational results are reported on simple examples from inventory control.", "e:keyword": ["Dynamic programming/optimal control:Markov finite state"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0146", "e:abstract": "We study a multi-item stochastic inventory system in which customers may order different but possibly overlapping subsets of items, such as a multiproduct assemble-to-order system. The goal is to determine the right base-stock level for each item and to identify the key driving factors. We formulate a cost-minimization model with order-based backorder costs and compare it with the standard single-item, newsvendor-type model with item-based backorder cost. We show that the solution of the former can be bounded by that of the latter with appropriately imputed parameters. Starting with this upper bound, the optimal base-stock levels of the order-based problem can be obtained in a greedy fashion. We also show that the optimal base-stock levels increase in replenishment lead times but may increase or decrease in lead-time variability and demand correlation. Finally, we devise closed-form approximations of the optimal base-stock levels to see more clearly their dependence on the system parameters.", "e:keyword": ["Inventory/production:stochastic", "Multi-item", "Assemble-to-order", "Component commonality", "Probability:multivariate Poisson process", "Correlation", "Optimization:submodular function minimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0147", "e:abstract": "This paper examines the stability of decentralized, multistage supply chains under arbitrary demand conditions. It looks for intrinsic properties of the inventory replenishment policies that hold for all customer demand processes and for policies with desirable properties.It is found that the overall conditions experienced by suppliers several stages removed from the final customer, e.g., the variances of the orders they receive and the inventories they keep, depend on the policy much more than on the demand process. A policy-specific but demand-independent upper bound for the order variance amplification factor of any decentralized policy is shown to exist, and its formula is presented. The bound is always tight for the suppliers at the end of a long chain so that a policy exhibits the “bullwhip effect” if and only if its bound is greater than 1.A simple necessary condition for bullwhip avoidance is also identified in terms of a policy's “gain.” Gain is the marginal change in average inventory induced by a policy when there is a small but sustained change in the demand rate. It is shown that <i>all</i> policies with positive gain produce the bullwhip effect if they do not use future order commitments. Because manufacturers can reduce costs by operating with positive gain, this explains the prevalence of the bullwhip effect.A family of commitment-based policies that can dynamically maintain any desired inventory level for any demand rate (e.g., achieve positive gain) without the bullwhip effect is also presented. The family includes just-in-time strategies as a special case. Simulation results are used as an illustration.", "e:keyword": ["Inventory/production", "Multistage systems", "Mathematics", "Systems solution"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0148", "e:abstract": "A general approach to improving simulation accuracy uses information about auxiliary control variables with known expected values to improve the estimation of unknown quantities. We analyze weighted Monte Carlo estimators that implement this idea by applying weights to independent replications. The weights are chosen to constrain the weighted averages of the control variables. We distinguish two cases (unbiased and biased), depending on whether the weighted averages of the controls are constrained to equal their expected values or some other values. In both cases, the number of constraints is usually smaller than the number of replications, so there may be many feasible weights. We select maximally uniform weights by minimizing a separable convex function of the weights subject to the control variable constraints. Estimators of this form arise (sometimes implicitly) in several settings, including at least two in finance: calibrating a model to market data (as in the work of Avellaneda et al. 2001) and calculating conditional expectations to price American options. We analyze properties of these estimators as the number of replications increases. We show that in the unbiased case, weighted Monte Carlo reduces asymptotic variance, and that all convex objective functions within a large class produce estimators that are very close to each other in a strong sense. In contrast, in the biased case the choice of objective function does matter. We show explicitly how the choice of objective determines the limit to which the estimator converges.", "e:keyword": ["Simulation: statistical analysis", "Finance: asset pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0149", "e:abstract": "This paper develops a stochastic general equilibrium inventory model for an oligopoly, in which all inventory constraint parameters are endogenously determined. We propose several systems of demand processes whose distributions are functions of all retailers' prices and all retailers' service levels. We proceed with the investigation of the equilibrium behavior of infinite-horizon models for industries facing this type of generalized competition, under demand uncertainty.We systematically consider the following three competition scenarios. (1) Price competition only: Here, we assume that the firms' service levels are exogenously chosen, but characterize how the price and inventory strategy equilibrium vary with the chosen service levels. (2) Simultaneous price and service-level competition: Here, each of the firms simultaneously chooses a service level and a combined price and inventory strategy. (3) Two-stage competition: The firms make their competitive choices sequentially. In a first stage, all firms simultaneously choose a service level; in a second stage, the firms simultaneously choose a combined pricing and inventory strategy with full knowledge of the service levels selected by all competitors. We show that in all of the above settings a Nash equilibrium of infinite-horizon stationary strategies exists and that it is of a simple structure, provided a Nash equilibrium exists in a so-called reduced game.We pay particular attention to the question of whether a firm can choose its service level on the basis of its own (input) characteristics (i.e., its cost parameters and demand function) only. We also investigate under which of the demand models a firm, under simultaneous competition, responds to a change in the exogenously specified characteristics of the various competitors by either: (i) adjusting its service level and price in the same direction, thereby compensating for price increases (decreases) by offering improved (inferior) service, or (ii) adjusting them in opposite directions, thereby simultaneously offering better or worse prices and service.", "e:keyword": ["Inventory/production policies", "Marketing/pricing", "Games/group decisions", "Noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0150", "e:abstract": "Albert Heijn, BV, a supermarket chain in the Netherlands, faces a vehicle routing and delivery scheduling problem once every three to six months. Given hourly demand forecasts for each store, travel times and distances, cost parameters, and various transportation constraints, the firm seeks to determine a weekly delivery schedule specifying the times when each store should be replenished from a central distribution center, and to determine the vehicle routes that service these requirements at minimum cost. We describe the development and implementation of a system to solve this problem at Albert Heijn. The system resulted in savings of 4% of distribution costs in its first year of implementation and is expected to yield 12%–20% savings as the firm expands its usage. It also has tactical and strategic advantages for the firm, such as in assessing the cost impact of various logistics and marketing decisions, in performance measurement, and in competing effectively through reduced lead time and increased frequency of replenishment.", "e:keyword": ["Inventory", "Inventory routing", "Networks", "Matchings", "Application to inventory routing", "Transportation", "Vehicle routing", "Algorithm and implementation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0151", "e:abstract": "Concavity cuts play an important role in several algorithms for concave minimization, such as pure cutting plane algorithms, conical algorithms, and branch-and-bound algorithms. For concave quadratic minimization problems Konno et al. (1998) have demonstrated that the lower the rank of the problem, i.e., the smaller the number of nonlinear variables, the deeper the concavity cuts usually turn out to be. In this paper we examine the case where the number of nonlinear variables of a concave minimization problem is large, but most of the objective value of a good solution is determined by a small number of variables only. We will discuss ways to exploit such a situation to derive deep cutting planes. To this end we apply concepts usually applied for efficiently solving low-rank concave minimization problems.", "e:keyword": ["Programming", "Nonlinear", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0152", "e:abstract": "We consider a queueing system with multitype customers and flexible (multiskilled) servers that work in parallel. If <i>Q<sub>i</sub></i> is the queue length of type <i>i</i> customers, this queue incurs cost at the rate of <i>C<sub>i</sub></i>(<i>Q<sub>i</sub></i>), where <i>C<sub>i</sub></i>(·) is increasing and convex. We analyze the system in heavy traffic (Harrison and Lopez 1999) and show that a very simple generalized <i>c</i>μ-rule (Van Mieghem 1995) minimizes both instantaneous and cumulative queueing costs, asymptotically, over essentially all scheduling disciplines, preemptive or non-preemptive. This rule aims at myopically maximizing the rate of decrease of the instantaneous cost at all times, which translates into the following: when becoming free, server <i>j</i> chooses for service a type <i>i</i> customer such that <i>i</i> ε arg max<i><sub>i</sub></i> <i>C</i>μ<i><sub>i</sub></i>(<i>Q<sub>i</sub></i>)μ<i><sub>ij</sub></i>, where μ<i><sub>ij</sub></i> is the average service rate of type <i>i</i> customers by server <i>j</i>.An analogous version of the generalized <i>c</i>μ-rule asymptotically minimizes delay costs. To this end, let the cost incurred by a type <i>i</i> customer be an increasing convex function <i>C<sub>i</sub></i>(<i>D</i>) of its sojourn time <i>D</i>. Then, server <i>j</i> always chooses for service a customer for which the value of <i>C</i>′<i><sub>i</sub></i>(<i>D</i>) μ<i><sub>ij</sub></i> is maximal, where <i>D</i> and <i>i</i> are the customer's sojourn time and type, respectively.", "e:keyword": ["Queues", "Diffusion models", "Networks", "Optimization", "Production/scheduling", "Sequencing/stochastic", "Networks", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0153", "e:abstract": "We consider a special case of the single-item, periodic-review inventory control problem with fixed plus linear ordering costs and an upper bound on production capacity. We assume that the fixed cost is large relative to the variable cost and restrict our analysis to full-capacity orders. We show that the optimal policy is a threshold policy, with respect to the inventory position, for a class of cost-to-go functions that include the class of convex functions.", "e:keyword": ["Production/inventory", "Capacitated", "Stochastic", "Threshold policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0154", "e:abstract": "The <i>constrained two-dimensional cutting</i> (C_TDC) problem consists of determining a <i>cutting pattern</i> of a set of <i>n</i> small rectangular piece types on a rectangular stock plate <i>S</i> with length <i>L</i> and width <i>W</i>, to maximize the sum of the profits of the pieces to be cut. Each piece type <i>i</i>, <i>i</i>=1,…,<i>n</i>, is characterized by a length <i>l<sub>i</sub></i>, a width <i>w<sub>i</sub></i>, a profit (or weight) <i>c<sub>i</sub></i>, and an upper demand value <i>b<sub>i</sub></i>. The upper demand value is the maximum number of pieces of type <i>i</i> that can be cut on <i>S</i>. In this paper, we study the two-staged C_TDC problem, noted C_2TDC. It is a classical variant of the C_TDC where each piece is produced, in the final cutting pattern, by at most two cuts. We solve the C_2TDC problem using an exact algorithm that is mainly based on a bottom-up strategy. We introduce new lower and upper bounds and propose new strategies that eliminate several duplicate patterns. We evaluate the performance of the proposed exact algorithm on problem instances extracted from the literature and compare it to the performance of an existing exact algorithm.", "e:keyword": ["Dynamic programming", "Industries", "Programming:algorithms—branch-and-bound", "Simulation:applications", "Efficiency"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0155", "e:abstract": "One could argue that the Navy's most important resource is its personnel, and as such, workforce planning is a crucial task. We investigate a new model and solution technique that is designed to aid in optimizing the process of assigning sailors to jobs. This procedure attempts to achieve an increased level of sailor satisfaction by providing a list of possible jobs from which a sailor may choose. We show that the optimal partition provided by an interior-point algorithm is particularly useful when designing the job lists. This follows because a strictly complementary solution to the linear programming relaxation observes all possible optimal solutions to the original binary problem. The techniques developed rely on a continuous parametric analysis, and we show that the parameterization provides meaningful information about the structure of the optimal assignments.", "e:keyword": ["Assignment problems", "Interior-point algorithms", "Matchings", "Optimal partition", "Parametric analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0156", "e:abstract": "We consider the model where <i>N</i> queues (users) are served in discrete time by a generalized switch. The switch state is random, and it determines the set of possible service rate choices (scheduling decisions) in each time slot. This model is primarily motivated by the problem of scheduling transmissions of <i>N</i> data users in a shared time-varying wireless environment, but also includes other applications such as input-queued cross-bar switches and parallel flexible server systems.The objective is to find a scheduling strategy maximizing a concave utility function <i>H</i>(<i>u</i><sub>1</sub>,…,<i>u<sub>N</sub></i>), where <i>u<sub>n</sub></i>s are long-term average service rates (data throughputs) of the users, assuming users always have data to be served.We prove asymptotic optimality of the gradient scheduling algorithm (which generalizes the well-known proportional fair algorithm) for our model, which, in particular, allows for simultaneous service of multiple users and for discrete sets of scheduling decisions. Analysis of the transient dynamics of user throughputs is the key part of this work.", "e:keyword": ["Communications", "Queues:algorithms", "Networks", "Optimization", "Networks:stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0157", "e:abstract": "We present a model describing the demand dynamics of two new products competing for a limited target market. The demand trajectories of the two products are driven by a market saturation effect and an imitation effect reflecting the product experience of previous adopters. In this general setting, we provide analytical results for the sales trajectories and life-cycle sales of the competing products. We use these results to study the impact of launch time on overall life-cycle sales. We consider the perspective of one of the competing products and model the trade-off between the lost revenues resulting from a delayed launch and the lower unit-production costs. We find that the profit-maximizing launch time exhibits a counterintuitive behavior. In particular, we show that a firm facing a launch time delay from a competing product might benefit from accelerating its own product launch, as opposed to using the softened competitive situation to further improve its cost position. We identify conditions under which a marginal cost-benefit analysis leads to suboptimal launch-time decisions. Finally, we analyze the Nash equilibrium in launch-time decisions of the two competing products.", "e:keyword": ["New products:cross-functional performance metrics", "Marketing-operations coordination", "Competitive diffusion dynamics", "Cost of delay"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0158", "e:abstract": "The foundation for any decision is a clear statement of objectives. Attributes clarify the meaning of each objective and are required to measure the consequences of different alternatives. Unfortunately, insufficient thought typically is given to the choice of attributes. This paper addresses this problem by presenting theory and guidelines for identifying appropriate attributes. We define five desirable properties of attributes: they should be unambiguous, comprehensive, direct, operational, and understandable. Each of these properties is discussed and illustrated with examples, including several cases in which one or more of the desirable properties are not met. We also present a decision model for selecting among the different types of natural, proxy, and constructed attributes.", "e:keyword": ["Decision analysis:multiple criteria", "Organizational studies:effectiveness/performance", "Utility/preference:multiattribute"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0159", "e:abstract": "We analyze the price-formation process in an infinite-horizon oligopoly model where hydroelectric generators engage in dynamic price-based competition. The analysis focuses on the role of “indifference” prices, i.e., prices that equate the gains from releasing or storing water. Strategies where players bid their indifference prices and the marginal player undercuts the lowest-cost unsuccessful bidder constitute a Markov Perfect Equilibrium (MPE) under appropriate conditions. These conditions involve symmetric production capacity and nonfractional (i.e., “all or nothing”) output by successful bidders. Although the MPE solution represents an equilibrium consistent with dynamic strategic behavior, it requires computational sophistication by market participants. However, a basic “learning” procedure involving indifference prices converges to an MPE.", "e:keyword": ["Games: stochastic", "Noncooperative", "Natural resources: energy", "Water resources", "Economics", "Restructured electricity markets", "Dynamic auctions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0163", "e:abstract": "We consider the single-product lot-sizing problem over a finite planning horizon. Demand at each period is constant, and excess demand is completely backlogged. Holding and backlogging costs are proportional to the amount of inventory stocked or backlogged, while ordering cost is fixed, independent of the quantity ordered. The optimal policy targets to minimize the total relevant costs over the planning horizon. The key results of this paper are: (1) an explicit formula for the optimal total cost as a function of the model parameters and the number of cycles of the policy; (2) a new, polynomial-time algorithm which determines the overall optimal policy; and (3) stability regions for any solution considering simultaneous variations on all cost and demand parameters. The proposed algorithm is easy to implement and therefore is suitable for practical use.", "e:keyword": ["Inventory/production:lot-sizing problem with backlogging", "Inventory/production:stability regions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0164", "e:abstract": "Deterministic mathematical programming models that capture network effects play a predominant role in the theory and practice of airline revenue management. These models do not address important issues like demand uncertainty, nesting, and the dynamic nature of the booking process. Alternatively, the network problem can be broken down into leg-based problems for which there are satisfactory solution methods, but this approach cannot be expected to capture all relevant network aspects. In this paper, we propose a new algorithm that addresses these issues. Starting with any nested booking-limit policy, we combine a stochastic gradient algorithm and approximate dynamic programming ideas to improve the initial booking limits. Preliminary simulation experiments suggest that the proposed algorithm can lead to practically significant revenue enhancements.", "e:keyword": ["Simulation:applications", "Inventory:perishable items", "Transportation:airlines"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0165", "e:abstract": "We study the optimal production-inventory-outsourcing policy for a firm with Markovian in-house production capacity that faces independent stochastic demand and has the option to outsource. We find very simple optimal policy forms under fairly reasonable assumptions. In addition, when the capacity Markov process is stochastically monotone, the policy parameters decrease in the firm’s current capacity level under additional assumptions. All these results extend to the infinite-horizon and undiscounted-cost cases. We analyze comparative statics and the necessity of some technical conditions, and discuss when the outsourcing option is more valuable.", "e:keyword": ["Inventory/production: uncertainty", "Dynamic programming/optimal control: models", "Probability: Markov processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0166", "e:abstract": "This paper proposes a distributed solution approach to a certain class of dynamic resource allocation problems and develops a dynamic programming-based multiagent decision-making, learning, and communication mechanism. In the class of dynamic resource allocation problems we consider, a set of reusable resources of different types has to be assigned to tasks that arrive randomly over time. The assignment of a resource to a task removes the task from the system, modifies the state of the resource, and generates a contribution. We build a decision-making scheme where the decisions regarding the resources in different sets of states are made by different agents. We explain how to coordinate the actions of different agents using nonlinear functional approximations, and show that in a distributed setting, nonlinear approximations produce sequences of min-cost network flow problems that naturally yield integer solutions. We also experimentally compare the performances of the centralized and distributed solution strategies.", "e:keyword": ["Dynamic programming: approximate dynamic programming", "Computers: artificial intelligence", "Technology transportation: dynamic fleet management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0167", "e:abstract": "Hoist scheduling is a typical problem in the operation of electroplating systems. The cyclic scheduling policy is widely used in these systems in industry. Research on hoist scheduling has focused on the cyclic problem to minimize the cycle length. Most previous studies consider the single-hoist case. In practice, however, more than one hoist is often used in an electroplating line. This paper addresses the two-hoist, no-wait cyclic scheduling problem, in which the tank-processing times are constants and, upon completion of processing in a tank, the parts have to be moved to the next tank immediately. Based on the analysis of the problem properties, a polynomial algorithm is developed to obtain an optimal schedule. This algorithm first identifies a set of thresholds, which are special values of the cycle length, so that the feasibility property may change only at these thresholds. Feasibility checking is then carried out on each individual threshold in ascending order. The first feasible threshold found will be the optimal cycle length, and the corresponding feasible schedule is an optimal hoist schedule.", "e:keyword": ["Production/scheduling: cyclic", "Two hoists", "No-wait", "Noncrossing", "Manufacturing: automated electroplating systems", "Robotic cells"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0168", "e:abstract": "In this paper, we deal with the polyhedral description and the resolution of the Mixed General Routing Problem. This problem, in which the service activity occurs both at some of the nodes and at some of the arcs and edges of a mixed graph, contains a large number of important arc and node routing problems as special cases. Here, a large family of facet-defining inequalities, the Honeycomb inequalities, is described. Furthermore, a cutting-plane algorithm for this problem that incorporates new separation procedures for the K-C, Regular Path-Bridge, and Honeycomb inequalities is presented. Branch and bound is invoked when the final solution of the cutting-plane procedure is fractional. Extensive computational experiments over different sets of instances are included.", "e:keyword": ["Polyhedral combinatorics", "Rural postman problem", "General routing problem", "Mixed rural postman problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0169", "e:abstract": "We consider a spatial problem arising in forest harvesting. For regulatory reasons, blocks harvested should not exceed a certain total area, typically 49 hectares. Traditionally, this problem, called the adjacency problem, has been approached by forming a priori blocks from basic cells of 5 to 25 hectares and solving the resulting mixed-integer program. Superior solutions can be obtained by including the construction of blocks in the decision process. The resulting problem is far more complex combinatorially. We present an exact algorithmic approach that has yielded good results in computational tests. This solution approach is based on determining a strong formulation of the linear programming problem through a clique representation of a projected problem.", "e:keyword": ["Integer programming", "Cutting planes", "Environment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0170", "e:abstract": "Complex systems like semiconductor wafer fabrication facilities (fabs), networks of data switches, and large-scale call centers all demand efficient resource allocation. Deterministic models like linear programs (LP) have been used for capacity planning at both the design and expansion stages of such systems. LP-based planning is critical in setting a medium range or long-term goal for many systems, but it does not translate into a day-to-day operational policy that must deal with discreteness of jobs and the randomness of the processing environment.A stochastic processing network, advanced by J. Michael Harrison (2000, 2002, 2003), is a system that takes inputs of materials of various kinds and uses various processing resources to produce outputs of materials of various kinds. Such a network provides a powerful abstraction of a wide range of real-world systems. It provides high-fidelity stochastic models in diverse economic sectors including manufacturing, service, and information technology.We propose a family of maximum pressure service policies for dynamically allocating service capacities in a stochastic processing network. Under a mild assumption on network structure, we prove that a network operating under a maximum pressure policy achieves maximum throughput predicted by LPs. These policies are semilocal in the sense that each server makes its decision based on the buffer content in its serviceable buffers and their immediately downstream buffers. In particular, their implementation does not use arrival rate information, which is difficult to collect in many applications. We also identify a class of networks for which the nonpreemptive, non-processor-splitting version of a maximum pressure policy is still throughput optimal. Applications to queueing networks with alternate routes and networks of data switches are presented.", "e:keyword": ["Production/scheduling: sequencing", "Stochastic", "Queues: networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0171", "e:abstract": "Sensors in a data fusion environment over hostile territory are geographically dispersed and change location with time. To collect and process data from these sensors, an equally flexible network of fusion beds (i.e., clusterheads) is required. To account for the hostile environment, we allow communication links between sensors and clusterheads to be unreliable. We develop a mixed-integer linear programming (MILP) model to determine the clusterhead location strategy that maximizes the expected data covered minus the clusterhead reassignments, over a time horizon. A column generation (CG) heuristic is developed for this problem. Computational results show that CG performs much faster than a standard commercial solver, and the typical optimality gap for large problems is less than 5%. Improvements to the basic model in the areas of modeling link failure, consideration of bandwidth capacity, and clusterhead changeover cost estimation are also discussed.", "e:keyword": ["Wireless ad hoc networks", "Military applications", "Maximal expected coverage"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0172", "e:abstract": "We consider a model of a service system that delivers two nonsubstitutable services to a market of heterogenous users. The first service is delivered subject to a “guaranteed” (G) processing rate, and the second is a “best-effort” (BE) type service in which residual capacity not allocated to the guaranteed class is <i>shared</i> among BE users. Users, in turn, are sensitive to both price and congestion-related effects. The service provider’s objective is to optimally design the system so as to extract maximum revenues. The design variables in this problem consist of a pair of static prices for the two services, a policy that controls admission of G users into the system, and the mechanism by which users are informed of the state of congestion in the system. Because these objectives are difficult to address using exact analysis, we pursue approximations that are tractable and lead to structural insights. Specifically, we first solve a deterministic relaxation of the original objective to obtain a “fluid-optimal” solution that is subsequently evaluated and refined to account for stochastic fluctuations. Using diffusion limits, we derive approximations that yield the following structural results: (1) pricing rules derived from the deterministic analysis are “almost” optimal, (2) the optimal operational regime for the system is close to heavy traffic, and (3) real-time congestion notification results in increased revenues. Numerical results illustrate the accuracy of the proposed approximations and validate the aforementioned structural insights.", "e:keyword": ["Congestion notification", "Diffusion approximations", "Economics", "Halfin-Whitt regime", "Many server limits", "Pricing", "Queueing", "Revenue management", "Service differentiation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0173", "e:abstract": "In this note, we show that the expected delay cost for a <i>G/G/</i>1 queue is not necessarily convex in the arrival rate as sometimes claimed in the literature. We can prove, however, that the total expected delay cost rate is convex in the arrival rate. This cost rate is often of interest when designing queueing systems.", "e:keyword": ["Mathematics:convexity", "Queues:optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0174", "e:abstract": "Manufacturing firms routinely commit resources to increase yield rates through product- and process-improvement initiatives. Champions of such yield-improvement projects may assume that stochastically larger yield rates are beneficial. In this note, we show that this need not hold, even when the contingent production lot sizes are chosen optimally. We employ stochastic comparison techniques to show that a yield rate that is smaller in the convex order ensures higher expected profit, and we provide a distribution-free bound on the size of increase in expected profit. We also identify properties of yield-rate distributions that do make stochastically larger yield rates beneficial.", "e:keyword": ["Random yield", "Stochastic order relations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0175", "e:abstract": "This paper presents a simple model to determine the location strategies of two retail firms planning to open a number of stores in a geographical market. Firms try to maximize their profit under a leader-follower type competition in which the number of stores is made endogenous by the introduction of fixed costs. A novel methodology is developed in which firms’ strategies are defined in terms of their location densities. This methodology leads to a model that is solvable analytically, and to several results on competitive location strategies. First, it is shown that if the follower decides to enter a market, he enters at least as strongly as the leader. Second, the leader can effectively deter entry even if she is severely cost-disadvantaged. However, in some cases the leader is better off by allowing the follower to enter the market. Third, the leader may also let the follower enter the market in some situations where she has a cost advantage. It is also shown that in situations where both firms enter the market, their location strategies are quite insensitive to model parameters.", "e:keyword": ["Facilities/equipment planning", "Location", "Continuous"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0178", "e:abstract": "In this paper, we investigate the properties of the sampled version of the fictitious play algorithm, familiar from game theory, for games with identical payoffs, and propose a heuristic based on fictitious play as a solution procedure for discrete optimization problems of the form max{<i>u</i>(<i>y</i>): <i>y</i> = (<i>y</i><sup>1</sup>,…,<i>y<sup>n</sup></i>) ∈ (Y-script)<sup>1</sup>×⋯×(Y-script)<i><sup>n</sup></i>}, i.e., in which the feasible region is a Cartesian product of finite sets (Y-script)<sup>i</sup>, <i>i</i> ∈ <i>N</i> = {1,…,<i>n</i>}. The contributions of this paper are twofold. In the first part of the paper, we broaden the existing results on convergence properties of the fictitious play algorithm on games with identical payoffs to include an approximate fictitious play algorithm that allows for errors in players’ best replies. Moreover, we introduce sampling-based approximate fictitious play that possesses the above convergence properties, and at the same time provides a computationally efficient method for implementing fictitious play. In the second part of the paper, we motivate the use of algorithms based on sampled fictitious play to solve optimization problems in the above form with particular focus on the problems in which the objective function <i>u</i>(·) comes from a “black box,” such as a simulation model, where significant computational effort is required for each function evaluation.", "e:keyword": ["Games/group decisions:noncooperative", "Programming:algorithms/heuristic", "Transportation:route choice"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0179", "e:abstract": "No good formal arguments exist for a central question in biology: Why, in species that have sexual reproduction, are there usually only “males” and “females”? We present a nonlinear optimization model that supports the conclusion that having only two sexes maximizes long-run viability.", "e:keyword": ["Programming:nonlinear/quadratic", "Mathematics:convexity", "Modeling:applications in biology/sexual reproduction"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0180", "e:abstract": "This paper investigates the effect of patient choice on kidney allocation using the following sequential stochastic assignment model. There are <i>n</i> transplant patients to be allocated <i>n</i> kidneys that will arrive sequentially. Each patient and each kidney has its own type, kidney types are random and revealed upon arrival, and the reward from allocating a kidney to a particular patient depends on both their types. Patients may choose to accept or decline any kidney offer.The objective is to determine a kidney allocation policy that maximizes total expected reward subject to the constraint that patients will only accept offers that maximize their own expected reward. A partition policy, in which the space of kidney types is divided into different domains (each corresponding to a different patient type) and in which each kidney is allocated to the patient type corresponding to its domain, is shown to be asymptotically optimal when patients must accept all kidney offers. To reflect patient choice, an incentive compatibility condition is derived to ensure that the offers made by the allocation policy are never declined. This condition is then used to derive a “second-best” partition policy. A numerical example, based on data from the US transplantation system, demonstrates that patient choice may introduce substantial inefficiencies, but the second-best policy recovers all the losses by minimizing the variability in the type of offers expected by each patient. Thus, policy makers should explicitly recognize the effect of patient choice when designing a kidney allocation system.", "e:keyword": ["Kidney allocation:strategic behavior", "Incentive compatibility", "Assignment models:dynamic", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0181", "e:abstract": "This paper presents a spatially aggregated multiperiod taxi service model with endogenous service intensity. The whole day service period is divided into a number of subperiods; during each subperiod, taxi supply and customer demand characteristics are assumed to be uniform. Customer demand is period-specific and described as a function of waiting time and taxi fare. Taxi operating cost for each work shift consists of two components: one component a function of total service time and the other component period-dependent. Each taxi driver can work for one or more shifts each day and freely chooses the starting and ending time of each shift. Equilibrium of taxi services is obtained when taxi drivers cannot increase their individual profits by changing their individual working schedules. A novel clock network representation is proposed to characterize the multiperiod taxi service equilibrium problem. The problem of interest is formulated as a network equilibrium model with path-specific costs and arc-capacity constraints, which can be solved using conventional nonlinear network flow optimization methods. The proposed model can ascertain at equilibrium the service intensity and utilization rate of taxis and the level of service quality throughout the day. The information obtained is useful for the prediction of the effects of alternative government regulations on the equilibrium of demand and supply in the urban taxi industry.", "e:keyword": ["Transportation", "Models", "Taxis/limousines", "Networks/graphs:applications", "Flow algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0182", "e:abstract": "We consider a dynamic asset allocation problem formulated as a mean-shortfall model in discrete time. A characterization of the solution is derived analytically under general distributional assumptions for serially independent risky returns. The solution displays risk taking under shortfall, as well as a specific form of time diversification. Also, for a representative stock-return distribution, risk taking increases monotonically with the number of decision moments given a fixed horizon. This is related to the well-known casino effect arising in a downside-risk and expected return framework. As a robustness check, we provide results for a modified objective with a quadratic penalty on shortfall. An analytical solution for a single-stage setup is derived, and numerical results for the two-period model and time diversification are provided.", "e:keyword": ["Multistage stochastic programming", "Downside risk", "Asset/liability management", "Time diversification"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0183", "e:abstract": "We give a formal specification for a strategic network routing problem known as the convoy movement problem (CMP) and establish that the corresponding feasibility problem is NP-complete. We then introduce an integer programming (IP) model based on the concept of a time-space network and apply a Lagrangian relaxation to this model. We discuss how the dual function may be evaluated using a modified version of Dijkstra’s algorithm suitable to very large, implicitly defined graphs and show how heuristic solutions to the primal problem may be obtained. We present results for a number of instances of the CMP, most of which are based on real-world problems. The number of convoys in these instances varies between 15–25, and their movement time requires up to several thousand time units in networks ranging in size from a few dozen to several thousand vertices and edges. The most difficult instance tested involves 17 long convoys each taking four times the average link travel time to pass through a point in the network. This instance is solved within 3.3% of optimality in less than 3.5 hours of computing time on a Dell Precision 420 dual processor computer. Every other test instance is solved within 2% of the optimal value in less than 20 minutes of computing time.", "e:keyword": ["Military: logistics", "Transportation: vehicle routing", "Programming: integer", "Relaxation/subgradient"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0184", "e:abstract": "This paper analyzes a series inventory system with stationary costs and stochastic demand over an infinite horizon. A distinctive feature is that demand can be negative, representing returns from customers, as well as zero or positive. We observe that, as in a system with nonnegative demand, a stationary echelon base-stock policy is optimal here. However, the steady-state behavior of the system under such a policy is different from that in systems with nonnegative demands. We present an exact procedure and several approximations for computing the operating characteristics and system costs for any stationary echelon base-stock policy, and also describe an algorithm for computing a good policy. As an alternative to the echelon base-stock policy, we discuss a policy that uses only local information. Finally, we describe how to extend the analysis to the case where returns occur at multiple stages instead of just at the stage closest to demand, and the case where returns require a recovery lead time.", "e:keyword": ["Inventory/production: multiechelon", "Environment: product recovery and remanufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0185", "e:abstract": "Dynamic linear programming (LP) models for asset-liability management (ALM) are quite powerful and flexible but face two challenges: (1) many modeling choices, not all consistent with one another or with finance theory, and (2) solution difficulties due to the large number of scenarios obtained from standard interest-rate models. We first survey these modeling choices with a view to help researchers make self-consistent choices. Next, we review how the dynamic LP model for ALM and the representation of uncertainty therein have been simplified in the past to motivate new or hybrid models emphasizing tractability. To this end, we review existing static LP models as extreme modeling simplifications and aggregation as a simplification of uncertainty.", "e:keyword": ["Finance: portfolio", "Financial institutions: banks", "Information systems: decision support systems", "Programming: linear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0186", "e:abstract": "Huang et al. (2003) used the Cesaro limit of a savings function to determine the optimal special order in an EOQ model with single announced price increases over an infinite horizon. In this note, we point out that the savings function is not Cesaro summable. More importantly, no limiting argument for the cost function <i>d(t, Q<sub>s</sub>)</i> as <i>t</i>→∞ is necessary at all given that this function is periodic for which it suffices to optimize the integral of the function over any given period.", "e:keyword": ["Inventory", "Economic order quantity", "Special ordering for price increases"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0187", "e:abstract": "Selection procedures help identify the best of a finite set of simulated alternatives. Most work has measured the quality of a selection with the probability of correct selection, P(CS), but the expected opportunity cost of a potentially incorrect decision makes more sense in business contexts. This paper analyzes the first selection procedures that guarantee an upper bound for the expected opportunity cost, in a frequentist sense, of a potentially incorrect selection. The paper therefore bridges a gap between the indifference-zone approach (with frequentist guarantees, but for the P(CS)) and the Bayesian approach to selection procedures (which has considered the opportunity cost). We also provide “unexpected” expected opportunity cost guarantees for several existing indifference-zone selection procedures that were originally designed to provide P(CS) guarantees.", "e:keyword": ["Simulation: ranking and selection", "Statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0188", "e:abstract": "A controller dynamically chooses a state-dependent transmission rate on a static, point-to-point wireless link by varying transmission power over time. The transmitter is modeled as a finite-buffer Markovian queue with adjustable service rates. That is, data packets arrive to the system according to a Poisson process, and packet size is exponentially distributed. The controller chooses a transmission rate from a fixed set <i>A</i> of available values, depending on the backlog in the system. The objective is to minimize long-run average energy consumption subject to a quality-of-service constraint, which is expressed as an upper bound on the packet drop rate. An explicit formula is developed for the optimal transmission rate as a function of the packet queue length.", "e:keyword": ["Telecommunications: dynamic power control", "Dynamic programming: service rate control in queues", "Queues: dynamic control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0189", "e:abstract": "In biology, the protein structure alignment problem answers the question of how similar two proteins are. Proteins with strong physical similarities in their tertiary (folded) structure often have similar functions, so understanding physical similarity could be a key to developing protein-based medical treatments. One of the models for protein structure alignment is the maximum contact map overlap (CMO) model. The CMO model of protein structure alignment can be cast as a maximum clique problem on an appropriately defined graph. We exploit properties of these protein-based maximum clique problems to develop specialized preprocessing techniques and show how they can be used to more quickly solve contact map overlap instances to optimality.", "e:keyword": ["Networks/graphs:applications", "Heuristics", "Programming:integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0190", "e:abstract": "Providing accurate and automated input-modeling support is one of the challenging problems in the application of computer simulation of stochastic systems. The models incorporated in current input-modeling software packages often fall short because they assume independent and identically distributed processes, even though dependent time-series input processes occur naturally in the simulation of many real-life systems. Therefore, this paper introduces a statistical methodology for fitting stochastic models to dependent time-series input processes. Specifically, an automated and statistically valid algorithm is presented to fit autoregressive-to-anything (ARTA) processes with marginal distributions from the Johnson translation system to stationary univariate time-series data. ARTA processes are particularly well suited to driving stochastic simulations. The use of this algorithm is illustrated with examples.", "e:keyword": ["Simulation", "Statistical analysis:stochastic input modeling", "Statistics", "Correlation", "Estimation", "Time series:autoregressive processes", "Least-squares fitting"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0191", "e:abstract": "This article studies two types of flexibility used by firms to better respond to uncertain market conditions: resource flexibility and responsive pricing. We consider a situation in which a single flexible resource can be used to satisfy two distinct demand classes. While the resource capacity must be decided based on uncertain demand functions, the resource allocation as well as the pricing decision are made based on the realized demand functions.We characterize the effects of two key drivers of flexibility: demand variability and demand correlation, assuming normally distributed demand curve intercepts. Demand variability creates opportunity costs and, with fixed prices, decreases the firm’s profit. We show that with the additional flexibility gained from responsive pricing, the firm can maximize the benefits of favorable demand conditions and mitigate the effects of poor demand conditions, ultimately profiting from variability. Positive demand correlation, on the other hand, remains undesirable under responsive pricing. The optimal capacity of the flexible resource is always increasing in both demand variability and demand correlation. This contrasts with the scenarios based on fixed prices, highlighting the crucial difference that responsive pricing makes in the management of flexible resources. We further quantify the value of flexibility for the firm and its customers by considering, as a benchmark, a firm relying on two dedicated resources. The value of flexibility is most significant if the demand levels are highly variable and negatively correlated. In such cases, the firm benefits from demand variability due to responsive pricing, while facing limited demand risk due to resource flexibility. Finally, we endogenize the input price of the flexible resource by considering the pricing decision of the resource supplier.", "e:keyword": ["Inventory/production:uncertainty", "Pricing", "Flexible capacity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0192", "e:abstract": "One of the distinguishing features of a backbone link is that it is designed to carry traffic from a large number of end users. This results in a Normal distribution for the number of bytes or packets that arrive in a fixed-length time interval. Based on this observation, which is substantiated by data analysis, we present a simple model for the steady-state loss probability that can be solved in closed form. This model assumes that there is no buffer, so that issues raised by the correlation of counts that is characteristic of packet traffic are bypassed.The longest interval that captures the relevant statistical fluctuations of backbone traffic is one second. Data collection on live commercial networks is costly, so byte and packet counts are usually collected over much longer time intervals; five minutes is a lower bound. This creates no problem in estimating the mean of the Normal distribution, but it makes direct estimation of the variance for one-second counts infeasible. Routers collect <i>flow</i> data; flows are analogous to “calls” in telephony. By modeling the number of active flows as an <i>M/G/</i>∞ queue and assuming that packets in a flow are spread uniformly in time, an equation for the variance of (say) one-second counts in terms of measured quantities is derived. The efficacy of this formula is demonstrated by applying it to data.", "e:keyword": ["Queues:applications", "Approximations", "Statistics:data analysis", "Time series"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0193", "e:abstract": "We consider the allocation of capacity in a system in which rental equipment is accessed by two classes of customers. We formulate the problem as a continuous-time analogue of the one-shot allocation problems found in the more traditional literature on revenue management, and we analyze a queueing control model that approximates its dynamics. Our investigation yields three sets of results.First, we use dynamic programming to characterize properties of optimal capacity allocation policies. We identify conditions under which “complete sharing”—in which both classes of customers have unlimited access to the rental fleet—is optimal.Next, we develop a computationally efficient “aggregate threshold” heuristic that is based on a fluid approximation of the original stochastic model. We obtain closed-form expressions for the heuristic’s control parameters and show that the heuristic performs well in numerical experiments. The closed-form expressions also show that, in the context of the fluid approximation, revenues are concave and increasing in the fleet size.Finally, we consider the effect of the ability to allocate capacity on optimal fleet size. We show that the optimal fleet size under allocation policies may be lower, the same as, or higher than that under complete sharing. As capacity costs increase, allocation policies allow for larger relative fleet sizes. Numerical results show that, even in cases in which dollar profits under complete sharing may be close to those under allocation policies, the capacity reductions enabled by allocation schemes can help to lift profit margins significantly.", "e:keyword": ["Service systems", "Queueing control", "Stochastic knapsack", "Fluid models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0195", "e:abstract": "In this paper, we propose a stochastic version of the salvo model for modern naval surface combat. We derive expressions for the mean and variance of surviving force strengths and for the probabilities of the possible salvo outcomes in forms simple enough to be implemented in spreadsheet software. Numerical comparisons of the deterministic and stochastic models indicate that while the two models tend to provide similar estimates of the average number of ships surviving a salvo, this average by itself can be highly misleading with respect to the likely outcomes of the battle. Our results also suggest that a navy’s preferences for risk (variability) and armament (offensive versus defensive) will depend on not only its mission objectives but also on whether it expects to fight from a position of strength or of weakness.", "e:keyword": ["Warfare models: stochastic salvo combat", "Tactics/strategy: naval surface tactics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0196", "e:abstract": "The delivery scheduling problem studied in this paper was motivated by the operation in a large personal computer assembly plant, which was using multisourcing for some of its materials. The company’s objective was to design a delivery schedule so that the average inventory level in the factory was minimized. We show that the problem is intimately related to a classical inventory staggering problem, where the focus is on the computation of the peak inventory level associated with the replenishment policy. This connection allows us to show that the delivery scheduling problem is NP-hard. For the two-vendor case with <i>integral</i> replenishment intervals, we propose a generalized form of Homer’s scheduling heuristic and obtain performance bounds for the classical inventory staggering problem. Our analysis uses the Chinese remainder theorem in an interesting way. The approach can be generalized to the case with more than two vendors, leading to a strong linear-programming-based lower bound for the inventory staggering problem. We illustrate this technique for the case in which all the replenishment intervals are relatively prime, establishing a bound that is not greater than 140% of the optimal. We examine the implications of these results to the delivery scheduling problem.", "e:keyword": ["Logistics", "Inventory management", "Multisourcing", "Chinese remainder theorem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0197", "e:abstract": "The design of route guidance systems faces a well-known dilemma. The approach that theoretically yields the system-optimal traffic pattern may discriminate against some users in favor of others. Proposed alternate models, however, do not directly address the system perspective and may result in inferior performance. We propose a novel model and corresponding algorithms to resolve this dilemma. We present computational results on real-world instances and compare the new approach with the well-established traffic assignment model. The essence of this study is that system-optimal routing of traffic flow with explicit integration of user constraints leads to a better performance than the user equilibrium, while simultaneously guaranteeing superior fairness compared to the pure system optimum.", "e:keyword": ["Networks/graphs", "Multicommodity:theory", "Transportation:models", "Mathematics:combinatorics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0198", "e:abstract": "An agent (who may or may not want to be found) is located in one of two boxes. At time 0 suppose that he is in box <i>B</i>. With probability <i>p</i> he wishes to be found, in which case he has been asked to stay in box <i>B</i>. With probability <i>1−p</i> he tries to evade the searcher, in which case he may move between boxes <i>A</i> and <i>B</i>. The searcher looks into one of the boxes at times 1, 2, 3, … . Between each search the agent may change boxes if he wants. The searcher is trying to minimise the expected time to discovery. The agent is trying to minimise this time if he wants to be found, but trying to maximise it otherwise. This paper finds a solution to this game (in a sense defined in the paper), associated strategies for the searcher and each type of agent, and a continuous value function <i>v(p)</i> giving the expected time for the agent to be discovered. The solution method (which is to solve an associated zero-sum game) would apply generally to this type of game of incomplete information.", "e:keyword": ["Search and surveillance:rendezvous search", "Evasion search", "Games and group decisions:teams"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0199", "e:abstract": "We present a new multiseasonal, multiyear, natural gas market equilibrium model based on the concept of a competitive equilibrium involving the market participants: producers, storage reservoir operators, peak gas operators, pipeline operators, marketers, and consumers. The first three classes are depicted as price-takers consistent with perfect competition. The pipeline operations are described with regulated tariffs, but also involve “congestion pricing” as a mechanism to allocate scarce pipeline capacity. The marketers are price-takers in all markets except in sales to consumers, in which they compete as Nash-Cournot players. Finally, consumers are described by demand curves for each of the four sectors: residential, commercial, industrial, and electric power. We show that the equilibrium model is an instance of a mixed nonlinear complementarity problem (NCP) and provide sufficient detail not generally seen in previous complementarity models of natural gas. The NCP formulation is derived from considering the Karush-Kuhn-Tucker optimality conditions of the optimization problems faced by these participants. Under mild conditions, we show that this NCP has a solution, and under additional reasonable conditions, we show that the market prices are unique. We also validate the model on a representative sample network with nine market participants and three seasons, using four scenarios.", "e:keyword": ["Games/group decisions: noncooperative", "Programming: complementarity", "Industries: petroleum/natural gas", "Marketing: competitive strategy", "Natural resources: energy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0200", "e:abstract": "For a Markovian queueing network with two stations in tandem, finite intermediate buffer, and <i>M</i> flexible servers, we study how the servers should be assigned dynamically to stations to obtain optimal long-run average throughput. We assume that each server can work on only one job at a time, that several servers can work together on a single job, and that the travel times between stations are negligible. Under these assumptions, we completely characterize the optimal policy for systems with three servers. We also provide a conjecture for the structure of the optimal policy for systems with four or more servers that is supported by extensive numerical evidence. Finally, we develop heuristic server-assignment policies for systems with three or more servers that are easy to implement, robust with respect to the server capabilities, and generally appear to yield near-optimal long-run average throughput.", "e:keyword": ["Production/scheduling:flexible manufacturing/line balancing", "Dynamic programming/optimal control:applications", "Queues:tandem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0201", "e:abstract": "We consider facilities that follow a cyclic schedule to replenish the inventory of a set of items through production by a shared resource. We introduce a dynamic produce-up-to policy that recovers the target cyclic schedule after a single disruption, and is also shown to be effective when disruptions are more frequent. Our policy is more flexible than traditional recovery policies in that our policy is able to adjust the amount of idle time observed during recovery in response to disruptions, and yet re-establish the target idle time as the schedule recovers. This results in a policy that not only saves cost and time, but provides better schedule stability than other recovery procedures. Furthermore, unlike simple produce-up-to policies, our policy is anticipatory—replenishments will be speeded up or delayed, whichever is necessary, to help avoid congestion at the shared resource. In addition, recovery is controlled by a single “knob” or parameter that can tune recovery to be fast and aggressive (frequent setups and small batches) or slow and methodical (few setups and larger batches). Finally, our policy is easy to implement, augmenting a traditional produce-up-to policy with a simple set of counters that control replenishment decisions.", "e:keyword": ["Production/scheduling:recovering cyclic schedules", "Bucket brigades", "Mathematics:discrete dynamical systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0202", "e:abstract": "This paper concerns statistical disclosure control methods to minimize information loss while keeping small the disclosure risk from different data snoopers. This issue is of primary importance in practice for statistical agencies when publishing data. It is assumed that the sensitive data have been identified by practitioners in the statistical offices, and the paper addresses the secondary problem of protecting these data with different methods, all defined in a unified mathematical framework. A common definition of protection is used in four different methodologies. Two integer linear programming models described in the literature for the <i>cell suppression</i> methodology are extended to work also for the <i>controlled rounding</i> methodology. In addition, two relaxed variants are presented using two associated linear programming models, called <i>partial cell suppression</i> and <i>partial controlled rounding</i>, respectively. A final discussion shows how to combine the four methods and how to implement a cutting-plane approach for the exact and heuristic resolution of the combinatorial problems in practice. This approach was implemented in ARGUS, a software package of disclosure limitation tools.", "e:keyword": ["Statistical disclosure control", "Cell suppression", "Rounding", "Integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0203", "e:abstract": "We consider a supply chain consisting of one manufacturer (capacitated supplier) and two retailers. We characterize the manufacturer’s optimal production policy under selective-information sharing, in which the manufacturer receives demand and inventory information from only one of the two retailers. We show that the manufacturer’s optimal production policy is a state-dependent base-stock policy and that the base-stock levels have a monotonic structure. We also perform an extensive numerical study to examine how system factors affect the benefit of information sharing and the relative values of information from each retailer. In addition, we identify cases where the cost saving due to receiving information from only one retailer captures most of the saving that can be obtained when the information is received from both retailers. Finally, we investigate the cost effectiveness of echelon-stock policies in systems with full-information sharing and introduce the “information pooling effect” as well as economies of scale with respect to information sharing.", "e:keyword": ["Production/scheduling:applications", "Dynamic programming:applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0204", "e:abstract": "This paper presents a method to assign utility values when only partial information is available about the decision maker’s preferences. We introduce the notion of a utility density function and a maximum entropy principle for utility assignment. The maximum entropy utility solution embeds a large family of utility functions that includes the most commonly used functional forms. We discuss the implications of maximum entropy utility on the preference behavior of the decision maker and present an application to competitive bidding situations where only previous decisions are observed by each party. We also present minimum cross entropy utility, which incorporates additional knowledge about the shape of the utility function into the maximum entropy formulation, and work through several examples to illustrate the approach.", "e:keyword": ["Decision analysis", "Applications", "Utility/preference", "Estimation", "Multiattribute"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0205", "e:abstract": "The general routing problem (GRP) is the problem of finding a minimum length tour, visiting a number of specified vertices and edges in an undirected graph. In this paper, we describe how the well-known 2-opt and 3-opt local search procedures for node routing problems can be adapted to solve arc and general routing problems successfully. Two forms of the 2-opt and 3-opt approaches are applied to the GRP. The first version is similar to the conventional approach for the traveling salesman problem; the second version includes a dynamic programming procedure and explores a larger neighborhood at the expense of higher running times. Extensive computational tests, including ones on larger instances than previously reported in the arc routing literature, are performed with variants of both algorithms. In combination with the guided local search metaheuristic and the mechanisms of marking and neighbor lists, the procedures systematically detect optimal or high-quality solutions within limited computation time.", "e:keyword": ["Transportation", "Vehicle routing: local search", "2-opt", "Networks/graphs", "Traveling salesman: arc routing", "General routing problem", "Rural postman problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1040.0206", "e:abstract": "Increasing generalized failure rate (IGFR) distributions were introduced as a tool in the study of contracting mechanisms in supply chains. In this note, we compare and contrast the closure—and the lack thereof—of IGFR and increasing failure rate (IFR) distributions with respect to standard operations on random variables. Some implications of these results for the use of IGFR distributions in supply chain models are noted.", "e:keyword": ["Inventory/production:stochastic models", "Probability:distributions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0194", "e:abstract": "We consider the simultaneous seat-inventory control of a set of parallel flights between a common origin and destination with dynamic customer choice among the flights. We formulate the problem as an extension of the classic multiperiod, single-flight “block demand” revenue management model. The resulting Markov decision process is quite complex, owing to its multidimensional state space and the fact that the airline’s inventory controls do affect the distribution of demand. Using stochastic comparisons, consumer-choice models, and inventory-pooling ideas, we derive easily computable upper and lower bounds for the value function of our model. We propose simulation-based techniques for solving the stochastic optimization problem and also describe heuristics based upon an extension of a well-known linear programming formulation. We provide numerical examples.", "e:keyword": ["Dynamic programming/optimal control:applications", "Transportation:yield management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0210", "e:abstract": "In the three-dimensional bin packing problem the task is to orthogonally pack a given set of rectangular items into a minimum number of three-dimensional rectangular bins. We give a characterization of the algorithm proposed by Martello et al. (2000) for the exact solution of the problem, showing that not all orthogonal packings can be generated by the proposed algorithm. The packings, however, have the property of being robot packings, which is relevant in practical settings. References to the modified algorithm, which solves the orthogonal as well as robot packable three-dimensional problem, are given.", "e:keyword": ["Programming:integer algorithms", "Production/scheduling:cutting stock/trim"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0211", "e:abstract": "We consider three models of investments in generation capacity in restructured electricity systems that differ with respect to their underlying economic assumptions. The first model assumes a perfect, competitive equilibrium. It is very similar to the traditional capacity expansion models even if its economic interpretation is different. The second model (open-loop Cournot game) extends the Cournot model to include investments in new generation capacities. This model can be interpreted as describing investments in an oligopolistic market where capacity is simultaneously built and sold in long-term contracts when there is no spot market. The third model (closed-loop Cournot game) separates the investment and sales decision with investment in the first stage and sales in the second stage—that is, a spot market. This two-stage game corresponds to investments in merchant plants where the first-stage equilibrium problem is solved subject to equilibrium constraints. We show that despite some important differences, the open- and closed-loop games share many properties. One of the important results is that the prices and quantities produced in the closed-loop game, when the solution exists, fall between the prices and quantities in the open-loop game and the competitive equilibrium.", "e:keyword": ["Electric utilities", "Existence and characterization of equilibria", "Noncooperative games", "Programming", "Oligopolistic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0212", "e:abstract": "This paper presents fast algorithms for calculating mean-variance efficient frontiers when the investor can sell securities short as well as buy long, and when a factor and/or scenario model of covariance is assumed. Currently, fast algorithms for factor, scenario, or mixed (factor and scenario) models exist, but (except for a special case of the results reported here) apply only to portfolios of long positions. Factor and scenario models are used widely in applied portfolio analysis, and short sales have been used increasingly as part of large institutional portfolios. Generally, the critical line algorithm (CLA) traces out mean-variance efficient sets when the investor’s choice is subject to any system of linear equality or inequality constraints. Versions of CLA that take advantage of factor and/or scenario models of covariance gain speed by greatly simplifying the equations for segments of the efficient set. These same algorithms can be used, unchanged, for the long-short portfolio selection problem provided a certain condition on the constraint set holds. This condition usually holds in practice.", "e:keyword": ["Finance", "Portfolio:optimization with short sales"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0213", "e:abstract": "When considering problems of sequential decision making under uncertainty, two of the most interesting questions are: How does the value of the optimal decision variable change with an increase in risk? How does the value of the optimal decision variable change with a more informative signal? In this paper, we show that if the payoff function is separable in the random variable, then one model can simultaneously answer both questions. This result holds for the reaction functions and equilibria of noncooperative games, as well as for single decision makers, with virtually no restrictions on the payoff functions. This is useful because otherwise it is very difficult to get at general results on the impact of learning. Furthermore, we clarify why the impacts of risk and a more informative signal are different when the payoff function is nonlinear in the random variable. It is because the directional impacts of informativeness are independent of risk attitude; the impacts of risk are not.", "e:keyword": ["Decision analysis", "Theory", "Comparative statics of risk and learning", "Games", "Stochastic", "Impact of learning on equilibrium", "Natural resources", "Energy", "Climate change policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0214", "e:abstract": "This paper considers the problem of determining the distribution of the weight <i>W</i> of a minimum spanning tree for an undirected graph with edge weights that are independently distributed discrete random variables. Using the underlying fundamental cutsets and cycles associated with a spanning tree, we are able to obtain upper and lower bounds on the distribution of <i>W</i>. In turn, these are used to establish bounds on <i>E[W]</i>. Our general method for deriving these bounding distributions subsumes existing approximation methods in the literature. Computational results indicate that the new approximation methods provide excellent bounds for some challenging test networks.", "e:keyword": ["Stochastic networks: bounding distributions for minimum spanning tree weight", "Tree algorithms: fundamental cycles and cutsets"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0215", "e:abstract": "A superposition of a large number of infinite source Poisson inputs or that of a large number of ON-OFF inputs with heavy tails can look like either a fractional Brownian motion or a stable Lévy motion, depending on the magnification at which we are looking at the input process (Mikosch et al. 2002). In this paper, we investigate what happens to a queue driven by such inputs. Under such conditions, we study the output of a single fluid server and the behavior of a fluid queueing network. For the network we obtain random field limits describing the activity at different stations. In general, both kinds of stations arise in the same network: the stations of the first kind experience loads driven by a fractional Brownian motion, while the stations of the second kind experience loads driven by a stable Lévy motion.", "e:keyword": ["Queue", "Queueing network", "Output process", "Heavy-tailed distribution", "Long-range dependence", "Fractional Brownian motion", "Stable Lévy process", "Weak convergence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0216", "e:abstract": "Optimal solutions to Markov decision problems may be very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of these probabilities is far from accurate. Hence, estimation errors are limiting factors in applying Markov decision processes to real-world problems.We consider a robust control problem for a finite-state, finite-action Markov decision process, where uncertainty on the transition matrices is described in terms of possibly nonconvex sets. We show that perfect duality holds for this problem, and that as a consequence, it can be solved with a variant of the classical dynamic programming algorithm, the “robust dynamic programming” algorithm. We show that a particular choice of the uncertainty sets, involving likelihood regions or entropy bounds, leads to both a statistically accurate representation of uncertainty, and a complexity of the robust recursion that is almost the same as that of the classical recursion. Hence, robustness can be added at practically no extra computing cost. We derive similar results for other uncertainty sets, including one with a finite number of possible values for the transition matrices.We describe in a practical path planning example the benefits of using a robust strategy instead of the classical optimal strategy; even if the uncertainty level is only crudely guessed, the robust strategy yields a much better worst-case expected travel time.", "e:keyword": ["Dynamic programming: Markov", "Finite state", "Game theory", "Programming: convex", "Uncertainty", "Robustness", "Statistics: estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0217", "e:abstract": "We analyze the optimal behavior of two players who are lost on a planar surface and who want to meet each other in least expected time. They each know the initial distribution of the other’s location, but have no common labeling of points, and so cannot simply go to a location agreed to in advance. They have no compasses, so do not even have a common notion of North. For simplicity, we restrict their motions to the integer lattice <i>Z</i><sup>2</sup> (graph paper) and their motions to horizontal and vertical directions, as in the original work of Anderson and Fekete (2001).", "e:keyword": ["Search and surveillance: rendezvous search", "Games/group decisions: teams"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0218", "e:abstract": "The black and white traveling salesman problem (BWTSP) is defined on a graph <i>G</i> whose vertex set is partitioned into black and white vertices. The aim is to design a shortest Hamiltonian tour on <i>G</i> subject to cardinality and length constraints: both the number of white vertices as well as the length of the tour between two consecutive black vertices are bounded above. The BWTSP has applications in airline scheduling and in telecommunications. This paper proposes an integer linear formulation for the undirected BWTSP, as well as several classes of valid inequalities. An exact branch-and-cut algorithm is then developed. Extensive tests show that it can solve exactly instances involving up to 100 vertices. The algorithm can also be applied directly to solve unit demand vehicle routing problems of similar sizes.", "e:keyword": ["Traveling salesman problem", "Vehicle routing problem", "Valid inequalities", "Branch-and-cut", "Exact algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0219", "e:abstract": "This paper develops algorithms for the pricing of discretely sampled barrier, lookback, and hindsight options and discretely exercisable American options. Under the Black-Scholes framework, the pricing of these options can be reduced to evaluation of a series of convolutions of the Gaussian distribution and a known function. We compute these convolutions efficiently using the double-exponential integration formula and the fast Gauss transform. The resulting algorithms have computational complexity of <i>O(nN)</i>, where the number of monitoring/exercise dates is <i>n</i> and the number of sample points at each date is <i>N</i>, and our results show the error decreases exponentially with <i>N</i>. We also extend the approach and provide results for Merton’s lognormal jump-diffusion model.", "e:keyword": ["Finance: asset pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0222", "e:abstract": "Column generation is often used to solve problems involving set-partitioning constraints, such as vehicle-routing and crew-scheduling problems. When these constraints are in large numbers and the columns have on average more than 8–12 nonzero elements, column generation often becomes inefficient because solving the master problem requires very long solution times at each iteration due to high degeneracy. To overcome this difficulty, we introduce a dynamic constraint aggregation method that reduces the number of set-partitioning constraints in the master problem by aggregating some of them according to an equivalence relation. To guarantee optimality, this equivalence relation is updated dynamically throughout the solution process. Tests on the linear relaxation of the simultaneous vehicle and crew-scheduling problem in urban mass transit show that this method significantly reduces the size of the master problem, degeneracy, and solution times, especially for larger problems. In fact, for an instance involving 1,600 set-partitioning constraints, the master problem solution time is reduced by a factor of 8.", "e:keyword": ["Programming:linear", "Large-scale systems", "Column generation", "Degeneracy", "Dynamic constraint aggregation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0223", "e:abstract": "We investigate the polyhedral structure of the lot-sizing problem with inventory bounds. We consider two models, one with linear cost on inventory, the other with linear and fixed costs on inventory. For both models, we identify facet-defining inequalities that make use of the inventory bounds explicitly and give exact separation algorithms. We also describe a linear programming formulation of the problem when the order and inventory costs satisfy the Wagner-Whitin nonspeculative property. We present computational experiments that show the effectiveness of the results in tightening the linear programming relaxations of the lot-sizing problem with inventory bounds and fixed costs.", "e:keyword": ["Lot sizing", "Facets", "Separation algorithms", "Computation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0225", "e:abstract": "Methods for selecting a research and development (R&D) project portfolio have attracted considerable interest among practitioners and academics. This notwithstanding, the industrial uptake of these methods has remained limited, partly because of the difficulties of capturing relevant concerns in R&D portfolio management. Motivated by these difficulties, we develop contingent portfolio programming (CPP), which extends earlier approaches in that it (i) uses states of nature to capture exogenous uncertainties, (ii) models resources through dynamic state variables, and (iii) provides guidance for the selection of an optimal project portfolio that is compatible with the decision maker’s risk attitude. Although CPP is presented here in the context of R&D project portfolios, it is applicable to a variety of investment problems where the dynamics and interactions of investment opportunities must be accounted for.", "e:keyword": ["Research and development: project selection", "Decision analysis: theory", "Programming: linear", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0226", "e:abstract": "To gain insights into the design and control of manufacturing cells with automation, we study simple models of serial production systems where one flexible worker attends a set of automated stations. We (a) characterize the operational benefits of automation, (b) determine the most desirable placement of automation within a line, and (c) investigate how best to allocate labor dynamically in a line with manual and automatic equipment. We do this by first considering two-station Markov decision process models and then studying three-station simulations. Our results show that the capacity of production lines with automatic machines can be significantly lower than the rate of the bottleneck. We also show that automating a manual machine can have a dramatic effect on the average work-in-process (WIP) level, provided that labor is the system bottleneck. Once a machine becomes the bottleneck, the benefits from further automation are dramatically reduced. In general, we find that automation is more effective when placed toward the end of the line rather than toward the front. Finally, we show that automation level increases the priority workers should give to a station when selecting a work location.", "e:keyword": ["Cross-trained workers", "Serial line", "Markov decision process", "Automation", "Capacity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0227", "e:abstract": "Deterministic fluid models are developed to provide simple first-order performance descriptions for multiserver queues with abandonment under heavy loads. Motivated by telephone call centers, the focus is on multiserver queues with a large number of servers and nonexponential service-time and time-to-abandon distributions. The first fluid model serves as an approximation for the <i>G/GI/s+GI</i> queueing model, which has a general stationary arrival process with arrival rate <i>(lambda)</i>, independent and identically distributed (IID) service times with a general distribution, <i>s</i> servers and IID abandon times with a general distribution. The fluid model is useful in the overloaded regime, where <i>(lambda) > s</i>, which is often realistic because only a small amount of abandonment can keep the system stable. Numerical experiments, using simulation for <i>M/GI/s+GI</i> models and exact numerical algorithms for <i>M/M/s+M</i> models, show that the fluid model provides useful approximations for steady-state performance measures when the system is heavily loaded. The fluid model accurately shows that steady-state performance depends strongly upon the time-to-abandon distribution beyond its mean, but not upon the service-time distribution beyond its mean. The second fluid model is a discrete-time fluid model, which serves as an approximation for the <i>G<sub>t</sub> (n)/GI/s+GI</i> queueing model, having a state-dependent and time-dependent arrival process. The discrete-time framework is exploited to prove that properly scaled queueing processes in the queueing model converge to fluid functions as <i>s (rightarrow) (infinity)</i>. The discrete-time framework is also convenient for calculating the time-dependent fluid performance descriptions.", "e:keyword": ["Queues", "Approximations", "Multiserver queues with abandonment", "Queues", "Multichannel", "Approximation of non-Markovian multichannel queues with customer abandonment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0228", "e:abstract": "We present approximation algorithms for integrated logistics problems that combine elements of facility location and transport network design. We first study the problem where opening facilities incurs opening costs and transportation from the clients to the facilities incurs buy-at-bulk costs, and provide a combinatorial approximation algorithm. We also show that the integer-programming formulation of this problem has small integrality gap. We extend the model to the version when there is a bound on the number of facilities that may be opened.", "e:keyword": ["Facilities", "Location", "Discrete", "Networks/graphs", "Flow algorithms", "Transportation", "Models", "Networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0229", "e:abstract": "We analyze a periodic-review inventory model where the decision maker can buy from either of two suppliers. With the first supplier, the buyer incurs a high variable cost but negligible fixed cost; with the second supplier, the buyer incurs a lower variable cost but a substantial fixed cost. Consequently, ordering costs are piecewise linear and concave. We show that a reduced form of generalized (<i>s, S</i>) policy is optimal for both finite and (discounted) infinite-horizon problems, provided that the demand density is log-concave. This condition on the distribution is much less restrictive than in previous models. In particular, it applies to the normal, truncated normal, gamma, and beta distributions, which were previously excluded. We concentrate on the situation in which sales are lost, but explain how the policy, cost assumptions, and proofs can be altered for the case where excess demand is backordered. In the lost sales case, the optimal policy will have one of three possible forms: a base stock policy for purchasing exclusively at the high variable cost (HVC) supplier; an (<i>s</i><sub>LVC</sub>, <i>S</i><sub>LVC</sub>) policy for buying exclusively from the low variable cost (LVC) supplier; or a hybrid (<i>s</i>, <i>S</i><sub>HVC</sub>, <i>S</i><sub>LVC</sub>) policy for buying from both suppliers.", "e:keyword": ["Inventory/production", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0230", "e:abstract": "Traditional research on routing in queueing systems usually ignores service quality related factors. In this paper, we analyze the routing problem in a system where customers call back when their problems are not completely resolved by the customer service representatives (CSRs). We introduce the concept of call resolution probability, and we argue that it constitutes a good proxy for call quality. For each call, both the call resolution probability (<i>p</i>) and the average service time (1/μ) are CSR dependent. We use a Markov decision process formulation to obtain analytical results and insights about the optimal routing policy that minimizes the average total time of call resolution, including callbacks. In particular, we provide sufficient conditions under which it is optimal to route to the CSR with the highest call resolution rate (<i>p</i>μ) among those available. We also develop efficient heuristics that can be easily implemented in practice.", "e:keyword": ["Dynamic programming/optimal control", "Markov: infinite state", "Probability: stochastic model applications", "Queues: Markovian", "Multichannel"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0231", "e:abstract": "We describe JOINT DEFENDER, a new two-sided optimization model for planning the pre-positioning of defensive missile interceptors to counter an attack threat. In our basic model, a defender pre-positions ballistic missile defense platforms to minimize the worst-case damage an attacker can achieve; we assume that the attacker will be aware of defensive pre-positioning decisions, and that both sides have complete information as to target values, attacking-missile launch sites, weapon system capabilities, etc. Other model variants investigate the value of secrecy by restricting the attacker’s and/or defender’s access to information. For a realistic scenario, we can evaluate a completely transparent exchange in a few minutes on a laptop computer, and can plan near-optimal secret defenses in seconds. JOINT DEFENDER’s mathematical foundation and its computational efficiency complement current missile-defense planning tools that use heuristics or supercomputing. The model can also provide unique insight into the value of secrecy and deception to either side. We demonstrate with two hypothetical North Korean scenarios.", "e:keyword": ["Missile defense", "Optimization", "Bilevel integer linear program", "Mixed-integer linear program"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0234", "e:abstract": "Dantzig-Wolfe decomposition and column generation, devised for linear programs, is a success story in large-scale integer programming. We outline and relate the approaches, and survey mainly recent contributions, not yet found in textbooks. We emphasize the growing understanding of the dual point of view, which has brought considerable progress to the column generation theory and practice. It stimulated careful initializations, sophisticated solution techniques for the restricted master problem and subproblem, as well as better overall performance. Thus, the dual perspective is an ever recurring concept in our “selected topics.”", "e:keyword": ["Integer programming: column generation", "Dantzig-Wolfe decomposition", "Lagrangian relaxation", "Branch-and-bound", "Linear programming: large scale systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0237", "e:abstract": "We propose an optimization-via-simulation algorithm, called COMPASS, for use when the performance measure is estimated via a stochastic, discrete-event simulation, and the decision variables are integer ordered. We prove that COMPASS converges to the set of local optimal solutions with probability 1 for both terminating and steady-state simulation, and for both fully constrained problems and partially constrained or unconstrained problems under mild conditions.", "e:keyword": ["Simulation", "Design of experiments", "Optimization via simulation", "Programming", "Stochastic", "Adaptive random search"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0238", "e:abstract": "We propose a general methodology based on robust optimization to address the problem of optimally controlling a supply chain subject to stochastic demand in discrete time. This problem has been studied in the past using dynamic programming, which suffers from dimensionality problems and assumes full knowledge of the demand distribution. The proposed approach takes into account the uncertainty of the demand in the supply chain without assuming a specific distribution, while remaining highly tractable and providing insight into the corresponding optimal policy. It also allows adjustment of the level of robustness of the solution to trade off performance and protection against uncertainty. An attractive feature of the proposed approach is its numerical tractability, especially when compared to multidimensional dynamic programming problems in complex supply chains, as the robust problem is of the same difficulty as the nominal problem, that is, a linear programming problem when there are no fixed costs, and a mixed-integer programming problem when fixed costs are present. Furthermore, we show that the optimal policy obtained in the robust approach is identical to the optimal policy obtained in the nominal case for a modified and explicitly computable demand sequence. In this way, we show that the structure of the optimal robust policy is of the same base-stock character as the optimal stochastic policy for a wide range of inventory problems in single installations, series systems, and general supply chains. Preliminary computational results are very promising.", "e:keyword": ["Programming", "Linear", "Integer", "Stochastic", "Inventory", "Production", "Uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0239", "e:abstract": "This paper is a mechanism design study for a monopolist selling multiple identical items to potential buyers arriving over time. Participants in our model are time sensitive, with the same discount factor; potential buyers have unit demand and arrive sequentially according to a renewal process; and valuations are drawn independently from the same regular distribution. Invoking the revelation principle, we restrict our attention to direct dynamic mechanisms taking a sequence of valuations and arrival epochs as input. We define two properties (discreteness and stability), and prove under further distributional assumptions that we may at no cost of generality consider only mechanisms satisfying them. This effectively reduces the mechanism input to a sequence of valuations and leads to formulate the problem as a dynamic program (DP). As this DP is equivalent to a well-known infinite-horizon asset-selling problem, we finally characterize the optimal mechanism as a sequence of posted prices increasing with each sale. Remarkably, this result rationalizes somewhat the frequent restriction to dynamic pricing policies and impatient buyers assumption. Our numerical study indicates that, under various valuation distributions, the benefit of dynamic pricing over a fixed posted price may be small. Besides, posted prices are preferable to online auctions for a large number of items or high interest rate, but in other cases auctions are close to optimal and significantly more robust.", "e:keyword": ["Games/group decisions", "Bidding/auctions", "Mechanism design", "Dynamic programming/optimal control", "Applications", "Asset selling", "Optimal stopping", "Marketing", "Pricing", "Dynamic pricing", "Online auctions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0240", "e:abstract": "We consider the multiple depot vehicle scheduling problem (MDVSP) and propose a branch-and-bound algorithm for solving it that combines column generation, variable fixing, and cutting planes. We show that the solutions of the linear relaxation of the MDVSP contain many “odd cycles.” We derive a class of valid inequalities by extending the notion of odd cycle and describe a lifting procedure for these inequalities. We prove that the lifted inequalities represent, under certain conditions, facets of the underlying polytope. Finally, we present the results of a computational study comparing several strategies (variable fixing, cutting planes, mixed branching, and tree search) for solving the MDVSP.", "e:keyword": ["Transportation", "Scheduling", "Vehicles", "Multiple depot vehicle scheduling problem", "Networks/graphs", "Multicommodity", "Multicommodity formulation", "Programming", "Integer", "Algorithms", "Cutting plane/facet", "Branch-and-cut algorithm for the MDVSP"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0241", "e:abstract": "We examine the problem of determining the annual staffing level that minimizes total expected costs for a fire department, subject to minimum service-level-based staffing requirements. We develop a quantitative model that allows for stochastic temporary absences, permanent wastage, and limited hiring opportunities, and takes into account the unique firefighter work schedule. Our model is reminiscent of traditional newsvendor-type inventory models, but where the uncertainty is dependent on the decision variable. We derive easily implementable optimal staffing policies for both the continuous and discrete cases. We provide numerical results comparing our optimal solution to several heuristic staffing policies commonly used in practice, which show that our optimal policy can provide significant savings. We perform sensitivity analysis to generate managerial insights. We discuss an application of our model and analysis to the Cincinnati Fire Department.", "e:keyword": ["Workforce staffing", "Firefighters", "Emergency services", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0243", "e:abstract": "Researchers and practitioners frequently spend more time fine-tuning algorithms than designing and implementing them. This is particularly true when developing heuristics and metaheuristics, where the “right” choice of values for search parameters has a considerable effect on the performance of the procedure. When testing metaheuristics, performance typically is measured considering both the quality of the solutions obtained and the time needed to find them. In this paper, we describe the development of CALIBRA, a procedure that attempts to find the best values for up to five search parameters associated with a procedure under study. Because CALIBRA uses Taguchi’s fractional factorial experimental designs coupled with a local search procedure, the best values found are not guaranteed to be optimal. We test CALIBRA on six existing heuristic-based procedures. These experiments show that CALIBRA is able to find parameter values that either match or improve the performance of the procedures resulting from using the parameter values suggested by their developers. The latest version of CALIBRA can be downloaded for free from the website that appears in the online supplement of this paper at http://or.pubs.informs.org/Pages.collect.html.", "e:keyword": ["Parameter setting", "Taguchi design of experiments", "Heuristic search"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0246", "e:abstract": "All too often, the hard work that members of the scholarly community do to support our profession goes unrecognized. For the sixth year, the Editorial Board of <i>Operations Research</i> has decided to reward outstanding service to the journal’s scholarly mission. Associate editors and referees who did an exceptionally professional job by submitting timely, unbiased, and thoughtful reviews were considered for this award. This award is just a small token of the appreciation for exceptional work performed on behalf of <i>Operations Research</i> and our profession. We regret that the following names were inadvertently omitted from the list published in the July–August issue, and we thank these individuals for their effort to make <i>Operations Research</i> the premier journal of the field.The Editorial Board would like to thank the following individual who acted as a referee for papers considered or published during the 2004 calendar year. Without the assistance of our valued referees, it would be impossible for INFORMS to publish a journal of high professional standards. We regret that the following name was inadvertently omitted from the list published in the July-August issue, and we thank him for his effort to make <i>Operations Research</i> the premier journal of the field.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0247", "e:abstract": "The stochastic differential equations for affine jump diffusion models do not yield exact solutions that can be directly simulated. Discretization methods can be used for simulating security prices under these models. However, discretization introduces bias into the simulation results, and a large number of time steps may be needed to reduce the discretization bias to an acceptable level. This paper suggests a method for the exact simulation of the stock price and variance under Heston’s stochastic volatility model and other affine jump diffusion processes. The sample stock price and variance from the exact distribution can then be used to generate an unbiased estimator of the price of a derivative security. We compare our method with the more conventional Euler discretization method and demonstrate the faster convergence rate of the error in our method. Specifically, our method achieves an <i>O</i>(<i>s</i><sup>-1/2</sup>) convergence rate, where <i>s</i> is the total computational budget. The convergence rate for the Euler discretization method is <i>O</i>(<i>s</i><sup>-1/3</sup>) or slower, depending on the model coefficients and option payoff function.", "e:keyword": ["Simulation", "Efficiency", "Exact methods", "Finance", "Asset pricing", "Computational methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0249", "e:keyword": ["Professional: OR/MS history", "Editorial comments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0250", "e:abstract": "This article recalls some of George B. Dantzig’s many contributions to operations research and the management sciences in his ninety-year lifetime.", "e:keyword": ["Professional: OR/MS history", "Obituaries"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0251", "e:abstract": "The authors comment on Ding et al. (2002), and Ding, Bisi, and Puterman respond.", "e:keyword": ["Inventory/production: unknown demand", "Lost sales", "Optimal policies", "Dynamic programming: Bayesian Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0252", "e:abstract": "Developed by General Motors (GM), the Auto Choice Advisor website (<ext-link ext-link-type=\"uri\"  href=\" http://www.autochoiceadvisor.com\"> http://www.autochoiceadvisor.com</ext-link>) recommends vehicles to consumers based on their requirements and budget constraints. Through the website, GM has access to large quantities of data that reflect consumer preferences. Motivated by the availability of such data, we formulate a nonparametric approach to multiproduct pricing.We consider a class of models of consumer purchasing behavior, each of which relates observed data on a consumer’s requirements and budget constraint to subsequent purchasing tendencies. To price products, we aim at optimizing prices with respect to a sample of consumer data. We offer a bound on the sample size required for the resulting prices to be near-optimal with respect to the true distribution of consumers. The bound exhibits a dependence of <i>O(n log n)</i> on the number <i>n</i> of products being priced, showing that---in terms of sample complexity---the approach is scalable to large numbers of products.With regards to computational complexity, we establish that computing optimal prices with respect to a sample of consumer data is NP-complete in the strong sense. However, when prices are constrained by a price ladder---an ordering of prices defined prior to price determination---the problem becomes one of maximizing a supermodular function with real-valued variables. It is not yet known whether this problem is NP-hard. We provide a heuristic for our price-ladder-constrained problem, together with encouraging computational results.Finally, we apply our approach to a data set from the Auto Choice Advisor website. Our analysis provides insights into the current pricing policy at GM and suggests enhancements that may lead to a more effective pricing strategy.", "e:keyword": ["Pricing", "Multiproduct pricing", "Marketing", "Buyer behavior", "Choice models", "Estimation/statistical techniques", "Industries", "Automotive"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0253", "e:abstract": "In several disciplines as diverse as shape analysis, location theory, quality control, archaeology, and psychometrics, it can be of interest to fit a circle through a set of points. We use the result that it suffices to locate a center for which the variance of the distances from the center to a set of given points is minimal. In this paper, we propose a new algorithm based on iterative majorization to locate the center. This algorithm is guaranteed to yield a series of nonincreasing variances until a stationary point is obtained. In all practical cases, the stationary point turns out to be a local minimum. Numerical experiments show that the majorizing algorithm is stable and fast. In addition, we extend the method to fit other shapes, such as a square, an ellipse, a rectangle, and a rhombus by making use of the class of <i>l<sub>p</sub></i> distances and dimension weighting. In addition, we allow for rotations for shapes that might be rotated in the plane. We illustrate how this extended algorithm can be used as a tool for shape recognition.", "e:keyword": ["Mathematics: functions: majorizing functions", "Facilities: location: continuous", "Engineering: shape analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0254", "e:abstract": "This paper concerns a method for digital circuit optimization based on formulating the problem as a geometric program (GP) or generalized geometric program (GGP), which can be transformed to a convex optimization problem and then very efficiently solved. We start with a basic gate scaling problem, with delay modeled as a simple resistor-capacitor (RC) time constant, and then add various layers of complexity and modeling accuracy, such as accounting for differing signal fall and rise times, and the effects of signal transition times. We then consider more complex formulations such as robust design over corners, multimode design, statistical design, and problems in which threshold and power supply voltage are also variables to be chosen. Finally, we look at the detailed design of gates and interconnect wires, again using a formulation that is compatible with GP or GGP.", "e:keyword": ["Programming: geometric", "Engineering: computer-aided design", "Digital circuit optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0255", "e:abstract": "Past requirements-planning research has typically assumed that the firm’s demands are determined prior to production planning. In contrast, we explore a single-stage planning model that implicitly decides, through pricing decisions, the demand levels the firm should satisfy in order to maximize contribution to profit. We briefly discuss solution methods and properties for these problems when production capacities are unlimited. The key result of this work is a polynomial-time solution approach to the problem under time-invariant finite production capacities and piecewise-linear and concave revenue functions in price.", "e:keyword": ["Production/scheduling", "Planning", "Inventory/production", "Policies", "Marketing/pricing", "Programming", "Integer", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0256", "e:abstract": "We consider a system in which a single finished good is assembled from two components. Demand for the finished product is stochastic and stationary, and procurement and assembly lead times are constant. Unsatisfied demand is backordered. The inventory of each component or assembly is controlled by a separate firm using a base-stock policy. Each firm is charged holding costs on its own inventory, plus a share of the shortage cost due to backorders of the finished product.We investigate the equilibrium base-stock levels that arise in this system under both echelon and local base-stock policies. In both cases, the component firms’ base-stock levels are economic complements. We then examine the effect on system performance when one firm uses information about other firms’ pipeline inventory. We find that, under echelon base-stock policies, all firms benefit with the use of pipeline information. In contrast, under local policies, using pipeline information may actually increase costs for some firms (including the firm that makes direct use of the information). Also, we compare the behavior of the decentralized system with that of the assembly system under centralized control. Finally, we describe a payment scheme between the final assembler and the suppliers that allows the decentralized system to achieve the centralized solution.", "e:keyword": ["Games/group decisions", "Noncooperative", "Inventory/production", "Multi-item/echelon/stage", "Policies", "Review/lead times", "Uncertainty", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0257", "e:abstract": "This paper studies the <i>M/M/s+M</i> queue, i.e., the <i>M/M/s</i> queue with customer abandonment, also called the Erlang-A model, having independent and identically distributed customer abandon times with an exponential distribution (the +<i>M</i>), focusing on the case in which the arrival rate and the number of servers are large. The goal is to better understand the sensitivity of performance to changes in the model parameters: the arrival rate, the service rate, the number of servers, and the abandonment rate. Elasticities are used to show the percentage change of a performance measure caused by a small percentage change in a parameter. Elasticities are calculated using an exact numerical algorithm and simple finite-difference approximations. Insight is gained by applying fluid and diffusion approximations. The analysis shows that performance is quite sensitive to small percentage changes in the arrival rate or the service rate, but relatively insensitive to small percentage changes in the abandonment rate.", "e:keyword": ["Queues", "Multichannel", "Sensitivity analysis", "Queues", "Balking and reneging", "Sensitivity to changes in model parameters"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0258", "e:abstract": "A batch production process that is initially in the in-control state can fail with constant failure rate to the out-of-control state. The probability that a unit is conforming if produced while the process is in control is constant and higher than the respective constant conformance probability while the process is out of control. When production ends, the units are inspected in the order they have been produced. The objective is to design a production and inspection policy that guarantees a zero defective delivery in minimum expected total cost.The inspection problem is formulated as a partially observable Markov decision process (POMDP): Given the observations about the quality of the items that have already been inspected, the inspector should determine whether to inspect the next unit or stop inspection and possibly pay shortage costs. We show that the optimal policy is of the control limit threshold (CLT) type: The observations are used to update the probability that the production process was still in control while producing the candidate unit for inspection. The optimal policy is to continue inspection if and only if this probability exceeds a CLT value that may depend on the outstanding demand and the number of uninspected items. Structural properties satisfied by the various CLT values are presented.", "e:keyword": ["Dynamic programming", "Optimal control", "Markov", "Applications", "Probability", "Stochastic model application"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0259", "e:abstract": "In this paper we describe an efficient algorithm for solving novel optimization models arising in the context of multiperiod capacity expansion of optical networks. We assume that the network operator must make investment decisions over a multiperiod planning horizon while facing rapid changes in transmission technology, as evidenced by a steadily decreasing per-unit cost of capacity. We deviate from traditional and monopolistic models in which demands are given as input parameters, and the objective is to minimize capacity deployment costs. Instead, we assume that the carrier sets end-to-end prices of bandwidth at each period of the planning horizon. These prices determine the demands that are to be met, using a plausible and explicit price-demand relationship; the resulting demands must then be routed, requiring an investment in capacity. The objective of the optimization is now to simultaneously select end-to-end prices of bandwidth and network capacities at each period of the planning horizon, so as to maximize the overall net present value of expanding and operating the network. In the case of typical large-scale optical networks with protection requirements, the resulting optimization problems pose significant challenges to standard optimization techniques. The complexity of the model, its nonlinear nature, and the large size of realistic problem instances motivates the development of efficient and scalable solution techniques. We show that while general-purpose nonlinear solvers are typically not adequate for the task, a specialized decomposition scheme is able to handle large-scale instances of this problem in reasonable time, producing solutions whose net present value is within a small tolerance of the optimum.", "e:keyword": ["Communications", "Facility/equipment planning", "Capacity expansion", "Design", "Maintenance/replacement", "Programming", "Nonlinear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0260", "e:abstract": "We study the problem of tracking a financial benchmark---a continuously compounded growth rate or a stock market index---by dynamically managing a portfolio consisting of a small number of traded stocks in the market. In either case, we formulate the tracking problem as an instance of the stochastic linear quadratic control (SLQ), involving indefinite cost matrices. As the SLQ formulation involves a discounted objective over an infinite horizon, we first address the issue of stabilizability. We then use semidefinite programming (SDP) as a computational tool to generate the optimal feedback control. We present numerical examples involving stocks traded at the Hong Kong and New York Stock Exchanges to illustrate the various features of the model and its performance.", "e:keyword": ["Steady growth-rate tracking", "Stock-index tracking", "Stochastic linear quadratic control", "Semidefinite programming", "Stabilizability"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0261", "e:abstract": "We consider the problem of radiation therapy treatment planning for cancer patients. During radiation therapy, beams of radiation pass through a patient, killing both cancerous and normal cells. Thus, the radiation therapy must be carefully planned so that a clinically prescribed dose is delivered to targets containing cancerous cells, while nearby organs and tissues are spared. Currently, a technique called <i>intensity-modulated radiation therapy (IMRT)</i> is considered to be the most effective radiation therapy for many forms of cancer. In IMRT, the patient is irradiated from several beams, each of which is decomposed into hundreds of small beamlets, the intensities of which can be controlled individually. In this paper, we consider the problem of designing a treatment plan for IMRT when the orientations of the beams are given. We propose a new model that has the potential to achieve most of the goals with respect to the quality of a treatment plan that have been considered to date. However, in contrast with established mixed-integer and nonlinear programming formulations, we do so while retaining linearity of the optimization problem, which substantially improves the tractability of the optimization problem. Furthermore, we discuss how several additional quality and practical aspects of the problem that have been ignored to date can be incorporated into our linear model. We demonstrate the effectiveness of our approach on clinical data.", "e:keyword": ["Linear programming", "Applications", "Health care", "Treatment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0262", "e:abstract": "In this paper, we propose to solve the linear multicommodity flow problem using a partial Lagrangian relaxation. The relaxation is restricted to the set of arcs that are likely to be saturated at the optimum. This set is itself approximated by an active set strategy. The partial Lagrangian dual is solved with Proximal-ACCPM, a variant of the analytic center cutting-plane method. The new approach makes it possible to solve huge problems when few arcs are saturated at the optimum, as appears to be the case in many practical problems.", "e:keyword": ["Network", "Linear multicommodity flow", "Analytic center cutting-plane method", "Active set strategy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0263", "e:abstract": "Motivated by real applications, we consider the problem of shipping products to multiple customers from limited inventory. After formulating the optimization problems under different restrictions on partial shipments, we find that commercially available packages, applied directly, are unsatisfactory, as are simple greedy approaches. We develop a scheme of heuristics that enables the user to select a good balance between computation time and effectiveness. A detailed computational study of one- and two-period industrial-sized problems indicates that these heuristics are computationally practical and generate solutions that are, on average, within 3%--4% of the optimum.", "e:keyword": ["Partial shipments", "Integer programming models", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0264", "e:abstract": "We consider a power portfolio optimization model that is intended as a decision aid for scheduling and hedging (DASH) in the wholesale power market. Our multiscale model integrates the unit commitment model with financial decision making by including the forwards and spot market activity within the scheduling decision model. The methodology is based on a multiscale stochastic programming model that selects portfolio positions that perform well on a variety of scenarios generated through statistical modeling and optimization. When compared with several commonly used fixed-mix policies, our experiments demonstrate that the DASH model provides significant advantages.", "e:keyword": ["Programming", "Stochastic", "Industries", "Electric", "Finance", "Portfolio"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0249", "e:keyword": ["OR/MS history", "Editorial comments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0271", "e:abstract": "We present a periodic review inventory model with multiple delivery modes. While base-stock policies are optimal for one or two consecutive delivery modes, they are not so otherwise. For multiple consecutive delivery modes, we show that only the fastest two modes have optimal base stocks, and provide a simple counterexample to show that the remaining ones do not. We investigate why the base-stock policy is or is not optimal in different situations. This note is an abridged version of Feng et al. (2006).", "e:keyword": ["Inventory/production: uncertainty", "Multiple delivery modes", "Base-stock policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0242", "e:abstract": "Hospital diagnostic facilities, such as magnetic resonance imaging centers, typically provide service to several diverse patient groups: outpatients, who are scheduled in advance; inpatients, whose demands are generated randomly during the day; and emergency patients, who must be served as soon as possible. Our analysis focuses on two interrelated tasks: designing the outpatient appointment schedule, and establishing dynamic priority rules for admitting patients into service.We formulate the problem of managing patient demand for diagnostic service as a finite-horizon dynamic program and identify properties of the optimal policies. Using empirical data from a major urban hospital, we conduct numerical studies to develop insights into the sensitivity of the optimal policies to the various cost and probability parameters and to evaluate the performance of several heuristic rules for appointment acceptance and patient scheduling.", "e:keyword": ["Health care", "Diagnosis", "Hospitals", "Dynamic programming/optimal control", "Applications", "Models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0248", "e:abstract": "In a multiple-customer-class model of demand fulfillment for a single item, we consider the use of dynamic price discounts to encourage backlogging of demand for customer classes denied immediate service. Customers are assumed to arrive over several stages in a period, and customer classes are distinguished by their contractual price and sensitivity to discounts. Through dynamic programming we determine the optimal discounts to offer, assuming a linear model for the sensitivity of customers to such inducements. We show that customers are served in class order, and allocation of inventory to demand is determined by considering the current number of customers backlogged, as well as the current inventory position. Through comparison to a naive supplier allocating inventory first come/first served with no discounting, we show that profits are primarily influenced by the allocation of capacity, and the use of price discounts primarily benefits the second-class customers’ overall fill rate. Heuristics for implementation of the solution in real-time settings are given.", "e:keyword": ["Inventory/production", "Dynamic pricing", "Multiple class", "Dynamic programming", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0270", "e:abstract": "We consider the stochastic single-machine problem, when the objective is to minimize the <i>expected</i> total weighted completion time of a set of jobs that are released over time. We assume that the existence and the parameters of each job including its release date, weight, and expected processing times are not known until its release date. The actual processing times are not known until processing is completed. We analyze the performance of the on-line nonpreemptive weighted shortest expected processing time among available jobs (WSEPTA) heuristic. When a scheduling decision needs to be made, this heuristic assigns, among the jobs that have arrived but not yet processed, one with the largest ratio of its weight to its expected processing time. We prove that when the job weights and processing times are bounded and job processing times are mutually independent random variables, WSEPTA is asymptotically optimal for the single-machine problem. This implies that WSEPTA generates a solution whose relative error approaches zero as the number of jobs increases. This result can be extended to the stochastic flow shop and open shop problems, as well as models with stochastic job weights.", "e:keyword": ["Production/scheduling", "On-line single-machine and flow shop stochastic sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0272", "e:abstract": "Spatial considerations are important in conservation reserve design. A particularly important spatial requirement is the connectivity of selected sites. Direct connections between reserve sites increase the likelihood of species persistence by allowing dispersal and colonization of other areas within the network without species having to leave the reserve. The conventional set-covering and maximal-covering formulations of the reserve selection problem assume that species representation is the only criterion in site selection. This approach usually results in a small but highly fragmented reserve, which may not be desirable. We present a linear integer programming framework incorporating spatial contiguity as an additional site selection criterion. An empirical application to a data set on the occurrence of breeding birds in Berkshire, United Kingdom, demonstrates that site connectivity requires a significantly larger reserve. Incorporation of spatial criteria increases the computational complexity of the problem. To overcome this, we use a two-stage procedure where the original sites are aggregated first and an optimum solution is determined for the aggregate sites. Then, site selection is restricted to original sites included in the aggregate solution and a connected reserve is determined. In this particular application the above procedure generated a significantly more efficient reserve than a heuristic selection.", "e:keyword": ["Integer programming", "Applications", "Facility layout and location", "Reserve site selection with spatial considerations", "Networks/graphs", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0273", "e:abstract": "We study the joint inventory replenishment and pricing problem for production systems with random demand and yield. More specifically, we analyze the following single-item, periodic-review model. Demands in consecutive periods are independent random variables and their distributions are price sensitive. The production yield is uncertain so that the quantity received from a replenishment is a random variable whose distribution depends on the production quantity. Stockouts are fully backlogged. Our problem is to characterize the optimal dynamic policy that simultaneously determines the production quantity and the price for each period to maximize the total discounted profit. We show that the optimal replenishment policy is of a threshold type, i.e., it is optimal to produce if and only if the starting inventory in a period is below a threshold value, and that both the optimal production quantity and the optimal price in each period are decreasing in the starting inventory. We further study the operational effects of uncertain yield. We prove that, in the single-period case, the threshold of replenishment is independent of the yield variability, and, in the multiperiod case, it is higher in a system with uncertain yield than in one with certain yield. In addition, the system with uncertain yield always charges a higher price.", "e:keyword": ["Inventory control", "Uncertain yield", "Random demand", "Pricing", "Markov decision program"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0274", "e:abstract": "This paper discusses decomposition of a multiregional economic equilibrium model that is characterized by a cost minimizing, linear programming (LP) model of the supply side and a vector-valued function that gives demand prices as functions of the quantities demanded. Our motivation is to ease model development and maintenance by a solution method that links separately developed regional models only when a consistent multiregion solution is desired. A heuristic strategy is described to extend any existing (LP) decomposition principle to a procedure for decomposing an equilibrium model by region. This strategy is applied to extend Dantzig-Wolfe decomposition to the multiregional economic equilibrium model, and several theoretical results are derived for the resulting algorithm. The central result is a proof of asymptotic convergence, under usefully general conditions. The extended Dantzig-Wolfe procedure is illustrated with an existing, two-region model of Canadian energy supplies and demands.", "e:keyword": ["Natural resources", "Energy: multiregional economic equilibrium models", "Programming: decomposition", "Variational inequalities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0277", "e:abstract": "We give a branch-and-cut algorithm for solving linear programs (LPs) with continuous separable piecewise-linear cost functions (PLFs). Models for PLFs use continuous variables in special-ordered sets of type 2 (SOS2). Traditionally, SOS2 constraints are enforced by introducing auxiliary binary variables and other linear constraints on them. Alternatively, we can enforce SOS2 constraints by branching on them, thus dispensing with auxiliary binary variables. We explore this approach further by studying the inequality description of the convex hull of the feasible set of LPs with PLFs in the space of the continuous variables, and using the new cuts in a branch-and-cut scheme without auxiliary binary variables. We give two families of valid inequalities. The first family is obtained by lifting the convexity constraints. The second family consists of lifted cover inequalities. Finally, we report computational results that demonstrate the effectiveness of our cuts, and that branch-and-cut without auxiliary binary variables is significantly more practical than the traditional mixed-integer programming approach.", "e:keyword": ["Piecewise linear function", "Special-ordered set", "Branch-and-cut"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0278", "e:abstract": "Column generation is one of the most successful approaches for solving large-scale linear programming problems. However, degeneracy difficulties and long-tail effects are known to occur as problems become larger. In recent years, several stabilization techniques of the dual variables have proven to be effective. We study the use of two types of <i>dual-optimal inequalities</i> to accelerate and stabilize the whole convergence process. Added to the dual formulation, these constraints are satisfied by all or a subset of the dual-optimal solutions. Therefore, the optimal objective function value of the augmented dual problem is identical to the original one. Adding constraints to the dual problem leads to adding columns to the primal problem, and feasibility of the solution may be lost. We propose two methods for recovering primal feasibility and optimality, depending on the type of inequalities that are used. Our computational experiments on the binary and the classical cutting-stock problems, and more specifically on the so-called triplet instances, show that the use of relevant dual information has a tremendous effect on the reduction of the number of column generation iterations.", "e:keyword": ["Production/scheduling", "Cutting-stock problems", "Programming", "Linear large-scale systems", "Column generation", "Stabilization", "Dual cuts"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0279", "e:abstract": "The performance of a given portfolio policy can in principle be evaluated by comparing its expected utility with that of the optimal policy. Unfortunately, the optimal policy is usually not computable, in which case a direct comparison is impossible. In this paper, we solve this problem by using the given portfolio policy to construct an upper bound on the unknown maximum expected utility. This construction is based on a dual formulation of the portfolio optimization problem. When the upper bound is close to the expected utility achieved by the given portfolio policy, the potential utility loss of this policy is guaranteed to be small. Our algorithm can be used to evaluate portfolio policies in models with incomplete markets and position constraints. We illustrate our methodology by analyzing the static and myopic policies in markets with return predictability and constraints on short sales and borrowing.", "e:keyword": ["Finance", "Portfolio", "Optimal control", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0280", "e:abstract": "We consider the supply chain of a manufacturer who produces time-sensitive products that have a large variety, a short life cycle, and are sold in a very short selling season. The supply chain consists of multiple overseas plants and a domestic distribution center (DC). Retail orders are first processed at the plants and then shipped from the plants to the DC for distribution to domestic retailers. Due to variations in productivity and labor costs at different plants, the processing time and cost of an order are dependent on the plant to which it is assigned. We study the following static and deterministic order assignment and scheduling problem faced by the manufacturer before every selling season: Given a set of orders, determine which orders are to be assigned to each plant, find a schedule for processing the assigned orders at each plant, and find a schedule for shipping the completed orders from each plant to the DC, such that a certain performance measure is optimized. We consider four different performance measures, all of which take into account both delivery lead time and the total production and distribution cost. A problem corresponding to each performance measure is studied separately. We analyze the computational complexity of various cases of the problems by either proving that a problem is intractable or providing an efficient exact algorithm for the problem. We propose several fast heuristics for the intractable problems. We analyze the worst-case and asymptotic performance of the heuristics and also computationally evaluate their performance using randomly generated test instances. Our results show that the heuristics are capable of generating near-optimal solutions quickly.", "e:keyword": ["Production/scheduling", "Approximation/heuristic", "Deterministic sequencing", "Manufacturing", "Performance/productivity", "Transportation", "Scheduling", "Programming", "Integer", "Algorithms", "Heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0281", "e:abstract": "We present fully sequential procedures for steady-state simulation that are designed to select the best of a finite number of simulated systems when “best” is defined by the largest or smallest long-run average performance. We also provide a framework for establishing the asymptotic validity of such procedures and prove the validity of our procedures. An example based on the <i>M/M</i>/1 queue is given.", "e:keyword": ["Simulation", "Ranking and selection", "Steady-state simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0282", "e:abstract": "Distributions with an increasing generalized failure rate (IGFR) have useful applications in pricing and supply chain contracting problems. We provide alternative characterizations of the IGFR property that lead to simplify verifying whether the IGFR condition holds. We also relate the limit of the generalized failure rate and the moments of a distribution.", "e:keyword": ["Probability distributions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0283", "e:abstract": "In the dial-a-ride problem, users formulate requests for transportation from a specific origin to a specific destination. Transportation is carried out by vehicles providing a shared service. The problem consists of designing a set of minimum-cost vehicle routes satisfying capacity, duration, time window, pairing, precedence, and ride-time constraints. This paper introduces a mixed-integer programming formulation of the problem and a branch-and-cut algorithm. The algorithm uses new valid inequalities for the dial-a-ride problem as well as known valid inequalities for the traveling salesman, the vehicle routing, and the pick-up and delivery problems. Computational experiments performed on randomly generated instances show that the proposed approach can be used to solve small to medium-size instances.", "e:keyword": ["Transportation", "Vehicle routing", "Programming", "Cutting plane"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0284", "e:abstract": "We consider a multiproduct supply-demand network equilibrium model on the basis of Wardrop’s equilibrium principle. We prove that such a network equilibrium model with both a single criterion and multiple criteria are each equivalent to a vector variational inequality. For the case with multiple criteria, we derive the necessary and sufficient conditions for network equilibrium in terms of a vector variational inequality by Gerstewitz’s function when the cost function is vector valued. This result is derived based on conditions that are weaker than those for many existing results. We follow with an example to illustrate the application of the theoretical results.", "e:keyword": ["Network equilibrium", "Variational inequality", "Wardrop’s equilibrium principle", "Supply-demand network"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0285", "e:abstract": "This paper analyzes a call center model with <i>m</i> customer classes and <i>r</i> agent pools. The model is one with doubly stochastic arrivals, which means that the <i>m</i>-vector (lambda) of instantaneous arrival rates is allowed to vary both temporally and stochastically. Two levels of call center management are considered: staffing the <i>r</i> pools of agents, and dynamically routing calls to agents. The system manager’s objective is to minimize the sum of personnel costs and abandonment penalties. We consider a limiting parameter regime that is natural for call centers and relatively easy to analyze, but apparently novel in the literature of applied probability. For that parameter regime, we prove an asymptotic lower bound on expected total cost, which uses a strikingly simple distillation of the original system data. We then propose a method for staffing and routing based on linear programming (LP), and show that it achieves the asymptotic lower bound on expected total cost; in that sense the proposed method is <i>asymptotically optimal</i>.", "e:keyword": ["Stochastic model applications", "Call centers", "Queueing", "Dynamic routing", "Fluid limits", "Doubly stochastic", "Asymptotic analysis", "Discrete review", "Performance bounds", "Abandonments", "Staffing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0286", "e:abstract": "Mixed-integer programs (MIPs) involving logical implications modeled through big-M coefficients are notoriously among the hardest to solve. In this paper, we propose and analyze computationally an automatic problem reformulation of quite general applicability, aimed at removing the model dependency on the big-M coefficients. Our solution scheme defines a <i>master</i> integer linear problem (ILP) with no continuous variables, which contains combinatorial information on the feasible integer variable combinations that can be “distilled” from the original MIP model. The master solutions are sent to a <i>slave</i> linear program (LP), which validates them and possibly returns combinatorial inequalities to be added to the current master ILP. The inequalities are associated to minimal (or irreducible) infeasible subsystems of a certain linear system, and can be separated efficiently in case the master solution is integer. The overall solution mechanism closely resembles the Benders' one, but the cuts we produce are purely combinatorial and do not depend on the big-M values used in the MIP formulation. This produces an LP relaxation of the master problem which can be considerably tighter than the one associated with original MIP formulation. Computational results on two specific classes of hard-to-solve MIPs indicate that the new method produces a reformulation which can be solved some orders of magnitude faster than the original MIP model.", "e:keyword": ["Mixed-integer programs", "Benders' decomposition", "Branch and cut", "Computational analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0287", "e:abstract": "The Bass Model (BM) is a widely-used framework in marketing for the study of new-product sales growth. Its usefulness as a demand model has also been recognized in production, inventory, and capacity-planning settings. The BM postulates that the cumulative number of adopters of a new product in a large population approximately follows a deterministic trajectory whose growth rate is governed by two parameters that capture (i) an individual consumer's intrinsic interest in the product, and (ii) a positive force of influence on other consumers from existing adopters. A finite-population pure-birth-process (re)formulation of the BM, called the Stochastic Bass Model (SBM), was proposed recently by the author in a previous paper, and it was shown that if the size of the population in the SBM is taken to infinity, then the SBM and the BM agree (in probability) in the limit. Thus, the SBM “expands” the BM in the sense that for any given population size, it is a well-defined model. In this paper, we exploit this expansion and introduce a further extension of the SBM in which demands of a product in successive time periods are governed by a history-dependent family of SBMs (one for each period) with different population sizes. A sampling theory for this extension, which we call the Piecewise-Diffusion Model (PDM), is also developed. We then apply the theory to a typical product example, demonstrating that the PDM is a remarkably accurate and versatile framework that allows us to better understand the underlying dynamics of new-product demands over time. Joint movement of price and advertising levels, in particular, is shown to have a significant influence on whether or not consumers are “ready” to participate in product purchase.", "e:keyword": ["Marketing: new products", "Buyer behavior", "Pricing", "Probability: diffusion", "Inventory/production: stochastic", "Nonstationary demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0288", "e:abstract": "Transit and peering arrangements among Internet backbone providers (IBPs) are essential for the global delivery of communication services on the Internet. In addition, to support delay-sensitive applications (e.g., streaming and multimedia applications) it is important for IBPs to maintain high service quality even if the network is congested. One promising approach is to establish interconnection agreements among providers to dynamically trade network capacity. To make such interconnections possible in a competitive setting, we propose a pricing scheme that considers factors such as network utilization, link capacity, and the cost structure of the interconnecting participants. Our analyses show that the common sender keeps all (SKA) mode of settlement does not provide adequate incentives for collaboration; rather, the provider that delivers the packets should be suitably compensated at an equilibrium price. Two price equilibria are identified: The first favors slower IBPs, whereas the other is congestion based and can be more beneficial for faster IBPs. When cost asymmetries exist, the lower cost IBP needs to offer a price discount to induce participation. We show that a usage-based, utilization-adjusted interconnection agreement could align the costs and revenues of the providers while allowing them to meet more stringent quality of service requirements.", "e:keyword": ["Internet backbone", "Network connectivity", "Queueing", "Quality of service", "Game theory", "Equilibrium strategies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0289", "e:abstract": "Motivated by recent electronic marketplaces, we consider a single-product make-to-stock manufacturing system that uses two alternative selling channels: long-term contracts and a spot market of electronic orders. At time 0, the risk-averse manufacturer selects the long-term contract price, at which point buyers choose one of the two channels. The resulting long-term contract demand is a deterministic fluid, while the spot-market demand is modeled as a stochastic renewal process. An exponential reflected random walk model is used to model the spot-market price, which is correlated with the spot-market demand process. The manufacturer accepts or rejects each electronic order, and long-term contracts and accepted electronic orders are backordered if necessary. The manufacturer’s control problem is to select the optimal long-term contract price as well as the optimal production (i.e., busy/idle) and electronic-order admission policies to maximize revenue minus inventory holding and backorder costs. Under heavy-traffic conditions, the problem is approximated by a diffusion-control problem, and analytical approximations are used to derive a policy that is simple, and reasonably accurate and robust.", "e:keyword": ["Inventory/production", "Stochastic", "Approximations/heuristics", "Queues", "Diffusion models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0290", "e:abstract": "We analyze a multiechelon inventory system with inventory stages arranged in series. In addition to traditional forward material flows, used products are returned to a recovery facility, where they can be stored, disposed, or remanufactured and shipped to one of the stages to re-enter the forward flow of material. This system combines the key elements of two simpler systems: the series system studied by Clark and Scarf (1960) and the single-stage remanufacturing systems studied by Simpson (1978) and Inderfurth (1997). We focus on identifying the structure of the optimal remanufacturing/ordering/disposal policy for such a system. In particular, we investigate whether the optimal policy inherits the basic structural properties of the simpler systems. We show that if remanufactured items flow into the most upstream stage, then this is the case. Specifically, the system can be solved by decomposition into a sequence of single-stage systems, with each downstream stage following an echelon base-stock policy and the most upstream stage following a three-parameter policy with a simple (and intuitive) structure. We show that similar results hold when remanufactured products flow into a downstream stage; however, in this case some modifications must be made. In particular, the definition of echelon inventory must be adjusted for stages upstream of the remanufacturing stage, and disposal of used items can no longer be allowed. We also compare the information required for managing this system to that required in the Clark and Scarf or Inderfurth settings, and we point out how the requirements are somewhat different depending on whether remanufacturing occurs upstream or downstream.", "e:keyword": ["Inventory/production", "Multiechelon", "Environment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0291", "e:abstract": "For a discrete-time finite-state Markov chain, we develop an adaptive importance sampling scheme to estimate the expected total cost before hitting a set of terminal states. This scheme updates the change of measure at every transition using constant or decreasing step-size stochastic approximation. The updates are shown to concentrate asymptotically in a neighborhood of the desired zero-variance estimator. Through simulation experiments on simple Markovian queues, we observe that the proposed technique performs very well in estimating performance measures related to rare events associated with queue lengths exceeding prescribed thresholds. We include performance comparisons of the proposed algorithm with existing adaptive importance sampling algorithms on some examples. We also discuss the extension of the technique to estimate the infinite horizon expected discounted cost and the expected average cost.", "e:keyword": ["Probability", "Markov processes", "Queues", "Markovian", "Simulation", "Efficiency"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0292", "e:abstract": "The well-known and established global optimality conditions based on the Lagrangian formulation of an optimization problem are consistent if and only if the duality gap is zero. We develop a set of global optimality conditions that are structurally similar but are consistent for any size of the duality gap. This system characterizes a primal--dual optimal solution by means of primal and dual feasibility, primal Lagrangian (epsilon)-optimality, and, in the presence of inequality constraints, a relaxed complementarity condition analogously called (delta)-complementarity. The total size (epsilon) + (delta) of those two perturbations equals the size of the duality gap at an optimal solution. Further, the characterization is equivalent to a near-saddle point condition which generalizes the classic saddle point characterization of a primal--dual optimal solution in convex programming. The system developed can be used to explain, to a large degree, when and why Lagrangian heuristics for discrete optimization are successful in reaching near-optimal solutions. Further, experiments on a set-covering problem illustrate how the new optimality conditions can be utilized as a foundation for the construction of new Lagrangian heuristics. Finally, we outline possible uses of the optimality conditions in column generation algorithms and in the construction of core problems.", "e:keyword": ["Programming", "Integer", "Theory", "Global optimality conditions", "Programming", "Integer", "Algorithms", "Lagrangian heuristics", "Column generation algorithms", "Core problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0293", "e:abstract": "This paper presents a new heuristic algorithm for the two-dimensional irregular stock-cutting problem, which generates significantly better results than the previous state of the art on a wide range of established benchmark problems. The developed algorithm is able to pack shapes with a traditional line representation, and it can also pack shapes that incorporate circular arcs and holes. This in itself represents a significant improvement upon the state of the art. By utilising hill climbing and tabu local search methods, the proposed technique produces 25 new best solutions for 26 previously reported benchmark problems drawn from over 20 years of cutting and packing research. These solutions are obtained using reasonable time frames, the majority of problems being solved within five minutes. In addition to this, we also present 10 new benchmark problems, which involve both circular arcs and holes. These are provided because of a shortage of realistic industrial style benchmark problems within the literature and to encourage further research and greater comparison between this and future methods.", "e:keyword": ["Search production/scheduling", "Cutting-stock/trim", "Production/scheduling", "Approximations/heuristic", "Computers/computer science", "Artificial intelligence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0294", "e:abstract": "We present a two-state, one-control model of a seller's decision about how good a “deal” to give customers when price and quantity are observable, but the customer does not observe quality until after committing to the transaction, so the quality of the bargain affects future demand only indirectly by influencing the reputation of the seller. This situation describes well the markets for many illicit drugs. Analysis reveals two optimal strategies: converging to a unique steady state with a positive sales volume or exploiting the initial customer base by selling cut-rate products until the seller's reputation is so bad that sales go to zero. Numerical analysis with parameters based on the U.S. cocaine market shows that there is a so-called Dechert-Nishimura-Skiba (DNS) curve in the state space, where the seller is indifferent between these two strategies. Convergence to the steady state with positive sales is oscillatory. Because different sellers may be at different phases of the oscillation at any given time, this oscillation implies that a population of homogenous sellers can generate dispersion in market prices---a phenomenon characteristic of drug markets that has been difficult to explain.", "e:keyword": ["Dynamic programming/optimal control: multiple equilibria", "Dechert-Nishimura-Skiba thresholds", "Marketing: pricing", "Competition strategy", "Judicial/legal: drug markets", "Prices"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0295", "e:abstract": "Data envelopment analysis (DEA) is a mathematical approach to measuring the relative efficiency of peer decision-making units (DMUs). It is particularly useful when no a priori information is available on the trade-offs or relationships among various performance measures. A shortcoming of the DEA model, however, is its inability to provide a measure of <i>absolute</i> performance for the DMUs under investigation. Traditionally, in the service sector, this has not been an issue that one could address, because performance standards in that sector have been difficult to establish. However, in those settings where it has become feasible to develop such standards, it is desirable to build these into DEA performance evaluation, thereby enhancing the capability of the tool. While there have been some attempts to incorporate standards into the DEA structure, these approaches have generally been <i>indirect</i>, in the sense that they have focused primarily on restricting the DEA dual multipliers. This paper introduces a new way of building performance standards into the model. Utilizing the conventional DEA framework and a set of activity matrices, a set of standard DMUs can be generated and incorporated <i>directly</i> into the analysis. We show that under normal circumstances, these generated DMUs are efficient relative to the normal ones, and therefore form a type of outer frontier against which regular units can be evaluated. The proposed approach is applied to a sample of 100 branches of a major Canadian bank, where time standards are used to generate a set of standard bank branches.", "e:keyword": ["Organizational studies: productivity", "Financial institutions: banks", "Programming: linear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0296", "e:abstract": "We consider the problem of dynamically cross-selling products (e.g., books) or services (e.g., travel reservations) in the e-commerce setting. In particular, we look at a company that faces a stream of stochastic customer arrivals and may offer each customer a choice between the requested product and a package containing the requested product as well as another product, what we call a “packaging complement.” Given consumer preferences and product inventories, we analyze two issues: (1) how to select packaging complements, and (2) how to price product packages to maximize profits.We formulate the cross-selling problem as a stochastic dynamic program blended with combinatorial optimization. We demonstrate the state-dependent and dynamic nature of the optimal package selection problem and derive the structural properties of the dynamic pricing problem. In particular, we focus on two practical business settings: with (the Emergency Replenishment Model) and without (the Lost-Sales Model) the possibility of inventory replenishment in the case of a product stockout. For the Emergency Replenishment Model, we establish that the problem is separable in the initial inventory of all products, and hence the dimensionality of the dynamic program can be significantly reduced. For both models, we suggest several packaging/pricing heuristics and test their effectiveness numerically.", "e:keyword": ["Dynamic programming/optimal control", "Models", "Marketing", "Pricing", "Buyer behavior"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0297", "e:abstract": "The multiple-family economic lot scheduling problem with safety stocks (MFELSP-SS) with normally distributed, time-stationary demand is considered in a manufacturing setting where the relevant costs include family setup costs, item setup costs, and inventory holding costs for both cycle and safety stocks. A family is a subset of the items that share a common family setup with its associated setup cost and setup time. Each item within the family may have its own setup time and setup cost. The families form a partition of the set of items manufactured on a single facility. The safety stock level for any item is a function of the time interval between production runs for the item, the service level specified, and the variance of its demand. We consider safety stocks explicitly in the formulation, as their holding costs vary nontrivially with the model’s decision variables. The MFELSP-SS differs from multilevel inventory models with family setups in that the former assumes noninstantaneous inventory replenishment and considers the cost of holding safety stocks; the latter assumes instantaneous replenishment and does not directly assess the impact of safety stock levels on the total cost. An efficient solution procedure is developed for this model. Properties of the nonconvex feasible space are identified and used in the solution approach. The solution to the mathematical model comprises the basic period length, the family multipliers, and the item multipliers that give the lowest total cost of setups and carrying inventory. The family multipliers and item multipliers are restricted to integer powers of two.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Multi-item", "Multifamily", "Safety stock", "Production/scheduling", "Applications", "Approximations/heuristics", "Cyclic schedule"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0298", "e:abstract": "This paper considers a multiproduct, single-server production system where both setup times and costs are incurred whenever the server changes product. The system is make-to-order with a per unit backlogging cost. The objective is to minimize the long-run average cost per unit time. Using a fluid model, we provide a closed-form lower bound on system performance. This bound is also shown to provide a lower bound for stochastic systems when scheduling is local or static, but is only an approximation when scheduling is global or dynamic. The fluid bound suggests both local and global scheduling heuristics, which are tested for the stochastic system via a simulation study.", "e:keyword": ["Production/scheduling", "Stochastic sequencing", "Queues", "Limit theorems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0301", "e:abstract": "We consider the two-machine open shop and two-machine flow shop scheduling problems in which each machine has to be maintained exactly once during the planning period, and the duration of each of these intervals depends on its start time. The objective is to minimize the maximum completion time of all activities to be scheduled. We resolve complexity and approximability issues of these problems. The open shop problem is shown to be polynomially solvable for quite general functions defining the length of the maintenance intervals. By contrast, the flow shop problem is proved binary NP-hard and pseudopolynomially solvable by dynamic programming. We also present a fully polynomial approximation scheme and a fast 3/2-approximation algorithm.", "e:keyword": ["Production/scheduling: approximations/heuristic", "Multiple machine", "Inventory/production: maintenance/replacement"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0302", "e:abstract": "We consider the multiproduct and multicomponent assemble-to-order (ATO) systems where the replenishment lead times of the components are stochastic, sequential, and independent of the system state. The component inventories are either controlled by the continuous-time base-stock policies, namely, a base-stock ATO system, or by the continuous-time batch-ordering policies, namely, a batch-ordering ATO system. This paper develops the following results: First, for a base-stock ATO system with a single end product and renewal demand arrivals, we characterize the probability distribution of the delivery lead time, i.e., the time it takes to satisfy a demand. The exact analysis allows us to provide simple proofs for the important system properties. Second, for a base-stock ATO system with multiple end products and demand following independent Poisson processes, we characterize the dependence among the stockout delays of the components. We show that a multiproduct ATO system can be decomposed into multiple single-product subsystems with each subsystem corresponding to one product. The analysis allows us to develop two numerical methods to evaluate the performance of the base-stock ATO systems of medium to large sizes. A hypothetical example inspired by a real-world problem is presented. Third, for a batch-ordering ATO system, we develop efficient numerical methods for performance evaluation based on Monte Carlo simulation. Given the sample size, the number of products, and the reorder points, the computational complexity of the methods is no more than that of sorting a set of real numbers, where the set size equals to the sum of the batch sizes of all components. Finally, we characterize the impact of the dependence among the components on various ATO systems, and discuss the limits of the approach.", "e:keyword": ["Inventory/production: multi-item", "Operating characteristics", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0303", "e:abstract": "We consider interruptible electricity contracts issued by an electricity retailer that allow for interruptions to electric service in exchange for either an overall reduction in the price of electricity delivered or for financial compensation at the time of interruption. We provide a structural model to determine electricity prices based on stochastic models of supply and demand. We use stochastic dynamic programming to value interruptible contracts from the point of view of an electricity retailer, and describe the optimal interruption strategy. We also demonstrate that structural models can be used to value contracts in competitive markets. Our numerical results indicate that, in a deregulated market, interruptible contracts can help alleviate supply problems associated with spikes of price and demand and that competition between retailers results in lower value and less frequent interruption.", "e:keyword": ["Natural resources: energy", "Interruptible electricity", "Dynamic programming/optimal control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0304", "e:abstract": "The spiral-down effect occurs when incorrect assumptions about customer behavior cause high-fare ticket sales, protection levels, and revenues to systematically decrease over time. If an airline decides how many seats to protect for sale at a high fare based on past high-fare sales, while neglecting to account for the fact that availability of low-fare tickets will reduce high-fare sales, then high-fare sales will decrease, resulting in lower future estimates of high-fare demand. This subsequently yields lower protection levels for high-fare tickets, greater availability of low-fare tickets, and even lower high-fare ticket sales. The pattern continues, resulting in a so-called spiral down. We develop a mathematical framework to analyze the process by which airlines forecast demand and optimize booking controls over a sequence of flights. Within the framework, we give conditions under which spiral down occurs.", "e:keyword": ["Pricing: revenue management", "Forecasting: estimation and control", "Probability: applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0305", "e:abstract": "Motivated by the recent adoption of tactical pricing strategies in manufacturing settings, this paper studies a problem of dynamic pricing for a multiproduct make-to-order system. Specifically, for a multiclass <i>M<sub>n</sub></i>/<i>M</i>/1 queue with controllable arrival rates, general demand curves, and linear holding costs, we study the problem of maximizing the expected revenues minus holding costs by selecting a pair of dynamic pricing and sequencing policies. Using a deterministic and continuous (fluid model) relaxation of this problem, which can be justified asymptotically as the capacity and the potential demand grow large, we show the following: (i) greedy sequencing (i.e., the <i>c</i>(mu)-rule) is optimal, (ii) the optimal pricing and sequencing decisions decouple in finite time, after which (iii) the system evolution and thus the optimal prices depend only on the total workload. Building on (i)--(iii), we propose a one-dimensional workload relaxation to the fluid pricing problem that is simpler to analyze, and leads to intuitive and implementable pricing heuristics. Numerical results illustrate the near-optimal performance of the fluid heuristics and the benefits from dynamic pricing.", "e:keyword": ["Revenue management", "Yield management", "Dynamic pricing", "Queuing", "Sequencing", "Fluid models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0306", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0307", "e:abstract": "In urban transportation planning, it has become critical (1) to determine the travel time of a traveler and how it is affected by congestion, and (2) to understand how traffic distributes in a transportation network. In the first part of this paper, we derive an analytical function of travel time, based on the theory of kinematic waves. This travel-time function integrates the traffic dynamics as well as the effects of shocks. Numerical examples demonstrate the quality of the analytical function, in comparison with simulated travel times. In the second part of this paper, we incorporate the travel-time model within a dynamic user equilibrium (DUE) setting. We prove that the travel-time function is continuous and strictly monotone if the flow varies smoothly. We illustrate how the model applies to solve a large network assignment problem through a numerical example.", "e:keyword": ["Transportation: models assignment", "Mode/route choice"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0308", "e:abstract": "As a model of make-to-order production, we consider an admission control problem for a multiclass, single-server queue. The production system serves multiple demand streams, each having a rigid due-date lead time. To meet the due-date constraints, a system manager may reject orders when a backlog of work is judged to be excessive, thereby incurring lost revenues. The system manager strives to minimize long-run average lost revenues by dynamically making admission control and sequencing decisions. Under heavy-traffic conditions the scheduling problem is approximated by a Brownian control problem, which is solved explicitly. Interpreting this solution in the context of the original queueing system, a nested threshold policy is proposed. A simulation experiment is performed to demonstrate the effectiveness of this policy.", "e:keyword": ["Production/scheduling", "Make-to-order production systems", "Queues", "Dynamic scheduling via heavy-traffic approximations", "Dynamic programming", "Admission control in queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0309", "e:abstract": "We present a dynamic programming algorithm for solving the single-unit commitment (1UC) problem with ramping constraints and arbitrary convex cost functions. The algorithm is based on a new approach for efficiently solving the single-unit economic dispatch (ED) problem with ramping constraints and arbitrary convex cost functions, improving on previously known ones that were limited to piecewise-linear functions. For simple convex functions, such as the quadratic ones typically used in applications, the solution cost of all the involved (ED) problems, consisting of finding an optimal primal and dual solution, is <i>O</i>(<i>n</i><sup>3</sup>). Coupled with a special visit of the state-space graph in the dynamic programming algorithm, this approach enables one to solve (1UC) with simple convex functions in <i>O</i>(<i>n</i><sup>3</sup>) overall.", "e:keyword": ["Dynamic programming", "Unit commitment problem", "Ramping constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0310", "e:abstract": "From the 1970s onwards, the OR community in Britain engaged in ongoing debate on the future of the discipline, the product of an emerging “crisis of confidence” engendered in part by the end of the “golden age of western economic growth” and the associated downsizing, or abolition, of practitioner groups in the corporate industrial sector. In addition, reservations were expressed concerning the increasing “mathematization” of academic OR in the context of the established “hard” or “classical” paradigm. In this respect, British operations researchers, aided and abetted by a number of American colleagues (notably Ackoff, Churchman, and Miser), engaged in a fundamental reappraisal of the OR methodological repertoire and its client base. Thus, in Britain, a new phase in the history of OR was inaugurated whereby the “positivist/scientist” approach bequeathed by the wartime pioneers was subject to challenge and qualification. Whilst some elements in the American OR community empathized with the emergent British critique, the response (notwithstanding Ackoff et al.) was, on the whole, relatively muted. This conservative American response provides one part of the rationale for this paper. The key issue here is to compare and contrast the tone and content of the Anglo-American debate on the future of OR after 1970: In simple terms, why did British OR practitioners and academics (especially the latter) respond so vigorously to the post-1970 OR critique in marked contrast to their American counterparts? In explaining the differential response, the paper will emphasize the interplay among an array of political, intellectual, and economic factors.", "e:keyword": ["OR/MS philosophy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0311", "e:abstract": "Screening experiments are performed to eliminate unimportant factors so that the remaining important factors can be more thoroughly studied in later experiments. Sequential bifurcation (SB) is a recent screening method that is well suited for simulation experiments; the challenge is to prove the “correctness” of the results. This paper proposes controlled sequential bifurcation, a procedure that incorporates a hypothesis-testing approach into SB to control error and power. A detailed algorithm is given, conditions that guarantee performance are provided, and an empirical evaluation is presented.", "e:keyword": ["Simulation: design of experiments", "Statistical analysis", "Factor screening", "Sequential bifurcation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0312", "e:abstract": "The introduction of digital terrestrial broadcasting all over Europe requires a complete and challenging replanning of in-place analog systems. However, an abrupt migration of resources (transmitters and frequencies) from analog to digital networks cannot be accomplished because the analog services must be preserved temporarily. Hence, a multiobjective problem arises, in which several networks sharing a common set of resources have to be designed. This problem is referred to as the network packing problem. In Italy, this problem is particularly challenging because of a large number of transmitters, orographical features, and strict requirements imposed by Italian law. In this paper, we report our experience in developing solution methods at the major Italian broadcaster Radiotelevisione Italiana (RAI S.p.A.). We propose a two-stage heuristic. In the first stage, emission powers are assigned to each network separately. In the second stage, frequencies are assigned to all networks so as to minimize the loss from mutual interference. A software tool incorporating our methodology is currently in use at RAI to help discover and select high-quality alternatives for the deployment of digital equipment.", "e:keyword": ["Communications: frequency and power assignment", "Integer programming", "Heuristic: neighborhood search"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0313", "e:abstract": "Multiechelon inventory optimization is increasingly being applied by business users as new tools expand the class of network topologies that can be optimized. In this paper, we formalize a topology that we call networks with clusters of commonality (CoC), which captures a large class of real-world supply chains that contain component commonality. Viewed as a modified network, a CoC network is a spanning tree where the nodes in the modified network are themselves maximal bipartite subgraphs in the original network. We first present algorithms to identify these networks and then present a single-state-variable dynamic program for optimizing safety stock levels and locations. We next present two reformulations of the dynamic program that significantly reduce computational complexity while preserving the optimality of the resulting solution. This work both incorporates arbitrary safety stock cost functions and makes possible optimizing a large class of practically useful but previously intractable networks. It has been successfully applied at several Fortune 500 companies, including the recent Edelman finalist project at Hewlett Packard described in detail in Billington et al. (2004).", "e:keyword": ["Multiechelon inventory system", "Safety stock optimization", "Dynamic programming application", "Component commonality", "Networks with clusters of commonality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0314", "e:abstract": "We study mixed-integer programming formulations, based upon variable disaggregation, for generic multicommodity network flow problems with nonconvex piecewise linear costs, a problem class that arises frequently in many application domains in telecommunications, transportation, and logistics. We present several structural results for these formulations, and we analyze the results of extensive experiments on a large set of instances with various characteristics. In particular, we show that the linear programming relaxation of an extended disaggregated model approximates the objective function by its lower convex envelope in the space of commodity flows. Together, the theoretical and computational results allow us to suggest which formulation might be the most appropriate, depending on the characteristics of the problem instances.", "e:keyword": ["Mathematics", "Piecewise linear", "Networks/graphs", "Multicommodity", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0317", "e:abstract": "The problem of finding a maximin Latin hypercube design in two dimensions can be described as positioning <i>n</i> nonattacking rooks on an <i>n</i> × <i>n</i> chessboard such that the minimal distance between pairs of rooks is maximized. Maximin Latin hypercube designs are important for the approximation and optimization of black-box functions. In this paper, general formulas are derived for maximin Latin hypercube designs for general <i>n</i>, when the distance measure is <i>l</i>(infinity) or <i>l</i><sup>1</sup>. Furthermore, for the distance measure <i>l</i><sup>2</sup>, we obtain maximin Latin hypercube designs for <i>n</i> (le) 70 and approximate maximin Latin hypercube designs for other values of <i>n</i>. All these maximin Latin hypercube designs can be downloaded from the website http://www.spacefillingdesigns.nl. We show that the reduction in the maximin distance caused by imposing the Latin hypercube design structure is small. This justifies the use of maximin Latin hypercube designs instead of unrestricted designs.", "e:keyword": ["Branch-and-bound", "Circle packing", "Latin hypercube design", "Mixed-integer programming", "Noncollapsing", "Space-filling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0318", "e:abstract": "This paper extends previous work on the distribution-free newsvendor problem, where only partial information about the demand distribution is available. More specifically, the analysis assumes that the demand distribution <i>f</i> belongs to a class of probability distribution functions (pdf) (F-script) with mean (mu) and standard deviation (sigma). While previous work has examined the expected value of distribution information (EVDI) for a particular order quantity and a particular pdf <i>f</i>, this paper aims at computing the maximum EVDI over all <i>f</i> (in) (F-script) for any order quantity. In addition, an optimization procedure is provided to calculate the order quantity that minimizes the maximum EVDI.", "e:keyword": ["Inventory/production: perishable/aging items", "Inventory/production: uncertainty", "Decision analysis: applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0319", "e:abstract": "This paper considers a generic one-warehouse multiple-retailer inventory system under continuous review, where customers provide perfect advance-order information. More specifically, each customer order entails a due date specifying when the customer wants the item delivered. The information is perfect in the sense that a placed order cannot be revised. With the intent of using the advance-order information fully throughout the supply chain, each installation replenishes its stock using order base-stock policies (see Hariharan and Zipkin 1995). As for stock allocation, the presence of advance-order information at the central warehouse raises important questions regarding when reservations should be made for different retailers, i.e., how to make best use of the available temporal information to allocate items to retailers. Exact and approximate cost evaluation techniques are presented for the general case, the <i>general reservation policy</i>, as well as for the two special cases of reserving as early as possible, the <i>complete reservation policy</i>, and as late as possible, the <i>last-minute allocation policy</i>. A numerical study illustrates the performance of the proposed heuristics and provides insights on the value of using advance-order information in supply chain inventory control.", "e:keyword": ["Inventory/production", "Multiechelon", "Stochastic", "Advance-order information", "Reservation", "Allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0320", "e:abstract": "We develop a method for computing efficient patient-specific drug protocols. Using this method, we identify two general categories of anticancer drug protocols, depending on the temporal cycle parameters of the host and cancer cells: a one-time intensive treatment, or a series of nonintensive treatments. Our method is based on a theoretical and experimental work showing that treatment efficacy can be improved by determining the dosing frequency on the drug-susceptible target and host cell-cycle parameters. Simulating the patient's pharmaco-dynamics in a simple model for cell population growth, we calculate the number of drug susceptible cells at every moment of therapy. Local search heuristics are then used to conduct a search for the desired solution, as defined by our criteria. These criteria include the patient's state at the end of a predetermined time period, the number of cancer and host cells at the end of treatment, and the time to the patient's cure. The process suggested here does not depend on the exact biological assumptions of the model, thus enabling its use in a more complex description of the system. We test three solution methods. Simulated annealing is compared to threshold acceptance and old bachelor acceptance, which are less known variants to this method. The conclusions concerning the three approximation methods are that good results can be achieved by choosing the proper parameters for each of the methods, but the computational effort required for achieving good results is much greater in simulated annealing than in the other methods. Also, a large number of iterations does not guarantee better solution quality, and resources would be better used in several short searches with different parameter values than in one long search.", "e:keyword": ["Health care: treatment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0321", "e:abstract": "High-volume, multistage continuous production flow through a re-entrant factory is modeled through a conservation law for a continuous-density variable on a continuous-production line augmented by a state equation for the speed of the production along the production line. The resulting nonlinear, nonlocal hyperbolic conservation law allows fast and accurate simulations. Little's law is built into the model. It is argued that the state equation for a re-entrant factory should be nonlinear. Comparisons of simulations of the partial differential equation (PDE) model and discrete-event simulations are presented. A general analysis of the model shows that for any nonlinear state equation there exist two steady states of production below a critical start rate: A high-volume, high-throughput time state and a low-volume, low-throughput time state. The stability of the low-volume state is proved. Output is controlled by adjusting the start rate to a changed demand rate. Two linear factories and a re-entrant factory, each one modeled by a hyperbolic conservation law, are linked to provide proof of concept for efficient supply chain simulations. Instantaneous density and flux through the supply chain as well as work in progress (WIP) and output as a function of time are presented. Extensions to include multiple product flows and preference rules for products and dispatch rules for re-entrant choices are discussed.", "e:keyword": ["Production/scheduling: approximations", "Simulations: efficiency", "Mathematics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0322", "e:abstract": "In this paper, we propose a model of intertemporal choice that explicitly incorporates satiation due to previous consumption in the evaluation of the utility of current consumption. In the discounted utility (DU) model, the utility of consumption is evaluated afresh in each time period. In our model, the utility of current consumption represents an incremental utility from the past level. When the time interval between consumption periods is large, and there are, therefore, no carryover effects, our model coincides with the DU model. For short time intervals between consumption periods, the satiation due to previous consumption lowers the utility of current consumption. Several implications of our model are examined, and comparisons with the DU model and the habituation model are made.", "e:keyword": ["Satiation", "Time preference", "Local substitution"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0323", "e:abstract": "The multilevel generalized assignment problem (MGAP) is a variation of the generalized assignment problem, in which agents can execute tasks at different efficiency levels with different costs. We present a branch-and-price algorithm that is the first exact algorithm for the MGAP. It is based on a decomposition into a master problem with set-partitioning constraints and a pricing subproblem that is a multiple-choice knapsack problem. We report on our computational experience with randomly generated instances with different numbers of agents, tasks, and levels; and with different correlations between cost and resource consumption for each agent-task-level assignment. Experimental results show that our algorithm is able to solve instances larger than those of the maximum size considered in the literature to proven optimality.", "e:keyword": ["Integer programming: branch-and-price", "Production-scheduling: generalized assignment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0324", "e:abstract": "An innovative approach to DNA sequencing by hybridization utilizes isothermic oligonucleotide libraries. In this paper, we demonstrate the utility of a genetic algorithm for the combinatorial portion of this new approach by incorporating characteristics of DNA sequencing by hybridization in addition to isothermic oligonucleotide libraries. Specialized crossover and mutation operators were developed for this purpose. After initial experiments for parameter adjustment, the performance of the genetic algorithm approach was evaluated with respect to previous methods in the literature. The results indicate that the proposed new approach is superior to previous approaches. The proposed new crossover operator that inherits some features of the structured weighted combinations might also be of value for some other combinatorial problems, including the traveling salesman problem.", "e:keyword": ["Analysis of algorithms: metaheuristics", "Health care: bioinformatics", "Health care: DNA sequencing", "Programming: integer", "Algorithms: heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0325", "e:abstract": "This research concerns a new family of capacitated multi-item lot-sizing problems, namely, lot-sizing problems with time windows. Two classes of the problem are analyzed and solved using different Lagrangian heuristics. Capacity constraints and a subset of time window constraints are relaxed resulting in particular single-item time window problems that are solved in polynomial time. Other relaxations leading to the classical Wagner-Whitin problem are also tested. Several smoothing heuristics are implemented and tested, and their results are compared. The gaps between lower and upper bounds for most problems are very small (less than 1%). Moreover, the proposed algorithms are robust and do not seem to be too affected when different parameters of the problem are varied.", "e:keyword": ["Production/scheduling", "Approximations/heuristic: Lagrangian heuristics", "Production/scheduling", "Planning: capacitated multi-item lot sizing", "Programming", "Integer", "Relaxation/subgradient: Lagrangian relaxations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0326", "e:abstract": "Consider <i>n</i> manufacturers, each producing a different product and selling it to a market, either directly or through a common retailer. The <i>n</i> products are perfectly complementary in the sense that they are always sold and consumed jointly or in sets of one unit of each. Demand for the products during a selling season is both price sensitive and uncertain. Each of the <i>n</i> manufacturers faces the problem of choosing a production quantity and a selling price for his product. Two settings are considered, regarding the decision sequence of the <i>n</i> manufacturers: They are either simultaneous or sequential. The retailer, when present, employs a consignment-sales contract with revenue sharing to bind her relationship with the manufacturers and to extract profit for herself. Using a multiplicative demand model in this paper, we fully characterize individual firms’ decisions in equilibria, under each of the two game settings, and derive closed-form performance measures, both for the channel and for individual channel members. These closed-form solutions allow us to explore the effects of channel structure and parameters on firms’ decisions and performance that lead to conclusions of managerial interest.", "e:keyword": ["Games/group decisions: noncooperative", "Inventory/production: uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0327", "e:abstract": "The United States Coast Guard (USCG), now part of the Department of Homeland Security, has the mission to secure the U.S. coastline using a combination of air and sea capabilities. This paper focuses on an application of operations research techniques at the USCG to improve the performance of its aircraft service parts supply chain. We focused on evaluating the supply chain benefits from linking the aircraft maintenance database with the aircraft parts inventory database. This required us to (a) develop an approach to link the databases and (b) use aircraft maintenance information to improve the inventory management of service parts at the USCG. We first used mathematical programming tools to merge the maintenance database with the demand database. We then developed state-dependent supply replenishment policies that use part-age information to manage the service parts supply chain. We show that one of the proposed policies permits analytic estimation of the benefits of linking the data sets. The impact of these inventory policies was evaluated using empirical demand data for 41 critical parts over a five-year period. Computational results suggest that our proposed policies can lead to significant reductions in inventory cost over the current system, as high as 70% for some parts. Based on the insights from this study, the USCG is currently contracting with commercial vendors to develop an operational database and decision-support implementation across all parts.", "e:keyword": ["Inventory: applications", "Maintenance policies", "Military: cost effectiveness"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0328", "e:abstract": "This paper describes a decision analysis methodology to evaluate academic programs. It avoids the shortcomings of the well-known evaluations of universities and academic programs produced by the public media. In addition to evaluating traditional departments and schools, the methodology is designed to evaluate interdisciplinary programs or fields that typically span many areas of a university, such as operations research, risk analysis, and decision science. We first discuss general principles of using this methodology for the evaluation of disciplinary or interdisciplinary academic programs. Next, we apply this methodology to interdisciplinary graduate decision science programs in United States universities, focusing on both prescriptive decision analysis and descriptive decision research. Finally, we suggest how the methodology might be implemented to evaluate operations research programs.", "e:keyword": ["Academic program evaluation", "Decision analysis", "Applications", "Education systems", "Operations", "Utility/preference", "Multiattribute"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0329", "e:abstract": "The only available therapy for patients with end-stage liver disease is organ transplantation. In the United States, patients with end-stage liver disease are placed on a waiting list and offered livers based on location and waiting time, as well as current and past health. Although there is a shortage of cadaveric livers, 45% of all cadaveric liver offers are declined by the first transplant surgeon and/or patient to whom they are offered. We consider the decision problem faced by these patients: Should an offered organ of a given quality be accepted or declined? We formulate a Markov decision process model in which the state of the process is described by patient state and organ quality. We use a detailed model of patient health to estimate the parameters of our decision model and implicitly consider the effects of the waiting list through our patient-state-dependent definition of the organ arrival probabilities. We derive structural properties of the model, including a set of intuitive conditions that ensure the existence of control-limit optimal policies. We use clinical data in our computational experiments, which confirm that the optimal policy is typically of control-limit type.", "e:keyword": ["Dynamic programming/optimal control", "Applications and Markov", "Infinite horizon", "Health care", "Treatment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0331", "e:abstract": "The location and operation of harvest machinery, along with the design and construction of access roads, are important problems faced by forestry planners, making up about 55% of total production costs. One of the main challenges consists of finding a design that will minimize the cost of installation and operation of harvest machinery, road construction, and timber transport, while complying with the technical restrictions that apply to the operation of harvesting equipment and road construction. We can model the network design problem as a mixed-integer linear programming problem. This model is fed with cartographic information, provided by a geographic information system (GIS), along with technical and economic parameters determined by the planner. We developed a specialized heuristic for the problem to obtain solutions that enable harvesting economically profitable volumes at a low cost. This methodology was programmed into a computer system known as PLANEX and is being applied in nine forestry companies that report important benefits from its use.", "e:keyword": ["Natural resources", "Forestry", "Discrete location", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0332", "e:abstract": "This paper deals with the conflict between simplicity and optimality in searching for a stationary target whose location is distributed in two dimensions, thus continuing an analysis that was begun in World War II. The search is assumed to be of the “piled-slab” type, where each slab consists of a uniform search of some simple region. The measure of simplicity is the number of regions (smaller is simpler). If each of a fixed number of elliptical regions is searched randomly, we find the optimal region size and the optimal division of effort between regions. Rectangular regions are also considered, as are problems where the regional searches are according to the inverse-cube law, instead of random search. There is a strong tendency for optimal inverse-cube law searches to consist of a single slab. We also consider problems where the amount of effort for each region is optimized myopically, with no consideration for the search of future regions.", "e:keyword": ["Search and surveillance", "Probability applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0333", "e:abstract": "A common practice among large retailers is the low-price guarantee, rebating consumers if they find an identical product cheaper elsewhere. This provides consumers with some level of comfort in their purchase decision. A similar low-price guarantee is provided by numerous service industries that allow reservation of capacity, yet do not penalize the consumer for failure to keep that reservation---examples include hotels and car rental. Given that a consumer is not required to keep the reservation, they may make another reservation, either at a competing firm or the same firm, if future prices decline. The increasing availability of pricing information on the Internet affords consumers the opportunity to be more strategic in their purchasing behavior. As consumers, we are able to quickly and easily check prices from numerous service or goods providers. The ease of price information potentially makes these guarantees very costly to the service or good provider. We analyze the implied costs associated with these guarantees by making analogies to financial options. Motivation for this research comes from a large car rental firm, Dollar Thrifty Automotive Group Inc., that considered offering a low-price guarantee to all consumers that book a reservation though their website.", "e:keyword": ["Finance: asset pricing", "Inventory/production: perishable items", "Transportation: automobile"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0334", "e:abstract": "Many problems in finance can be formulated as high-dimensional integrals, which are often attacked by quasi-Monte Carlo (QMC) algorithms. To enhance QMC algorithms, dimension reduction techniques, such as the Brownian bridge (BB) and principal component analysis (PCA), are used to reduce the effective dimension. This paper explores in depth the effects of these techniques on the dimension structure of some typical high-dimensional problems from finance: the pricing of path-dependent options and bond valuation according to term structure models. By deriving explicit expressions for the underlying integrands and the associated weights that control the relative importance of different variables, and by investigating the variance ratios, the effective dimensions, the mean dimension, and their limiting behavior as the nominal dimension tends to infinity, we show theoretically and empirically how and to what extent the BB and PCA algorithms change the dimension structure (including the degree of additivity) of the underlying functions. They change the functions to be strongly weighted and substantially reduce the effective dimensions and the mean dimension; and they enhance the degree of additivity, which is particularly important for QMC. Moreover, the resulting functions are of low effective dimension, not only in the superposition sense, but also in the truncation sense. The variance ratios, the effective dimensions, and the mean dimension associated with these techniques are very insensitive to the nominal dimension (they are essentially constant), which highlights the possibility of removing the curse of dimensionality when dimension reduction techniques are used in combination with QMC. A counterexample is also shown for which the BB and PCA may increase the effective dimension. The investigation provides further insight into the effects of dimension reduction techniques.", "e:keyword": ["Simulation", "Applications: quasi-Monte Carlo methods", "Finance", "Asset pricing: option pricing", "Finance", "Asset pricing: bond valuation", "Statistics", "Analysis of variance: effective dimension", "Statistics", "Analysis of variance: dimension reduction"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0335", "e:abstract": "This paper considers an inventory and production-planning problem for a contract manufacturer who anticipates an order of a single product, but with uncertain quantity. To meet the challenges of long component procurement lead times and limited assembly capacity, which may render production time insufficient to assemble total order quantity, the manufacturer may need to procure components or even assemble some quantities of the final product before receiving the confirmation of the actual order quantity. We present profit-maximization models that make optimal inventory and production decisions in the above assemble-to-order environment. We also consider the option of outsourcing, so that the manufacturer can outsource part of his production to an external facility that also has limited capacity. We establish structural properties of optimal solutions and develop efficient solution procedures for the proposed problems. We also provide sensitivity analysis of the optimal decisions, and some managerial insights.", "e:keyword": ["Inventory/production: assemble-to-order systems", "Component procurement lead times", "Demand uncertainty", "Capacity management", "Outsourcing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0336", "e:abstract": "This paper studies a one-shot inventory replenishment problem with dynamic pricing. The customer arrival rate is assumed to follow a geometric Brownian motion. Homogeneous customers have an isoelastic demand function and do not behave strategically. We find a closed-form optimal pricing policy, which utilizes current demand information. Under this pricing policy the inventory trajectory is deterministic, and a retailer sells all inventory. We show that dynamic pricing coordinated with the inventory decision achieves significantly higher profits than does static pricing. Furthermore, under oligopolistic competition we establish a weak perfect Bayesian equilibrium for the price and inventory replenishment game. We find the pricing equilibrium to be cooperative even in a noncooperative environment, but that inventory competition results in overstock and damages profits. Finally, we examine the trade-off between dynamic pricing and price precommitment and find that flexible pricing is still beneficial, provided competition is not too intense.", "e:keyword": ["Differential games", "Dynamic pricing", "Geometric Brownian motion", "Martingales", "Revenue management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0337", "e:abstract": "We analyze a general market for an industry of competing service facilities. Firms differentiate themselves by their price levels and the waiting time their customers experience, as well as different attributes not determined directly through competition. Our model therefore assumes that the expected demand experienced by a given firm may depend on all of the industry’s price levels as well as a (steady-state) waiting-time standard, which each of the firms announces and commits itself to by proper adjustment of its capacity level. We focus primarily on a separable specification, which in addition is linear in the prices. (Alternative nonseparable or nonlinear specifications are discussed in the concluding section.) We define a firm’s service level as the difference between an upper-bound benchmark for the waiting-time standard (<i>w̅</i>) and the firm’s actual waiting-time standard.Different types of competition and the resulting equilibrium behavior may arise, depending on the industry dynamics through which the firms select their strategic choices. In one case, firms may initially select their waiting-time standards, followed by a selection of their prices in a second stage (service-level first). Alternatively, the sequence of strategic choices may be reversed (price first) or, as a third alternative, the firms may make their choices simultaneously (simultaneous competition). We model each of the service facilities as a single-server <i>M/M/</i>1 queueing facility, which receives a given firm-specific price for each customer served. Each firm incurs a given cost per customer served as well as cost per unit of time proportional to its adopted capacity level.", "e:keyword": ["Queues", "Multichannel", "Games", "Noncooperative", "Marketing", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0338", "e:abstract": "We consider a finite-horizon, periodic-review inventory model with demand forecasting updates following the martingale model of forecast evolution (MMFE). The optimal policy is a state-dependent base-stock policy, which, however, is computationally intractable to obtain. We develop tractable bounds on the optimal base-stock levels and use them to devise a general class of heuristic solutions. Through this analysis, we identify a necessary and sufficient condition for the myopic policy to be optimal. Finally, to assess the effectiveness of the heuristic policies, we develop upper bounds on their value loss relative to optimal cost. These solution bounds and cost error bounds also work for general dynamic inventory models with nonstationary and autocorrelated demands. Numerical results are presented to illustrate the results.", "e:keyword": ["Inventory", "Forecasting", "MMFE", "Approximation", "Error bounds"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0339", "e:abstract": "We study a finite-horizon discrete-time model of due-date setting (equivalently, reserving capacity) in a make-to-order setting, where demands arrive from two different classes of customers. Demands in each period are stochastic. The two customer classes penalize with different margins the lead times quoted to them, which (once quoted) are to be satisfied reliably. We first derive the optimal policy for reserving capacity that maps to quoted due dates. We use the insights from its structure to develop a novel approximation that provides near-optimal solutions quickly. Currently available heuristics are tested and are found to be considerably less effective than the above approximation.", "e:keyword": ["Production policies", "Operating characteristics", "Approximation", "Heuristic", "Stochastic models", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0340", "e:abstract": "One of the historic roles of operations research (OR) people in the problem domains they enter is that of missionary, bringing their OR techniques of quantitative modeling, system perspective, and planning to the fields where those approaches have not yet taken hold. One of the most primitive of social systems in that regard is the criminal justice system responsible for society’s response to crime. Over the past 40 years, I and a number of colleagues have been involved in this missionary function. The issues addressed have involved modeling of criminal careers as a stochastic process, bringing those analyses to the assessment of incapacitation effects of incarceration, review of trends in incarceration and factors contributing to those trends, and an examination of the interaction of incarceration and drug markets, with an analysis of some of the unintended counterproductive effects of that incarceration response to the drug problem.", "e:keyword": ["Judicial/legal/crime", "Penal system", "Probability/stochastic model applications", "Government/programs", "Simulation/applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0341", "e:abstract": "In this paper, we introduce the lexicographically minimum load linear programming problem, and we provide a polynomial approach followed by the proof of correctness. This problem has applications in numerous areas where it is desirable to achieve an equitable distribution or sharing of resources. We consider the application of our technique to the problem of lexicographically minimum load in capacitated multicommodity networks and discuss a special nonlinear case, the so-called Kleinrock load function. We next define the lexicographically maximum load linear programming problem and deduce a similar approach. An application in the lexicographically maximum concurrent flow problem is depicted followed by a discussion on the minimum balance problem as a special case of the lexicographically maximum load problem.", "e:keyword": ["Networks/graphs", "Multicommodity", "Theory", "Programming", "Linear algorithms", "Multiple criteria", "Games/group decisions", "Cooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0342", "e:abstract": "Most previous Nash-Cournot models of competition among electricity generators have assumed smooth demand (price) functions, facilitating computation and proofs of existence and uniqueness. However, nonsmooth demand functions are an important feature of real power markets due, for example, to price caps and generator recognition of transmission constraints that limit exports. A more general model of Nash-Cournot competition on networks is proposed that accounts for these features by including (1) concave piecewise-linear demand curves and (2) joint constraints that include variables from other generating companies within the profit maximization problems for individual generators. The piecewise demand curves imply, in general, a nonmonotone multivalued variational inequality problem. Thus, for instance, imposition of a price cap can destroy the uniqueness properties found in previous models, so that distinct solutions can yield different sets of profits for market participants. The joint constraints turn the equilibrium problem into a quasi-variational inequality, which also can yield multiple solutions. The formulation poses computational challenges that can cause Lemke’s algorithm to fail; a restricted formulation is proposed that can be solved by that algorithm.", "e:keyword": ["Games/group decisions", "Noncooperative", "Industries", "Electric", "Mathematics", "Piecewise linear", "Programming", "Complementarity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0343", "e:abstract": "In many situations, performance on several attributes is important. Moreover, a decision maker’s utility may depend not on the absolute level of performance on each attribute, but rather on whether that level of performance meets a target, in which case the decision maker is said to be target oriented. For example, typical attributes in new product development include cost, quality, and features, and the corresponding targets might be the best performance on these attributes by competing products. Targets and performance levels typically are uncertain and often are dependent. We develop a model that allows for uncertain dependent targets and uncertain dependent performance levels, and we study implications for decision making in this general multiattribute target-oriented setting. We consider the impact on expected utility of modifying key characteristics of performance (or target) distributions: location, spread, and degree of dependence. In particular, we show that explicit consideration of dependence is important, and we establish when increasing or decreasing dependence is beneficial. We illustrate the results numerically with a normal model and discuss some extensions and implications.", "e:keyword": ["Decision analysis", "Multiattribute performance targets", "Utility/preference", "Multiattribute utility", "Target-oriented utility"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0344", "e:abstract": "We present a new model for revenue management of product sales that incorporates both dynamic pricing and a price guarantee. The guarantee provides customers with compensation if, prior to a fixed future date, the price of the product drops below a level specified at the time of purchase. We consider the problem of simultaneously determining optimal dynamic price and guarantee policies for items from a fixed stock when demand depends both on the price and on the parameters of the price guarantee. The model can be used for pricing any items with limited availability over a fixed time horizon. We formulate this model as a discrete-time optimal control problem, prove the existence of its optimal solution, explore some of the structural properties of the solution, present lower-bounding heuristics for solving the problem, and report numerical results.", "e:keyword": ["Inventory/production", "Perishable/aging items", "Marketing/pricing", "Uncertainty", "Dynamic programming/optimal control", "Applications", "Probability", "Stochastic model applications", "Programming", "Nonlinear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0345", "e:abstract": "In the realm of service parts management, customer relationships are often established through service agreements that extend over months or years. These agreements typically apply to a piece of equipment that the customer has purchased, and they specify the type and timing of service that will be provided. If a customer operates in multiple locations, service agreements may cover several pieces of equipment at several locations. In this paper, we describe a continuous-review inventory model for a multi-item, multiechelon service parts distribution system in which time-based service-level requirements exist. Our goal is to determine base-stock levels for all items at all locations so that the service-level requirements are met at minimum investment. We derive exact time-based fill-rate expressions for each item within its distribution channel, as well as approximate expressions for the gradients of these fill-rate functions. Using these results, we develop an intelligent greedy algorithm that can be used to find near-optimal solutions to large-scale problems quickly, as well as a Lagrangian-based approach that provides both near-optimal solutions and good lower bounds with increased computational effort. We demonstrate the effectiveness and scalability of these algorithms on three example problems.", "e:keyword": ["Inventory/production", "Multiechelon", "Multi-item", "Service-level constraints", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0346", "e:abstract": "We consider an infinite-horizon deterministic joint replenishment problem with first order interaction. Under this model, the setup transportation/reorder cost associated with a group of retailers placing an order at the same time equals some group-independent major setup cost plus retailer-dependent minor setup costs. In addition, each retailer is associated with a retailer-dependent holding-cost rate. The structure of optimal replenishment policies is not known, thus research has focused on optimal power-of-two (POT) policies. Following this convention, we consider the cost allocation problem of an optimal POT policy among the various retailers. For this sake, we define a characteristic function that assigns to any subset of retailers the average-time total cost of an optimal POT policy for replenishing the retailers in the subset, under the assumption that these are the only existing retailers. We show that the resulting transferable utility cooperative game with this characteristic function is concave. In particular, it is a totally balanced game, namely, this game and any of its subgames have nonempty core sets. Finally, we give an example for a core allocation and prove that there are infinitely many core allocations.", "e:keyword": ["Inventory/production", "Infinite-horizon", "Deterministic", "Multiretailer", "Lot-sizing", "Games", "Cooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0347", "e:abstract": "We present tractable algorithms to assess the sensitivity of a stochastic dynamic fleet management model to fleet size and load availability. In particular, we show how to compute the change in the objective function value in response to an additional vehicle or an additional load introduced into the system. The novel aspect of our approach is that it does not require multiple simulations with different values of the model parameters, and in this respect it differs from trial-and-error-based “what-if” analyses. Numerical experiments show that the proposed methods are accurate and computationally attractive.", "e:keyword": ["Transportation", "Models", "Network", "Dynamic programming/optimal control", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0348", "e:abstract": "In this paper, we analyze a facility location model where facilities may be subject to disruptions, causing customers to seek service from the operating facilities. We generalize the classical <i>p</i>-median problem on a network to explicitly include the failure probabilities, and analyze structural and algorithmic aspects of the resulting model. The optimal location patterns are seen to be strongly dependent on the probability of facility failure, with facilities becoming more centralized, or even co-located, as the failure probability grows. Several exact and heuristic solution approaches are developed. Results of numerical experiments are reported.", "e:keyword": ["Facilities/equipment planning", "Location discrete", "Reliability", "p-median"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0351", "e:abstract": "Bilateral supply contracts are widely used despite the presence of spot markets. In this paper, we provide a potential explanation for this prevalence of supply contracts even when spot markets are liquid and without delivery lag. Specifically, we consider the determination of an equilibrium forward contract on a nonstorable commodity between two firms that have mean-variance preferences over their risky profits and negotiate the forward contract through a Nash bargaining process. We derive the unique equilibrium forward contract in closed form and provide an extensive analysis. We show that it is the risk-hedging benefit from a forward that justifies its prevalence in spite of liquid spot markets. In addition, while a forward does not affect production decisions due to the presence of spot markets, it does affect inventory decisions of the storable input factor due to its hedging effect against the inventory risk. We also show that price volatilities and correlations are important determinants of the equilibrium contract. In particular, the equilibrium forward price can be nonmonotonic in the spot price volatility and can decrease as the initial spot price increases.", "e:keyword": ["Finance", "Futures price", "Hedging", "Spot market", "Inventory/production", "Dual sourcing", "Supply contract"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0352", "e:abstract": "We consider robust optimization to cope with uncertainty about the stock return process in one-period option hedging problems. The robust approach relates portfolio choice to uncertainty, making more cautious hedges when uncertainty is high. We represent uncertainty by a set of plausible expected returns of the underlying stocks and show that for this set the robust problem is a second-order cone program that can be solved efficiently. We apply the approach to find an optimal portfolio to hedge an index option.", "e:keyword": ["Robust optimization", "Portfolio optimization", "Nonnegative cones", "Hedging"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0353", "e:abstract": "We provide a method for deriving robust solutions to certain stochastic optimization problems, based on mean-covariance information about the distributions underlying the uncertain vector of returns. We prove that for a general class of objective functions, the robust solutions amount to solving a certain deterministic parametric quadratic program. We first prove a general projection property for multivariate distributions with given means and covariances, which reduces our problem to optimizing a univariate mean-variance robust objective. This allows us to use known univariate results in the multidimensional setting, and to add new results in this direction. In particular, we characterize a general class of objective functions (the so-called one- or two-point support functions), for which the robust objective is reduced to a deterministic optimization problem in one variable. Finally, we adapt a result from Geoffrion (1967a) to reduce the main problem to a parametric quadratic program. In particular, our results are true for increasing concave utilities with convex or concave-convex derivatives. Closed-form solutions are obtained for special discontinuous criteria, motivated by bonus- and commission-based incentive schemes for portfolio management. We also investigate a multiproduct pricing application, which motivates extensions of our results for the case of nonnegative and decision-dependent returns.", "e:keyword": ["Programming", "Stochastic", "Quadratic", "Decision analysis", "Risk", "Finance", "Portfolio"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0354", "e:abstract": "In this paper, we propose a novel optimization algorithm for examination timetabling. It works by alternating two phases; one based on a stochastic local search and the other on a deterministic local search. The stochastic phase is fundamentally based on biased random sampling that iteratively constructs schedules according to a matrix whose entries are the probability with which exams can be assigned to time slots. The deterministic phase, instead, consists of assigning (according to a given ordering) each exam sequentially to the time slot that causes the lowest increase in the schedule penalty. After a schedule is constructed, swap operations are executed to improve performance. These two phases are coupled and made closely interactive by tunnelling information on what has happened during one phase to the successive ones. Moreover, the length of a phase and the parameter framework to be used in a new phase are automatically determined by a record of the process. We tested the proposed technique on known benchmarks, and a comparison with 17 algorithms drawn from the state of the art appears to show that our algorithm is able to improve best-known results. In particular, in reference to uncapacitated problems, i.e., the ones without room constraints, our algorithm bested the state of the art in 70% to 90% of the tested instances, while in capacitated problems with overnight conflicts (second-order conflicts), it was superior to all the other algorithms.", "e:keyword": ["Scheduling", "Heuristic", "Deterministic algorithm", "Stochastic algorithm", "Biased random sampling", "Examination timetabling", "Local search"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0355", "e:abstract": "In this paper, we use a real-options framework to value a power plant. The real option to commit or decommit a generating unit may be exercised on an hourly basis to maximize expected profit while subject to intertemporal operational constraints. The option-exercising process is modeled as a multistage stochastic problem. We develop a framework for generating discrete-time price lattices for two correlated Ito processes for electricity and fuel prices. We show that the proposed framework exceeds existing approaches in both lattice feasibility and computational efficiency. We prove that this framework guarantees existence of branching probabilities at all nodes and all stages of the lattice if the correlation between the two Ito processes is no greater than 4/(sqrt)35 (approx) 0.676. With price evolution represented by a lattice, the valuation problem is solved using stochastic dynamic programming. We show how the obtained power plant value converges to the true expected value by refining the price lattice. Sensitivity analysis for the power plant value to changes of price parameters is also presented.", "e:keyword": ["Natural resources", "Energy", "Finance", "Asset pricing", "Real options"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0356", "e:abstract": "We consider a supply chain operating in an uncertain environment: The customers’ demand is characterized by a discrete probability distribution. A probabilistic programming approach is adopted for constructing an inventory-production-distribution plan over a multiperiod planning horizon. The plan does not allow the backlogging of the unsatisfied demand, and minimizes the costs of the supply chain while enabling it to reach a prescribed nonstockout service level. It is a strategic plan that hedges against undesirable outcomes, and that can be adjusted to account for possible favorable realizations of uncertain quantities. A modular, integrated, and computationally tractable method is proposed for the solution of the associated stochastic mixed-integer optimization problems containing joint probabilistic constraints with dependent right-hand side variables. The concept of <i>p</i>-efficiency is used to construct a finite number of demand trajectories, which in turn are employed to solve problems with joint probabilistic constraints. We complement this idea by designing a preordered set-based preprocessing algorithm that selects a subset of promising <i>p</i>-efficient demand trajectories. Finally, to solve the resulting disjunctive mixed-integer programming problem, we implement a special column-generation algorithm that limits the risk of congestion in the resources of the supply chain. The methodology is validated on an industrial problem faced by a large chemical supply chain and turns out to be very efficient: it finds a solution with a minimal integrality gap and provides substantial cost savings.", "e:keyword": ["Inventory/production", "Multistage supply chain", "Programming", "Integer", "Stochastic", "Transportation", "Scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0357", "e:abstract": "We introduce a generalization of <i>K</i>-concavity termed <i>weak</i> (<i>K</i><sub>1</sub>, <i>K</i><sub>2</sub>)-<i>concavity</i> and show how it can be used to analyze certain dynamic systems arising in capacity management. We show that weak (<i>K</i><sub>1</sub>, <i>K</i><sub>2</sub>)-concavity has two fundamental properties that are relevant for the analysis of such systems: First, it is preserved for linear interpolations; second, it is preserved for certain types of linear extensions. In capacity management problems where both buying and selling capacity involve a fixed cost plus a proportional cost/revenue term, interpolations and extensions are fundamental building blocks of the optimality analysis. In the context of the capacity management problem studied by Ye and Duenyas (2007), we show that weak (<i>K</i><sub>1</sub>, <i>K</i><sub>2</sub>)-concavity is sufficient to prove the general structure of the optimal policy established in that paper.", "e:keyword": ["Dynamic programming", "Optimal control", "Models", "Inventory", "Production", "Policies", "Mathematics", "Functions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0358", "e:abstract": "In the last few decades, the resource-constrained project-scheduling problem has become a popular problem type in operations research. However, due to its strongly NP-hard status, the effectiveness of exact optimisation procedures is restricted to relatively small instances. In this paper, we present a new genetic algorithm (GA) for this problem that is able to provide near-optimal heuristic solutions. This GA procedure has been extended by a so-called decomposition-based genetic algorithm (DBGA) that iteratively solves subparts of the project. We present computational experiments on two data sets. The first benchmark set is used to illustrate the performance of both the GA and the DBGA. The second set is used to compare the results with current state-of-the-art heuristics and to show that the procedure is capable of producing consistently good results for challenging problem instances. We illustrate that the GA outperforms all state-of-the-art heuristics and that the DBGA further improves the performance of the GA.", "e:keyword": ["Production/scheduling", "Approximations/heuristic", "Project management", "Resource constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0359", "e:abstract": "Contract manufacturers sell capacity under different terms to different buyers. In a common practice, the supplier offers a market standard price and lead-time combination for products listed in its catalog. At the same time, it strikes contracts with some high-volume customers who require recurring delivery of a custom product at a short notice, usually timed to meet the buyer’s production schedule. Whereas the manufacturer is obligated to satisfy demand from these customers, it can dynamically choose which transactional orders to accept. In this paper, we analyze two scenarios. In the first scenario, the manufacturer produces contractual orders on a make-to-order basis, and in the second, it maintains finished goods inventory for such orders. At each decision epoch, the manufacturer decides which transactional orders to accept to maximize its long-run expected profit. When contractual items are made to stock, the manufacturer also chooses how much extra capacity to allocate to contractual items, over and above what is needed to meet realized demand. We establish the structure of the optimal policies and propose scalable heuristics when optimal policies are hard to compute/implement. Our models are then used to study the effect of demand variability on the optimal profit. We also show how the amount of capacity available, relative to demand, changes the desirability of serving either transactional-only or both markets.", "e:keyword": ["Manufacturing", "Contract manufacturing", "Strategy", "Inventory/production", "Applications", "Policies", "Dynamic programming/optimal control", "Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0360", "e:abstract": "Motivated by the significant advances in integer optimization in the past decade, we introduce mixed-integer optimization methods to the classical statistical problems of classification and regression and construct a software package called CRIO (classification and regression via integer optimization). CRIO separates data points into different polyhedral regions. In classification each region is assigned a class, while in regression each region has its own distinct regression coefficients. Computational experimentations with generated and real data sets show that CRIO is comparable to and often outperforms the current leading methods in classification and regression. We hope that these results illustrate the potential for significant impact of integer optimization methods on computational statistics and data mining.", "e:keyword": ["Programming", "Integer", "Applications", "Statistics", "Nonparametric"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0367", "e:abstract": "Model reference adaptive search (MRAS) for solving global optimization problems works with a parameterized probabilistic model on the solution space and generates at each iteration a group of candidate solutions. These candidate solutions are then used to update the parameters associated with the probabilistic model in such a way that the future search will be biased toward the region containing high-quality solutions. The parameter updating procedure in MRAS is guided by a sequence of implicit probabilistic models we call reference models. We provide a particular algorithm instantiation of the MRAS method, where the sequence of reference models can be viewed as the generalized probability distribution models for estimation of distribution algorithms (EDAs) with proportional selection scheme. In addition, we show that the model reference framework can also be used to describe the recently proposed cross-entropy (CE) method for optimization and to study its properties. Hence, this paper can also be seen as a study on the effectiveness of combining CE and EDAs. We prove global convergence of the proposed algorithm in both continuous and combinatorial domains, and we carry out numerical studies to illustrate the performance of the algorithm.", "e:keyword": ["Programming", "Nondifferentiable", "Nonlinear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0368", "e:abstract": "We formally derive the standard deterministic linear program (LP) for bid-price control by making an affine functional approximation to the optimal dynamic programming value function. This affine functional approximation gives rise to a new LP that yields tighter bounds than the standard LP. Whereas the standard LP computes static bid prices, our LP computes a time trajectory of bid prices. We show that there exist dynamic bid prices, optimal for the LP, that are individually monotone with respect to time. We provide a column generation procedure for solving the LP within a desired optimality tolerance, and present numerical results on computational and economic performance.", "e:keyword": ["Revenue management", "Pricing", "Network", "Bid prices", "Dynamic programming/optimal control", "Applications", "Approximate"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0369", "e:abstract": "Higher-dimensional orthogonal packing problems have a wide range of practical applications, including packing, cutting, and scheduling. Combining the use of our data structure for characterizing feasible packings with our new classes of lower bounds, and other heuristics, we develop a two-level tree search algorithm for solving higher-dimensional packing problems to optimality. Computational results are reported, including optimal solutions for all two-dimensional test problems from recent literature.This is the third in a series of articles describing new approaches to higher-dimensional packing.", "e:keyword": ["Production", "Scheduling", "Cutting stock", "Trim"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0370", "e:abstract": "This paper considers the provisioning of transmission line bandwidth on a private network with given traffic routing for the purpose of distribution of video-on-demand service with guaranteed end-to-end quality of service. We present an architecture for video-on-demand service delivery and model the assignment of bandwidth in the distribution network as a constrained, nonlinear optimization problem. To solve this optimization problem, we develop three new auction algorithm-based solution procedures. The optimization problem assumes that there is a functional relationship between the maximum acceptable end-to-end delay and the bandwidth requirement for the links in the distribution network. Absent reliable video traffic data models for MPEG-2 format, we sample a large number of DVD-recorded movies to form a basis for randomly generated aggregate traffic streams. The aggregate traffic streams are used in a simulation experiment to measure the maximum transmission buffer occupancy for each given traffic stream for different transmission rates. Based on this simulation experiment, we derive an empirical transmission line provisioning function that guarantees delivery of all video frames without frame loss within a maximum frame delay tolerance. To illustrate the effectiveness of the proposed solution procedure for the bandwidth assignment problem, we solve to near optimality 570 small problem instances under three demand structures and 100 large problem instances with uniformly distributed demand.", "e:keyword": ["Telecommunications", "Mobile networks", "Revenue management", "Queueing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0371", "e:abstract": "We combine mixed-integer linear programming (MILP) and constraint programming (CP) to solve an important class of planning and scheduling problems. Tasks are allocated to facilities using MILP and scheduled using CP, and the two are linked via logic-based Benders decomposition. Tasks assigned to a facility may run in parallel subject to resource constraints (cumulative scheduling). We solve problems in which the objective is to minimize cost, makespan, or total tardiness. We obtain significant computational speedups, of several orders of magnitude for the first two objectives, relative to the state of the art in both MILP and CP. We also obtain better solutions and bounds for problems than cannot be solved to optimality.", "e:keyword": ["Integer programming", "Benders decomposition", "Production/scheduling", "Planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0372", "e:abstract": "In an assemble-to-order system, a wide variety of products are rapidly assembled from component inventories in response to customer orders. We assume that orders must be filled within a product-specific target lead time. In the event that some of the components required to fill an order are out of stock, these components must be expedited. The objective is to minimize the expected infinite-horizon discounted cost of primary component production and expediting. Our formulation captures financial holding costs but implicitly assumes that physical holding costs are negligible. The controls are (1) sequencing orders for assembly, (2) primary component production, and (3) component expediting. We prove that the multidimensional assemble-to-order control problem separates into single-item inventory control problems. In particular, under an optimal policy for assembly sequencing, the optimal production and expediting policy for each component is independent of all other components. Hence, the literature on single-item inventory management with expediting or lost sales is directly relevant to the control of assemble-to-order systems.", "e:keyword": ["Inventory", "Production", "Assemble-to-order", "Delay constraints", "Expediting", "Stochastic optimal control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0373", "e:abstract": "This paper develops simple approximate methods to analyze a two-stage distribution system consisting of one warehouse and multiple retailers with stochastic demand. We consider local and central control schemes. The main ideas are based on relaxing and or decomposing the system into more manageable newsvendor-type subsystems. We also provide bounds on the optimal policy and the optimal expected cost. We show that one of the heuristics is asymptotically optimal in the number of retailers. These results provide practically useful techniques as well as insights into stock-positioning issues and the drivers of system performance.", "e:keyword": ["Inventory", "Production", "Multi-item", "Echelon", "Stage", "Approximation", "Heuristics", "Uncertainty", "Stochastic", "Operating characteristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0374", "e:abstract": "For a real-world problem---transporting pallets between warehouses to guarantee sufficient supply for known and additional stochastic demand---we propose a solution approach via convex relaxation of an integer programming formulation, suitable for online optimization. The essential new element linking routing and inventory management is a convex piecewise-linear cost function that is based on minimizing the expected number of pallets that still need transportation. For speed, the convex relaxation is solved approximately by a bundle approach yielding an online schedule in five to 12 minutes for up to three warehouses and 40,000 articles; in contrast, computation times of state-of-the-art LP solvers are prohibitive for online application. In extensive numerical experiments on a real-world data stream, the approximate solutions exhibit negligible loss in quality; in long-term simulations the proposed method reduces the average number of pallets needing transportation due to short-term demand to less than half the number observed in the data stream.", "e:keyword": ["Convex relaxation", "Integer programming", "Stochastic demand", "Network models", "Large-scale problems", "Bundle method", "Logistics", "Vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0375", "e:abstract": "We introduce and investigate approximations for the probability distribution of the maximum of <i>n</i> independent and identically distributed nonnegative random variables, in terms of the number <i>n</i> and the first few moments of the underlying probability distribution, assuming the distribution is unbounded above but does not have a heavy tail. Because the mean of the underlying distribution can immediately be factored out, we focus on the effect of the squared coefficient of variation (SCV, <i>c</i><sup>2</sup>, variance divided by the square of the mean). Our starting point is the classical extreme-value theory for representative distributions with the given SCV---mixtures of exponentials for <i>c</i><sup>2</sup> (ge) 1, convolutions of exponentials for <i>c</i><sup>2</sup> (le) 1, and gamma for all <i>c</i><sup>2</sup>. We develop approximations for the asymptotic parameters and evaluate their performance. We show that there is a minimum threshold <i>n</i><sup>*</sup>, depending on the underlying distribution, with <i>n</i> (ge) <i>n</i><sup>*</sup> required for the asymptotic extreme-value approximations to be effective. The threshold <i>n</i><sup>*</sup> tends to increase as <i>c</i><sup>2</sup> increases above one or decreases below one.", "e:keyword": ["Probability", "Distributions", "Maximum of independent random variables", "Probability", "Distributions", "Two-moment approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0376", "e:abstract": "We consider a single-item, periodic-review, serial inventory/production system, with linear inventory-holding and penalty costs. To facilitate shipment consolidation and capacity planning, we assume that the system has implemented fixed replenishment intervals; each stage is allowed to order only at given equidistant times. Further, for each stage except the most downstream one, the replenishment interval is assumed to be an integer multiple of the replenishment interval of the next downstream stage. This reflects the fact that the further upstream in a supply chain, the higher setup times and costs tend to be, and thus larger batches are desired. Our model with fixed replenishment intervals is a direct generalization of the serial model of Clark and Scarf (1960). For this generalized model, we prove the optimality of base-stock policies, we derive newsboy equations for the optimal base-stock levels, and we describe an efficient exact solution procedure for the case with mixed Erlang demands. Finally, we present extensions to assembly systems and to systems with a modified fill-rate constraint instead of backorder costs.", "e:keyword": ["Inventory/production", "Multiechelon", "Stochastic demand", "Fixed replenishment intervals", "Optimal policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0377", "e:abstract": "This technical note presents a screening technique for using chance-constrained programming to achieve overall system (i.e., joint) reliability when there is statistical dependence between constraints representing an ambient air-quality requirement at different geographical locations. The technique is intended to determine whether the full analysis of row interdependence, which requires more intensive programming and larger computational effort, is warranted, by examining a possible spectrum of solutions at three extreme cases of row dependence. The technique is illustrated for airborne particulate emissions control, in which the overall cost of controlling particulate emissions from two electrostatic precipitators is minimized in a manner that maintains ground-level particulate concentration at all receptors with a prescribed reliability.In accordance with the theory presented here, such screening is achieved by setting the required reliability values of individual constraints according to assumptions of complete codependence, zero codependence, and complete negative codependence. In application, these reliability values represent the probability of achieving the ambient concentration standard at several receptor locations. The results of the screening technique are compared to those of two more computationally intensive methods for achieving overall system reliability. It is found that, for a given example, the screening technique brackets the results of those full-analysis methods for row dependence, as expected.", "e:keyword": ["Environment", "Air quality control", "Programming", "Joint chance constraint"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0378", "e:abstract": "We present structural and computational investigations of a new class of weak forecast horizons---minimal forecast horizons under the assumption that future demands are integer multiples of a given positive real number---for a specific class of dynamic lot-size (DLS) problems. Apart from being appropriate in most practical instances, the discreteness assumption offers a significant reduction in the length of a minimal forecast horizon over the one using the classical notion of continuous future demands. We provide several conditions under which a discrete-demand forecast horizon is also a continuous-demand forecast horizon. We also show that the increase in the cost resulting from using a discrete minimal forecast horizon instead of the classical minimal forecast horizon is modest. The discreteness assumption allows us to characterize forecast horizons as feasibility/optimality questions in 0-1 mixed-integer programs. On an extensive test bed, we demonstrate the computational tractability of the integer programming approach. Owing to its prevalence in practice, our computational experiments emphasize the special case of integer future demands.", "e:keyword": ["Inventory/production", "Planning horizons", "Programming", "Integer", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0379", "e:abstract": "We study a new class of decentralized algorithms for discrete optimization via simulation, which is inspired by the fictitious play algorithm applied to games with identical interests. In this approach, each component of the solution vector of the optimization model is artificially assumed to have a corresponding “player,” and the interaction of these players in simulation allows for exploration of the solution space and, for some problems, ultimately results in the identification of the optimal solution. Our algorithms also allow for correlation in players’ decision making, a key feature when simulation output is shared by multiple decision makers. We first establish convergence under finite sampling to equilibrium solutions. In addition, in the context of discrete network flow models, we prove that if the underlying link cost functions are convex, then our algorithms converge almost surely to an optimal solution.", "e:keyword": ["Simulation", "Games/group decisions", "Networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0380", "e:abstract": "When a manufacturer places repeated orders with a supplier to meet changing production requirements, he faces the challenge of finding the right balance between holding costs and the operational costs involved in adjusting the shipment sizes. We consider an inventory whose content fluctuates as a Brownian motion in the absence of control. At any moment, a controller can adjust the inventory level by any positive or negative quantity, but incurs both a fixed cost and a cost proportional to the magnitude of the adjustment. The inventory level must be nonnegative at all times and continuously incurs a linear holding cost. The objective is to minimize long-run average cost. We show that control band policies are optimal for the average cost Brownian control problem and explicitly calculate the parameters of the optimal control band policy. This form of policy is described by three parameters {<i>q,Q,S</i>}, 0 < <i>q</i> (le) <i>Q</i> < <i>S</i>. When the inventory falls to zero (rises to <i>S</i>), the controller expedites (curtails) shipments to return it to <i>q</i> (<i>Q</i>). Employing apparently new techniques based on methods of Lagrangian relaxation, we show that this type of policy is optimal even with constraints on the size of adjustments and on the maximum inventory level. We also extend these results to the discounted cost problem.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1060.0386", "e:abstract": "In this paper, we consider the optimal management of capacity when a firm faces fixed costs and variable costs to purchase capacity. The firm can also salvage capacity and receive a variable value per unit capacity salvaged, but faces a (different) fixed cost in this case. Each period, the firm faces a stochastic demand, and maintenance costs for capacity that it decides to keep. The firm would thus like to decide how much capacity it should purchase or salvage each period.We introduce a new concept, which we call (<i>K</i><sub>1</sub>, <i>K</i><sub>2</sub>)-concavity, and show that the profit-to-go function satisfies this property. This enables us to characterize the structure of an optimal policy, which is rather complex, consisting of multiple regions in which different decisions are made. We show how special cases of this problem (e.g., no fixed costs, expansion or contraction not allowed) reduce to well-known results, and how (<i>K</i><sub>1</sub>, <i>K</i><sub>2</sub>)-concavity is a generalization of concavity, <i>K</i>-concavity, and sym-<i>K</i>-concavity. We also show how different lead times for purchasing or salvaging capacity can be integrated into the model. Finally, we extend the model to the case where demand is Markov modulated, and a portion of capacity can deteriorate in each period.", "e:keyword": ["Dynamic programming/optimal control", "Models", "Facilities/equipment planning", "Capacity expansion", "Inventory production", "Policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0364", "e:abstract": "We study the integrated operational and financial hedging decisions faced by a global firm who sells to both home and foreign markets. Production occurs either at a single facility located in one of the markets or at two facilities, one in each market. The company has to invest in capacity before the selling season starts when the demand in both markets and the currency exchange rate are uncertain. The currency exchange rate risk can be hedged by delaying allocation of the capacity to specific markets until both the currency and demand uncertainties are resolved and/or by buying financial option contracts on the currency exchange rate when capacity commitment is made. A mean-variance utility function is used to model the firm’s risk aversion in decision making. We derive the joint optimal capacity and financial option decision, and analyze the impact of the delayed allocation option and the financial options on capacity commitment and the firm’s performance. We show that the firm’s financial hedging strategy ties closely to, and can have both quantitative and qualitative impact on, the firm’s operational strategy. The use, or lack of use of financial hedges, can go beyond affecting the magnitude of capacity levels by altering global supply chain structural choices, such as the desired location and number of production facilities to be employed to meet global demand.", "e:keyword": ["Inventory/production", "Capacity", "Allocation", "Stochastic", "Finance", "Hedging", "Currency exchange rate", "Utility", "Mean-variance"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0383", "e:abstract": "We study the problem of minimizing the maximum latency of flows in networks with congestion. We show that this problem is NP-hard, even when all arc latency functions are linear and there is a single source and sink. Still, an optimal flow and an equilibrium flow share a desirable property in this situation: All flow-carrying paths have the same length, i.e., these solutions are “fair,” which is in general not true for optimal flows in networks with nonlinear latency functions. In addition, the maximum latency of the Nash equilibrium, which can be computed efficiently, is within a constant factor of that of an optimal solution. That is, the so-called price of anarchy is bounded. In contrast, we present a family of instances with multiple sources and a single sink for which the price of anarchy is unbounded, even in networks with linear latencies. Furthermore, we show that an <i>s</i>-<i>t</i>-flow that is optimal with respect to the average latency objective is near-optimal for the maximum latency objective, and it is close to being fair. Conversely, the average latency of a flow minimizing the maximum latency is also within a constant factor of that of a flow minimizing the average latency.", "e:keyword": ["Networks/graphs", "Multicommodity", "Theory", "Games/group decisions", "Noncooperative", "Nonatomic", "Transportation", "Models", "Network"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0384", "e:abstract": "In theory, the mathematically elegant Vickrey-Clarke-Groves process offers perfect efficiency with dominant truth-revealing strategies. However, it has many serious practical problems. This paper describes these problems and argues that research that aims to maintain the dominant truth-revealing strategies while compromising on the other practical issues is of limited practical value.", "e:keyword": ["Auctions", "Mechanism design", "Truth-revealing strategies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0385", "e:abstract": "In the area of dynamic revenue management, optimal pricing policies are typically computed on the basis of an underlying demand rate model. From the perspective of applications, this approach implicitly assumes that the model is an accurate representation of the real-world demand process and that the parameters characterizing this model can be accurately calibrated using data. In many situations, neither of these conditions are satisfied. Indeed, models are usually simplified for the purpose of tractability and may be difficult to calibrate because of a lack of data. Moreover, pricing policies that are computed under the assumption that the model is correct may perform badly when this is not the case. This paper presents an approach to single-product dynamic revenue management that accounts for errors in the underlying model at the optimization stage. Uncertainty in the demand rate model is represented using the notion of relative entropy, and a tractable reformulation of the “robust pricing problem” is obtained using results concerning the change of probability measure for point processes. The optimal pricing policy is obtained through a version of the so-called Isaacs’ equation for stochastic differential games, and the structural properties of the optimal solution are obtained through an analysis of this equation. In particular, (i) closed-form solutions for the special case of an exponential nominal demand rate model, (ii) general conditions for the exchange of the “max” and the “min” in the differential game, and (iii) the equivalence between the robust pricing problem and that of single-product revenue management with an exponential utility function without model uncertainty, are established through the analysis of this equation.", "e:keyword": ["Robust control", "Model uncertainty", "Ambiguity", "Rate control", "Revenue management", "Dynamic pricing", "Girsanov Theorem", "Stochastic differential games", "Relative entropy", "Exponential utility", "Risk-sensitive control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0387", "e:abstract": "In the <i>m</i>-peripatetic salesman problem (<i>m</i>-PSP), the aim is to determine <i>m</i> edge disjoint Hamiltonian cycles of minimum total cost on a graph. This article introduces new valid inequalities and polyhedral results for the <i>m</i>-PSP. An improved 2-index branch-and-cut algorithm is developed. Tests performed on randomly generated and TSPLIB Euclidean instances indicate that this algorithm can solve instances with more than double the size of what was previously achievable.", "e:keyword": ["Mathematics", "Combinatorics", "Networks/graphs", "Traveling salesman", "Programming", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0388", "e:abstract": "In designing a production model for firms that generate multiple outputs, we take as a starting point that such multioutput production refers to economies of scope, which in turn originate from joint input use and input externalities. We provide a nonparametric characterization of cost-efficient behavior under these conditions, and subsequently institute necessary and sufficient conditions for data consistency with such efficient behavior that only include observed firm demand and supply data. We illustrate our methodology by examining the cost efficiency of research programs in economics and business management faculties of Dutch universities. This application shows that the proposed methodology may entail robust conclusions regarding cost-efficiency differences between universities within specific specialization areas, even when using shadow prices to evaluate the different inputs.", "e:keyword": ["Production behavior", "Multiproduct firms", "Input externalities", "Joint input use", "Economies of scope", "Nonparametric tests"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0389", "e:keyword": ["Facilities", "Equipment planning", "Location", "Continuous"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0392", "e:abstract": "We consider a family of <i>N</i> items that are produced in, or obtained from, the same production facility. Demands are deterministic for each item and each period within a given horizon of <i>T</i> periods. If in a given period an order is placed, setup costs are incurred. The aggregate order size is constrained by a capacity limit. The objective is to find a lot-sizing strategy that satisfies the demands for all items over the entire horizon without backlogging, and that minimizes the sum of inventory-carrying costs, fixed-order costs, and variable-order costs.All demands, cost parameters, and capacity limits may be time dependent. In the basic joint setup cost (JS) model, the setup cost of an order does not depend on the composition of the order. The joint and item-dependent setup cost (JIS) model allows for item-dependent setup costs in addition to the joint setup costs. We develop and analyze a class of so-called progressive interval heuristics. A progessive interval heuristic solves a JS or JIS problem over a progressively larger time interval, always starting with period 1, but fixing the setup variables of a progressively larger number of periods at their optimal values in earlier iterations. Different variants in this class of heuristics allow for different degrees of flexibility in adjusting continuous variables determined in earlier iterations of the algorithm.For the JS-model and the two basic implementations of the progressive interval heuristics, we show under some mild parameter conditions that the heuristics can be designed to be (epsilon)-optimal for any desired value of (epsilon) > 0 with a running time that is polynomially bounded in the size of the problem. They can also be designed to be simultaneously asymptotically optimal and polynomially bounded.A numerical study covering both the JS and JIS models shows that a progressive interval heuristic generates close-to-optimal solutions with modest computational effort and that it can be effectively used to solve large-scale problems.", "e:keyword": ["Inventory", "Production", "Multi-item", "Echelon", "Approximation", "Heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0393", "e:abstract": "We consider the dynamic pricing problem of a monopolist firm in a market with repeated interactions, where demand is sensitive to the firm’s pricing history. Consumers have memory and are prone to human decision-making biases and cognitive limitations. As the firm manipulates prices, consumers form a reference price that adjusts as an anchoring standard based on price perceptions. Purchase decisions are made by assessing prices as discounts or surcharges relative to the reference price in the spirit of prospect theory.We prove that optimal pricing policies induce a perception of monotonic prices, whereby consumers always perceive a discount, respectively surcharge, relative to their expectations. The effect is that of a skimming or penetration strategy. The firm’s optimal pricing path is monotonic on the long run, but not necessarily at the introductory stage. If consumers are loss averse, we show that optimal prices converge to a constant steady-state price, characterized by a simple implicit equation; otherwise, the optimal policy cycles. The range of steady states is wider the more loss averse consumers are. Steady-state prices decrease with the strength of the reference effect and with customers’ memory, all else equal. Offering lower prices to frequent customers may be suboptimal, however, if these are less sensitive to price changes than occasional buyers.If managers ignore such long-term implications of their pricing strategy, the model indicates that they will systematically price too low and lose revenue. Our results hold under very general reference dependent demand models.", "e:keyword": ["Dynamic programming", "Deterministic", "Marketing", "Pricing", "Promotion", "Buyer behavior", "Inventory policies", "Marketing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0394", "e:abstract": "We deal with an inventory system with limited storage space for a single item or multiple items. For the single-item system, customers' demand is stochastic. The inventory is controlled by a continuous-review (<i>r, Q</i>) policy. Goods are replenished to the inventory system with a constant lead time. An optimization problem with a storage-space constraint is formulated for computing a single-item (<i>r, Q</i>) policy that minimizes the long-run average system cost. Based on some existing results in the single-item (<i>r, Q</i>) policy without a storage-space constraint in the literature, useful structural properties of the optimization problem are attained. An efficient algorithm with polynomial time computational complexity is then proposed for obtaining the optimal solutions. For the multi-item system, each item possesses its particular customers' demand that is stochastic, its own (<i>r, Q</i>) policy that controls the inventory, and its individual lead time that is constant. An important issue in such inventory systems is the allocation of the storage space to the items and the values of <i>r</i> and <i>Q</i> for each item. We formulate an optimization problem with a storage-space constraint for multi-item (<i>r, Q</i>) policies. Based on the results in the single-item (<i>r, Q</i>) policy with a storage-space constraint, we find useful structural properties of the optimization problem. An efficient algorithm with polynomial time computational complexity is then proposed for obtaining undominated solutions.", "e:keyword": ["Inventory/production", "(r", "Q) policy", "Stochastic demand", "Programming", "Storage-space constraint", "Algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0395", "e:abstract": "The tactical planning process of an airline is typically decomposed into several stages among which fleeting, aircraft routing, and crew pairing form the core. In such a decomposed and sequential approach, the output of fleeting forms the input to aircraft routing and crew pairing. In turn, the output to aircraft routing is part of the input to crew pairing. Due to this decomposition, the resulting solution is often suboptimal. We propose a model that completely integrates the fleeting and crew-pairing stages and guarantees feasibility of plane-count feasible aircraft routings, but neglects aircraft maintenance constraints. We design two solution methodologies to solve the model. One is based on a combination of Lagrangian relaxation and column generation, while the other one is a Benders decomposition approach. We conduct computational experiments for a variety of instances obtained from a major carrier.", "e:keyword": ["Crew scheduling", "Capacity planning", "Large-scale optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0396", "e:abstract": "We consider the problem of locating a spherical circle with respect to existing facilities on a sphere, such that the sum of distances between the circle and the facilities is minimized or such that the maximum distance is minimized. The problem properties are analyzed, and we give solution procedures. When the circle to be located is restricted to be a great circle, some simplifications are possible. The models may be used in preliminary studies on the location of large linear facilities on the earth’s surface, such as superhighways, pipelines, and transmission lines, or in totally different contexts such as search-and-rescue missions and medical or biological studies.", "e:keyword": ["Facilities/equipment planning", "Location", "Continuous", "Circle on sphere"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0397", "e:abstract": "We propose a new resolution algorithm, called resolution branch and bound (RBB), where a branch-and-bound scheme is empowered by exploiting the information contained in a family of closed subproblems, collected by a full resolution phase. In particular, we use this information to define a new branching rule that seems able to reduce the risk of incurring inappropriate branchings. We apply RBB and the proposed branching rule to the maximum weighted stable set problem, as its features allow us to speed up a time-consuming step in the full resolution phase. To compute upper bounds, we generalize to the weighted case the polynomial time procedure provided by Mannino and Sassano [Mannino, C., A. Sassano. 1994. An exact algorithm for the maximum stable set problem. <i>Computational Optim. Appl.</i> <b>3</b> 243--258] for the unweighted case. Computational results validate the effectiveness of the provided branching rule and the good performance of RBB on many DIMACS benchmarks.", "e:keyword": ["Mathematics", "Combinatorics", "Programming", "Integer", "Algorithms", "Branch and bound", "Networks/graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0398", "e:abstract": "The operations research literature contains numerous studies on the design and application of optimization and heuristic solution procedures. These studies identify a particular optimization problem, suggest a general solution procedure, and then customize that procedure to improve its efficiency and/or accuracy. In contrast, this paper shows how to use existing solution procedures more effectively. We develop a methodology for predicting the relative performance of alternative procedures, using easily computed problem characteristics. This methodology enables us, for any given data set, to preselect a solution procedure. We apply this preselection methodology to the 0-1 knapsack problem for which two successful optimization procedures, dynamic programming and branch-and-search, are available. Extensive computational testing indicates that substantial savings in average computation time are achieved. The benefits of our work include faster and cheaper identification of effective solution procedures, as well as an improved understanding of the relationship between problem characteristics and the performance of various procedures. Our methodology can be applied to many optimization problems to develop easily implemented guidelines for selecting appropriate solution procedures.", "e:keyword": ["Computational testing", "Prediction of solution procedure performance", "Choice of solution procedure", "Statistical analysis", "Programming", "Integer", "Algorithms", "Heuristics", "Analysis of algorithms", "Suboptimal algorithms", "Statistics", "Data analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0399", "e:abstract": "The focus of this “OR framing paper” is to introduce the operations research (OR) community to the need for new mathematical modeling of an influenza pandemic and its control. By reviewing relevant history and literature, one key concern that emerges relates to how a population’s heterogeneity may affect disease progression. Another is to explore within a modeling framework “social distancing” as a disease progression control method, where social distancing refers to steps aimed at reducing the frequency and intensity of daily human-to-human contacts. To depict social contact behavior of a heterogeneous population susceptible to infection, a nonhomogeneous probabilistic mixing model is developed. Partitioning the population of susceptibles into subgroups, based on frequency of daily human contacts and infection propensities, a stylistic difference equation model is then developed depicting the day-to-day evolution of the disease. This simple model is then used to develop a preliminary set of results. Two key findings are (1) early exponential growth of the disease may be dominated by susceptibles with high human contact frequencies and may not be indicative of the general population’s susceptibility to the disease, and (2) social distancing may be an effective nonmedical way to limit and perhaps even eradicate the disease. Much more decision-focused research needs to be done before any of these preliminary findings may be used in practice.", "e:keyword": ["Health care", "Epidemiology", "Philosophy of modeling", "Community", "Probability", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0400", "e:abstract": "We consider congestion pricing as a mechanism for sharing bandwidth in communication networks, and model the interaction among the users as a game. We propose a decentralized algorithm for the users that is based on the history of the price process, where user response to congestion prices is analogous to “fictitious play” in game theory, and show that this results in convergence to the unique Wardrop equilibrium. We further show that the Wardrop equilibrium coincides with the welfare-maximizing capacity allocation.", "e:keyword": ["Communications", "Computer networks", "Games", "Noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0401", "e:abstract": "We develop an algorithm for determining the highest among the class of lower bounds proposed by Atkins and Iyogun (1988) for the joint replenishment problem (JRP) with stochastic demands. The proposed algorithm is simple and does not require many more calculations than an existing lower bound based on equalization of expected runout times.", "e:keyword": ["Inventory/production", "Stochastic model", "Joint replenishment", "Lower bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0402", "e:abstract": "In this paper, we explore the impact of decentralized decision making on the behavior of multiproduct assembly systems. Specifically, we consider a system where three components (two product specific and one common) are used to produce two end products to satisfy stochastic customer demands. We study the system under both centralized and decentralized decision making. In the decentralized system, we prove that for any set of wholesale prices, there exists a unique Pareto-optimal equilibrium in the suppliers’ capacity game. We show that the assembler’s optimal wholesale prices lie in one of two regions---one leads to capacity imbalance and one does not. We use these results to derive insights regarding the inefficiencies that decentralization can cause in such systems. In particular, several of our findings indicate that outsourcing the management of component supplies may inhibit the use of operational hedging approaches for managing uncertainty.", "e:keyword": ["Games", "Group decisions", "Noncooperative", "Inventory", "Production", "Multi-item", "Echelon", "Stage", "Uncertainty", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0403", "e:abstract": "In a two-sided version of the famous secretary problem, employers search for a secretary at the same time as secretaries search for an employer. Nobody accepts being put on hold, and nobody is willing to take part in more than <i>N</i> interviews. Preferences are independent, and agents seek to optimize the expected rank of the partner they obtain among the <i>N</i> potential partners. We find that in any subgame perfect equilibrium, the expected rank grows as the square root of <i>N</i> (whereas it tends to a constant in the original secretary problem). We also compute how much agents can gain by cooperation.", "e:keyword": ["Games/group decisions", "Strategic secretary problem", "Dynamic programming/optimal control", "Optimal stopping"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0404", "e:abstract": "Although the textbook Dantzig-Wolfe decomposition reformulation for the capacitated lot-sizing problem, as already proposed by Manne [Manne, A. S. 1958. Programming of economic lot sizes. <i>Management Sci.</i> <b>4</b>(2) 115--135], provides a strong lower bound, it also has an important structural deficiency. Imposing integrality constraints on the columns in the master program will not necessarily give the optimal integer programming solution. Manne's model contains only production plans that satisfy the Wagner-Whitin property, and it is well known that the optimal solution to a capacitated lot-sizing problem will not necessarily satisfy this property. The first contribution of this paper answers the following question, unsolved for almost 50 years: If Manne's formulation is not equivalent to the original problem, what is then a correct reformulation? We develop an equivalent mixed-integer programming (MIP) formulation to the original problem and show how this results from applying the Dantzig-Wolfe decomposition to the original MIP formulation. The set of extreme points of the lot-size polytope that are needed for this MIP Dantzig-Wolfe reformulation is much larger than the set of dominant plans used by Manne. We further show how the integrality restrictions on the original setup variables translate into integrality restrictions on the new master variables by separating the setup and production decisions. Our new formulation gives the same lower bound as Manne's reformulation. Second, we develop a branch-and-price algorithm for the problem. Computational experiments are presented on data sets available from the literature. Column generation is accelerated by a combination of simplex and subgradient optimization for finding the dual prices. The results show that branch-and-price is computationally tractable and competitive with other state-of-the-art approaches found in the literature.", "e:keyword": ["Integer programming", "Algorithms", "Decomposition", "Column generation", "Branch-and-price", "Inventory/production: lot sizing", "Setup times"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0405", "e:abstract": "John Deere & Company (Deere), one of the world’s leading producers of machinery, manufactures products composed of various features, within which a customer may select one of a number of possible options. On any given Deere product line, there may be tens of thousands of combinations of options (configurations) that are feasible. Maintaining such a large number of configurations inflates overhead costs; consequently, Deere wishes to reduce the number of configurations from their product lines without upsetting customers or sacrificing profits. In this paper, we provide a detailed explanation of the marketing and operational methodology used, and tools built, to evaluate the potential for streamlining two product lines at Deere. We illustrate our work with computational results from Deere, highlighting important customer behavior characteristics that impact product line diversity. For the two very different studied product lines, a potential increase in profit from 8% to 18% has been identified, possible through reducing the number of configurations by 20% to 50% from present levels, while maintaining the current high customer service levels. Based on our analysis and the insights it generated, Deere recently implemented a new product line strategy. We briefly detail this strategy, which has thus far increased profits by tens of millions of dollars.", "e:keyword": ["Industries", "Machinery", "Marketing", "Retail/product line optimization", "Production", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0406", "e:abstract": "We consider two models of stochastic serial inventory systems with economies of scale for which the forms of optimal policies are known. In the first model, each stage has a fixed-order quantity, while in the second model, there is a fixed-order cost for external supplies. For each model, we show that the optimal policy parameters can be bounded and approximated by a series of independent, single-stage optimal policy parameters. We further construct closed-form bounds and approximations for the single-stage solutions and apply them to the serial systems. These results provide simple and effective solutions that will help to facilitate implementations in practice. They also allow us to see the connections between the serial and single-stage systems and sharpen our intuition on optimal policy parameters and system behavior.", "e:keyword": ["Inventory policies", "Bounds", "Approximations", "Batch ordering"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0407", "e:abstract": "Data perturbation and query restriction are two methods developed to protect confidential data in statistical databases. In the former, the data is systematically changed to yield answers to queries that are statistically similar to those that would have resulted from the original data. The latter provides exact answers to queries as long as the risk of exact disclosure of confidential data does not become too great. We present a new methodology to combine these techniques so that the advantages of both are captured. The model is appropriate and computationally viable for large databases whether the queries are linear or nonlinear. The query restriction phase consists of finding an optimal subset of queries to answer exactly without compromising the database. This is an (N-script) (P-script)-hard problem with a matroid intersection structure that lends itself to an efficient greedy heuristic. Then, given the queries that are answered exactly, we implement a data perturbation phase that provides stochastic protection and consistency. We present computational results on a large database with both linear and nonlinear queries. The results indicate that many queries can be answered exactly and the proposed perturbation approach provides more accurate answers than the standard perturbation method.", "e:keyword": ["Statistical databases", "Database security", "Query restriction", "Data perturbation", "Matroid intersection", "Hilbert spaces", "Nonlinear least-squares estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0408", "e:abstract": "Motivated by one of the leading intermodal logistics suppliers in the United States, we consider an internal pricing mechanism for managing a fleet of service units (shipping containers) flowing in a closed queueing network. Nodes represent geographic locations, and arcs represent travel between them. Customer requests for arcs arrive over time, and the problem is to find an accept/reject policy that maximizes the long-run time average reward rate from accepting requests. We formulate the problem as a semi-Markov decision process and give a simple linear program that provides an upper bound on the optimal reward rate. Using Palm calculus, we derive a nonlinear program that approximately captures queueing and stockout effects on the network. Using its optimal Lagrange multipliers, we construct a simple functional approximation to the dynamic programming value function. The resulting policy is computationally efficient and produces superior economic performance as compared with other policies. Furthermore, it provides a methodologically grounded solution to the firm's internal pricing problem.", "e:keyword": ["Dynamic programming/optimal control", "Semi-Markov", "Programming", "Nonlinear", "Queues", "Networks", "Industries", "Transportation/shipping"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0409", "e:abstract": "Assortment planning at a retailer entails both selecting the set of products to be carried and setting inventory levels for each product. We study an assortment planning model in which consumers might accept substitutes when their favorite product is unavailable. We develop an algorithmic process to help retailers compute the best assortment for each store. First, we present a procedure for estimating the parameters of substitution behavior and demand for products in each store, including the products that have not been previously carried in that store. Second, we propose an iterative optimization heuristic for solving the assortment planning problem. In a computational study, we find that its solutions, on average, are within 0.5% of the optimal solution. Third, we establish new structural properties (based on the heuristic solution) that relate the products included in the assortment and their inventory levels to product characteristics such as gross margin, case-pack sizes, and demand variability. We applied our method at Albert Heijn, a supermarket chain in The Netherlands. Comparing the recommendations of our system with the existing assortments suggests a more than 50% increase in profits.", "e:keyword": ["Inventory", "Multi-item", "Stochastic", "Applications", "Heuristics", "Marketing", "Retailing", "Estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0410", "e:abstract": "We study the optimal control of a firm with two capacitated manufacturing plants situated in two distinct geographical regions. Demands from each region are mostly satisfied by the local plant. However, if necessary, some of the newly arrived demands can be designated to be served by the other, more remote, plant. The sources of the above virtual lateral transshipments, unlike the ones involved in the real lateral transshipments, do not need to have nonnegative inventory levels throughout the transshipment processes. We develop results about the simultaneous preservation of supermodularity and diagonal dominances of the cost functions and then use these results to derive structural results for the optimal policies. Specifically, to perform optimally, we find that each plant should adopt a modified base-stock policy in observance of its own capacity, while the base level should mildly decrease in the other plant's starting inventory level. Our computational results illustrate the benefits realizable by using transshipment.", "e:keyword": ["Dynamic programming/optimal control", "Markov", "Inventory/production", "Multiechelon", "Manufacturing", "Strategy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0411", "e:abstract": "In most deterministic manufacturing decision models, demand is either known or induced by pricing decisions in the period that the demand is experienced. However, in more realistic market scenarios consumers make purchase decisions with respect to price, not only in the current period, but also in past and future periods. We model a joint manufacturing/pricing decision problem, accounting for that portion of demand realized in each period that is induced by the interaction of pricing decisions in the current period and in previous periods. We formulate a mathematical programming model and develop solution techniques. We identify structural properties of our models and develop closed-form solutions and effective heuristics for various special cases of our models. Finally, we conduct extensive computational experiments to quantify the effectiveness of our heuristics and to develop managerial insights.", "e:keyword": ["Inventory/production", "Uncertainty", "Deterministic", "Planning horizons", "Operating characteristics", "Marketing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0412", "e:abstract": "We study conflict and cooperation issues in supply chain manufacturing. Consider an assembly system where suppliers provide parts to a manufacturer. A product cannot be delivered until all its parts have been supplied. The manufacturer performs nonbottleneck operations, for example, outsourced assembly, packaging, and delivery for each product. Two classical scheduling objectives are considered: minimization of the total completion time and of the maximum lateness. We analyze how far from optimal the best schedule for a suppliers' scheduling problem can be for the corresponding manufacturer's problem, and vice versa. To resolve these conflicts, we consider four alternative scenarios for the relative bargaining power of the suppliers and the manufacturer, and in each case describe a practical mechanism for cooperation between the decision makers. Evaluating the cost of conflict and the benefit of cooperation in these scenarios requires the solution of various scheduling problems by the suppliers, the manufacturer, and the overall system. For all these scheduling problems, we provide either an efficient algorithm or a proof of intractability. Moreover, for two problems that we show are intractable, we describe heuristics and analyze their worst case performance or demonstrate asymptotic optimality of their solutions. We demonstrate computationally that the cost saving realized by cooperation between the decision makers is significant in many cases. Extensions of our models to consider bottleneck operations at the manufacturer and transportation times are also developed.", "e:keyword": ["Supply chain scheduling", "Decision making", "Cooperation", "Cost of conflict", "Cooperation", "Benefit"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0413", "e:abstract": "One of the most common approaches for multiobjective optimization is to generate the whole or partial efficient frontier and then decide about the preferred solution in a higher-level decision-making process. In this paper, a new method for generating the efficient frontier for multiobjective problems is developed, called the diversity maximization approach (DMA). This approach is capable of solving mixed-integer and combinatorial problems. The DMA finds Pareto optimal solutions by maximizing a proposed diversity measure and guarantees generating the complete set of efficient points. Given a subset of the efficient frontier, DMA finds the next Pareto optimal solution which, combined with the existing ones, yields the most diversified subset of efficient points. This solution is defined as <i>the most diverse solution</i>. In fact, it aims to maximize the distance between the new efficient point and the closest point in the given subset of the efficient frontier. The proposed approach can be applied to any problem that can be solved for the single-objective case. We can use the DMA by solving directly a modified version of the mixed-integer programming (MIP) formulation of the multiobjective problem. In this case, the Pareto optimal solutions are found sequentially in an iterative way. Consequently, as we terminate the procedure before completion, a partial efficient frontier is available. The diversity measure assures that in every stage of the procedure, the partial efficient frontier is well diversified. This partial efficient frontier can be perceived as a filtered set of the complete efficient frontier and can be used by the decision maker in case the complete efficient frontier contains too many points. An additional way of using DMA is by incorporating it in a problem oriented branch-and-bound algorithm. Detailed examples of both approaches are given.", "e:keyword": ["Programming", "Decision making", "Multiple criteria", "Production/scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0414", "e:abstract": "Technological advances in digitalization and communications technologies have aggravated the information goods piracy problem. In contrast to previous literature which mainly considers solutions, such as law enforcement or technology protection that work on increasing individual piracy costs to alleviate the piracy problem, we consider using versioning as a potential instrument to fight piracy. We show that while a single version is the optimal strategy for an information goods provider absent piracy, the presence of piracy may lead firms to offer more than one quality, and versioning can be an effective and profitable instrument to fight piracy for digital information goods under some conditions. Our results indicate that the incentive to version is greater when the piracy cost is in a certain range. This strategic interaction between piracy cost and product line design has an interesting implication on optimal investment in piracy control. We also find that versioning can act as both a strategic substitute and a strategic complement to other instruments that increase consumer piracy costs. We further provide a general model, as a nonlinear mixed-integer program, that would assist an information goods provider in determining how many versions to offer, at what quality levels and prices.", "e:keyword": ["Information systems", "Digital goods piracy", "Programming", "Nonlinear mixed integer", "Marketing", "Versioning", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0415", "e:abstract": "The ordered open-end bin-packing problem is a variant of the bin-packing problem in which the items to be packed are sorted in a given order and the capacity of each bin can be exceeded by the last item packed into the bin. We present a branch-and-price algorithm for its exact optimization. The pricing subproblem is a special variant of the binary knapsack problem, in which the items are ordered and the last one does not consume capacity. We present a specialized optimization algorithm for this subproblem. The speed of the column generation algorithm is improved by subgradient optimization steps, allowing for multiple pricing and variable fixing. Computational results are presented on instances of different sizes and items with different correlations between their size and their position in the given order.", "e:keyword": ["Combinatorial optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0416", "e:abstract": "A model of two-settlement electricity markets is introduced, which accounts for flow congestion, demand uncertainty, system contingencies, and market power. We formulate the subgame perfect Nash equilibrium for this model as an equilibrium problem with equilibrium constraints (EPEC), in which each firm solves a mathematical program with equilibrium constraints (MPEC). The model assumes linear demand functions, quadratic generation cost functions, and a lossless DC network, resulting in equilibrium constraints as a parametric linear complementarity problem (LCP). We introduce an iterative procedure for solving this EPEC through repeated application of an MPEC algorithm. This MPEC algorithm is based on solving quadratic programming subproblems and on parametric LCP pivoting. Numerical examples demonstrate the effectiveness of the MPEC and EPEC algorithms and the tractability of the model for realistic-size power systems.", "e:keyword": ["Noncooperative games", "Cournot equilibrium", "Electricity market", "Two settlements", "Programming", "Mathematical program with equilibrium constraints", "Equilibrium problem with equilibrium constraints", "Linear complementarity problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0249", "e:keyword": ["OR/MS history", "Editorial comments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0419", "e:abstract": "We propose a new computational method for the valuation of options in jump-diffusion models. The option value function for European and barrier options satisfies a partial integrodifferential equation (PIDE). This PIDE is commonly integrated in time by implicit-explicit (IMEX) time discretization schemes, where the differential (diffusion) term is treated implicitly, while the integral (jump) term is treated explicitly. In particular, the popular IMEX Euler scheme is first-order accurate in time. Second-order accuracy in time can be achieved by using the IMEX midpoint scheme. In contrast to the above approaches, we propose a new high-order time discretization scheme for the PIDE based on the extrapolation approach to the solution of ODEs that also treats the diffusion term implicitly and the jump term explicitly. The scheme is simple to implement, can be added to any PIDE solver based on the IMEX Euler scheme, and is remarkably fast and accurate. We demonstrate our approach on the examples of Merton's and Kou's jump-diffusion models, the diffusion-extended variance gamma model, as well as the two-dimensional Duffie-Pan-Singleton model with correlated and contemporaneous jumps in the stock price and its volatility. By way of example, pricing a one-year double-barrier option in Kou's jump-diffusion model, our scheme attains accuracy of 10<sup>-5</sup> in 72 time steps (in 0.05 seconds). In contrast, it takes the first-order IMEX Euler scheme more than 1.3 million time steps (in 873 seconds) and the second-order IMEX midpoint scheme 768 time steps (in 0.49 seconds) to attain the same accuracy. Our scheme is also well suited for Bermudan options. Combining simplicity of implementation and remarkable gains in computational efficiency, we expect this method to be very attractive to financial engineering modelers.", "e:keyword": ["Finance", "Asset pricing", "Option pricing", "Barrier options", "Bermudan options", "Probability", "Markov processes", "Jump-diffusion processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0420", "e:abstract": "Bollapragada and Morton (1999) present several well-performing heuristics for solving the periodic inventory problem with random yield and demand. Their approach is essentially based on a transformation of the single-period problem into a standard newsvendor problem with deterministic yield and random demand which, however, is supply dependent. In our note, we show that their evaluation of the respective optimality condition is not correct. This explains the steady deterioration of their myopic heuristics for parameter constellations that correspond to increasing service levels. Some computational investigations reveal that the performance of the heuristics can become quite poor if service levels are high and exceed those values for which results are reported in the original study. Nonetheless, up to now these heuristics are still the best ones available for solving the joint random yield problem.", "e:keyword": ["Inventory/production", "Heuristics", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0421", "e:abstract": "This paper characterizes a supply function equilibrium in an auction market constrained by limited capacities of links in a transportation network and limited input/output capacities of participants. The formulation is adapted to a wholesale spot market for electricity managed by the operator of the transmission system. The results are derived using the calculus of variations to obtain the Euler conditions and the transversality conditions that characterize a Nash equilibrium in an auction in which bids are as supply functions, and quantities and payments are based either on nodal prices or pay-as-bid.", "e:keyword": ["Industries", "Electric", "Games/group decisions", "Bidding/auctions", "Networks/graphs", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0422", "e:abstract": "This study extends the classical network median problem by considering the stochastic nature of demand. Assuming that the demand weights associated with nodes are independent discrete random variables, we introduce a chance-constrained programming model to define a (beta)-reliable median of the network. It is shown that the (beta)-reliable median problem is NP-hard. Exact solution procedures and a normal approximation algorithm are developed to search for the (beta)-reliable median. Their performance is evaluated by computational experiments.", "e:keyword": ["Facilities/equipment planning", "Location", "Networks/graphs", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0423", "e:abstract": "Rosling (2002) presents and analyzes five different cost-rate functions for single-item inventory models. The functions are referred to as Models 1--5. These errata correct an erroneous comment concerning Model 3 and render a rigorous argument for the functional form of Model 2.", "e:keyword": ["Inventory", "Single-item cost functions", "Stochastic demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0425", "e:abstract": "We present a new interactive hybrid approach for solving multicriteria optimization problems where features of approximation methods and interactive approaches are incorporated. We produce rough approximations of the nondominated set and let the decision maker indicate with the help of reference points where to refine the approximation. In this way, (s)he iteratively directs the search toward the best nondominated solution. After the decision maker has identified the most interesting region of the nondominated set, the final solution can be fine-tuned with existing interactive methods. We suggest different ways of updating the reference point as well as discuss visualizations that can be used in comparing different nondominated solutions. The new method is computationally relatively inexpensive and easy to use for the decision maker.", "e:keyword": ["Programming", "Multiple criteria", "Nonlinear", "Interactive methods", "Reference point"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0426", "e:abstract": "The confidentiality-via-camouflage (CVC) procedure was recently proposed as an alternative to existing procedures such as data perturbation for protecting the confidentiality of numerical data. In this paper, we show that CVC, implemented with certain parameters, could potentially disclose confidential information. We identify the conditions under which such compromise will occur. We provide new derivations for the database administrator to select CVC parameters to avoid such disclosure. We also derive CVC parameters that allow the database administrator to evaluate the trade-off between disclosure risk and data utility, and we provide an expression to evaluate partial value disclosure risk of CVC. Thus, the results of this study should aid the database administrator in evaluating the applicability of CVC.", "e:keyword": ["Computers", "Databases", "Protecting confidential data", "Information systems", "Decision-support systems", "Answering ad-hoc queries"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0427", "e:abstract": "When a marketer in an interactive environment decides which messages to send to her customers, she may send messages currently thought to be most promising (exploitation) or use poorly understood messages for the purpose of information gathering (exploration). We assume that customers are already clustered into homogeneous segments, and we consider the adaptive learning of message effectiveness within a customer segment. We present a Bayesian formulation of the problem in which decisions are made for batches of customers simultaneously, although decisions may vary within a batch. This extends the classical multiarmed bandit problem for sampling one-by-one from a set of reward populations. Our solution methods include a Lagrangian decomposition-based approximate dynamic programming approach and a heuristic based on a known asymptotic approximation to the multiarmed bandit solution. Computational results show that our methods clearly outperform approaches that ignore the effects of information gain.", "e:keyword": ["Dynamic programming/optimal control", "Relaxations", "Multiarmed bandit problem", "Marketing", "Advertising and media"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0428", "e:abstract": "We describe a two-stage robust optimization approach for solving network flow and design problems with uncertain demand. In two-stage network optimization, one defers a subset of the flow decisions until after the realization of the uncertain demand. Availability of such a recourse action allows one to come up with less conservative solutions compared to single-stage optimization. However, this advantage often comes at a price: two-stage optimization is, in general, significantly harder than single-stage optimization.For network flow and design under demand uncertainty, we give a characterization of the first-stage robust decisions with an exponential number of constraints and prove that the corresponding separation problem is (N-script)(P-script)-hard even for a network flow problem on a bipartite graph. We show, however, that if the second-stage network topology is totally ordered or an arborescence, then the separation problem is tractable.Unlike single-stage robust optimization under demand uncertainty, two-stage robust optimization allows one to control conservatism of the solutions by means of an allowed “budget for demand uncertainty.” Using a budget of uncertainty, we provide an upper bound on the probability of infeasibility of a robust solution for a random demand vector.We generalize the approach to multicommodity network flow and design, and give applications to lot-sizing and location-transportation problems. By projecting out second-stage flow variables, we define an upper bounding problem for the two-stage min-max-min optimization problem. Finally, we present computational results comparing the proposed two-stage robust optimization approach with single-stage robust optimization as well as scenario-based two-stage stochastic optimization.", "e:keyword": ["Network/graphs", "Applications", "Programming", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0429", "e:abstract": "Traditional inventory models focus on risk-neutral decision makers, i.e., characterizing replenishment strategies that maximize expected total profit, or equivalently, minimize expected total cost over a planning horizon. In this paper, we propose a framework for incorporating risk aversion in multiperiod inventory models as well as multiperiod models that coordinate inventory and pricing strategies. We show that the structure of the optimal policy for a decision maker with exponential utility functions is almost identical to the structure of the optimal risk-neutral inventory (and pricing) policies. These structural results are extended to models in which the decision maker has access to a (partially) complete financial market and can hedge its operational risk through trading financial securities. Computational results demonstrate that the optimal policy is relatively insensitive to small changes in the decision-maker's level of risk aversion.", "e:keyword": ["Inventory/production", "Policies", "Uncertainty", "Decision analysis", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0430", "e:abstract": "We analyze the effect of price and order postponement in a decentralized newsvendor model with multiplicative and price-dependent demand, wherein the manufacturer sets the wholesale price, and possibly offers a buyback rate, and the retailer determines the order quantity and retail price. Such postponement strategies can be used by the retailer by delaying his operational decisions (order quantity and retail price) until after demand uncertainty is observed. We show how the equilibrium values of the contract parameters and profits are affected by (i) vertical competition, (ii) type of contract (wholesale price-only or buyback), (iii) demand distribution, (iv) form of the expected demand function, and (v) the timing of the retailer's operational decisions. Although in most cases postponement is quite beneficial for the channel members, we show that for some model parameters, due to vertical competition, the expected value of perfect information about demand for price postponement and order postponement may be negative for the channel and even, surprisingly, for both members. We also show that when a buyback option is offered, neither order postponement nor price postponement has an effect on the equilibrium wholesale price, profit allocation ratio between the manufacturer and the retailer, and channel efficiency, and that the equilibrium wholesale price, expected retail price, profit allocation ratio between the manufacturer and the retailer, and channel efficiency in the model with buyback options under either order or price postponement further coincide with their counterparts in the corresponding deterministic model.", "e:keyword": ["Inventory/production", "Policies", "Uncertainty", "Games/group decisions", "Noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0431", "e:abstract": "We study a bilevel noncooperative game-theoretic model of restructured electricity markets, with locational marginal prices. Each player in this game faces a bilevel optimization problem that we model as a mathematical program with equilibrium constraints (MPEC). The corresponding game is an example of an equilibrium program with equilibrium constraints (EPEC). We establish sufficient conditions for the existence of pure-strategy Nash equilibria for this class of bilevel games and give some applications. We show by examples the effect of network transmission limits, i.e., congestion, on the existence of equilibria. Then we study, for more general equilibrium programs with equilibrium constraints, the weaker pure-strategy concepts of local Nash and Nash stationary equilibria. We pose the latter as solutions of complementarity problems (CPs) and show their equivalence with the former in some cases. Finally, we present numerical examples of methods that attempt to find local Nash equilibria or Nash stationary points of randomly generated electricity market games.", "e:keyword": ["Government", "Energy policies", "Electricity market", "Industries", "Electric", "Economic dispatch", "Game/group decisions", "Noncooperative", "Bidding/auctions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0432", "e:abstract": "The Capacitated <i>m</i>-Ring-Star Problem (C<i>m</i>RSP) is the problem of designing a set of rings that pass through a central depot and through some transition points and/or customers, and then assigning each nonvisited customer to a visited point or customer. The number of customers visited and assigned to a ring is bounded by an upper limit: the capacity of the ring. The objective is to minimize the total routing cost plus assignment costs. The problem has practical applications in the design of urban optical telecommunication networks. This paper presents and discusses two integer programming formulations for the C<i>m</i>RSP. Valid inequalities are proposed to strengthen the linear programming relaxation and are used as cutting planes in a branch-and-cut approach. The procedure is implemented and tested on a large family of instances, including real-world instances, and the good performance of the proposed approach is demonstrated.", "e:keyword": ["Networks/graphs", "Optical network design", "Programming", "Integer", "Cutting plane", "Branch-and-cut algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0433", "e:abstract": "Value at Risk (VaR) has emerged in recent years as a standard tool to measure and control the risk of trading portfolios. Yet, existing theoretical analysis of the optimal behavior of a trader subject to VaR limits has produced a negative view of VaR as a risk-control tool. In particular, VaR limits have been found to induce increased risk exposure in some states and an increased probability of extreme losses. However, these conclusions are based on models that are either static or dynamically inconsistent. In this paper, we formulate a dynamically consistent model of optimal portfolio choice subject to VaR limits and show that the concerns expressed in earlier papers do not apply if, consistently with common practice, the VaR limit is reevaluated dynamically. In particular, we find that the optimal risk exposure of a trader subject to a VaR limit is always lower than that of an unconstrained trader and that the probability of extreme losses is also lower. We also consider risk limits formulated in terms of tail conditional expectation (TCE), a coherent risk measure often advocated as an alternative to VaR, and show that in our dynamic setting it is always possible to transform a TCE limit into an equivalent VaR limit, and conversely.", "e:keyword": ["Investment", "Management", "Portfolio"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0434", "e:abstract": "In this paper, we apply game theory to identify equilibrium strategies for both attacker and defender in a fully endogenous model of resource allocation for countering terrorism and natural disasters. The key features of our model include balancing protection from terrorism and natural disasters, and describing the attacker choice by a continuous level of effort rather than a discrete choice (i.e., attack or not). Interestingly, in a sequential game, increased defensive investment can lead an attacker to either increase his level of effort (to help compensate for the reduced probability of damage from an attack), or decrease his level of effort (because attacking has become less profitable). This can either reduce or increase the effectiveness of investments in protection from intentional attack, and can therefore affect the relative desirability of investing in protection from natural disasters.", "e:keyword": ["Decision analysis", "Risk", "Games/group decisions", "Noncooperative", "Utility/preference", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0435", "e:abstract": "Wireless sensor networks pose numerous fundamental coordination problems. For example, in a number of application domains including homeland security, environmental monitoring, and surveillance for military operations, a network's ability to efficiently manage power consumption is extremely critical because direct user intervention after initial deployment is severely limited. In these settings, limited battery life gives rise to the basic coordination problem of maintaining coverage while maximizing the network's lifetime. In this paper, we propose a distributed scheme for efficient power management in sensor networks that is guaranteed to identify suboptimal topologies in an online fashion. Our scheme is based on a general (game-theoretic) mathematical structure that induces a natural mapping between the informational layer and the physical layer. We provide sufficient conditions for the convergence of the algorithm to a pure Nash equilibrium and characterize the performance of the algorithm in terms of coverage. We also present encouraging performance results on a MicaZ testbed as well as on large-scale topologies (obtained via simulation).", "e:keyword": ["Sensor networks", "Game theory", "Distributed algorithms", "Power management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0436", "e:abstract": "Seasonal products have an effective inventory deadline, a time by which the inventory must be ready to distribute. The deadline creates an incentive to start early with production. However, opportunities to gather information that might change production decisions provide an incentive to defer the start of production. We study the resultant dynamic decision problem with alternatives that commit to one of several courses of action now and an alternative to defer the commitment to gather more information about the possible consequences of each alternative. The deadline increases the effective cost of gathering information because that cost includes the value sacrificed by reducing the time available to produce inventory. We frame our model using the annual influenza vaccine composition decision: deciding between strains of the virus to include, which must happen in the spring to allow time for vaccine production before the fall flu season begins. Our analysis describes the optimal decision strategies for this commit-or-defer decision. Many insights are drawn from this model that could contribute to more informed flu vaccine composition decisions. We comment on the relevance of this commit-or-defer decision model to a firm's production decisions for other seasonal products with an inventory deadline such as fashion goods.", "e:keyword": ["Decision analysis", "Sequential", "Dynamic programming/optimal control", "Applications", "Government", "Regulations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0437", "e:abstract": "We consider the problem of maximizing capacity in a queueing network with flexible servers, where the classes and servers are subject to failure. We assume that the interarrival and service times are independent and identically distributed, that routing is probabilistic, and that the failure state of the system can be described by a Markov process that is independent of the other system dynamics. We find that the maximal capacity is tightly bounded by the solution of a linear programming problem and that the solution of this problem can be used to construct timed, generalized round-robin policies that approach the maximal capacity arbitrarily closely. We then give a series of structural results for our policies, including identifying when server flexibility can completely compensate for failures and when the implementation of our policies can be simplified. We conclude with a numerical example that illustrates some of the developed insights.", "e:keyword": ["Queues", "Networks", "Optimization", "Manufacturing", "Performance", "Productivity", "Production/scheduling", "Flexible manufacturing", "Line balancing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0438", "e:abstract": "We present a new model for optimal dynamic pricing of perishable services or products that incorporates a simple risk measure permitting control of the probability that total revenues fall below a minimum acceptable level. The formulation assumes that sales must occur within a finite time period, that there is a finite---possibly large---set of available prices, and that demand follows a price-dependent, nonhomogeneous Poisson process. This model is particularly appropriate for applications in which attainment of a revenue target is an important consideration for managers; for example, in event management, in seasonal clearance of high-value items, or for business subunits operating under performance targets. We formulate the model as a continuous-time optimal control problem, obtain optimality conditions, explore structural properties of the solution, and report numerical results on problems of realistic size.", "e:keyword": ["Inventory/production", "Perishable/aging items", "Marketing/pricing", "Uncertainty", "Dynamic programming/optimal control", "Applications", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0439", "e:abstract": "A notorious open problem in the field of rendezvous search is to decide the rendezvous value of the symmetric rendezvous search problem on the line, when the initial distance between the two players is two. We show that the symmetric rendezvous value is within the interval (4.1520, 4.2574), which considerably improves the previous best-known result (3.9546, 4.3931). To achieve the improved bounds, we call upon results from absorbing Markov chain theory and mathematical programming theory---particularly fractional quadratic programming and semidefinite programming. Moreover, we also establish some important properties of this problem, which could be of independent interest and useful for resolving this problem completely. Finally, we conjecture that the symmetric rendezvous value is asymptotically equal to 4.25 based on our numerical calculations.", "e:keyword": ["Search and surveillance", "Rendezvous search", "Games/group decisions", "Teams", "Analysis of algorithms", "Suboptimal algorithms", "Symmetric rendezvous search", "Game theory", "Approximation algorithm", "Semidefinite programming relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0440", "e:abstract": "The weapon-target assignment (WTA) problem is a fundamental problem arising in defense-related applications of operations research. This problem consists of optimally assigning <i>n</i> weapons to <i>m</i> targets so that the total expected survival value of the targets after all the engagements is minimal. The WTA problem can be formulated as a nonlinear integer programming problem and is known to be NP-complete. No exact methods exist for the WTA problem that can solve even small-size problems (for example, with 20 weapons and 20 targets). Although several heuristic methods have been proposed to solve the WTA problem, due to the absence of exact methods, no estimates are available on the quality of solutions produced by such heuristics. In this paper, we suggest integer programming and network flow-based lower-bounding methods that we obtain using a branch-and-bound algorithm for the WTA problem. We also propose a network flow-based construction heuristic and a very large-scale neighborhood (VLSN) search algorithm. We present computational results of our algorithms, which indicate that we can solve moderately large instances (up to 80 weapons and 80 targets) of the WTA problem optimally and obtain almost optimal solutions of fairly large instances (up to 200 weapons and 200 targets) within a few seconds.", "e:keyword": ["Military", "Targeting", "Programming", "Nonlinear", "Networks", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0441", "e:abstract": "In this paper, we introduce an approach for constructing uncertainty sets for robust optimization using new deviation measures for random variables termed the <i>forward and backward deviations</i>. These deviation measures capture distributional asymmetry and lead to better approximations of chance constraints. Using a linear decision rule, we also propose a tractable approximation approach for solving a class of multistage chance-constrained stochastic linear optimization problems. An attractive feature of the framework is that we convert the original model into a second-order cone program, which is computationally tractable both in theory and in practice. We demonstrate the framework through an application of a project management problem with uncertain activity completion time.", "e:keyword": ["Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0442", "e:abstract": "Random walks have been used extensively within operations research models such as inventory systems and single-server queues to estimate performance measures. In this paper, we use sample-path analysis to express the steady-state probability of a one-sided regulated random walk to increase and be above a threshold, referred to as the last-come-first-serve (LCFS) backlog probability. We approximate the LCFS backlog probability under mild assumptions on the distribution of the random walk's steps and provide its exact expression when the steps are exponentially distributed, and a closed-form approximation when the steps are normally distributed. In our numerical experiments, the average relative gap between the approximated LCFS backlog probabilities and their simulated values is 5.13%. We further show that the LCFS backlog probability is an upper bound on the loss probability---the probability that a two-sided regulated random walk is at a boundary. This bound is tighter than the backlog probability---the probability that a random walk ever crosses a threshold---that also bounds the loss probability. In an inventory application, we demonstrate that using the LCFS backlog probability rather than the backlog probability reduces the inventory level required to satisfy a service-level constraint on the percentage of orders backlogged. In our examples, this reduction leads to cost savings of 31% on average.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Base-stock level for a single item", "Probability", "Random walk", "Bounds on threshold-related probabilities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0443", "e:abstract": "We describe an optimization method to approximate the arrival-rate function of a nonhomogeneous Poisson process based on observed arrival data. We estimate the function by cubic splines, using an optimization model based on the maximum-likelihood principle. A critical feature of the model is that the splines are constrained to be nonnegative everywhere. We enforce these constraints by using a characterization of nonnegative polynomials by positive semidefinite matrices. We also describe versions of our model that allow for periodic arrival-rate functions and input data of limited time precision. We formulate the estimation problem as a convex nonlinear program, and solve it with standard nonlinear optimization packages. We present numerical results using both an actual record of e-mail arrivals over a period of 60 weeks, and artificially generated data sets. We also present a cross-validation procedure for determining an appropriate number of spline knots to model a set of arrival observations.", "e:keyword": ["Nonlinear programming", "Applications", "Probability", "Statistics", "Nonparametric"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0444", "e:abstract": "We consider a scenario in which a single Red wishes to shoot at a collection of Blue targets, one at a time, to maximise some measure of return obtained from Blues killed before Red’s own (possible) demise. Such a situation arises in various military contexts, such as the conduct of air defence by Red in the face of Blue SEAD (suppression of enemy air defences). A class of decision processes called multiarmed bandits has been previously deployed to develop optimal policies for Red, in which she attaches a calibrating (Gittins) index to each Blue target and optimally shoots next at the Blue with the largest index value. The current paper seeks to elucidate how a range of developments of index theory are able to accommodate features of such problems, which are of practical military import. Such features include levels of risk to Red that are policy dependent, Red having imperfect information about the Blues she faces, an evolving population of Blue targets, and the possibility of Red disengagement. The paper concludes with a numerical study that both compares the performance of (optimal) index policies to a range of competitors and also demonstrates the value to Red of (optimal) disengagement.", "e:keyword": ["Military", "Logistics", "Tactics/strategy", "Dynamic programming/optimal control", "Applications", "Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0445", "e:abstract": "We consider a broad class of stochastic dynamic programming problems that are amenable to relaxation via decomposition. These problems comprise multiple subproblems that are independent of each other except for a collection of coupling constraints on the action space. We fit an additively separable value function approximation using two techniques, namely, Lagrangian relaxation and the linear programming (LP) approach to approximate dynamic programming. We prove various results comparing the relaxations to each other and to the optimal problem value. We also provide a column generation algorithm for solving the LP-based relaxation to any desired optimality tolerance, and we report on numerical experiments on bandit-like problems. Our results provide insight into the complexity versus quality trade-off when choosing which of these relaxations to implement.", "e:keyword": ["Dynamic programming/optimal control", "Approximate dynamic programming", "Lagrangian optimization", "Discounted infinite horizon", "Linear programming", "Column generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0446", "e:abstract": "We describe a model of the market for petroleum tank vessels used for planning by Maritrans, Inc. This model is an enhanced version of an earlier model and more closely approximates the market for transportation services. Because of the better representation, we found that the market, which is defined around an index for transportation services, has the potential for multiple equilibria. We present how the model has been used in making major decisions at Maritrans and show how the index design leads to an anomaly where demand could increase with increasing prices, leading to the potential for multiple equilibria. We have not observed this phenomenon in the market. However, with the advent of forward markets for transportation services, known as freight-forward markets, if multiple equilibria do appear, it could become profitable for a player to move a market from one equilibrium to another.", "e:keyword": ["Forecasting", "Economics", "Finance", "Forward markets", "Transportation", "Energy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0447", "e:abstract": "We look for protocols (service disciplines) setting an upper bound on the slowdown (expected sojourn time divided by job size) a job may face, irrespective of the processing times of other jobs. We call this worst slowdown the <i>liability</i> of a job. In a scheduling problem with identical release dates, allowing the server to randomize the order of service cuts almost in half the liability profiles feasible under deterministic protocols. The same is true if cash transfers are feasible and users have linear waiting costs. When release times are arbitrary, we characterize liability functions implementable by a deterministic online protocol, where the liability depends on the number of jobs in the queue upon release of the job. In an <i>M/D</i>/1 queue with a given arrival rate, the liability of a job is at least its slowdown when all other jobs are of the same size. We conjecture that this liability is feasible online, and identify a probabilistic protocol exceeding it by at most 130%.", "e:keyword": ["Scheduling", "Queueing", "Slowdown", "Probabilistic protocols"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0448", "e:abstract": "Every fall, millions of Americans enter betting pools to pick winners of the weekly NFL football games. In the spring, NCAA tournament basketball pools are even more popular. In both cases, teams that are popularly perceived as “favorites” gain a disproportionate share of entries. In large pools there can be a significant advantage to picking upsets that differentiate your picks from the crowd. In this paper, we present a model of betting pools that incorporates pool participant behavior. We use the model to derive strategies that maximize the expected return on a bet in both football pools and tournament-style pools. These strategies significantly outperform strategies based on maximizing score or number of correct picks---often by orders of magnitude.", "e:keyword": ["Games/group decisions", "Gambling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0449", "e:abstract": "This paper presents a branch-and-cut-and-price algorithm for the vehicle-routing problem with time windows. The standard Dantzig-Wolfe decomposition of the arc flow formulation leads to a set-partitioning problem as the master problem and an elementary shortest-path problem with resource constraints as the pricing problem. We introduce the subset-row inequalities, which are Chvatal-Gomory rank-1 cuts based on a subset of the constraints in the master problem. Applying a subset-row inequality in the master problem increases the complexity of the label-setting algorithm used to solve the pricing problem because an additional resource is added for each inequality. We propose a modified dominance criterion that makes it possible to dominate more labels by exploiting the step-like structure of the objective function of the pricing problem. Computational experiments have been performed on the Solomon benchmarks where we were able to close several instances. The results show that applying subset-row inequalities in the master problem significantly improves the lower bound and, in many cases, makes it possible to prove optimality in the root node.", "e:keyword": ["Transportation", "Vehicle routing", "Programming", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0450", "e:abstract": "We consider online routing optimization problems where the objective is to minimize the time needed to visit a set of locations under various constraints; the problems are online because the set of locations are revealed incrementally over time. We consider two main problems: (1) the online traveling salesman problem (TSP) with precedence and capacity constraints, and (2) the online TSP with <i>m</i> salesmen. For both problems we propose online algorithms, each with a competitive ratio of 2; for the <i>m</i>-salesmen problem, we show that our result is best-possible. We also consider polynomial-time online algorithms.We then consider resource augmentation, where we give the online servers additional resources to offset the powerful offline adversary advantage. In this way, we address a main criticism of competitive analysis. We consider the cases where the online algorithm has access to faster servers, servers with larger capacities, additional servers, and/or advanced information. We derive improved competitive ratios. We also give lower bounds on the competitive ratios under resource augmentation, which in many cases are tight and lead to best-possible results.Finally, we study online algorithms from an asymptotic point of view. We show that, under general stochastic structures for the problem data, unknown and unused by the online player, the online algorithms are almost surely asymptotically optimal. Furthermore, we provide computational results that show that the convergence can be very fast.", "e:keyword": ["Online optimization", "Transportation", "Analysis of algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0451", "e:abstract": "This paper proposes a general framework for a large class of multiperiod principal-agent problems. In this framework, a principal has a primary stake in the performance of a system but delegates its control to an agent. The underlying system is a Markov decision process, where the state of the system can only be observed by the agent but the agent's action is observed by both parties. This paper develops a dynamic programming algorithm to derive optimal long-term contracts for the principal. The principal indirectly controls the underlying system by offering the agent a menu of continuation utility vectors along public information paths; the agent's best response, expressed in his choice of continuation utilities, induces truthful state revelation and results in actions that maximize the principal's expected payoff. This problem is meaningful to the operations research community because it can be framed as the problem of optimally designing the reward structure of a Markov decision process with hidden states and has many applications of interest as discussed in this paper.", "e:keyword": ["Games", "Stochastic", "Dynamic principal-agent model", "Adverse selection", "Dynamic programming", "Markov", "Decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0455", "e:abstract": "We study a stochastic network that consists of a set of servers processing multiple classes of jobs. Each class of jobs requires a concurrent occupancy of several servers while being processed, and each server is shared among the job classes in a head-of-the-line processor-sharing mechanism. The allocation of the service capacities is a real-time control mechanism: in each network state, the resource allocation is the solution to an optimization problem that maximizes a general utility function. Whereas this resource allocation optimizes in a “greedy” fashion with respect to each state, we establish its asymptotic optimality in terms of (a) deriving the fluid and diffusion limits of the network under this allocation scheme, and (b) identifying a cost function that is minimized in the diffusion limit, along with a characterization of the so-called fixed-point state of the network.", "e:keyword": ["Stochastic processing network", "Concurrent resource occupancy", "Utility-maximizing resource allocation", "Fluid limit", "Diffusion limit", "Resource pooling", "Heavy-traffic optimality", "Lyapunov function"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0456", "e:abstract": "A basket default swap is a derivative security tied to an underlying basket of corporate bonds or other assets subject to credit risk. The value of the contract depends on the joint distribution of the default times of the underlying assets. Valuing a basket default swap often entails Monte Carlo simulation of these default times. For baskets of high-quality credits and for swaps that require multiple defaults to trigger payment, pricing the swap is a rare-event simulation problem. The Joshi-Kainth algorithm is an innovative importance-sampling technique for this problem that forces a predetermined number of defaults to occur on each path. This paper analyzes, extends, and improves the Joshi-Kainth algorithm. We show that, in its original form, the algorithm can actually increase variance; we present an alternative that is guaranteed to reduce variance, even when defaults are not rare. Along the way, we provide a rigorous underpinning in a setting sufficiently general to include both the original method and the version proposed here.", "e:keyword": ["Finance", "Securities", "Simulation", "Efficiency", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0457", "e:abstract": "Stochastic optimization, especially multistage models, is well known to be computationally excruciating. Moreover, such models require exact specifications of the probability distributions of the underlying uncertainties, which are often unavailable. In this paper, we propose tractable methods of addressing a general class of multistage stochastic optimization problems, which assume only limited information of the distributions of the underlying uncertainties, such as known mean, support, and covariance. One basic idea of our methods is to approximate the recourse decisions via decision rules. We first examine linear decision rules in detail and show that even for problems with complete recourse, linear decision rules can be inadequate and even lead to infeasible instances. Hence, we propose several new decision rules that improve upon linear decision rules, while keeping the approximate models computationally tractable. Specifically, our approximate models are in the forms of the so-called second-order cone (SOC) programs, which could be solved efficiently both in theory and in practice. We also present computational evidence indicating that our approach is a viable alternative, and possibly advantageous, to existing stochastic optimization solution techniques in solving a two-stage stochastic optimization problem with complete recourse.", "e:keyword": ["Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0458", "e:abstract": "Following the multistage design approach, we propose two asymptotically efficient truthful double auction mechanisms, the BC-LP mechanism and the MBC mechanism, for an exchange market with many buyers and sellers. In this market, each buyer wants to procure a bundle of commodities and each seller supplies one unit of a commodity. Furthermore, various transaction-related costs will be incurred when a buyer trades with a seller. We prove that under both mechanisms, truthful bidding is a dominant strategy for all buyers and sellers when the buyers' bundle information and the transaction cost information are common knowledge. The BC-LP mechanism can be implemented by just solving two linear programs, whereas the MBC mechanism has a higher complexity. The empirical study shows that the MBC mechanism achieves higher efficiency over the BC-LP mechanism and that both outperform the KSM-TR mechanism, the only known truthful mechanism for a more restrictive exchange market.", "e:keyword": ["Games/group decisions", "Bidding/auctions", "Information systems", "Analysis and design"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0459", "e:abstract": "We report on the application of operations research to a very complex scheduling and dispatching problem. Scheduling and dispatching are never easy, but the scheduling of concrete deliveries is particularly difficult for several reasons: (1) concrete is an extremely perishable product---it can solidify in the truck if offloading is delayed by a few hours; (2) customer orders are extremely unpredictable and volatile---orders are often canceled or drastically changed at the last minute; (3) the concrete company overbooks by as much as 20% to compensate for customer unpredictability; (4) many orders require synchronized deliveries by multiple trucks; (5) when a truck arrives at a customer site, the customer may not be ready for the delivery, or a storm may negate the ability to use the concrete; and (6) most of the travel takes place in highly congested urban areas, making travel times highly variable.To assist the dispatchers, schedulers, and order takers at this company, we designed and implemented a decision-support tool consisting of both planning and execution tools. The modules determine whether new orders should be accepted, when drivers should arrive for work, the real-time assignment of drivers to delivery loads, the dispatching of these drivers to customers and back to plants, and the scheduling of the truck loadings at the plants.For the real-time dispatching and order-taking decisions, optimization models are solved to within 1% of optimality every five minutes throughout the day. This nearly continuous reoptimization of the entire system allows quick reactions to changes. The modeling foundation is a time-space network with integer side constraints. We describe each of the models and explain how we handle imperfect data. We also detail how we overcome a variety of implementation issues.The success of this project can be measured, most importantly, by the fact that the tool is being ported by the parent company, Florida Rock, to each of its other ready-mix concrete companies. Second, the corporation is sufficiently convinced of its importance that they have begun promoting this methodology as a “best practice” at the World of Concrete and ConAgg industry conventions.", "e:keyword": ["Vehicle scheduling", "Real-time concrete truck scheduling", "Integer programming applications", "Dispatching", "Network flows", "Time-space networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0460", "e:abstract": "A DEA-based stochastic frontier estimation framework is presented to evaluate contextual variables affecting productivity that allows for both one-sided inefficiency deviations as well as two-sided random noise. Conditions are identified under which a two-stage procedure consisting of DEA followed by ordinary least squares (OLS) regression analysis yields consistent estimators of the impact of contextual variables. Conditions are also identified under which DEA in the first stage followed by maximum likelihood estimation (MLE) in the second stage yields consistent estimators of the impact of contextual variables. This requires the contextual variables to be independent of the input variables, but the contextual variables may be correlated with each other. Monte Carlo simulations are carried out to compare the performance of our two-stage approach with one-stage and two-stage parametric approaches. Simulation results indicate that DEA-based procedures with OLS, maximum likelihood, or even Tobit estimation in the second stage perform as well as the best of the parametric methods in the estimation of the impact of contextual variables on productivity. Simulation results also indicate that DEA-based procedures perform better than parametric methods in the estimation of individual decision-making unit (DMU) productivity. Overall, the results establish DEA as a nonparametric stochastic frontier estimation (SFE) methodology.", "e:keyword": ["Organizational studies", "Productivity", "Effectiveness/performance", "Statistics", "Nonparametric", "Probability", "Stochastic model applications", "Simulation", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0461", "e:abstract": "In this paper, we establish an exact framework for a class of supply chains with at most one directed path between every two stages. External demands follow compound Poisson processes, the transit times are stochastic, sequential, and exogenous, and each stage controls its inventory by an installation base-stock policy under continuous review. Unsatisfied demand at each stage is fully backordered. This class of supply chains includes assembly, distribution, tree, and two-level general networks as special cases. We characterize the stockout delay for each unit of demand at each stage of the supply chain by developing an exact and unified approach that applies to various network topologies. We also present tractable approximations and decompositions that facilitate efficient evaluation and optimization (up to the approximations) of the base-stock policies in industry-size problems with a tree structure. We demonstrate the effectiveness of the solution by numerical studies.", "e:keyword": ["Inventory/production", "Multi-item/echelon", "Operating characteristics", "Approximations", "Lead times", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0462", "e:abstract": "We study a stationary, single-stage inventory system, under periodic review, with fixed ordering costs and multiple sales levers (such as pricing, advertising, etc.). We show the optimality of (<i>s, S</i>)-type policies in these settings under both the backordering and lost-sales assumptions. Our analysis is constructive and is based on a condition that we identify as being key to proving the (<i>s, S</i>) structure. This condition is entirely based on the single-period profit function and the demand model. Our optimality results complement the existing results in this area.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Marketing", "Pricing", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0463", "e:abstract": "We compare two inventory systems, one in which excess demand is lost and the other in which excess demand is backordered. Both systems are reviewed periodically. They experience the same sequence of identically and independently distributed random demands. Holding and shortage costs are considered. The holding cost parameter is identical; however, the cost of a lost sale could be different from the per-period cost of backlogging a unit sale. When these costs are equal, we prove that the optimal expected cost for managing the system with lost sales is lower. When the cost of a lost sale is greater, we establish a relationship between these parameters that ensures that the reverse inequality is true. These results are useful for designing inventory systems. We also introduce a new stochastic comparison technique in this paper.", "e:keyword": ["Inventory", "Production", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0464", "e:abstract": "Motivated by workforce planning problems in health care, professional, warranty, and repair services, we propose modeling service centers that are exclusively dedicated to fixed client constituencies as closed multiserver queueing systems, a framework we refer to as <i>membership services</i>. We provide fluid and diffusion approximations of the number of users within the membership who are requesting service. The approximations are obtained via many-server limit theorems, where the limiting regime assumptions of each theorem correspond to a particular staffing strategy a manager might employ. Accordingly, we propose staffing rules designed to meet a certain desired performance criterion. In particular, when the objective is to minimize the staffing size subject to a constraint on the probability of delay for a service-requesting customer, we suggest staffing rules inspired by the so-called quality- and efficiency-driven (QED), or Halfin-Whitt, limiting regime. Numerical evaluations of our proposed QED scheme indicate that, although justified for large systems, the staffing rule performs well for memberships of all sizes.", "e:keyword": ["Queues", "Diffusion models", "Limit theorems", "Staffing", "Stochastic model applications", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0470", "e:abstract": "A classic application of the linear assignment problem is the assignment of people to jobs (or jobs to people). In this context, it is interesting to measure competition for jobs and to generate a suitable list of jobs from which a person can choose; the length of the list is a parameter. A known list-generation procedure is based on an interior-point method followed by a parametric analysis. We describe a more efficient procedure, exploiting linear assignment theory and shortest-path computations. Further, we propose an alternative list-generation procedure, based on a special type of dual values for the linear assignment problem.", "e:keyword": ["Assignment problems", "Matchings", "Parametric analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0471", "e:abstract": "We consider the notoriously difficult discrete-time inventory model with stochastic demands, a constant lead time, and lost sales. We show that the effective state space is a relatively manageable compact set. Then, we test various plausible heuristics. We find that several perform reasonably well, although none is perfect. However, the standard base-stock policy (a direct analogue of the optimal policy for a backlog system) performs badly. We also show that the optimal cost is increasing in the lead time.", "e:keyword": ["Inventory", "Lost sales", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0472", "e:abstract": "Forest ecosystem management often requires spatially explicit planning because the spatial arrangement of harvests has become a critical economic and environmental concern. Recent research on exact methods has addressed both the design and the solution of forest management problems with constraints on the clearcut size, but where simultaneously harvesting two adjacent stands in the same period does not necessarily exceed the maximum opening size. Two main integer programming approaches have been proposed for this area restriction model. However, both encompass an exponential number of variables or constraints. In this work, we present a new integer programming model with a polynomial number of variables and constraints. Branch and bound is used to solve it. The model was tested with both real and hypothetical forests ranging from 45 to 1,363 polygons. Results show that the proposed model's solutions were within or slightly above 1% of the optimal solution and were obtained in a short computation time.", "e:keyword": ["Forest management", "Harvest scheduling", "Spatial modeling", "Integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0474", "e:abstract": "Most examples of cycling in the simplex method are given without explanation of how they were constructed. An exception is Beale's example built around the geometry of the dual simplex method in the plane [Beale, E. 1955. Cycling in the dual simplex method. <i>Naval Res. Logist. Quart.</i> <b>2</b>(4) 269--275]. Using this approach, we give a simple geometric explanation for a number of examples of cycling in the simplex method, including Hoffman's original example [Hoffman, A. 1953. <i>Cycling in the Simplex Algorithm</i>. National Bureau of Standards, Washington, D.C.]. This gives rise to a simple method for generating examples with cycles.", "e:keyword": ["Linear programming theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0475", "e:abstract": "To estimate the variance parameter (i.e., the sum of covariances at all lags) for a steady-state simulation output process, we formulate certain statistics that are computed from overlapping batches separately and then averaged over all such batches. We form overlapping versions of the area and Cramér--von Mises estimators using the method of standardized time series. For these estimators, we establish (i) their limiting distributions as the sample size increases while the ratio of the sample size to the batch size remains fixed; and (ii) their mean-square convergence to the variance parameter as both the batch size and the ratio of the sample size to the batch size increase. Compared with their counterparts computed from nonoverlapping batches, the estimators computed from overlapping batches asymptotically achieve reduced variance while maintaining the same bias as the sample size increases; moreover, the new variance estimators usually achieve similar improvements compared with the conventional variance estimators based on nonoverlapping or overlapping batch means. In follow-up work, we present several analytical and Monte Carlo examples, and we formulate efficient procedures for computing the overlapping estimators with only order-of-sample-size effort.", "e:keyword": ["Simulation", "Statistical analysis", "Steady-state variance estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0476", "e:abstract": "An airline's fleet typically contains multiple aircraft families, each having a specific cockpit design and crew requirement. Each aircraft family contains multiple aircraft types having different capacities. Given a flight schedule network, the fleet assignment model is concerned with assigning aircraft to flight legs to maximize profits with respect to captured itinerary-based demand. However, because of related yield management and crew-scheduling regulations, in particular, this decision needs to be made well in advance of departures when market demand is still highly uncertain, although subsequently at a later stage, reassignments of aircraft types within a given family can be made when demand forecasts improve, while preserving crew schedules. In this paper, we propose a two-stage stochastic mixed-integer programming approach in which the first stage makes only higher-level family-assignment decisions, while the second stage performs subsequent family-based type-level assignments according to forecasted market demand realizations. By considering demand uncertainty up-front at the initial fleeting stage, we inject additional flexibility in the process that offers more judicious opportunities for later revisions. We conduct a polyhedral analysis of the proposed model and develop suitable solution approaches. Results of some numerical experiments are presented to exhibit the efficacy of using the stochastic model as opposed to the traditional deterministic model that considers only expected demand, and to demonstrate the efficiency of the proposed algorithms as compared with solving the model using its deterministic equivalent.", "e:keyword": ["Industries", "Transportation", "Programming", "Integer", "Stochastic", "Large-scale systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0477", "e:abstract": "In this paper, we consider numerous inventory control problems for which the base-stock policies are known to be optimal, and we propose stochastic approximation methods to compute the optimal base-stock levels. The existing stochastic approximation methods in the literature guarantee that their iterates converge, but not necessarily to the optimal base-stock levels. In contrast, we prove that the iterates of our methods converge to the optimal base-stock levels. Moreover, our methods continue to enjoy the well-known advantages of the existing stochastic approximation methods. In particular, they only require the ability to obtain samples of the demand random variables, rather than to compute expectations explicitly, and they are applicable even when the demand information is censored by the amount of available inventory.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Dynamic programming/optimal control", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0478", "e:abstract": "Transportation and production contracts often specify the frequency and volume reserved by the supplier for a particular customer's deliveries. This practice motivated Henig et al. (Henig, M., Y. Gerchak, R. Ernst, D. Pyke. 1997. An inventory model embedded in designing a supply contract. <i>Management Sci.</i> <b>43</b> 184--189) to study a periodic-review inventory-control model where ordering cost is zero if the order quantity does not exceed a given contract volume and is linear in the excess quantity otherwise. This paper addresses the same problem but with a fixed cost if the order quantity is above the contract volume. The fixed cost may represent the cost of disruption for the supplier (finding more trucks, arranging extra processing capacity, persuading other customers to wait, etc.) as well as additional administrative costs. Also, suppliers may impose such costs simply to induce desired behavior by buyers. This order-cost function is neither convex nor concave. The classical inventory models with fixed costs are special cases with contract volume zero. We partially characterize the optimal policy for this system and develop a simple, effective heuristic policy. We also apply the model to a production-control problem in which an incentive is provided for not ordering over a certain quota.", "e:keyword": ["Periodic-review inventory systems", "Optimal control", "(s", "S) policies", "K-convexity", "Incentives", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0479", "e:abstract": "In 1958, Wagner and Whitin published a seminal paper on the deterministic uncapacitated lot-sizing problem, a fundamental model that is embedded in many practical production planning problems. In this paper, we consider a basic version of this model in which problem parameters are stochastic: the stochastic uncapacitated lot-sizing problem. We define the production-path property of an optimal solution for our model and use this property to develop a backward dynamic programming recursion. This approach allows us to show that the value function is piecewise linear and right continuous. We then use these results to show that a full characterization of the optimal value function can be obtained by a dynamic programming algorithm in polynomial time for the case that each nonleaf node contains at least two children. Moreover, we show that our approach leads to a polynomial-time algorithm to obtain an optimal solution to any instance of the stochastic uncapacitated lot-sizing problem, regardless of the structure of the scenario tree. We also show that the value function for the problem without setup costs is continuous, piecewise linear, and convex, and therefore an even more efficient dynamic programming algorithm can be developed for this special case.", "e:keyword": ["Lot sizing", "Stochastic programming", "Dynamic programming", "Production/scheduling", "Planning", "Programming", "Integer", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0480", "e:abstract": "The question of when to initiate HIV treatment is considered the most important question in HIV care today. Benefits of delaying therapy include avoiding the negative side effects and toxicities associated with the drugs, delaying selective pressures that induce the development of resistant strains of the virus, and preserving a limited number of treatment options. On the other hand, the risks of delayed therapy include the possibility of irreversible damage to the immune system, development of AIDS-related complications, and death. We use Markov decision processes to develop the first HIV optimization models that aim to maximize the expected lifetime or quality-adjusted lifetime of a patient. We prove conditions that establish structural properties of the optimal solution and compare them to our data and results. Model solutions, based on clinical data, support a strategy of treating HIV earlier in its course as opposed to recent trends toward treating it later.", "e:keyword": ["Dynamic programming", "Markov decision processes", "Health-care treatment", "HIV/AIDS", "Stochastic modeling applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0481", "e:abstract": "Transportation companies that operate in seasonal markets where differences in demand occur in peak and nonpeak periods often negotiate for some form of demand smoothing with the customer---i.e., the shipper. In this paper, we study a shipper's transportation procurement model, in which the shipper gives assurances, through volume guarantees negotiated with the transportation companies, that shipments made in nonpeak periods will be commensurate with shipments in peak periods. The shipper uses the model in an auction process, in which the transportation companies bid for routes giving prices and capacity limits, to procure freight services from the companies, which minimizes its total transportation costs. The model is formulated as an integer programming problem that is shown to be strongly (N-script) (P-script)-hard even for a single-route network. We develop a solution approach that builds on the solution of the subproblem when only one transportation company is available to construct heuristic algorithms, including a linear programming relaxation-based method. Worst-case analysis is given, and the effectiveness of the algorithms is tested in numerical experimentation. By examining parameter sensitivity, insight is provided on how the algorithms can be used by the shipper for making procurement decisions. The practical usefulness of the model and the solution approaches is substantiated by its deployment with a multinational shipper.", "e:keyword": ["Transportation", "Models", "Assignment", "Transportation allocation", "Programming", "Integer applications", "Large-scale integer optimization", "Industrial", "Transportation/shipping", "Transportation procurement optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0482", "e:abstract": "We provide a new approach to the structural analysis of the standard lost-sales inventory system. This approach is, we think, easier to work with than the original one. We also derive new bounds on the optimal policy. Then, we show that more variable demand leads to higher cost. Finally, we extend the analysis to several important variations of the basic model.", "e:keyword": ["Inventory", "Lost sales", "Discrete convexity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0483", "e:abstract": "A system consisting of two buffers, each with independent fluid sources, is considered in this paper. Due to ease of implementation, the output capacities for the two buffers depend on the workload of only one of the buffers that is measured. A threshold-based policy is considered to dynamically assign output capacities for both buffers. Marginal workload distributions for the two buffers need to be evaluated for this policy. The key contribution of this paper is the performance analysis to derive the workload distribution in the two buffers. In addition, the paper also provides some guidelines to choose the output capacities for the two buffers as well as a mathematical program to determine an optimal threshold to dynamically switch between output capacities. Further, various applications of such systems to computer-communication networks are discussed.", "e:keyword": ["Probability", "Stochastic model applications", "Queues", "Information systems", "Management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0484", "e:abstract": "Radiation therapy is subject to uncertainties that need to be accounted for when determining a suitable treatment plan for a cancer patient. For lung and liver tumors, the presence of breathing motion during treatment is a challenge to the effective and reliable delivery of the radiation. In this paper, we build a model of motion uncertainty using probability density functions that describe breathing motion, and provide a robust formulation of the problem of optimizing intensity-modulated radiation therapy. We populate our model with real patient data and measure the robustness of the resulting solutions on a clinical lung example. Our robust framework generalizes current mathematical programming formulations that account for motion, and gives insight into the trade-off between sparing the healthy tissues and ensuring that the tumor receives sufficient dose. For comparison, we also compute solutions to a nominal (no uncertainty) and margin (worst-case) formulation. In our experiments, we found that the nominal solution typically underdosed the tumor in the unacceptable range of 6% to 11%, whereas the robust solution underdosed by only 1% to 2% in the worst case. In addition, the robust solution reduced the total dose delivered to the main organ-at-risk (the left lung) by roughly 11% on average, as compared to the margin solution.", "e:keyword": ["Linear programming", "Applications", "Health care", "Treatment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0485", "e:abstract": "We study the effectiveness of simple heuristics in multiattribute decision making. We consider the case of an additive separable utility function with nonnegative, nonincreasing attribute weights. In this case, cumulative dominance ensures that the so-called cumulative dominance compliant heuristics will choose a best alternative. For the case of binary attribute values and under two probabilistic models of the decision environment generalizing a simple Bernoulli model, we obtain the probabilities of simple and cumulative dominance. In contrast with the probability of simple dominance, the probability of cumulative dominance is shown to be large in many cases, explaining the effectiveness of cumulative dominance compliant heuristics in those cases. Additionally, for the subclass of the so-called fully cumulative dominance compliant heuristics, we obtain an upper bound for the expected loss that only depends on the weights being nonnegative and nonincreasing. The low values of the upper bound for cases in which the probability of cumulative dominance is not large provide an additional explanation for the effectiveness of fully cumulative dominance compliant heuristics. Examples of cumulative dominance compliant heuristics and fully cumulative dominance compliant heuristics are discussed, including the deterministic elimination by aspects (<i>DEBA</i>) heuristic that motivated our work.", "e:keyword": ["Multiattribute decision making", "Deterministic elimination", "Aspects", "Cumulative dominance", "Binary attributes", "Performance", "Simple heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0486", "e:abstract": "Traditional stochastic inventory models assume full knowledge of the demand probability distribution. However, in practice, it is often difficult to completely characterize the demand distribution, especially in fast-changing markets. In this paper, we study the newsvendor problem with partial information about the demand distribution (e.g., mean, variance, symmetry, unimodality). In particular, we derive the order quantities that minimize the newsvendor's maximum regret of not acting optimally. Most of our solutions are tractable, which makes them attractive for practical application. Our analysis also generates insights into the choice of the demand distribution as an input to the newsvendor model. In particular, the distributions that maximize the entropy perform well under the regret criterion. Our approach can be extended to a variety of problems that require a robust but not conservative solution.", "e:keyword": ["Distribution-free inventory policy", "Newsvendor model", "Robust optimization", "Entropy", "Value of information", "Semi-infinite linear optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0487", "e:abstract": "In this paper, we examine the cross-efficiency concept in data envelopment analysis (DEA). Cross efficiency links one decision-making unit's (DMU) performance with others and has the appeal that scores arise from peer evaluation. However, a number of the current cross-efficiency approaches are flawed because they use scores that are arbitrary in that they depend on a particular set of optimal DEA weights generated by the computer code in use at the time. One set of optimal DEA weights (possibly out of many alternate optima) may improve the cross efficiency of some DMUs, but at the expense of others. While models have been developed that incorporate secondary goals aimed at being more selective in the choice of optimal multipliers, the alternate optima issue remains. In cases where there is competition among DMUs, this situation may be seen as undesirable and unfair. To address this issue, this paper generalizes the original DEA cross-efficiency concept to game cross efficiency. Specifically, each DMU is viewed as a player that seeks to maximize its own efficiency, under the condition that the cross efficiency of each of the other DMUs does not deteriorate. The average game cross-efficiency score is obtained when the DMU's own maximized efficiency scores are averaged. To implement the DEA game cross-efficiency model, an algorithm for deriving the best (game cross-efficiency) scores is presented. We show that the optimal game cross-efficiency scores constitute a Nash equilibrium point.", "e:keyword": ["Data envelopment analysis", "Game", "Cross efficiency", "Nash equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0488", "e:abstract": "We study a problem faced by a third-party logistics provider (3PL) who needs to coordinate shipments between suppliers and customers through a consolidation center in a distribution network. Products from a supplier have one release time and are consolidated into a single shipment to the consolidation center. At the center, products to the same destination are also consolidated into a single shipment, and the consolidation time can be as early as possible or as late as possible, depending on the customer requirement and cost structure. The 3PL needs to determine the pickup times from the suppliers, delivery times to the customers, and the transportation options while considering product release times, latest arrival times, different consolidation policies, and the transportation and storage costs involved. In this paper, we formulate this problem as a nonlinear optimization problem, show it is an NP-hard problem, and develop a dual-based solution method for the general problem. Utilizing the problem's special structure, we show that the Lagrangian dual of the general problem can be solved optimally as a linear program, thus allowing us to accelerate the computation of a lower bound to the optimal objective function value. The experimental results show that the dual-based algorithm provides solutions with objective function values, which are on average within 3.24% of optimality. We also consider a version of the problem where each customer orders products from all suppliers, for which we develop a polynomial-time algorithm.", "e:keyword": ["Shipment consolidation", "Distribution network optimization", "Logistics coordination"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0489", "e:abstract": "Firms compete in supply functions when they offer a schedule of prices and quantities into a market; for example, this occurs in many wholesale electricity markets. We study the equilibrium behaviour when firms differ, both with regard to their costs and their capacities. We characterize strong equilibrium solutions in which, given the other players' supply functions, optimal profits are achieved for every demand realisation. If the demand can be low enough for it to be met economically with supply from just one firm, then the supply function equilibria are ordered in a natural way. We consider equilibria in which, for the highest levels of demand, all but one of the firms have reached their capacity limit. We show that there can be at most one supply function equilibrium with this property. We also propose a new numerical method to find asymmetric supply function equilibria, using piecewise-linear approximations and a discretization of the demand distribution. We show that this approach has good theoretical convergence behaviour. Finally, we present numerical results from an implementation using GAMS to demonstrate that the approach is effective in practice.", "e:keyword": ["Supply function equilibrium", "Electricity markets", "Numerical solution of ordinary differential equations", "Piecewise-linear approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0490", "e:abstract": "We formulate and analyze a spatial queueing model concerning a terrorist who is attempting to drive a nuclear or radiological weapon toward a target in a city center. In our model, imperfect radiation sensors form a circular wall around the periphery of the city, and vehicles setting off sensor alarms (representing a terrorist or a nuisance alarm) arrive randomly at the perimeter of a circle (representing the wall of sensors) and drive toward the center of the circle. Interdiction vehicles, one in each wedge of the circle, chase the alarm-generating vehicles. We derive an accurate mathematical expression for the mean damage inflicted by a terrorist in this system in terms of the arrival rate of alarm-generating vehicles and the number of interdiction vehicles. Our results suggest that detection-interdiction systems using current technology are capable of mitigating the damage from a nuclear weapon made of plutonium, but not one made of uranium or a radiological weapon.", "e:keyword": ["Government", "Defense", "Queues", "Approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0491", "e:abstract": "We consider the one-warehouse multiple retailer inventory model with a submodular joint setup cost function. The objective of this model is to determine an inventory replenishment policy that minimizes the long-run average system cost over an infinite time horizon. Although the optimal policy for this problem is still unknown, a class of easy-to-implement power-of-two policies are 98% effective. This paper focuses on how the cost, under an optimal power-of-two policy, should be allocated to the retailers. This question generates an interesting cooperative game. We prove that this cooperative game has a nonempty core. The key to our result is a strong duality theorem for the one-warehouse multiple retailer problem under power-of-two policies.", "e:keyword": ["Cooperative game", "Joint replenishment", "Strong duality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0494", "e:abstract": "Inventory sharing through transshipment has attracted a great deal of attention from researchers and practitioners due to its potential for increasing service levels while simultaneously decreasing stock levels. In this paper, we analyze the optimal production and transshipment policy for a two-location make-to-stock queueing system with exponential production and interarrival times. A key feature of our model is that we allow transshipments to be triggered by both demand arrivals and production completions. Thus, transshipment is used to achieve production flexibility through inventory reallocation, as well as to fill emergency demands. We also consider capacity issues in transshipment by modeling each location as a single-server, make-to-stock queueing system. In this setting, we prove that the optimal production policy for each location belongs to the “hedging point” family of policies, while the optimal demand filling policy belongs to the “state-dependent rationing” family of policies. We analyze the structural properties of the optimal policy and provide conditions under which the optimal policy can be simplified. Given the complex nature of the optimal policy, we develop three easy-to-implement heuristics that work very well for a large range of cost parameters.", "e:keyword": ["Inventory transshipment", "Inventory/production systems", "Make-to-stock systems", "Optimal control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0495", "e:abstract": "A multivariate Bayesian control chart for monitoring process mean under the assumption that the vector of process observations follows a multivariate normal distribution is considered. Traditional control charts such as Hotelling's <i>T</i><sup>2</sup>, EWMA, and CUSUM charts have been applied to control industrial processes characterized by several measurable variables. It is well known that these traditional, non-Bayesian process control techniques are not optimal, but very few results regarding the structure of the Bayesian control policy have been reported in the literature, all dealing with the univariate, finite-horizon case. In this paper, we formulate the multivariate Bayesian process control problem in the optimal stopping framework. The objective is to find a stopping rule under partial observations, minimizing the long-run expected average cost per unit time for a given sample size and sampling interval. Under standard operating and cost assumptions, it is proved that a control limit policy is optimal, and an algorithm is presented to find the optimal control limit and the minimum average cost.", "e:keyword": ["Reliability", "Quality control", "Replacement/renewal", "Dynamic programming/optimal control", "Models", "Statistics", "Bayesian"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0496", "e:abstract": "We show that multigrid ideas can be used to reduce the computational complexity of estimating an expected value arising from a stochastic differential equation using Monte Carlo path simulations. In the simplest case of a Lipschitz payoff and a Euler discretisation, the computational cost to achieve an accuracy of <i>O</i>((epsilon)) is reduced from <i>O</i>((epsilon)<sup>-3</sup>) to <i>O</i>((epsilon)<sup>-2</sup> (log (epsilon))<sup>2</sup>). The analysis is supported by numerical results showing significant computational savings.", "e:keyword": ["Analysis of algorithms", "Computational complexity", "Finance", "Simulation", "Efficiency"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0497", "e:abstract": "This paper examines a two-tier assemble-to-order system. Customer orders for various products must be filled within the product-specific target lead time, or become lost sales. A product can be assembled instantaneously if its required components are in stock at the assembly facility. The production facility for each component is geographically distant from the assembly facility, and the transportation lead time is deterministic. Each shipment of components incurs a fixed cost and a variable cost per unit. The system manager must initially commit to the production capacity for each component. Then, in response to customer orders, he must dynamically manage production (expediting and salvaging) and shipping for each component, and the sequence of customer orders for assembly (how scarce components are allocated to outstanding orders). The objective is to minimize expected discounted costs for lost sales, production, and shipping. This discounted formulation accounts for financial inventory holding costs but not physical inventory holding costs. The main result is that as the order arrival rate for each product becomes large and the discount rate becomes small, a simple threshold policy with independent control of each component is asymptotically optimal. The policy is parameterized by five numbers for each component. Expressions for these parameters, the expected discounted cost, and the long-run average rates of salvaging and expediting are obtained by solving an approximating Brownian control problem. In a numerical example from the computer industry, the Brownian approximation is remarkably accurate.", "e:keyword": ["Inventory/production", "Assemble-to-order", "Stochastic optimal control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0498", "e:abstract": "Information measures arise in many disciplines, including forecasting (where scoring rules are used to provide incentives for probability estimation), signal processing (where information gain is measured in physical units of relative entropy), decision analysis (where new information can lead to improved decisions), and finance (where investors optimize portfolios based on their private information and risk preferences). In this paper, we generalize the two most commonly used parametric families of scoring rules and demonstrate their relation to well-known generalized entropies and utility functions, shedding new light on the characteristics of alternative scoring rules as well as duality relationships between utility maximization and entropy minimization. In particular, we show that weighted forms of the pseudospherical and power scoring rules correspond exactly to measures of relative entropy (divergence) with convenient properties, and they also correspond exactly to the solutions of expected utility maximization problems in which a risk-averse decision maker whose utility function belongs to the linear-risk-tolerance family interacts with a risk-neutral betting opponent or a complete market for contingent claims in either a one-period or a two-period setting. When the market is incomplete, the corresponding problems of maximizing linear-risk-tolerance utility with the risk-tolerance coefficient (beta) are the duals of the problems of minimizing the pseudospherical or power divergence of order (beta) between the decision maker's subjective probability distribution and the set of risk-neutral distributions that support asset prices.", "e:keyword": ["Decision analysis", "Theory", "Probability", "Entropy", "Utility/preference", "Theory", "Finance", "Portfolio"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0500", "e:abstract": "Assurance region (AR) restrictions on multipliers in data envelopment analysis (DEA) have been applied extensively in many performance measurement settings. They facilitate the derivation of multiplier values that reflect the reality of the problem situation under study. In measuring the operational efficiency of bank branches, for example, output multipliers would generally represent unit processing times for branch transactions such as deposits. AR restrictions on these multipliers are intended to ensure that the (multiplier) values assigned to the various outputs are relatively of the proper size. Current AR-DEA models presume that multiplier restrictions apply uniformly across all decision-making units (DMUs) in the analysis set. Such models can have severe shortcomings, however, in those situations where different circumstances prevail for some DMUs than for others. In the context of bank branches, for example, two sets of branches, whose transaction times are known to be different from each other, would generally require different sets of AR restrictions. This paper presents a methodology for incorporating multiple sets of AR restrictions, with each reflecting the context for a particular subset of DMUs. The resulting modified DEA model, referred to as CAR-DEA, evaluates performance in a manner that more accurately captures the circumstances in which the DMUs operate.", "e:keyword": ["Organizational studies", "Productivity", "Decision analysis", "Multiple criteria", "Programming", "Linear", "DEA", "Assurance regions", "AR", "Context-dependent", "CAR-DEA"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0501", "e:abstract": "A complex product, such as a software system, is often inspected more than once in a sequential manner to further improve its quality and reliability. In such a case, a particularly important task is to accurately estimate the number of errors still remaining in the product after a series of multiple inspections. In the paper, we first develop a maximum likelihood method of estimating both the number of undiscovered errors in the product and the detection probability. We then compare its performance with that of an existing estimation method that has several limitations. We also propose a Bayesian method with noninformative priors, which performs very well in a Monte Carlo simulation study. As the prior knowledge is elicited and incorporated in the analysis, the prediction accuracy of the Bayesian method improves even further. Thus, it would be worthwhile to use various estimation methods and compare their estimates in a specific inspection environment.", "e:keyword": ["Reliability", "Inspection", "Statistics", "Bayesian", "Probability", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0502", "e:abstract": "A facility must be located within a given region taking two criteria of equity and efficiency into account. Equity is sought by minimizing the inequality in the facility-inhabitant distances, as measured by the sum of the absolute differences between all pairs of squared Euclidean distances from inhabitants to the facility. This measure meets the Pigou-Dalton condition of transfers and can easily be minimized. Efficiency is measured through optimizing the sum of squared inhabitant-facility distances, either to be minimized or maximized for an attracting or repellent facility, respectively. Geometric localization results are obtained for the whole set of Pareto-optimal solutions for each of the two resulting bicriteria problems within a convex polygonal region. A polynomial procedure is developed to obtain the full bicriteria plot, both trade-off curves, and the corresponding efficient sets.", "e:keyword": ["Facilities planning", "Equity continuous location", "Programming", "Bicriteria"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0503", "e:abstract": "The airline fleet assignment problem addresses the question of how to best assign aircraft fleet types to scheduled flight legs. This paper presents the subnetwork fleet assignment model: a model that employs composite decision variables representing the simultaneous assignment of fleet types to subnetworks of one or more flight legs. The formulation is motivated by the need to better model the revenue side of the objective function. We present a solution method designed to balance revenue approximation and model tractability. Computational results suggest that the approach yields profit improvements over comparable models and that it is computationally tractable for problems of practical size.", "e:keyword": ["Transportation", "Airline models", "Programming", "Dantzig-Wolfe decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0504", "e:abstract": "Recently, the so-called class of SMART scheduling policies has been introduced to formalize the common heuristic of “biasing toward small jobs.” We study the tail of the sojourn-time (response-time) distribution under both SMART policies and the foreground-background policy (FB) in the GI/GI/1 queue. We prove that these policies behave very well under heavy-tailed service times. Specifically, we show that the sojourn-time tail under all SMART policies and FB is similar to that of the service-time tail, up to a constant, which makes the SMART class superior to first-come-first-served (FCFS). In contrast, for light-tailed service times, we prove that the sojourn-time tail under FB and SMART is larger than that under FCFS. However, we show that the sojourn-time tail for a job of size <i>y</i> under FB and all SMART policies still outperforms FCFS as long as <i>y</i> is not too large.", "e:keyword": ["Queues", "Priority", "Limit theorems", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0505", "e:abstract": "We utilise and develop Whittle's restless bandit formulation to analyse a simple class of inventory routing problems with direct deliveries. These routing problems arise from the practice of vendor-managed inventory replenishment and concern the optimal replenishment of a collection of inventory holding locations controlled centrally by a decision maker who is able to monitor inventory levels throughout the network. We develop a notion of location indexability from a Lagrangian relaxation of the problem and show that (subject to mild conditions) the locations are indeed indexable. We thus have a collection of location indices in closed form, namely, real-valued functions of the inventory level (one for each location), which measure in a natural way (namely, as a fair charge for replenishment) each location's priority for inclusion in each day's deliveries. We discuss how to use such location indices to construct heuristics for replenishment and assess a greedy index heuristic in a numerical study where it performs strongly. A simpler approximate index analysis is available for the case in which the demand at each location is Poisson. This analysis permits a more explicit characterisation of the range of holding cost rates for which (approximate) location indexability is guaranteed.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Models", "Inventory/production", "Policies", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0506", "e:abstract": "This paper describes the evolution and application of a novel approach for forecasting drug demand in markets where supply limitations have significantly curtailed sales volumes and thus reduced the usefulness of conventional sales-based forecasting methods. This occurs frequently with biological (biotech) drugs. We use methods from decision analysis to explicitly model the variability in epidemiological data together with the variability in treatment modalities to estimate <i>latent therapeutic demand</i> (LTD)---the underlying demand that captures how physicians would prescribe treatment and how patients would comply if ample supplies of drugs were available and affordable. Our approach evolved from efforts to help Bayer Biological Products with strategic decisions regarding its drug for treating hemophilia A, the future of which had been clouded for several years, primarily due to a lack of confidence in demand estimates. Use of the LTD model resulted in a better understanding of the therapeutic needs of the global hemophilia community and helped Bayer make good decisions. We believe this approach is widely applicable to forecasting potential demand for supply-constrained as well as brand-new drugs, and thus can be very useful in helping both drug manufacturers and health-care agencies worldwide to ensure adequate supplies of critical drugs.", "e:keyword": ["Decision analysis", "Applications", "Forecasting", "Drug demand", "Industries", "Pharmaceutical"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0507", "e:abstract": "Local distribution companies (LDCs) play the role of purchasing and delivering natural gas to their consumers, and state regulators oversee the pricing of natural gas to consumers. The common method of regulation, based on the cost of service, provides arguably little incentive for the LDC to optimally manage their procurement activities. In the light of recent deregulation and other changes, benchmarking-based regulatory schemes are being increasingly perceived as the right direction to pursue. Various states are experimenting with simple benchmark mechanisms that have inherent deficiencies and are often criticized. In this paper, we propose and characterize a new kind of benchmark that we call a <i>policy benchmark</i> as a mechanism for regulation. Using variance as the measure of risk, we formulate the regulator's and the LDC's problems as multiple-objective optimizations. We provide rigorous characterizations of the dominance frontiers for a two-stage model. We also provide multistage formulations that take into account various natural gas market microstructures. We compute solutions under parameters estimated from relevant real-world data and illustrate that the structures of the dominance frontiers remain unaltered from the characterizations provided by a stylized two-stage model.", "e:keyword": ["Natural gas", "Regulation", "Benchmarks", "Linear incentive contracts", "Policy benchmarks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1070.0508", "e:abstract": "In this paper, we study capacitated dynamic lot-sizing problems with or without backorders, under the assumption that production costs are linear, that is, there are no setup costs. These two dynamic lot-sizing problems (with or without backorders) are minimum-cost flow problems on an underlying network that possess a special structure. We show how the well-known successive shortest-path algorithm for the minimum-cost flow problem can be used to solve these problems in <i>O</i>(<i>n</i><sup>2</sup>) time, where <i>n</i> is the number of periods in the dynamic lot-sizing problems, and how, with the use of dynamic trees, we can solve these problems in <i>O</i>(<i>n</i> log <i>n</i>) time. Our algorithm also extends to the dynamic lot-sizing problem with integer variables and convex production costs with running time <i>O</i>(<i>n</i> log <i>n</i> log <i>U</i>), where <i>U</i> is the largest demand value.", "e:keyword": ["Inventory/production", "Lot sizing", "Production/scheduling", "Networks/graphs", "Flow algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0249", "e:keyword": ["Professional", "Journal policies", "OR/MS history", "Editorial comments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0513", "e:abstract": "We consider the risk of a portfolio comprising loans, bonds, and financial instruments that are subject to possible default. In particular, we are interested in performance measures such as the probability that the portfolio incurs large losses over a fixed time horizon, and the expected excess loss given that large losses are incurred during this horizon. Contrary to the normal copula that is commonly used in practice (e.g., in the CreditMetrics system), we assume a portfolio dependence structure that is semiparametric, does not hinge solely on correlation, and supports extremal dependence among obligors. A particular instance within the proposed class of models is the so-called <i>t</i>-copula model that is derived from the multivariate Student <i>t</i> distribution and hence generalizes the normal copula model. The size of the portfolio, the heterogeneous mix of obligors, and the fact that default events are rare and mutually dependent make it quite complicated to calculate portfolio credit risk either by means of exact analysis or naïve Monte Carlo simulation. The main contributions of this paper are twofold. We first derive sharp asymptotics for portfolio credit risk that illustrate the implications of extremal dependence among obligors. Using this as a stepping stone, we develop importance-sampling algorithms that are shown to be asymptotically optimal and can be used to efficiently compute portfolio credit risk via Monte Carlo simulation.", "e:keyword": ["Portfolio", "Credit", "Asymptotics", "Expected shortfall", "Simulation", "Importance sampling", "Rare events", "Risk management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0514", "e:abstract": "We study the important problem of how a supplier should optimally share the consequences of demand uncertainty (i.e., the cost of inventory excesses and shortages) with a retailer in a two-level supply chain facing a finite planning horizon. In particular, we characterize a multiperiod contract form, the <i>promised lead-time contract</i>, that reduces the supplier's risk from demand uncertainty and the retailer's risk from uncertain inventory availability. Under the contract terms, the supplier guarantees on-time delivery of complete orders of any size after the promised lead time. We characterize the optimal promised lead time and the corresponding payments that the supplier should offer to minimize her expected inventory cost, while ensuring the retailer's participation. In such a supply chain, the retailer often holds private information about his shortage cost (or his service level to end customers). Hence, to understand the impact of the promised lead-time contract on the supplier's and the retailer's performance, we study the system under <i>local control with full information</i> and <i>local control with asymmetric information.</i> By comparing the results under these information scenarios to those under a <i>centrally controlled system</i>, we provide insights into stock positioning and inventory risk sharing. We quantify, for example, how much and when the supplier and the retailer overinvest in inventory as compared to the centrally controlled supply chain. We show that the supplier faces more inventory risk when the retailer has private service-level information. We also show that a supplier located closer to the retailer is affected less by information asymmetry. Next, we characterize when the supplier should optimally choose not to sign a promised lead-time contract and consider doing business under other settings. In particular, we establish the optimality of a <i>cutoff level policy.</i> Finally, under both full and asymmetric service-level information, we characterize conditions when optimal promised lead times take extreme values of the feasible set, yielding the supplier to assume all or none of the inventory risk---hence the name <i>all-or-nothing solution.</i> We conclude with numerical examples demonstrating our results.", "e:keyword": ["Inventory/production", "Multi-item/echelon/stage", "Uncertainty", "Stochastic", "Operating characteristics", "Game theory", "Mechanism design", "Finite types", "Screening"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0515", "e:abstract": "In this paper, we address the optimal joint control of inventory and transshipment for a firm that produces in two locations and faces capacity uncertainty. Capacity uncertainty (e.g., due to downtime, quality problems, yield, etc.) is a common feature of many production systems, but its effects have not been explored in the context of a firm that has multiple production facilities. We first characterize the optimal production and transshipment policies and show that uncertain capacity leads the firm to ration the inventory that is available for transshipment to the other location and characterize the structure of this rationing policy. Then, we characterize the optimal production policies at both locations, which are defined by state-dependent produce-up-to thresholds. We also describe sensitivity of the optimal production and transshipment policies to problem parameters and, in particular, explain how uncertain capacity can lead to counterintuitive behavior, such as produce-up-to limits decreasing for locations that face stochastically higher demand. We finally explore, through a numerical study, when the optimal policy is most likely to yield significant benefits compared to simple policies.", "e:keyword": ["Dynamic programming", "Application", "Inventory/production", "Uncertainty/stochastic", "Reliability", "Capacity", "Uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0519", "e:abstract": "<i>This paper is dedicated to Arthur Geoffrion, who serves as role model of a great researcher, educator, and practitioner.</i>We believe that research, teaching, and practice are becoming increasingly disengaged from one another in the OR/MS ecosystem. This ecosystem comprises researchers, educators, and practitioners in its core along with end users, universities, and funding agencies. Continuing disengagement will result in OR/MS occupying only niche areas and disappearing as a distinct field even though its tools would live on. To understand the reasons for this disengagement better and to engender discussion among academics and practitioners on how to counter it, we present the ecosystem's strengths, weaknesses, opportunities, and threats. Incorporated in this paper are insights from a cluster of sessions at the 2006 INFORMS meeting in Pittsburgh (“Where Do We Want to Go in OR/MS?”) and from the literature.", "e:keyword": ["Operations research", "Management science", "SWOT analysis", "Ecosystem", "Change management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0520", "e:abstract": "We consider the vehicle-routing problem with stochastic demands (VRPSD) under reoptimization. We develop and analyze a finite-horizon Markov decision process (MDP) formulation for the single-vehicle case and establish a partial characterization of the optimal policy. We also propose a heuristic solution methodology for our MDP, named partial reoptimization, based on the idea of restricting attention to a subset of all the possible states and computing an optimal policy on this restricted set of states. We discuss two families of computationally efficient partial reoptimization heuristics and illustrate their performance on a set of instances with up to and including 100 customers. Comparisons with an existing heuristic from the literature and a lower bound computed with complete knowledge of customer demands show that our best partial reoptimization heuristics outperform this heuristic and are on average no more than 10%--13% away from this lower bound, depending on the type of instances.", "e:keyword": ["Dynamic programming", "Application", "Heuristics", "Network/graphs", "Stochastic model", "Transportation", "Vehicle-routing problem", "Stochastic demands"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0521", "e:abstract": "We study the performance of a stylized supply chain where two firms, a retailer and a producer, compete in a Stackelberg game. The retailer purchases a single product from the producer and afterward sells it in the retail market at a stochastic clearance price. The retailer, however, is budget constrained and is therefore limited in the number of units that he may purchase from the producer. We also assume that the retailer's profit depends in part on the realized path or terminal value of some observable stochastic process. We interpret this process as a financial process such as a foreign exchange rate or interest rate. More generally, the process can be interpreted as any relevant economic index. We consider a variation (the flexible contract) of the traditional wholesale price contract that is offered by the producer to the retailer. Under this flexible contract, at <i>t</i> = 0 the producer offers a menu of wholesale prices to the retailer, one for each realization of the financial process up to a future time (tau). The retailer then commits to purchasing at time (tau) a variable number of units, with the specific quantity depending on the realization of the process up to time (tau). Because of the retailer's budget constraint, the supply chain might be more profitable if the retailer was able to shift some of the budget from states where the constraint is not binding to states where it is binding. We therefore consider a variation of the flexible contract, where we assume that the retailer is able to trade dynamically between zero and (tau) in the financial market. We refer to this variation as the <i>flexible contract with hedging</i>. We compare the decentralized competitive solution for the two contracts with the solutions obtained by a central planner. We also compare the supply chain's performance across the two contracts. We find, for example, that the producer always prefers the flexible contract with hedging to the flexible contract without hedging. Depending on model parameters, however, the retailer might or might not prefer the flexible contract with hedging.", "e:keyword": ["Finance", "Portfolio", "Management", "Inventory/production", "Applications", "Procurement contract", "Financial constraints", "Supply chain coordination"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0522", "e:abstract": "Motivated by the challenges faced by the telecom industry during the past decade, in this paper we study a dynamic capacity expansion problem for service firms. There is a random demand for the firm's capacity in each period: the demand in excess of the capacity is lost, and revenue is generated for the fulfilled demand. At the beginning of each period, the firm might increase its capacity through purchasing equipment for immediate delivery, which is constrained by a random supply limit, or it might sign a future contract for equipment delivery in the following period. We assume that the firm's capacity might partially become obsolete due to natural deterioration or technology innovation. We aim at characterizing optimal capacity expansion strategies and comparing the profit functions as well as the optimal control policies of different options. Specifically, we show that the optimal capacity expansion policy for the current period is determined by a base-stock policy. Compared with the case where no future contracts are available, the optimal control parameters of capacity expansion are always smaller. We further show that when the obsolescence rate is deterministic, the optimal policy for capacity expansion through future contracts is also a base-stock type. The results are extended to the cases with stochastically dependent capacity supply limits and stochastically dependent demand processes, which establish the robustness of the optimal policy in various market conditions.", "e:keyword": ["Capacity expansion", "Future contracts", "Base-stock policy", "Submodularity", "Stochastic order"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0523", "e:abstract": "This paper reports on an application of network-flow integer programming to a vacation timeshare exchange problem. A typical timeshare owner has purchased yearly access to a specific week at a specific resort. The resulting lack of vacation variety is mitigated by systems that allow owners to exchange owned weeks for different weeks at different resorts according to their preferences, the assessed value of what they are exchanging, their contractual priority, and resort availability. The timeshare exchange problem is similar to other preference-based assignment problems such as labor scheduling, preferential bidding, and traditional timetabling, but different in the formulation of the objective function. This paper demonstrates how the effectiveness of timeshare exchange processes can be improved through mathematical optimization, as measured by increased satisfaction of participant preferences. Optimization also presents exchange managers with the opportunity to more precisely manage preference and priority trade-offs among various classes of participants. The trade-off decisions are aided by sensitivity analysis utilizing a minmax criterion.", "e:keyword": ["Operations research applications", "Optimization", "Service sector"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0524", "e:abstract": "We introduce the pseudoflow algorithm for the maximum-flow problem that employs only pseudoflows and does not generate flows explicitly. The algorithm solves directly a problem equivalent to the minimum-cut problem---the maximum blocking-cut problem. Once the maximum blocking-cut solution is available, the additional complexity required to find the respective maximum-flow is <i>O</i>(<i>m</i> log <i>n</i>). A variant of the algorithm is a new parametric maximum-flow algorithm generating all breakpoints in the same complexity required to solve the constant capacities maximum-flow problem. The pseudoflow algorithm has also a simplex variant, pseudoflow-simplex, that can be implemented to solve the maximum-flow problem. One feature of the pseudoflow algorithm is that it can initialize with any pseudoflow. This feature allows it to reach an optimal solution quickly when the initial pseudoflow is “close” to an optimal solution. The complexities of the pseudoflow algorithm, the pseudoflow-simplex, and the parametric variants of pseudoflow and pseudoflow-simplex algorithms are all <i>O</i>(<i>mn</i> log <i>n</i>) on a graph with <i>n</i> nodes and <i>m</i> arcs. Therefore, the pseudoflow-simplex algorithm is the fastest simplex algorithm known for the parametric maximum-flow problem. The pseudoflow algorithm is also shown to solve the maximum-flow problem on <i>s</i>, <i>t</i>-tree networks in linear time, where <i>s, t</i>-tree networks are formed by joining a forest of capacitated arcs, with nodes <i>s</i> and <i>t</i> adjacent to any subset of the nodes.", "e:keyword": ["Flow algorithms", "Parametric flow", "Normalized tree", "Lowest label", "Pseudoflow algorithm", "Maximum flow"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0525", "e:abstract": "We consider the planning and scheduling of production in a multitask/multistage batch manufacturing process typical of industries such as chemical manufacturing, food processing, and oil refining. We allow instances in which multiple sequences of tasks may be used to produce end products. We formulate the problem as a mixed-integer linear program and show that the linear programming relaxation has a large integrality gap and requires significant computational effort to solve to optimality for large instances. Using echelon inventory, we construct a new family of valid inequalities for this problem. The formulation with the additional constraints leads to a significantly tighter linear programming relaxation and to greatly reduced solution times for the mixed-integer linear program.", "e:keyword": ["Production planning/scheduling", "Echelon inventory", "Integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0526", "e:abstract": "The defined-benefit pension system poses substantial, long-term risks for the U.S. economy. We describe a flexible asset-liability management (ALM) system for pension planning. The primary goals are to improve the performance and survivability of the pension trust. We first employ a stochastic program for enhancing investment strategies in light of company and other goals and pension risk constraints. The results are linked with a policy simulator for further analysis. We illustrate the concepts via two disparate real-world companies. The first is a slowly growing auto company, and the second a profitable pharmaceutical enterprise. We show that a stochastic program can help in the process of discovering sound policy rules. The ALM system has been employed extensively throughout the world by a large global actuarial firm.", "e:keyword": ["Finance", "Corporate", "Portfolio", "Programming", "Stochastic", "Simulation", "Applications", "Decision analysis", "Multiple criteria", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0527", "e:abstract": "Annual influenza outbreaks incur great expenses in both human and monetary terms, and billions of dollars are being allocated for influenza pandemic preparedness in an attempt to avert even greater potential losses. Vaccination is a primary weapon for fighting influenza outbreaks. The influenza vaccine supply chain has characteristics that resemble the newsvendor problem but possesses several characteristics that distinguish it from many other supply chains. Differences include a nonlinear value of sales (caused by the nonlinear health benefits of vaccination that are due to infection dynamics) and vaccine production yield issues. We show that production risks, taken currently by the vaccine manufacturer, lead to an insufficient supply of vaccine. Several supply contracts that coordinate buyer (governmental public health service) and supplier (vaccine manufacturer) incentives in many other industrial supply chains cannot fully coordinate the influenza vaccine supply chain. We design a variant of the cost-sharing contract and show that it provides incentives to both parties so that the supply chain achieves global optimization and hence improves the supply of vaccines.", "e:keyword": ["Inventory/production", "Health care", "Epidemiology", "Industries", "Pharmaceutical"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0528", "e:abstract": "We analyze the problem of allocating cost savings from sharing demand information in a three-level supply chain with a manufacturer, a distributor, and a retailer. To find a unique allocation scheme, we use concepts from cooperative game theory. First, we analytically compute the expected cost incurred by the manufacturer and then use simulation to obtain expected costs for the distributor and the retailer. We construct a three-person cooperative game in characteristic-function form and derive necessary conditions for the stability of each of five possible coalitions. To divide the cost savings between two members, or among three supply chain members, we use various allocation schemes. We present numerical analyses to investigate the impacts of the demand autocorrelation coefficient, (rho), and the unit holding and shortage costs on the allocation scheme.", "e:keyword": ["Inventory/production", "Information sharing", "Games/group decisions", "Cooperative", "Supply chain management", "Cooperative game theory", "Nash arbitration scheme", "Constrained core", "Shapley value", "Constrained nucleolus solution", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0529", "e:abstract": "Motivated by the question of which point-estimator digits to report in a statistical experiment, we study the probabilistic behavior of the digits as a function of the true performance measure and the point estimator's standard error. We investigate the family of Leading-Digit Rules, which guarantees that every unreported digit has correctness probability below a given threshold. Choosing the threshold to be about 0.198 yields Yoneda's rule. The easy-to-implement rule that reports the point estimate through the leading digit of the standard error has threshold (approximately) 0.117, which is not much larger than the one-in-ten probability of a uniformly distributed random digit being correct.", "e:keyword": ["Statistical experiments", "Point estimator", "Standard error"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0530", "e:abstract": "This note studies the optimal dynamic decision-making problem for a retailer in a price-sensitive, multiplicative demand framework. Our model incorporates lost sales, holding cost, fixed and variable procurement costs, as well as salvage value. We characterize the structure of the retailer's (discounted) expected profit-maximizing dynamic inventory policy for both finite and infinite selling horizon problems.", "e:keyword": ["Dynamic pricing and inventory control", "Multiplicative demand", "Fixed cost", "Lost sales"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0531", "e:abstract": "Quantiles of a random performance serve as important alternatives to the usual expected value. They are used in the financial industry as measures of risk and in the service industry as measures of service quality. To manage the quantile of a performance, we need to know how changes in the input parameters affect the output quantiles, which are called quantile sensitivities. In this paper, we show that the quantile sensitivities can be written in the form of conditional expectations. Based on the conditional-expectation form, we first propose an infinitesimal-perturbation-analysis (IPA) estimator. The IPA estimator is asymptotically unbiased, but it is not consistent. We then obtain a consistent estimator by dividing data into batches and averaging the IPA estimates of all batches. The estimator satisfies a central limit theorem for the i.i.d. data, and the rate of convergence is strictly slower than <i>n</i><sup>-1/3</sup>. The numerical results show that the estimator works well for practical problems.", "e:keyword": ["Quantile", "Value-at-risk", "Sensitivity analysis", "Simulation", "Statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0532", "e:abstract": "Motivated by make-to-order production systems, we consider a dynamic control problem for a multiclass, parallel-server queueing system. The production system serves multiple classes of customers who require rigid due-date lead times and may cancel their order subject to a cancellation penalty. To meet the due-date constraints, a system manager may outsource orders when the backlog of work is judged excessive, thereby incurring outsourcing costs. The system manager strives to minimize long-run average costs by dynamically making outsourcing and resource allocation decisions. Under heavy-traffic conditions, the scheduling problem is approximated by a Brownian control problem. Interpreting the solution of the Brownian control problem in the context of the original queueing system, a nongreedy outsourcing and resource allocation policy is proposed. A simulation experiment is performed to demonstrate the effectiveness of this policy.", "e:keyword": ["Make-to-order production", "Parallel-server queues", "Heavy-traffic approximations", "Diffusion models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0533", "e:abstract": "This paper studies the performance impact of making delay announcements to arriving customers who must wait before starting service in a many-server queue with customer abandonment. The queue is assumed to be invisible to waiting customers, as in most customer contact centers, when contact is made by telephone, e-mail, or instant messaging. Customers who must wait are told upon arrival either the delay of the last customer to enter service or an appropriate average delay. Models for the customer response are proposed. For a rough-cut performance analysis, prior to detailed simulation, two approximations are proposed: (1) the equilibrium delay in a deterministic fluid model, and (2) the equilibrium steady-state delay in a stochastic model with fixed delay announcements. These approximations are shown to be effective in overloaded regimes, where delay announcements are important, by making comparisons with simulations. Within the fluid model framework, conditions are established for the existence and uniqueness of an equilibrium delay, where the actual delay coincides with the announced delay. Multiple equilibria can occur if a key monotonicity condition is violated.", "e:keyword": ["Queues", "Applications", "Approximations", "Balking and reneging"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0534", "e:abstract": "A firm often makes an adoption decision regarding an improvement of one technology depending on changes in other technologies. For example, a manufacturer with a serial production line considers jointly upgrading multiple machines, or a firm producing an assembled product considers improving several components simultaneously. Economies or diseconomies of scope in the fixed cost of adoption when multiple improvements are undertaken at the same time generate an economic dependence among the technological innovations. Although the literature on technological innovations has attributed slow adoption mainly to uncertainties outside the firm, this paper shows that the economic dependence that inherently defines cost relationships inside the firm can significantly influence the timing of adoption. Furthermore, this impact is not unidirectional: economic dependence can either expedite or delay the adoption of an improved technology.", "e:keyword": ["Dynamic programming", "Facility/equipment planning", "Technology"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0535", "e:abstract": "This paper addresses the problem of finding cutting planes for multistage stochastic integer programs. We give a general method for generating cutting planes for multistage stochastic integer programs based on combining inequalities that are valid for the individual scenarios. We apply the method to generate cuts for a stochastic version of a dynamic knapsack problem and for stochastic lot-sizing problems. We give computational results, which show that these new inequalities are very effective in a branch-and-cut algorithm.", "e:keyword": ["Programming", "Integer", "Cutting plane/facet", "Programming", "Stochastic", "Inventory/production", "Lot sizing", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0536", "e:abstract": "In this paper, we study dynamic supplier alliances in a decentralized assembly system. We examine a supply chain in which <i>n</i> suppliers sell complementary components to a downstream assembler, who faces a price-sensitive deterministic demand. We analyze alliance/coalition formation between suppliers, using a two-stage approach. In Stage 1, suppliers form coalitions that each agree to sell a kit of components to the assembler. In Stage 2, coalitions make wholesale price decisions, whereas the assembler buys the components (kits) from the coalitions and sets the selling price of the product. Stage 2 is modeled as a competitive game, in which the primary competition is vertical (i.e., supplier coalitions compete against the downstream assembler), and the secondary competition is horizontal, in that coalitions compete against each other. Here, we consider three modes of competition---Supplier Stackelberg, Vertical Nash, and Assembler Stackelberg models---that correspond to different power structures in the market. In Stage 1, we analyze the stability of coalition structures. We assume that suppliers are farsighted, that is, each coalition considers the possibility that once it acts, another coalition may react, and a third coalition might in turn react, and so on. Using this framework, we predict the structure of possible supplier alliances as a function of the power structure in the market, the number of suppliers, and the structure of the demand.", "e:keyword": ["Assembly systems", "Coalitions", "Cooperative games"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0540", "e:abstract": "We consider a market with two suppliers and a set of buyers in search of procurement contracts with one of the suppliers. In particular, each buyer needs to process a certain volume of work, and each supplier's ability to process the customers' requests is constrained by a production capacity. The procurement contracts include guarantees that the products will be available when needed, and the buyers select a supplier based on their service delivery offers. The suppliers are modeled as make-to-stock queues and compete for the buyers' business. The main objective of this paper is to determine how the procurement contracts are established between buyers and suppliers. Because each buyer selects a single supplier to establish the sourcing relationship, the game fails to have a pure-strategy Nash equilibrium. Instead, an equilibrium is defined as the limit equilibrium of some discrete action games.", "e:keyword": ["Facilities/equipment planning", "Capacity expansion", "Games", "Noncooperative", "Inventory/production", "Multi-item", "Queues", "Priority"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0541", "e:abstract": "We propose a static approximation of dynamic demand substitution behavior based on a fluid network model and a service-inventory mapping. This approximation greatly enhances our ability to analyze the interdependent inventory/service, price, and product assortment decisions in noncompetitive and competitive scenarios with demand substitution. We demonstrate that the approximation is well behaved and then apply it to two previously intractable applications. First, we study a price and service competition between single-product retailers. After establishing a unique pure-strategy Nash equilibrium, we find that competition results in lower price, higher demand, and a higher level of inventory. We also observe that the aggregate profit and inventory level increase to positive constants as the number of retailers goes to infinity. Second, we study a duopolistic competition on price, service, and product assortment. We establish a pure-strategy Nash equilibrium for the product assortment competition and identify a condition for uniqueness. We find that competition on both price and product assortment results in lower price and less variety for each competitor, but the total number of products and the aggregate inventory level in a duopoly market are both likely to be higher than in a monopolistic market.", "e:keyword": ["Competition", "Demand substitution", "Logit demand", "Pricing", "Product assortment", "Service"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0542", "e:abstract": "In addition to having uncertain patient arrivals, primary-care clinics also face uncertainty arising from patient choices. Patients have different perceptions of the acuity of their need, different time-of-day preferences, as well as different degrees of loyalty toward their designated primary-care provider (PCP). Advanced access systems are designed to reduce wait and increase satisfaction by allowing patients to choose either a same-day or a scheduled future appointment. However, the clinic must carefully manage patients' access to physicians' slots to balance the needs of those who book in advance and those who require a same-day appointment. On the one hand, scheduling too many appointments in advance can lead to capacity shortages when same-day requests arrive. On the other hand, scheduling too few appointments increases patients' wait time, patient-PCP mismatch, and the possibility of clinic slots going unused.The capacity management problem facing the clinic is to decide which appointment requests to accept to maximize revenue. We develop a Markov decision process model for the appointment-booking problem in which the patients' choice behavior is modeled explicitly. When the clinic is served by a single physician, we prove that the optimal policy is a threshold-type policy as long as the choice probabilities satisfy a weak condition. For a multiple-doctor clinic, we partially characterize the structure of the optimal policy. We propose several heuristics and an upper bound. Numerical tests show that the two heuristics based on the partial characterization of the optimal policy are quite accurate. We also study the effect on the clinic's optimal profit of patients' loyalty to their PCPs, total clinic load, and load imbalance among physicians.", "e:keyword": ["Health-care operations", "Patient choice", "Capacity reservation", "Revenue management", "Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0543", "e:abstract": "We formulate a finite-horizon nonstationary dynamic single-asset assembly problem, which covers both a liquid-asset assembly problem, based on the work of Rosling published in 1989, where the single asset is the single product being assembled, and a fixed-asset assembly problem, in which the single asset is production capacity. In the latter case, capacity is assembled over time from components and may be used to manufacture many products. In the spirit of Rosling, we provide conditions under which it can be solved by an equivalent analogous serial model in the form developed by Clark and Scarf in 1960, with a separate state variable for the level of assets in each stage of completion, dramatically simplifying the problem and its solution. In the liquid-asset case, we extend Rosling's 1989 work by including nonstationary demands, costs, and revenues in a finite-horizon setting. In the fixed-asset (capacity) expansion case, we show that capacity should be assembled in a balanced way and derive the optimal timing and extent of delays (in previously initiated capacity expansions). Our basic capacity expansion model is deterministic, so under our conditions, it is optimal never to delay a schedule and the state space reduces to a single dimension. However, in the Markov-modulated case, in which cost parameters and customer demand distributions can be influenced by a randomly and exogenously evolving state (of the economy), we illustrate that delays can be optimal.", "e:keyword": ["Dynamic programming", "State space reduction", "Facilities/equipment planning", "Dynamic assembly", "Inventory/production", "Multiechelon nonstationary assembly"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0545", "e:abstract": "In many service industries, companies compete with each other on the basis of the waiting time their customers experience, along with the price they charge for their service. A firm's waiting-time standard may either be defined in terms of the expected value or a given, for example 95%, percentile of the steady state waiting-time distribution. We investigate how a service industry's competitive behavior depends on the characteristics of the service providers' queueing systems. We provide a unifying approach to investigate various standard single-stage systems covering the spectrum from M/M/1 to general G/GI/s systems, along with open Jackson networks to represent multistage service systems. Assuming that the capacity cost is proportional with the service rates, we refer to its dependence on (i) the firm's demand rate, and (ii) the waiting-time standard as the capacity cost function. We show that across the above broad spectrum of queueing models, the capacity cost function belongs to a specific four-parameter class of function, either exactly or as a close approximation. We then characterize how this capacity cost function impacts the equilibrium behavior in the industry. We give separate treatments to the case where the firms compete in terms of (i) prices (only), (ii) their service level or waiting-time standard (only), and (iii) simultaneously in terms of both prices and service levels. The firms' demand rates are given by a general system of equations of the prices and waiting-time standards in the industry.", "e:keyword": ["Queues", "Multichannel games", "Noncooperative marketing", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0547", "e:abstract": "We propose a heuristic for finding base order quantities for stochastic inventory models. The heuristic includes two steps. The first clusters the stages according to cost parameters. The second solves a single-stage problem for each cluster with the original problem data. In a numerical study, we show that the heuristic is near optimal.", "e:keyword": ["Multiecholon", "(R", "nQ) policies", "Simple solution"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0548", "e:abstract": "The one-dimensional facility layout problem is concerned with arranging <i>n</i> departments of given lengths on a line, while minimizing the weighted sum of the distances between all pairs of departments. The problem is NP-hard because it is a generalization of the minimum linear arrangement problem. In this paper, a 0-1 quadratic programming model consisting of only O(<i>n</i><sup>2</sup>) 0-1 variables is proposed for the problem. Subsequently, this model is cast as an equivalent mixed-integer program and then reduced by preprocessing. Next, additional redundant constraints are introduced and linearized in a higher space to achieve an equivalent mixed 0-1 linear program, whose continuous relaxation provides an approximation of the convex hull of solutions to the quadratic program. It is shown that the resulting mixed 0-1 linear program is more efficient than previously published mixed-integer formulations. In the computational results, several problem instances taken from the literature were efficiently solved to optimality. Moreover, it is now possible to efficiently solve problems of a larger size.", "e:keyword": ["Facilities/equipment planning", "Layout", "Programming", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0549", "e:abstract": "This paper examines the impact of customer order sizes on a make-to-stock system with multiple demand classes. We first characterize the manufacturer's optimal production and rationing policies when the demand is nonunitary and lost if unsatisfied. We also investigate the optimal policies of a backorder system with two demand classes and fixed order sizes. Through a numerical study, we show the effects of batch orders on the manufacturer's inventory cost as well as on the benefit of optimal stock rationing. It is shown that batch ordering may reduce the manufacturer's overall cost if carefully introduced in a first-come-first-served (FCFS) system. With the same effective demand rates, the customers' order sizes also have a strong impact on the benefit of optimal stock rationing.", "e:keyword": ["Batch ordering", "Make-to-stock", "Stock rationing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0550", "e:abstract": "Virtual nesting is a popular capacity control strategy in network revenue management. In virtual nesting, products (itinerary-fare-class combinations) are mapped (“indexed”) into a relatively small number of “virtual classes” on each resource (flight leg) of the network. Nested protection levels are then used to control the availability of these virtual classes; specifically, a product request is accepted if and only if its corresponding virtual class is available on each resource required. Bertsimas and de Boer proposed an innovative simulation-based optimization method for computing protection levels in a virtual nesting control scheme [Bertsimas, D., S. de Boer. 2005. Simulation-based booking-limits for airline revenue management. <i>Oper. Res.</i> <b>53</b> 90--106]. In contrast to traditional heuristic methods, this simulation approach captures the true network revenues generated by virtual nesting controls. However, because it is based on a discrete model of capacity and demand, the method has both computational and theoretical limitations. In particular, it uses first-difference estimates, which are computationally complex to calculate exactly. These gradient estimates are then used in a steepest-ascent-type algorithm, which, for discrete problems, has no guarantee of convergence. In this paper, we analyze a continuous model of the problem that retains most of the desirable features of the Bertsimas-de Boer method, yet avoids many of its pitfalls. Because our model is continuous, we are able to compute gradients exactly using a simple and efficient recursion. Indeed, our gradient estimates are often an order of magnitude faster to compute than first-difference estimates, which is an important practical feature given that simulation-based optimization is computationally intensive. In addition, because our model results in a smooth optimization problem, we are able to prove that stochastic gradient methods are at least locally convergent. On several test problems using realistic networks, the method is fast and produces significant performance improvements relative to the protection levels produced by heuristic virtual nesting schemes. These results suggest it has good practical potential.", "e:keyword": ["Stochastic algorithm", "Stochastic gradients", "Revenue management", "Capacity control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0551", "e:abstract": "We analyze a planning model for a firm or public organization that needs to cover uncertain demand for a given item by procuring supplies from multiple sources. Each source faces a random yield factor with a general probability distribution. The model considers a single demand season. All supplies need to be ordered before the start of the season. The planning problem amounts to selecting which of the given set of suppliers to retain, and how much to order from each, so as to minimize total procurement costs while ensuring that the uncertain demand is met with a given probability. The total procurement costs consist of variable costs that are proportional to the total quantity delivered by the suppliers, and a fixed cost for each participating supplier, incurred irrespective of his supply level. Each potential supplier is characterized by a given fixed cost and a given distribution of his random yield factor. The yield factors at different suppliers are assumed to be independent of the season's demand, which is described by a general probability distribution. Determining the optimal set of suppliers, the aggregate order and its allocation among the suppliers, on the basis of the exact shortfall probability, is prohibitively difficult. We have therefore developed two approximations for the shortfall probability. Although both approximations are shown to be highly accurate, the first, based on a large-deviations technique (LDT), has the advantage of resulting in a rigorous upper bound for the required total order and associated costs. The second approximation is based on a central limit theorem (CLT) and is shown to be asymptotically accurate, whereas the order quantities determined by this method are asymptotically optimal as the number of suppliers grows. Most importantly, this CLT-based approximation permits many important qualitative insights.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Operating characteristics", "Reliability", "Failure models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0552", "e:abstract": "We examine a possibly capacitated, periodically reviewed, single-stage inventory system where replenishment can be obtained either through a regular fixed lead time channel, or, for a premium, via a channel with a smaller fixed lead time. We consider the case when the unsatisfied demands are backordered over an infinite horizon, introducing the easily implementable, yet informationally rich dual-index policy. We show very general separability results for the optimal parameter values, providing a simulation-based optimization procedure that exploits these separability properties to calculate the optimal inventory parameters within seconds. We explore the performance of the dual-index policy under stationary demands as well as capacitated production environments, demonstrating when the dual-sourcing option is most valuable. We find that the optimal dual-index policy mimics the behavior of the complex, globally optimal state-dependent policy found via dynamic programming: the dual-index policy is nearly optimal (within 1% or 2%) for the majority of cases, and significantly outperforms single sourcing (up to 50% better). Our results on optimal dual-index parameters are generic, extending to a variety of complex and realistic scenarios such as nonstationary demand, random yields, demand spikes, and supply disruptions.", "e:keyword": ["Inventory/production", "Infinite horizon", "Policies", "Review/lead times", "Dual index", "Dual supply", "Uncertainty", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0553", "e:abstract": "We consider a multiperiod inventory system of a perishable product with unobservable lost sales. Demand distribution parameters are unknown and are updated periodically using the Bayesian approach based on the censored historical sales data. We develop an explicit expression of the first-order condition for optimality that demonstrates the key trade-off of the problem. The result generalizes partial characterizations of this trade-off in the literature. It shows that the myopic solution is a lower bound on the optimal inventory level. It also enables us to quantify the expected marginal value of information.", "e:keyword": ["Stochastic inventory control", "Censored demand data", "Bayesian models", "Exact analysis", "Sample-path approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0555", "e:abstract": "The stochastic knapsack has been used as a model in wide-ranging applications from dynamic resource allocation to admission control in telecommunication. In recent years, a variation of the model has become a basic tool in studying problems that arise in revenue management and dynamic/flexible pricing, and it is in this context that our study is undertaken. Based on a dynamic programming formulation and associated properties of the value function, we study in this paper a class of control that we call switch-over policies---start by accepting only orders of the highest price, and switch to including lower prices as time goes by, with the switch-over times optimally decided via convex programming. We establish the asymptotic optimality of the switch-over policy, and develop pricing models based on this policy to optimize the price reductions over the decision horizon.", "e:keyword": ["Revenue management", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0556", "e:abstract": "We introduce and study a randomized quasi-Monte Carlo method for the simulation of Markov chains up to a random (and possibly unbounded) stopping time. The method simulates <i>n</i> copies of the chain in parallel, using a (<i>d+1</i>)-dimensional, highly uniform point set of cardinality <i>n</i>, randomized independently at each step, where <i>d</i> is the number of uniform random numbers required at each transition of the Markov chain. The general idea is to obtain a better approximation of the state distribution, at each step of the chain, than with standard Monte Carlo. The technique can be used in particular to obtain a low-variance unbiased estimator of the expected total cost when state-dependent costs are paid at each step. It is generally more effective when the state space has a natural order related to the cost function.We provide numerical illustrations where the variance reduction with respect to standard Monte Carlo is substantial. The variance can be reduced by factors of several thousands in some cases. We prove bounds on the convergence rate of the worst-case error and of the variance for special situations where the state space of the chain is a subset of the real numbers. In line with what is typically observed in randomized quasi-Monte Carlo contexts, our empirical results indicate much better convergence than what these bounds guarantee.", "e:keyword": ["Simulation", "Efficiency", "Probability", "Markov processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0557", "e:abstract": "We propose an algorithm to calculate confidence intervals for the values of hedging parameters of discretely exercisable options using Monte Carlo simulation. The algorithm is based on a combination of the duality formulation of the optimal stopping problem for pricing discretely exercisable options and Monte Carlo estimation of hedging parameters for European options. We show that the width of the confidence interval for a hedging parameter decreases, with an increase in the computer budget, asymptotically at the same rate as the width of the confidence interval for the price of the option. The method can handle arbitrary payoff functions, general diffusion processes, and a large number of random factors. We also present a fast, heuristic, alternative method and use our method to evaluate its accuracy.", "e:keyword": ["Finance", "Derivative hedging"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0558", "e:abstract": "This paper develops rare-event simulation methods for the estimation of portfolio credit risk---the risk of losses to a portfolio resulting from defaults of assets in the portfolio. Portfolio credit risk is measured through probabilities of large losses, which are typically due to defaults of many obligors (sources of credit risk) to which a portfolio is exposed. An essential element of a portfolio view of credit risk is a model of dependence between these sources of credit risk: large losses occur rarely and are most likely to result from systematic risk factors that affect multiple obligors. As a consequence, estimating portfolio credit risk poses a challenge both because of the rare-event property of large losses and the dependence between defaults. To address this problem, we develop an importance sampling technique within the widely used Gaussian copula model of dependence. We focus on difficulties arising in multifactor models---that is, models in which multiple factors may be common to multiple obligors, resulting in complex dependence between defaults. Our importance sampling procedure shifts the mean of the common factor to increase the frequency of large losses. In multifactor models, different combinations of factor outcomes and defaults can produce large losses, so our method combines multiple importance sampling distributions, each associated with a shift in the mean of common factors. We characterize “optimal” mean shifts. Finding these points is both a combinatorial problem and a convex optimization problem, so we address computational aspects of this step as well. We establish asymptotic optimality results for our method, showing that---unlike standard simulation---it remains efficient as the event of interest becomes rarer.", "e:keyword": ["Simulation", "Efficiency", "Variance reduction", "Probability", "Large deviations", "Finance", "Credit risk", "Portfolio"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0559", "e:abstract": "A methodology is given for modeling the dynamics of discrete-event stochastic systems as optimization problems. The intent is to provide a means to utilize the rich mathematical theory and algorithms of optimization in the study of this important class of systems. A procedure for mapping a simulation event relationship graph into a mixed-integer program is presented, along with examples of queueing networks and manufacturing systems that illustrate the approach. Several potential applications are examined, including automatic constraint generation for optimal resource scheduling, representations of max-plus algebra models for queueing system dynamics, response gradient estimation, and an unconventional technique for simulating queueing systems using virtual resources that are identified from the optimization models for these systems.", "e:keyword": ["Simulation", "Methodology", "System dynamics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0560", "e:abstract": "A continuous-review two-echelon inventory system with one central warehouse and a number of nonidentical retailers is considered. The retailers face independent Poisson demand and apply standard (<i>R, Q</i>) policies. The retailer order quantities are fixed integer multiples of a certain batch size, representing the smallest pallet or container size transported in the system. A warehouse order may consist of one or several such batches. We derive a new policy for warehouse ordering, which is optimal in the broad class of position-based policies relying on complete information about the retailer inventory positions, transportation times, cost structures, and demand distributions at all facilities. The exact analysis of the new policy includes a method for determining the expected total inventory holding and backorder costs for the entire system. The class of position-based policies encompasses both the traditional installation-stock and echelon-stock (<i>R, Q</i>) policies, as well as the more sophisticated policies recently analyzed in the literature. The value of more carefully incorporating a richer information structure into the warehouse ordering policy is illustrated in a numerical study.", "e:keyword": ["Inventory/production", "Multiechelon", "Policies", "Continuous review", "Stochastic", "Poisson demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0561", "e:abstract": "This article presents a risk-sensitive model for managing perishable products assuming the supplier is averse to the variation of revenues. While traditional risk-neutral revenue management models offer optimal strategies in the long run, they are exposed to the variation of revenue flows. If a short-term revenue target is a primary concern for the supplier, the risk-neutral assumption fails to provide the best policy needed. The proposed model uses an exponential function with a risk-sensitive parameter instead of the conventional risk-neutral objective. The risk parameter measures how the supplier is sensitive to the deviation of revenues. We show that the new objective function captures the supplier's risk behavior. We develop a recursive procedure for the optimal solution in closed form. The optimal policy has attractive properties such as nested active price set, monotonicity with respect to the remaining time and inventory, and threshold-type control. When the supplier is more sensitive to the uncertain revenue flows, the risk-sensitive model leads to more conservative pricing policies. Finally, we show that the risk-neutral model is a special case of the proposed framework.", "e:keyword": ["Revenue management", "Risk sensitive", "Exponential utility"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0562", "e:abstract": "We seek optimal inventory levels and prices of multiple products in a given assortment in a newsvendor model (single period, stochastic demand) under price-based substitution, but not stockout-based substitution. We address a demand model involving multiplicative uncertainty, motivated by market share models often used in marketing. The pricing problem that arises is known not to be well behaved in the sense that, in its deterministic version, the objective function is not jointly quasi-concave in prices. However, we find that the objective function is still reasonably well behaved in the sense that there is a unique solution to the first-order conditions, and this solution is optimal for our problem.", "e:keyword": ["Inventory/production", "Multi-item", "Pricing", "Stochastic", "Marketing", "Choice models", "Pricing", "Retailing", "Wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0564", "e:abstract": "This paper describes the financial planning model InnoALM we developed at Innovest for the Austrian pension fund of the electronics firm Siemens. The model uses a multiperiod stochastic linear programming framework with a flexible number of time periods of varying length. Uncertainty is modeled using multiperiod discrete probability scenarios for random return and other model parameters. The correlations across asset classes, of bonds, stocks, cash, and other financial instruments, are state dependent using multiple correlation matrices that correspond to differing market conditions. This feature allows InnoALM to anticipate and react to severe as well as normal market conditions. Austrian pension law and policy considerations can be modeled as constraints in the optimization. The concave risk-averse preference function is to maximize the expected present value of terminal wealth at the specified horizon net of expected discounted convex (piecewise-linear) penalty costs for wealth and benchmark targets in each decision period. InnoALM has a user interface that provides visualization of key model outputs, the effect of input changes, growing pension benefits from increased deterministic wealth target violations, stochastic benchmark targets, security reserves, policy changes, etc. The solution process using the IBM OSL stochastic programming code is fast enough to generate virtually online decisions and results and allows for easy interaction of the user with the model to improve pension fund performance. The model has been used since 2000 for Siemens Austria, Siemens worldwide, and to evaluate possible pension fund regulation changes in Austria.", "e:keyword": ["Scenarios", "Correlation matrices", "Pension fund planning", "Stochastic linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0565", "e:abstract": "This paper presents a large-scale computationally intensive model for understanding the dynamic strategic evolution of electricity-generating asset portfolios in response to various market interventions, and the consequent longer-term effects of such changes on market structure and prices. We formulate a multistage model involving a Cournot representation of the wholesale electricity market, the performance of which then determines plant trading between players and the coevolution of market structure. An algorithm to model this game is presented. We apply this model to the full England and Wales system, as it was in 2000, and simulate the strategic responses to divestiture, capacity targets, and the two market mechanism variants of pool and bilateral market clearing.", "e:keyword": ["Games/group decisions", "Noncooperative", "Government", "Energy policies", "Regulations", "Industries", "Electricity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0566", "e:abstract": "Mean-variance portfolios constructed using the sample mean and covariance matrix of asset returns perform poorly out of sample due to estimation error. Moreover, it is commonly accepted that estimation error in the sample mean is much larger than in the sample covariance matrix. For this reason, researchers have recently focused on the minimum-variance portfolio, which relies solely on estimates of the covariance matrix, and thus usually performs better out of sample. However, even the minimum-variance portfolios are quite sensitive to estimation error and have unstable weights that fluctuate substantially over time. In this paper, we propose a class of portfolios that have better stability properties than the traditional minimum-variance portfolios. The proposed portfolios are constructed using certain <i>robust</i> estimators and can be computed by solving a <i>single</i> nonlinear program, where robust estimation and portfolio optimization are performed in a single step. We show analytically that the resulting portfolio weights are less sensitive to changes in the asset-return distribution than those of the traditional portfolios. Moreover, our numerical results on simulated and empirical data confirm that the proposed portfolios are more stable than the traditional minimum-variance portfolios, while preserving (or slightly improving) their relatively good out-of-sample performance.", "e:keyword": ["Finance", "Portfolio", "Investment", "Economics", "Econometrics", "Portfolio choice", "Minimum-variance portfolios", "Estimation error", "Robust statistics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0567", "e:abstract": "During the past few years, there has been a trend to enrich traditional revenue management models built upon the independent demand paradigm by accounting for customer choice behavior. This extension involves both modeling and computational challenges. One way to describe choice behavior is to assume that each customer belongs to a <i>segment</i>, which is characterized by a <i>consideration set</i>, i.e., a subset of the products provided by the firm that a customer views as options. Customers choose a particular product according to a multinomial-logit criterion, a model widely used in the marketing literature. In this paper, we consider the choice-based, deterministic, linear programming model (CDLP) of Gallego et al. (2004) [Gallego, G., G. Iyengar, R. Phillips, A. Dubey. 2004. Managing flexible products on a network. Technical Report CORC TR-2004-01, Department of Industrial Engineering and Operations Research, Columbia University, New York], and the follow-up dynamic programming decomposition heuristic of van Ryzin and Liu (2008) [van Ryzin, G. J., Q. Liu. 2008. On the choice-based linear programming model for network revenue management. <i>Manufacturing Service Oper. Management</i> <b>10</b>(2) 288--310]. We focus on the more general version of these models, where customers belong to overlapping segments. To solve the CDLP for real-size networks, we need to develop a column generation algorithm. We prove that the associated column generation subproblem is indeed NP-hard and propose a simple, greedy heuristic to overcome the complexity of an exact algorithm. Our computational results show that the heuristic is quite effective and that the overall approach leads to high-quality, practical solutions.", "e:keyword": ["Analysis of algorithms", "Computational complexity", "Marketing", "Choice models", "Programming", "Fractional", "Linear applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0568", "e:abstract": "Cross-selling is becoming an increasingly prevalent practice in call centers, due, in part, to its unique capability to allow firms to dynamically segment their callers and customize their product offerings accordingly. This paper considers a call center with cross-selling capability that serves a pool of customers that are differentiated in terms of their revenue potential and delay sensitivity. It studies the operational decisions of staffing, call routing, and cross-selling under various forms of customer segmentation. It derives near-optimal controls in each of the settings analyzed, and characterizes the impact of a more refined customer segmentation on the structure of these policies and the center's profitability.", "e:keyword": ["Call centers", "Cross-selling", "Queueing systems", "Revenue management", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0569", "e:abstract": "This paper concerns a two-person zero-sum game between an infiltrator and a defender. The infiltrator wants to pass through a channel, while the defender wants to detect the infiltrator by laying static underwater devices across the channel. Previous work assumed the infiltrator has zero width. We consider an extension when the infiltrator has a positive width, and a proportion of its width needs to be detected by the underwater devices for a positive identification. We show that the positive-width problem reduces to the zero-width problem by an appropriate transformation.", "e:keyword": ["Games/group decisions", "Noncooperative", "Ambush game", "Military", "Search/surveillance"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0570", "e:abstract": "We develop a goal-driven stochastic optimization model that considers a random objective function in achieving an aspiration level, target, or goal. Our model maximizes the shortfall-aware aspiration-level criterion, which encompasses the probability of success in achieving the aspiration level and an expected level of underperformance or shortfall. The key advantage of the proposed model is its tractability. We can obtain its solution by solving a small collection of stochastic linear optimization problems with objectives evaluated under the popular conditional-value-at-risk (CVaR) measure. Using techniques in robust optimization, we propose a decision-rule-based deterministic approximation of the goal-driven optimization problem by solving subproblems whose number is a polynomial with respect to the accuracy, with each subproblem being a second-order cone optimization problem (SOCP). We compare the numerical performance of the deterministic approximation with sampling-based approximation and report the computational insights on a multiproduct newsvendor problem.", "e:keyword": ["Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0571", "e:abstract": "A co-epidemic arises when the spread of one infectious disease stimulates the spread of another infectious disease. Recently, this has happened with human immunodeficiency virus (HIV) and tuberculosis (TB). We develop two variants of a co-epidemic model of two diseases. We calculate the basic reproduction number (R<sub>0</sub>), the disease-free equilibrium, and the quasi-disease-free equilibria, which we define as the existence of one disease along with the complete eradication of the other disease, and the co-infection equilibria for specific conditions. We determine stability criteria for the disease-free and quasi-disease-free equilibria. We present an illustrative numerical analysis of the HIV-TB co-epidemics in India that we use to explore the effects of hypothetical prevention and treatment scenarios. Our numerical analysis demonstrates that exclusively treating HIV or TB may reduce the targeted epidemic, but can subsequently exacerbate the other epidemic. Our analyses suggest that coordinated treatment efforts that include highly active antiretroviral therapy for HIV, latent TB prophylaxis, and active TB treatment may be necessary to slow the HIV-TB co-epidemic. However, treatment alone may not be sufficient to eradicate both diseases. Increased disease prevention efforts (for example, those that promote condom use) may also be needed to extinguish this co-epidemic. Our simple model of two synergistic infectious disease epidemics illustrates the importance of including the effects of each disease on the transmission and progression of the other disease.", "e:keyword": ["Health care", "Epidemiology", "Treatment", "Simulation", "Applications", "Infectious disease co-epidemic", "Epidemic control", "HIV", "Tuberculosis", "India"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0572", "e:abstract": "We present the results of a computational investigation of the pseudoflow and push-relabel algorithms for the maximum flow and minimum <i>s</i>-<i>t</i> cut problems. The two algorithms were tested on several problem instances from the literature. Our results show that our implementation of the pseudoflow algorithm is faster than the best-known implementation of push-relabel on most of the problem instances within our computational study.", "e:keyword": ["Flow algorithms", "Parametric flow", "Normalized tree", "Lowest label", "Pseudoflow algorithm", "Maximum flow"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0573", "e:abstract": "This paper proposes a new classification technique, called support feature machine (SFM), for multidimensional time-series data. The proposed technique was applied to the classification of abnormal brain activity represented in electroencephalograms (EEGs). First, the dynamical properties of EEGs from each electrode were extracted. These dynamical profiles were put in SFM, which is an optimization model that maximizes classification accuracy by selecting electrodes (features) that correctly classify unlabeled EEG samples based on the nearest-neighbor classification rule. The empirical studies were performed on the EEG data sets collected from 10 subjects. The performance of SFM was assessed and compared with the ones achieved by the traditional <i>k</i>-nearest-neighbor classifier and support vector machines (SVMs). The results show that SFM achieved, on average, over 90% correct classification and outperformed other classification techniques. In the validation step, SFM correctly classified unseen preseizure and normal EEGs with over 73% accuracy.", "e:keyword": ["Medical diagnosis", "Nearest neighbor", "Classification", "Multidimensional time series", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0575", "e:abstract": "Many primary care offices and other medical practices regularly experience long backlogs for appointments. These backlogs are exacerbated by a significant level of last-minute cancellations or “no-shows,” which have the effect of wasting capacity. In this paper, we conceptualize such an appointment system as a single-server queueing system in which customers who are about to enter service have a state-dependent probability of not being served and may rejoin the queue. We derive stationary distributions of the queue size, assuming both deterministic as well as exponential service times, and compare the performance metrics to the results of a simulation of the appointment system. Our results demonstrate the usefulness of the queueing models in providing guidance on identifying patient panel sizes for medical practices that are trying to implement a policy of “advanced access.”", "e:keyword": ["Health care", "Queues", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0576", "e:abstract": "In this paper, an intelligent decision-making framework (DMF) is developed to help decision makers identify cost-effective ozone control policies. High concentrations of ozone at the ground level continue to be a serious problem in numerous U.S. cities. Our DMF searches for dynamic and targeted control policies that require a lower total reduction of emissions than current control strategies based on the “trial and error” approach typically employed by state government decision makers. Our DMF utilizes a rigorous stochastic dynamic programming (SDP) formulation and incorporates an atmospheric chemistry module to model how ozone concentrations change over time. Within the atmospheric chemistry module, methods from design and analysis of computer experiments are employed to create SDP state transition equation metamodels, and critical dimensionality reduction is conducted to reduce the state-space dimension in solving our SDP problem. Results are presented from a prototype DMF for the Atlanta metropolitan region.", "e:keyword": ["Environment", "Dynamic programming", "Applications", "Statistics", "Data analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0577", "e:abstract": "We study the problem faced by a monopolistic company that is dynamically pricing a perishable product or service and simultaneously learning the demand characteristics of its customers. In the learning procedure, the company observes the sales history over consecutive learning stages and predicts consumer demand by applying an <i>aggregating algorithm</i> (AA) to a pool of online stochastic predictors. Numerical implementation uses finite-sample distribution approximations that are periodically updated using the most recent sales data. These are subsequently altered with a random step characterizing the stochastic predictors. The company's pricing policy is optimized with a simulation-based procedure integrated with AA. The methodology of the paper is general and independent of specific distributional assumptions. We illustrate this procedure on a demand model for a market in which customers are aware that pricing is dynamic, may time their purchases strategically, and compete for a limited product supply. We derive the form of this demand model using a game-theoretic consumer choice model and study its structural properties. Numerical experiments demonstrate that the learning procedure is robust to deviations of the actual market from the model of the market used in learning.", "e:keyword": ["Marketing", "Pricing", "Games", "Stochastic", "Artificial intelligence", "Online learning"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0579", "e:abstract": "Disruptions in infrastructure networks to transport material, energy, and information can have serious economic, and even catastrophic, consequences. Since these networks require enormous investments, network service providers emphasize both survivability and cost effectiveness in their topological design decisions. This paper addresses the survivable network design problem, a core model incorporating the cost and redundancy trade-offs facing network planners. Using a novel connectivity upgrade strategy, we develop several families of inequalities to strengthen a multicommodity flow-based formulation for the problem, and show that some of these inequalities are facet defining. By increasing the linear programming lower bound, the valid inequalities not only lead to better performance guarantees for heuristic solutions, but also accelerate exact and approximate solution methods. We also consider a heuristic strategy that sequentially rounds the fractional values, starting with the linear programming solution to our strong model. Extensive computational tests confirm that the valid inequalities, added via a cutting plane algorithm, and the heuristic procedure are very effective, and their performance is robust to changes in the network dimensions and connectivity structure. Our solution approach generates tight lower and upper bounds with average gaps that are less than 1.2% for various problem sizes and connectivity requirements.", "e:keyword": ["Integer programming", "Cutting plane/facet", "Networks/graphs", "Applications", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0580", "e:abstract": "We develop the first algorithmic approach to compute provably good ordering policies for a multiperiod, capacitated, stochastic inventory system facing stochastic nonstationary and correlated demands that evolve over time. Our approach is computationally efficient and guaranteed to produce a policy with total expected cost no more than twice that of an optimal policy. As part of our computational approach, we propose a novel scheme to account for backlogging costs in a capacitated, multiperiod environment. Our cost-accounting scheme, called the <i>forced marginal backlogging cost-accounting scheme</i>, is significantly different from the period-by-period accounting approach to backlogging costs used in dynamic programming; it captures the long-term impact of a decision on system performance in the presence of capacity constraints. In the likely event that the per-unit order costs are large compared to the holding and backlogging costs, a transformation of cost parameters yields a significantly improved guarantee. We also introduce new semimyopic policies based on our new cost-accounting scheme to derive bounds on the optimal base-stock levels. We show that these bounds can be used to effectively improve <i>any</i> policy. Finally, empirical evidence is presented that indicates that the typical performance of this approach is significantly stronger than these worst-case guarantees.", "e:keyword": ["Stochastic inventory control", "Heuristics", "Approximation algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0581", "e:abstract": "In this paper, we address the problem of generating a discrete representation of the nondominated frontier in multiple objective linear problems. We find a surface that approximates the shape of the nondominated frontier. Utilizing the surface, we generate a set of discrete points that is representative of the frontier. Our experience on randomly generated problems demonstrates that the approach performs well in terms of both the quality of the representation and the computation time.", "e:keyword": ["Programming", "Multiple criteria", "Discrete representation", "Nondominated frontier"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0582", "e:abstract": "We consider a perishable inventory system with Poisson demands, fixed shelf lives, constant lead times, and lost sales in the presence of nonnegligible fixed ordering costs. The inventory control policy employed is the continuous-review (<i>Q</i>,<i>r</i>) policy, where <i>r</i><<i>Q</i>. The system is modeled using an embedded Markov process approach by introducing the concept of the effective shelf life of a batch in use. Using the stationary distribution of the effective shelf life, we obtain the expressions for the operating characteristics and construct the expected cost rate function for the inventory system. Our numerical study indicates that the determination of the policy parameters exactly as modeled herein results in significant improvements in cost rates with respect to a previously proposed heuristic. We also compare the (<i>Q</i>,<i>r</i>) policy with respect to a time-based benchmark policy and find that the (<i>Q</i>,<i>r</i>) policy might be impractical for rare events, but overall appears to be a good heuristic policy.", "e:keyword": ["Inventory", "Perishables", "Lot size-reorder point policy", "Lost sales", "Effective shelf life"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0583", "e:abstract": "This paper addresses the problem of determining the optimal integrations and consolidations of air cargo shipments. A freight forwarder arranges for the execution of many jobs (shipments) on behalf of several clients. Each job consists of a number of sequential activities, such as pickup, truck transportation, warehousing, and air transportation. Each activity can be performed by a number of processing units, such as trucking companies, warehouses, and airlines. If a number of consecutive activities of a job are integrated, i.e., performed by the same processing unit, then it typically costs less. If a number of similar activities of different jobs are consolidated, i.e., performed by the same processing unit, then it also typically costs less. Given a number of jobs and processing units, the sets of activities that can be integrated and consolidated, and the associated costs, the decision problem is to determine which processing unit should perform each activity of each job to minimize the total cost. This problem of assigning shipment activities to processing units is formulated as a linear 0--1 program. Principal properties of the model are established. Exploiting the special structure of this model, we design a solution procedure that includes heuristics and a branch-and-bound algorithm.", "e:keyword": ["Transportation", "Freight handling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0584", "e:abstract": "Many hospitals face the problem of insufficient capacity to meet demand for inpatient beds, especially during demand surges. This results in quality degradation of patient care due to large delays from admission time to the hospital until arrival at a floor. In addition, there is loss of revenue because of the inability to provide service to potential patients. A solution to the problem is to proactively transfer patients between floors in anticipation of a demand surge. Optimal reallocation poses an extraordinarily complex problem that can be modeled as a finite-horizon Markov decision process. Based on the optimization model, a decision-support system has been developed and implemented at Windham Hospital in Willimantic, Connecticut. Projections from an initial trial period indicate very significant financial gains of about 1% of their total revenue, with no negative impact on any standard quality of care or staffing effectiveness indicators. In addition, the hospital showed a marked improvement in quality of care because of a resulting decrease of almost 50% in the average time that an admitted patient has to wait from admission until being transferred to a floor.", "e:keyword": ["Hospitals", "Health care", "Dynamic programming", "Markov decision processes", "Decision-support systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0585", "e:abstract": "To ensure quality from outsourced call centers, firms sign <i>service-level agreements</i> (SLAs). These define service measures such as what constitutes an acceptable delay or an acceptable abandonment rate. They may also dictate penalties for failing to meet agreed-upon targets. We introduce a <i>period-based SLA</i> that measures performance over a short duration such as a rush hour. We compare it to alternate SLAs that measure service by individual and over a long horizon. To measure the service levels for these SLAs, we develop several approximations. We approximate the probability an acceptable delay is met by generalizing the heavy-traffic quality and efficiency driven regime. We also provide a new approximation for the abandonment rate. Further, we prove a central limit theorem for the probability of meeting a service level measured by the percentage of customers acceptably served during a period. We demonstrate how an outsourced call center operating in an environment with uncertain demand and abandonment can determine its staffing policy to maximize the expected profit for these SLAs. Numerical experiments demonstrate a high degree of accuracy for the approximations and the resulting staffing levels. We indicate several salient features of the behavior of the period-based SLA.", "e:keyword": ["Queues", "Applications", "Approximations", "Balking and reneging"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0586", "e:abstract": "Many optimization problems are formulated as generalized geometric programming (GGP) containing signomial terms <i>f</i>(<b>X</b>)·<i>g</i>(<b>Y</b>), where <b>X</b> and <b>Y</b> are continuous and discrete free-sign vectors, respectively. By effectively convexifying <i>f</i>(<b>X</b>) and linearizing <i>g</i>(<b>Y</b>), this study globally solves a GGP with a lower number of binary variables than are used in current GGP methods. Numerical experiments demonstrate the computational efficiency of the proposed method.", "e:keyword": ["Programming", "Geometric", "Generalized geometric programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0587", "e:abstract": "We present a novel formulation for the service network design problem in the context of large-scale, less-than-truckload (LTL) freight operations. The formulation captures the basic network design constraints; the load-planning requirement that all freight at a location, irrespective of the freight's origin, loads to the same next terminal; and other important LTL-specific requirements. Our modeling scheme fragments the underlying massive network design model with up to 1.3 million 0--1 variables and 1.3 million rows into a separate and efficient integer programming (IP) problem for each destination terminal along with a coordinating master network design problem. We produce high-quality solutions in very reasonable CPU times ((sim)2 hours) using slope scaling and load-planning tree generation with corresponding potential annual savings of $20--25 million dollars for the target company for which the research was conducted.", "e:keyword": ["Integer programming", "Transportation", "Shipping", "Multicommodity networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0588", "e:abstract": "This paper analyzes the relationships between personal decisions and premature deaths in the United States. The analysis indicates that over one million of the 2.4 million deaths in 2000 can be attributed to personal decisions and could have been avoided if readily available alternative choices were made. Separate analyses indicate 46% of deaths due to heart disease and 66% of cancer deaths are attributable to personal decisions, about 55% of all deaths for ages 15--64 are attributable to personal decisions, and over 94% of the deaths attributable to personal decisions result in the death of the individual making the decisions. Relative to the current 45%, retrospective appraisal suggests that roughly 5% of deaths in 1900 and 20%--25% of deaths in 1950 could be attributed to personal decisions. These results suggest that more effort directed toward improving personal choices regarding life risks may be an effective and economical way to save lives.", "e:keyword": ["Risk", "Decision making", "Decision analysis", "Applications", "Health care", "Information systems", "Management", "Statistics", "Data analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0589", "e:abstract": "We study how innovators can optimally design licensing contracts when there is incomplete information on the licensee's valuation of the innovation, and limited control over the licensee's development efforts. A licensing contract typically contains an up-front payment, milestone payments at successful completion of a project phase, and royalties on sales. We use principal-agent models to formulate the licensor's contracting problem, and we find that under adverse selection, the optimal contract structure changes with the licensee's valuation of the innovation. As the licensee's valuation increases, the licensor's optimal level of involvement in the development---directly or through royalties---should decrease. Only a risk-averse licensor should include both up-front and milestone payments. Moral hazard alone is not detrimental to the licensor's value, but may create an additional value loss when combined with adverse selection. Our results inform managerial practice about the advantages and disadvantages of the different terms included in licensing contracts and recommend the optimal composition of the contract.", "e:keyword": ["Health care", "Pharmaceutical", "Research and development", "Innovation", "Principal-agent modeling", "Adverse selection", "Moral hazard", "Contract design"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0590", "e:abstract": "We present a method to dynamically schedule patients with different priorities to a diagnostic facility in a public health-care setting. Rather than maximizing revenue, the challenge facing the resource manager is to dynamically allocate available capacity to incoming demand to achieve wait-time targets in a cost-effective manner. We model the scheduling process as a Markov decision process. Because the state space is too large for a direct solution, we solve the equivalent linear program through approximate dynamic programming. For a broad range of cost parameter values, we present analytical results that give the form of the optimal linear value function approximation and the resulting policy. We investigate the practical implications and the quality of the policy through simulation.", "e:keyword": ["Health care", "Approximate dynamic programming", "Markov decision processes", "Patient scheduling", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0591", "e:abstract": "To calculate many of the important performance measures for an emergency response system, one requires knowledge of the probability that a particular server will respond to an incoming call at a particular location. Estimating these “dispatch probabilities” is complicated by four important characteristics of emergency service systems. We discuss these characteristics and extend previous approximation methods for calculating dispatch probabilities to account for the possibilities of workload variation by station, multiple vehicles per station, call- and station-dependent service times, and server cooperation and dependence.", "e:keyword": ["Health care", "Ambulance service", "Queues", "Approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0592", "e:abstract": "This paper presents an experimental investigation into the properties of the optimal communication spanning tree (OCST) problem. The OCST problem seeks a spanning tree that connects all the nodes and satisfies their communication requirements at a minimum total cost. The paper compares the properties of random trees to the properties of the best solutions for the OCST problem that are found using an evolutionary algorithm. The results show, on average, that the optimal solution and the minimum spanning tree (MST) share a higher number of links than the optimal solution and a random tree. Furthermore, optimal solutions for OCST problems with randomly chosen distance weights share a higher number of links with an MST than OCST problems with Euclidean distance weights. This intuitive similarity between optimal solutions and MSTs suggests that some heuristic optimization methods for OCST problems might be improved by starting with an MST. Using an MST as a starting solution for a greedy search in the tested cases either improves median running time up to a factor of 10 while finding solutions of the same quality, or increases solution quality up to a factor of 100 while using the same number of search steps in comparison to starting the greedy search from a random tree. Starting a local search, a simulated annealing approach and a genetic algorithm from an MST increases solution quality up to a factor of three in comparison to starting from a random solution.", "e:keyword": ["Networks/graphs", "Tree algorithms", "Communications", "Topological network design", "Simulation", "Statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0593", "e:abstract": "We propose a stochastic unit commitment model for a power generation company that takes part in an electricity spot market. The relevant feature of this model is its detailed representation of the spot market during a whole week, including seven day-ahead market sessions and the corresponding adjustment market sessions. The adjustment market sessions can be seen as an hour-ahead market mechanism. This representation takes into account the influence that the company's decisions exert on the market-clearing price by means of a residual demand curve for each market session. We introduce uncertainty in the form of several possible spot market outcomes for each day, which leads to a weekly scenario tree. The model also represents in detail the operation of the company's generation units. The model leads to large-scale mixed linear-integer problems that are hard to solve with commercial optimizers. This suggests the use of alternative solution methods. We test four solution approaches with a realistic numerical example in the context of the Spanish electricity spot market. The first is a direct solution with a commercial optimizer, which illustrates the mentioned limitations. The second is a standard Lagrangean relaxation algorithm. The third and fourth methods are two original variants of Benders decomposition for multistage stochastic integer programs. The first Benders decomposition algorithm builds approximations for the recourse function relaxing the integrality constraints of the subproblems. The second variant strengthens these cuts by performing one iteration of the Lagrangean of each subproblem. We analyze the advantages of these four methods and compare the results.", "e:keyword": ["Programming", "Stochastic", "Integer", "Lagrangean relaxation", "Benders decomposition", "Production/scheduling", "Planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0597", "e:abstract": "We propose a new method to compute bid prices in network revenue management problems. The novel aspect of our method is that it explicitly considers the temporal dynamics of the arrivals of the itinerary requests and generates bid prices that depend on the remaining leg capacities. Our method is based on relaxing certain constraints that link the decisions for different flight legs by associating Lagrange multipliers with them. In this case, the network revenue management problem decomposes by the flight legs, and we can concentrate on one flight leg at a time. When compared with the so-called deterministic linear program, we show that our method provides a tighter upper bound on the optimal objective value of the network revenue management problem. Computational experiments indicate that the bid prices obtained by our method perform significantly better than the ones obtained by standard benchmark methods.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0598", "e:abstract": "If the price of an asset follows a jump diffusion process, the market is in general incomplete. In this case, hedging a contingent claim written on the asset is not a trivial matter, and other instruments besides the underlying must be used to hedge in order to provide adequate protection against jump risk. We devise a dynamic hedging strategy that uses a hedge portfolio consisting of the underlying asset and liquidly traded options, where transaction costs are assumed present due to a relative bid-ask spread. At each rebalance time, the hedge weights are chosen to simultaneously (i) eliminate the instantaneous diffusion risk by imposing delta neutrality, and (ii) minimize an objective that is a linear combination of a jump risk and transaction cost penalty function. Because reducing the jump risk is a competing goal vis-à-vis controlling for transaction cost, the respective components in the objective must be appropriately weighted. Hedging simulations of this procedure are carried out, and our results indicate that the proposed dynamic hedging strategy provides sufficient protection against the diffusion and jump risk while not incurring large transaction costs.", "e:keyword": ["Finance", "Asset pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0599", "e:abstract": "In this paper, we study extensions of the classical Markowitz mean-variance portfolio optimization model. First, we consider that the expected asset returns are stochastic by introducing a probabilistic constraint, which imposes that the expected return of the constructed portfolio must exceed a prescribed return threshold with a high confidence level. We study the deterministic equivalents of these models. In particular, we define under which types of probability distributions the deterministic equivalents are second-order cone programs and give closed-form formulations. Second, we account for real-world trading constraints (such as the need to diversify the investments in a number of industrial sectors, the nonprofitability of holding small positions, and the constraint of buying stocks by lots) modeled with integer variables. To solve the resulting problems, we propose an <i>exact</i> solution approach in which the uncertainty in the estimate of the expected returns and the integer trading restrictions are <i>simultaneously</i> considered. The proposed algorithmic approach rests on a nonlinear branch-and-bound algorithm that features two new branching rules. The first one is a static rule, called <i>idiosyncratic risk branching</i>, while the second one is dynamic and is called <i>portfolio risk branching</i>. The two branching rules are implemented and tested using the open-source <monospace>Bonmin</monospace> framework. The comparison of the computational results obtained with state-of-the-art MINLP solvers (<monospace>MINLP_BB</monospace> and <monospace>CPLEX</monospace>) and with our approach shows the effectiveness of the latter, which permits to solve to optimality problems with up to 200 assets in a reasonable amount of time. The practicality of the approach is illustrated through its use for the construction of four fund-of-funds now available on the major trading markets.", "e:keyword": ["Programming", "Stochastic", "Integer", "Nonlinear", "Branch-and-bound", "Finance", "Portfolio", "Probability", "Distributions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0600", "e:abstract": "We consider the problem of sampling a point from an arbitrary distribution (pi) over an arbitrary subset <i>S</i> of an integer hyperrectangle. Neither the distribution (pi) nor the support set <i>S</i> are assumed to be available as explicit mathematical equations, but may only be defined through oracles and, in particular, computer programs. This problem commonly occurs in black-box discrete optimization as well as counting and estimation problems. The generality of this setting and high dimensionality of <i>S</i> precludes the application of conventional random variable generation methods. As a result, we turn to Markov chain Monte Carlo (MCMC) sampling, where we execute an ergodic Markov chain that converges to (pi) so that the distribution of the point delivered after sufficiently many steps can be made arbitrarily close to (pi). Unfortunately, classical Markov chains, such as the nearest-neighbor random walk or the coordinate direction random walk, fail to converge to (pi) because they can get trapped in isolated regions of the support set. To surmount this difficulty, we propose discrete hit-and-run (DHR), a Markov chain motivated by the hit-and-run algorithm known to be the most efficient method for sampling from log-concave distributions over convex bodies in <i>R<sup>n</sup></i>. We prove that the limiting distribution of DHR is (pi) as desired, thus enabling us to sample approximately from (pi) by delivering the last iterate of a sufficiently large number of iterations of DHR. In addition to this asymptotic analysis, we investigate finite-time behavior of DHR and present a variety of examples where DHR exhibits polynomial performance.", "e:keyword": ["Simulation", "Markov chain Monte Carlo", "Probability", "Random walk"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0601", "e:abstract": "This paper explores when it is important for firms to consider stockout-based substitution and competitor's inventory levels in making inventory decisions in the context of a duopoly model. To address this question, we consider a model where two newsvendors sell substitutable products in a market with aggregate market demand <i>D</i>. The two firms get a proportion <i>p</i> and (1 - <i>p</i>) of this demand, where <i>p</i> is random. We characterize the equilibrium inventory levels of the two firms in a single-period model and show the striking property that, under certain reasonable conditions on the cost parameters, the two firms ignore their competitor's inventory levels and potential substitution demand, i.e., their inventory decisions are decoupled. Furthermore, we show under slightly more restrictive conditions on the cost parameters that the single-period results can be extended to the case where <i>D</i> is random. Finally, we extend the decoupling property to a multiperiod periodic review scenario and show that the resulting Nash equilibrium can be characterized simply as the solution to a single-product dynamic newsvendor problem that ignores substitution demand.", "e:keyword": ["Inventory/production applications", "Uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0602", "e:abstract": "We consider a call center model with multiple customer classes and multiple server pools. Calls arrive randomly over time, and the instantaneous arrival rates are allowed to vary both temporally and stochastically in an arbitrary manner. The objective is to minimize the sum of personnel costs and expected abandonment penalties by selecting an appropriate staffing level for each server pool. We propose a simple and computationally tractable method for solving this problem that requires as input only a few system parameters and historical call arrival data for each customer class; in this sense the method is said to be <i>data-driven</i>. The efficacy of the proposed method is illustrated via numerical examples. An asymptotic analysis establishes that the prescribed staffing levels achieve near-optimal performance and characterizes the magnitude of the optimality gap.", "e:keyword": ["Stochastic model applications", "Stochastic networks", "Nonstationary queues", "Limit theorem", "Approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0603", "e:abstract": "The classical risk-neutral newsvendor problem is to decide the order quantity that maximizes the one-period expected profit. In this note, we consider a risk-averse newsvendor with stochastic price-dependent demand. We adopt Conditional Value-at-Risk (<monospace>CVaR</monospace>), a risk measure commonly used in finance, as the decision criterion. The aim of our study is to investigate the optimal pricing and ordering decisions in such a setting. For both additive and multiplicative demand models, we provide sufficient conditions for the uniqueness and existence of the optimal policy. Comparative statics show the monotonicity properties and other characteristics of the optimal pricing and ordering decisions. We also compare our results with those of the newsvendor with a risk-neutral attitude and a general utility function.", "e:keyword": ["Inventory", "Risk", "Perishable/aging items", "Marketing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0604", "e:abstract": "Latin hypercube designs (LHDs) play an important role when approximating computer simulation models. To obtain good space-filling properties, the maximin criterion is frequently used. Unfortunately, constructing maximin LHDs can be quite time consuming when the number of dimensions and design points increase. In these cases, we can use heuristical maximin LHDs. In this paper, we construct bounds for the separation distance of certain classes of maximin LHDs. These bounds are useful for assessing the quality of heuristical maximin LHDs. Until now only upper bounds are known for the separation distance of certain classes of unrestricted maximin designs, i.e., for maximin designs without a Latin hypercube structure. The separation distance of maximin LHDs also satisfies these “unrestricted” bounds. By using some of the special properties of LHDs, we are able to find new and tighter bounds for maximin LHDs. Within the different methods used to determine the upper bounds, a variety of combinatorial optimization techniques are employed. Mixed-integer programming, the traveling salesman problem, and the graph-covering problem are among the formulations used to obtain the bounds. Besides these bounds, also a construction method is described for generating LHDs that meet Baer's bound for the (l-script)<sup>(infinity)</sup> distance measure for certain values of <i>n</i>.", "e:keyword": ["Simulation", "Design of experiments", "Latin hypercube design", "Maximin", "Space-filling", "Mixed-integer programming", "Traveling salesman problem", "Graph covering"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0605", "e:abstract": "In this paper, we introduce the extended affinely adjustable robust counterpart to modeling and solving multistage uncertain linear programs with fixed recourse. Our approach first reparameterizes the primitive uncertainties and then applies the affinely adjustable robust counterpart proposed in the literature, in which recourse decisions are restricted to be linear in terms of the primitive uncertainties. We propose a special case of the extended affinely adjustable robust counterpart---the splitting-based extended affinely adjustable robust counterpart---and illustrate both theoretically and computationally that the potential of the affinely adjustable robust counterpart method is well beyond the one presented in the literature. Similar to the affinely adjustable robust counterpart, our approach ends up with deterministic optimization formulations that are tractable and scalable to multistage problems.", "e:keyword": ["Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0606", "e:abstract": "Recent efforts to develop a universal view of complex networks have created both excitement and confusion about the way in which knowledge of network structure can be used to understand, control, or design system behavior. This paper offers perspective on the emerging field of “network science” in three ways. First, it briefly summarizes the origins, methodological approaches, and most celebrated contributions within this increasingly popular field. Second, it contrasts the predominant perspective in the network science literature (that abstracts away domain-specific function and instead focuses on graph-theoretic measures of system structure and dynamics) with that of engineers and practitioners of decision science (who emphasize the importance of network performance, constraints, and trade-offs). Third, it proposes <i>optimization-based reverse engineering</i> to address some important open questions within network science from an operations research perspective. We advocate for increased, yet cautious, participation in this field by operations researchers.", "e:keyword": ["Networks/graphs", "Theory", "Philosophy of modeling", "Engineering"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0607", "e:abstract": "In an incremental optimization problem, we are given a feasible solution <i>x</i><sup>0</sup> of an optimization problem <i>P</i>, and we want to make an incremental change in <i>x</i><sup>0</sup> that will result in the greatest improvement in the objective function. In this paper, we study the incremental optimization versions of six well-known network problems. We present a strongly polynomial algorithm for the incremental minimum spanning tree problem. We show that the incremental minimum cost flow problem and the incremental maximum flow problem can be solved in polynomial time using Lagrangian relaxation. We consider two versions of the incremental minimum shortest path problem, where increments are measured via arc inclusions and arc exclusions. We present a strongly polynomial time solution for the arc inclusion version and show that the arc exclusion version is NP-complete. We show that the incremental minimum cut problem is NP-complete and that the incremental minimum assignment problem reduces to the minimum exact matching problem, for which a randomized polynomial time algorithm is known.", "e:keyword": ["Theory", "Distance algorithms", "Flow algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0608", "e:abstract": "We consider a make-to-order system where customers are dynamically quoted lead times (and prices). Customers are homogenous but have general (nonlinear) disutility for delay. Because the firm is a monopolist, the pricing problem is trivial and the dynamic problem reduces to one of lead-time quotation and order sequencing. We also consider the (static) problem of up-front capacity installation. We use a large-capacity asymptotic regime to make the problem tractable. We provide recommended policies for convex, concave, and convex-concave lead-time cost functions and prove that these policies are asymptotically optimal. The policies are both highly intuitive and readily implementable. Moreover, they provide delay guarantees for all served customers. They are tested numerically; we find that significant benefits can accrue by using the prescribed dynamic policies instead of first-come-first-served type policies.", "e:keyword": ["Production/scheduling", "Dynamic lead-time quotation", "Queues", "Limit theorems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0609", "e:abstract": "This paper revisits the finite-horizon model of a censored newsvendor by Ding et al. [Ding, X., M. L. Puterman, A. Bisi. 2002. The censored newsvendor and the optimal acquisition of information. <i>Oper. Res.</i> <b>50</b> 517--527]. An important result claimed there without a proper proof is that the myopic order quantity is always less than or equal to the optimal order quantity. Lu et al. [Lu, X., J. S. Song, K. Zhu. 2008. Analysis of perishable inventory systems with censored demand data. <i>Oper. Res.</i> <b>56</b>(4) 1034--1038.] supplied a correct proof of the result. We analyze the same model using the interesting concept of the unnormalized probability, which simplifies the dynamic programming equation considerably and facilitates the proof of the claim. Moreover, it produces the proof of the existence of an optimal solution for an infinite-horizon setting of the problem.", "e:keyword": ["Inventory/production", "Unknown demand", "Censoring", "Optimal policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0610", "e:abstract": "We examine a multiperiod capacity allocation model with upgrading. There are multiple product types, corresponding to multiple classes of demand, and the firm purchases capacity of each product before the first period. Within each period, after demand arrives, products are allocated to customers. Customers who arrive to find that their product has been depleted can be upgraded by at most one level. We show that the optimal allocation policy is a simple two-step algorithm: First, use any available capacity to satisfy same-class demand, and then upgrade customers until capacity reaches a protection limit, so that in the second step the higher-level capacity is rationed. We show that these results hold both when all capacity is salvaged at the end of the last demand period as well as when capacity can be replenished (in the latter case, an order-up-to policy is optimal for replenishment). Although finding the optimal protection limits is computationally intensive, we describe bounds for the optimal protection limits that take little effort to compute and can be used to effectively solve large problems. Using these heuristics, we examine numerically the relative value of strictly optimal capacity and dynamic rationing, the value of perfect demand information, and the impact of demand and economic parameters on the value of optimal substitution.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Multi-item", "Approximations/heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0611", "e:abstract": "Consumers or firms contemplating purchasing a new product or adopting a new technology are often plagued by uncertainty: Will the benefits outweigh the costs? Should we buy now or wait and gather more information? In this paper, we study a dynamic programming model of this technology adoption problem. In each period, the consumer decides whether to adopt the technology, reject it, or wait and gather additional information by observing a signal about the technology's benefit. The technology's actual benefit may be constant or changing stochastically over time. The dynamic programming state variable is a probability distribution that describes the consumer's beliefs about the benefits of the technology. We allow general probability distributions on benefits and general signal processes and assume that the consumer updates her beliefs over time using Bayes' rule. We are interested in structural properties of this model. We show that improving the technology's benefit need not make the consumer better off and that first-order stochastic dominance improvements in the consumer's distribution on benefits need not increase the consumer's value function. Nevertheless, the model possesses a great deal of structure. For example, we obtain monotonic value functions and policies if we order distributions using likelihood-ratio dominance rather than first-order stochastic dominance. We also examine convexity properties and provide many comparative statics results.", "e:keyword": ["Dynamic programming", "Decision analysis", "Sequential"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0612", "e:abstract": "Vaccination against infectious disease is hailed as one of the great achievements in public health. However, the United States Recommended Childhood Immunization Schedule is becoming increasingly complex as it is expanded to cover additional diseases. Moreover, biotechnology advances have allowed vaccine manufacturers to create combination vaccines that immunize against several diseases in a single injection. All these factors are creating a combinatorial explosion of alternatives and choices (each with a different cost) for public health policy makers, pediatricians, and parents/guardians (each with a different perspective). The General Vaccine Formulary Selection Problem (GVFSP) is introduced to model general childhood immunization schedules that can be used to illuminate these alternatives and choices by selecting a vaccine formulary that minimizes the cost of fully immunizing a child and the amount of extraimmunization. Both exact algorithms and heuristics for GVFSP are presented. A computational comparison of these algorithms and heuristics is presented for the 2006 Recommended Childhood Immunization Schedule, as well as several randomly generated childhood immunization schedules that are likely to be representative of future childhood immunization schedules. The results reported here provide both fundamental insights into the structure of the GVFSP models and algorithms and practical value for the public health community.", "e:keyword": ["Health care", "Pediatric immunization", "Vaccines", "Analysis of algorithms", "Computational complexity", "Programming", "Integer algorithms", "Heuristics", "Dynamic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0613", "e:abstract": "Dialysis is the most common therapy for patients afflicted with chronic kidney failure. Currently, little is known about the relationship between the timing of dialysis initiation and the therapy's cost and effectiveness. This paper examines the cost-effective initiation of dialysis and compares standard initiation criteria from the clinical literature to computationally derived strategies. Comparisons make use of a simulation model that integrates submodels of disease progression, hospitalization, transplantation, cost, and quality of life. The simulation model is also used by an approximate dynamic programming (ADP) algorithm to derive approximately optimal strategies that maximize patient welfare. Patient welfare is measured from the society's perspective and is defined as the product of the expected discounted quality-adjusted life years (QALYs) and a “value-of-life” parameter, minus the expected total discounted medical expenditures. Also considered is an alternative formulation in which the goal is to minimize the expected total discounted medical expenditures without affecting patient QALYs relative to current medical practice. Numerical results show that: (i) standard early initiation strategies, where once started on dialysis patients are kept on a fixed weekly program, have a limited potential, and (ii) early dialysis at an incrementally increasing dose customized to each patient can yield a significant cost advantage. These findings demonstrate computationally intensive models of disease progression, and therapy effectiveness can identify novel strategies for managing expensive medical therapies and a more efficient use of scarce health-care resources.", "e:keyword": ["Dynamic programming", "Semi-Markov", "Health care", "Treatment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0614", "e:abstract": "Questions regarding the relative value and frequency of mammography screening for premenopausal women versus postmenopausal women remain open due to the conflicting age-based dynamics of both the disease (increasing incidence, decreasing aggression) and the accuracy of the test results (increasing sensitivity and specificity). To investigate these questions, we formulate a partially observed Markov chain model that captures several of these age-based dynamics not previously considered simultaneously. Using sample-path enumeration, we evaluate a broad range of policies to generate the set of “efficient” policies, as measured by a lifetime breast cancer mortality risk metric and an expected mammogram count, from which a patient may select a policy based on individual circumstance. We demonstrate robustness with respect to small changes in the input data and conclude that, in general, to efficiently achieve a lifetime risk comparable to the current risk among U.S. women, screening should start relatively early in life and continue relatively late in life regardless of the screening interval(s) adopted. The frontier also exhibits interesting patterns with respect to policy type, where policy type is defined by the relationship between the screening interval prescribed in younger years and that prescribed later in life.", "e:keyword": ["Health care", "Diagnosis", "Probability", "Stochastic model applications", "Reliability", "Inspection"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0615", "e:abstract": "Consider a distribution system consisting of a set of retailers facing a single-period price-dependent demand of a single product. By taking advantage of the risk-pooling effect and the quantity/volume discount provided by suppliers or third-party carriers, the retailers may place joint orders and keep inventory at central warehouses before demand realization, and allocate inventory among themselves after demand realization to reduce their operating costs. Under rather general assumptions, we prove that there is a stable allocation of profits among the retailers in the sense that the resulting inventory centralization game has a nonempty core. We also show how to compute an allocation in the core.", "e:keyword": ["Games/group decisions", "Cooperative", "Inventory/production", "Uncertainty", "Stochastic", "Marketing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0616", "e:abstract": "Decathlon coaches usually spend days or even weeks on designing training schedules for their athletes. A major complication is the fact that the athlete has only limited time for an extensive range of training exercises, whereas the effects of exercises on the athlete's performance in the various events are interrelated. This paper presents a mathematical model for optimizing the use of the total available training time by assigning time to training exercises. The data used in this time capacity planning model concerns a decathlete preparing for the Olympic Games.", "e:keyword": ["Recreation and sports", "Applications", "Nonlinear programming", "Decision-support systems", "Information systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0617", "e:abstract": "To optimize revenue, service firms must integrate within their pricing policies the rational reaction of customers to their price schedules. In the airline or telecommunication industry, this process is all the more complex due to interactions resulting from the structure of the supply network. In this paper, we consider a streamlined version of this situation where a firm's decision variables involve both prices and investments. We model this situation as a joint design and pricing problem that we formulate as a mixed-integer bilevel program, and whose properties are investigated. In particular, we take advantage of a feature of the model that allows the development of an algorithmic framework based on Lagrangean relaxation. This approach is entirely novel, and numerical results show that it is capable of solving problems of significant sizes.", "e:keyword": ["Economics", "Pricing", "Games", "Integer programming", "Heuristics", "Transportation", "Programming", "Nonlinear", "Nondifferentiable"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0618", "e:abstract": "A dynamic model is built to study the water regulation of human body and related disorders, focusing on the fundamental feedback mechanisms involved in their normal and abnormal physiology. The simulation model is extended to include therapeutic interventions related to the most common body fluid disorder, namely, water intoxication/hyponatremia. The modeling approach is based on system dynamics methodology. Comparisons with experimental and field data show that the model adequately reproduces typical dynamics of the body fluid variables in their normal and diseased states. Finally, an interactive game version is developed to test the possible effects of alternative treatment options on a simulated patient. Simulation and game results reveal the subtleties involved during and after administration of various pharmacological interventions. For example, hypertonic saline should be administered concurrently and in delicate balance with drugs that increase urine flow. The simulator offers a virtual laboratory for experimental research and education on diagnosis and alternative therapies of body water disorders in general and hyponatremia in particular.", "e:keyword": ["Simulation", "System dynamics", "Health care", "Diagnosis/treatment", "Games"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0619", "e:abstract": "The rising cost of health care is one of the world's most important problems. Accordingly, predicting such costs with accuracy is a significant first step in addressing this problem. Since the 1980s, there has been research on the predictive modeling of medical costs based on (health insurance) claims data using heuristic rules and regression methods. These methods, however, have not been appropriately validated using populations that the methods have not seen. We utilize modern data-mining methods, specifically classification trees and clustering algorithms, along with claims data from over 800,000 insured individuals over three years, to provide rigorously validated predictions of health-care costs in the third year, based on medical and cost data from the first two years. We quantify the accuracy of our predictions using unseen (out-of-sample) data from over 200,000 members. The key findings are: (a) our data-mining methods provide accurate predictions of medical costs and represent a powerful tool for prediction of health-care costs, (b) the pattern of past cost data is a strong predictor of future costs, and (c) medical information only contributes to accurate prediction of medical costs of high-cost members.", "e:keyword": ["Health care", "Cost predictions", "Prediction algorithms", "Claims data"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0620", "e:abstract": "We show the optimality of state-dependent echelon base-stock policies in uncapacitated serial inventory systems with Markov-modulated demand and Markov-modulated stochastic lead times in the absence of order crossing. Our results cover finite-time horizon problems as well as infinite-time horizon formulations, with either a discounted or an average cost criterion. We employ a novel approach, based on a decomposition of the problem into a series of single-unit single-customer problems that are essentially decoupled. Besides providing a simple proof technique, this approach also gives rise to efficient algorithms for the calculation of the base-stock levels.", "e:keyword": ["Inventory/production", "Multiechelon", "Inventory/production: policies", "Review/lead times"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0621", "e:abstract": "We consider the problem of scheduling the storage and retrieval of containers in the storage area of a container terminal. Some arcs in the underlying directed network must be visited; other arcs may be---but need not be---visited. We can, therefore, consider this problem to be a special case of the directed rural postman problem. We show that this problem can be reformulated as an asymmetric Steiner traveling salesman problem. This reformulation can be efficiently solved to optimality by a combination of optimal assignments in bipartite networks for parts of the problem and dynamic programming for the connections between those parts.", "e:keyword": ["Transportation", "Freight/materials handling", "Facilities/equipment planning", "Scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0622", "e:abstract": "We study the upstream supplier's batch scheduling problem in a supply chain, which was defined by Hall and Potts [Hall, N. G., C. N. Potts. 2003. Supply chain scheduling: Batching and delivery. <i>Oper. Res.</i> <b>51</b>(4) 566--584]. The supplier has to manufacture multiple products and deliver them to customers in batches. There is an associated delivery cost with each batch. The objective of the supplier is to minimize the total inventory holding and delivery costs. We present simple approximation algorithms for this strongly NP-hard problem, which find a solution that is guaranteed to have a cost at most 3/2 times the minimum. We also prove that the approximation algorithms have worst-case bounds that vary parametrically with the data and that for realistic parameter values are much better than 3/2. The theoretical results are also supported by the findings of a computational study.", "e:keyword": ["Production/scheduling", "Sequencing", "Deterministic", "Single machine", "Manufacturing", "Performance/productivity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0623", "e:abstract": "This paper addresses a general class of capacity planning problems under uncertainty, which arises, for example, in semiconductor tool purchase planning. Using a scenario tree to model the evolution of the uncertainties, we develop a multistage stochastic integer programming formulation for the problem. In contrast to earlier two-stage approaches, the multistage model allows for revision of the capacity expansion plan as more information regarding the uncertainties is revealed. We provide analytical bounds for the <i>value of multistage stochastic programming</i> (VMS) afforded over the two-stage approach. By exploiting a special substructure inherent in the problem, we develop an efficient approximation scheme for the difficult multistage stochastic integer program and prove that the proposed scheme is asymptotically optimal. Computational experiments with realistic-scale problem instances suggest that the VMS for this class of problems is quite high; moreover, the quality and performance of the approximation scheme is very satisfactory. Fortunately, this is more so for instances for which the VMS is high.", "e:keyword": ["Facilities/equipment planning", "Capacity expansion", "Production/scheduling", "Planning", "Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0624", "e:abstract": "In Europe, two market designs are discussed for electricity trade and transmission. We argue that their performance in the presence of market power can be represented by two models from the literature. In contrast to examples for simple two-node networks, we show that in more complex networks a general ranking of both designs is not possible. Hence, computational models are required to evaluate the designs for realistic parameter assumptions. We extend existing formulations of both models to represent them as equilibrium problems with equilibrium constraints (EPEC) with equivalent representation of demand, fringe generation, and strategic generators. In a numerical simulation for the Northwestern European network, the integrated market design performs better. This difference illustrates the impact of small assumptions on the outcome of strategic models.", "e:keyword": ["Regulation", "Noncooperative games", "Complementarity programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0627", "e:abstract": "A semi-open queuing network (SOQN) is a special type of a queuing network consisting of two parts: an inner network with a population constraint and an external queue to accommodate jobs whose entrance is delayed. We first study an SOQN with a single class of jobs in tandem configuration and then extend our study to multiclass configurations. Multiclass SOQNs fall into two categories: general pallet and dedicated pallet SOQNs. For the general pallet case, we aggregate all classes and solve the resulting single-class SOQN. For the dedicated pallet case, we construct a method based on an existing product-form approximation method for general, multiclass closed networks. Our approximation method combines the matrix-geometric method with the decomposition-aggregation approach. Numerical results show that our approximations have desirable accuracy and efficiency.", "e:keyword": ["Semi-open queuing network", "Multiple class", "General pallet", "Dedicated pallet", "Tandem line", "Matrix-geometric method", "Experimental results"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0628", "e:abstract": "The purpose of this paper is to introduce the reader to the field of closed-loop supply chains with a strong business perspective, i.e., we focus on profitable value recovery from returned products. It recounts the evolution of research in this growing area over the past 15 years, during which it developed from a narrow, technically focused niche area to a fully recognized subfield of supply chain management. We use five phases to paint an encompassing view of this evolutionary process for the reader to understand past achievements and potential future operations research opportunities.", "e:keyword": ["Closed-loop supply chains", "Reverse logistics", "Remanufacturing", "Value-added recovery"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0629", "e:abstract": "We study a rental system where a fixed number of heterogeneous users rent one product at a time from a collection of reusable products. The online DVD rental firm Netflix provides the motivation. We assume that rental durations of each user are independent and identically distributed with finite mean. We study transient behavior in this system following the introduction of a new product that is desired by all the users. We represent the usage process for this new product in terms of an empirical distribution. This allows us to characterize the asymptotic behavior of the usage process as the number of users increases without bound, via appropriate versions of Glivenko-Cantelli and Donsker's theorems. Analyzing the usage process, we demonstrate that an increase in the variability of the rental duration distribution can actually help the firm by allowing it to set lower capacity levels to provide a desired quality of service. Further, we show that the firm is better off not imposing any deadlines for the return of the product.", "e:keyword": ["Queues", "Limit theorems", "Nonstationary"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0630", "e:abstract": "We model and analyze the process of passengers boarding an airplane. We show how the model yields closed-form estimates for the expected boarding time in many cases of interest. Comparison of our computations with previous work, based on discrete-event simulations, shows a high degree of agreement. Analysis of the model reveals a clear link between the efficiency of various airline boarding policies and a congestion parameter that is related to interior airplane design parameters, such as distance between rows. In particular, as congestion increases, random boarding becomes more attractive among row-based policies.", "e:keyword": ["Air transportation", "Stochastic modeling application", "Space-time geometry", "Airplane boarding"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0631", "e:abstract": "Many oligopolies operate as a repeated game. In such circumstances, it can be expected that profit-maximising participants may engage in implicit collusion to profitably increase spot market prices. This paper models the emergence of such implicit collusion in a stylised market model using a coevolutionary approach. Players bid supply functions made up of a finite number of linear pieces. Each player uses a genetic algorithm to find state-based strategies depending on the price and demand in the last period and the predicted demand in the next period. We consider a symmetric duopoly and demonstrate that collusive behaviour can be learned even when there is very limited information available to the participants. Moreover, we show a type of implicit collusive behaviour that occurs even though the system does not settle into a stable equilibrium. We use a wholesale electricity market, in which supply function bids are typical, as a motivating example throughout this paper.", "e:keyword": ["Noncooperative games", "Repeated games", "Implicit collusion", "Genetic algorithm", "Energy", "Electricity markets"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0632", "e:abstract": "We propose a general Markovian model for the optimal control of admissions and subsequent routing of customers for service provided by a collection of heterogeneous stations. Queue-length information is available to inform all decisions. Admitted customers will abandon the system if required to wait too long for service. The optimisation goal is the maximisation of reward rate earned from service completions, net of the penalties paid whenever admission is denied, and the costs incurred upon every customer loss through impatience. We show that the system is indexable under mild conditions on model parameters and give an explicit construction of an index policy for admission control and routing founded on a proposal of Whittle for restless bandits. We are able to gain insights regarding the strength of performance of the index policy from the nature of solutions to the Lagrangian relaxation used to develop the indices. These insights are strengthened by the development of performance bounds. Although we are able to assert the optimality of the index heuristic in a range of asymptotic regimes, the performance bounds are also able to identify instances where its performance is relatively weak. Numerical studies are used to illustrate and support the theoretical analyses.", "e:keyword": ["Admission control", "Customer impatience", "Dynamic programming", "Index policies", "Markov decision processes", "Monotone policies", "Restless bandits", "Routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0633", "e:abstract": "For the distribution of spare parts to car dealers, many automotive companies use a transport network of intermediate hubs or transport platforms, operated by a set of third-party logistics (3PL) partners. The optimization of this network, particularly the selection of 3PL providers and corresponding transport platforms, is a complex decision that needs to be supported by appropriate software tools. In this paper, we develop such a tool, implement it, and show its results on a real-life case study provided by Toyota. The tool is currently in active use at Toyota to study and improve the distribution of spare parts in Germany. Using a tabu search metaheuristic, the developed tool essentially solves a large location-routing problem, but has several innovative features to increase its usefulness. First, the tool generates a set of high-quality but structurally different solutions, rather than a single one. This increases Toyota's negotiating power, increases its ability to analyze its current transport network against possible alternatives, and allows it to quickly switch between different transport networks if unexpected events occur. Second, a commercial vehicle-routing solver is integrated into the tool, to allow for a far more realistic modeling of the vehicle-routing decision.", "e:keyword": ["Transportation", "Vehicle routing", "Facilities/equipment planning", "Location", "Discrete"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0635", "e:abstract": "We extend a previous model for scheduling commercial advertisements during breaks in television programming. The proposed extension allows differential weighting of conflicts between pairs of commercials. We formulate the problem as a capacitated generalization of the max <i>k</i>-cut problem in which the vertices of a graph correspond to commercial insertions and the edge weights to the conflicts between pairs of insertions. The objective is to partition the vertices into <i>k</i> capacitated sets to maximize the sum of conflict weights across partitions. We note that the problem is NP-hard. We extend a previous local-search procedure to allow for the differential weighting of edge weights. We show that for problems with equal insertion lengths and break durations, the worst-case bound on the performance of the proposed algorithm increases with the number of program breaks and the number of insertions per break, and that it is independent of the number of conflicts between pairs of insertions. Simulation results suggest that the algorithm performs well even if the problem size is small.", "e:keyword": ["Marketing", "Advertising", "Media", "Networks/graphs", "Maxcut", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0636", "e:abstract": "In many production/distribution systems, materials flow in fixed lot sizes (e.g., in full truckloads or full containers) and under regular schedules (e.g., delivery every week). In this paper, we study a multiechelon serial system with batch ordering and fixed replenishment intervals. We derive the optimal inventory control policy, provide a distribution-function solution for its optimal control parameters, and present an efficient algorithm for computing those parameters. Further, we show that the optimal expected system cost is minimized when the ordering times for all stages are synchronized. In contrast to the known approach in the literature that develops a lower bound for the average cost of a given period for the classical serial system, we develop a lower bound for the average total cost over an appropriately defined cycle and then construct a policy that reaches the lower bound. We also discuss its extension to the nonlinear shortage cost case (i.e., the nonlinear cost case). This paper generalizes several recent results on the analysis of multiechelon systems.", "e:keyword": ["Multiechelon system", "Optimal policy", "Synchronized ordering", "Nonlinear shortage costs", "Fixed replenishment intervals", "Batch ordering"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0637", "e:abstract": "In a combinational auction in which bidders can bid on any combination of goods, bid data can be of exponential size. We describe an innovative new combinatorial auction format in which bidders submit “matrix bids.” The advantage of this approach is that it provides bidders a mechanism to compactly express bids on every possible bundle. We describe many different types of preferences that can be modeled using a matrix bid, which is quite flexible, supporting additive, subadditive, and superadditive preferences simultaneously. To utilize the compactness of the matrix bid format in a more general preference environment, we describe a logical language with matrix bids as “atoms” and show that matrix bids compactly express preferences that require an exponential number of atoms in other bidding languages and are as expressive as the most sophisticated languages in the literature. We model the (N-script)(P-script)-hard winner-determination problem as a polynomially sized integer program, specifically an assignment problem with side constraints. We show the strength of this formulation with which we rapidly solve winner-determination problems with 72 unique items, indicating that this model may be well suited for practical implementation.", "e:keyword": ["Games/group decisions", "Bidding/auctions", "Information systems", "Decision support systems", "Integer programming", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0638", "e:abstract": "We consider the problem of allocating a fixed amount of an infinitely divisible resource among multiple competing, fully rational users. We study the efficiency guarantees that are possible when we restrict to mechanisms that satisfy certain scalability constraints motivated by large-scale communication networks; in particular, we restrict attention to mechanisms where users are restricted to one-dimensional strategy spaces. We first study the efficiency guarantees possible when the mechanism is not allowed to price differentiate. We study the worst-case efficiency loss (ratio of the utility associated with a Nash equilibrium to the maximum possible utility), and show that Kelly's proportional allocation mechanism minimizes the efficiency loss when users are price anticipating. We then turn our attention to mechanisms where price differentiation is permitted; using an adaptation of the Vickrey-Clarke-Groves class of mechanisms, we construct a class of mechanisms with one-dimensional strategy spaces where Nash equilibria are fully efficient. These mechanisms are shown to be fully efficient even in general convex environments, under reasonable assumptions. Our results highlight a fundamental insight in mechanism design: when the pricing flexibility available to the mechanism designer is limited, restricting the strategic flexibility of bidders may actually improve the efficiency guarantee.", "e:keyword": ["Game/group decisions", "Noncooperative", "Bidding/auctions", "Networks/graphs", "Theory", "Utility/preference", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0639", "e:abstract": "In this paper, we propose a simple heuristic approach for the inventory control problem with stochastic demand and multiplicative random yield. Our heuristic tries to find the best candidate within a class of policies that are referred to in the literature as the <i>linear inflation rule</i> (LIR) policies. Our approach is computationally fast, easy to implement, and intuitive to understand. Moreover, we find that in a significant number of instances our heuristic performs better than several other well-known heuristics that are available in the literature.", "e:keyword": ["Inventory/production", "Operating characteristics", "Policies", "Uncertainties", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0640", "e:abstract": "We consider a single-product revenue management problem where, given an initial inventory, the objective is to dynamically adjust prices over a finite sales horizon to maximize expected revenues. Realized demand is observed over time, but the underlying functional relationship between price and mean demand rate that governs these observations (otherwise known as the demand function or demand curve) is not known. We consider two instances of this problem: (i) a setting where the demand function is assumed to belong to a known parametric family with unknown parameter values; and (ii) a setting where the demand function is assumed to belong to a broad class of functions that need not admit any parametric representation. In each case we develop policies that learn the demand function “on the fly,” and optimize prices based on that. The performance of these algorithms is measured in terms of the <i>regret</i>: the revenue loss relative to the maximal revenues that can be extracted when the demand function is known prior to the start of the selling season. We derive lower bounds on the regret that hold for any admissible pricing policy, and then show that our proposed algorithms achieve a regret that is “close” to this lower bound. The magnitude of the regret can be interpreted as the economic value of prior knowledge on the demand function, manifested as the revenue loss due to model uncertainty.", "e:keyword": ["Revenue management", "Pricing", "Estimation", "Learning", "Exploration-exploitation", "Value of information", "Asymptotic analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0641", "e:abstract": "In 2004, Deere & Company's Commercial & Consumer Equipment Division (C&CE) engaged in a new logistics initiative to further enhance its outbound distribution network. The goal was to offer faster and more reliable replenishment to 2,500 North American independent dealers while keeping logistics costs in check by deploying different tactics during the peak (February--July) and offpeak (August--January) selling and shipping seasons. Deere and SmartOps worked together under a shared reward contract based on actual cost reductions accomplished (that are additive over the benefits that may have accrued for other reasons). Through careful analysis, validation, and verification of the data available, the team was able to develop detailed models of the current and alternative distribution systems. By formulating the key replenishment and transportation decisions and constraints as a mixed-integer mathematical program, the team was able to use powerful off-the-shelf solution software to find improvements. The ability to fix variables to perform what-if analysis also helped in the acceptance of the recommendations. Over a period of three years, Deere significantly improved service to 82% of their dealers (without any reduction in service to the other 18%) while reducing logistics costs by over $10 million. The novelty of this work stems from the dynamic seasonal optimization of Deere C&CE distribution network and replenishment decisions as a way of meeting service and cost reduction mandates, the creative use of tactical network optimization operations research models and what-if analysis to meet the implementation goals under time constraints and in the scrutiny given to the results.", "e:keyword": ["Industries", "Machinery", "Transportation", "Costs", "Network and location models", "Inventory/production", "Multistage"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0642", "e:abstract": "Train movements across railway stations are still operated by human dispatchers. Motivated by an application provided by Azienda Trasporti Milanesi (ATM), the major Italian municipal transport company, we developed a real-time automated traffic control system to operate trains in metro stations. The system optimally controls the trains in a metro station by identifying a suitable routing and by establishing an optimum schedule of the performed operations. For each candidate routing an instance of the blocking, no-wait job-shop scheduling problem with convex costs is solved to optimality by branch and bound. A new, effective lower bound is developed to speed up the enumeration process. Computational testing in a real environment proved that the algorithm is able to solve relevant practical instances within the very tight time limit imposed by the application. The system has been in operation in the Milan metro since July 2007. To our knowledge, this is the first example of successful application of optimization methods to real-time traffic control in metro stations.", "e:keyword": ["Transportation", "Routing", "Scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0643", "e:abstract": "A “proliferator” seeks to complete a first small batch of fission weapons as quickly as possible, whereas an “interdictor” wishes to delay that completion for as long as possible. We develop and solve a max-min model that identifies resource-limited interdiction actions that maximally delay completion time of the proliferator's weapons project, given that the proliferator will observe any such actions and adjust his plans to minimize that time. The model incorporates a detailed project-management (critical path method) submodel, and standard optimization software solves the model in a few minutes on a personal computer. We exploit off-the-shelf project-management software to manage a database, control the optimization, and display results. Using a range of levels for interdiction effort, we analyze a published case study that models three alternate uranium-enrichment technologies. The task of “cascade loading” appears in all technologies and turns out to be an inherent fragility for the proliferator at all levels of interdiction effort. Such insights enable policy makers to quantify the effects of interdiction options at their disposal, be they diplomatic, economic, or military.", "e:keyword": ["Government", "Defense", "Foreign policy", "Military", "Targeting", "Programming", "Integer", "Linear", "Project management", "CPM"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0644", "e:abstract": "We study a simple protocol for communication networks, in which users get no receipt acknowledgment of their requests. As a result, users hold partial and differential information over the state of the protocol. We characterize optimal behavior by viewing the protocol as a stochastic game with partial observation. We also study two classes of protocols that generalize this protocol.", "e:keyword": ["Games/group decisions", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0645", "e:abstract": "We consider a revenue-maximizing make-to-order manufacturer that serves a market of price- and delay-sensitive customers and operates in an environment in which the market size varies stochastically over time. A key feature of our analysis is that no model is assumed for the evolution of the market size. We analyze two main settings: (i) the size of the market is observable at any point in time; and (ii) the size of the market is not observable and hence cannot be used for decision making. We focus on high-volume systems that are characterized by large processing capacities and market sizes, and where the latter fluctuate on a slower timescale than that of the underlying production system dynamics. We develop an approach to tackle such problems that is based on an asymptotic analysis and that yields near-optimal policy recommendations for the original system via the solution of a stochastic fluid model.", "e:keyword": ["Revenue management", "Dynamic pricing", "Market uncertainty", "Queueing", "State-dependent queues", "Asymptotic analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0646", "e:abstract": "In this paper, we propose a methodology for constructing uncertainty sets within the framework of robust optimization for linear optimization problems with uncertain parameters. Our approach relies on decision maker risk preferences. Specifically, we utilize the theory of <i>coherent risk measures</i> initiated by Artzner et al. (1999) [Artzner, P., F. Delbaen, J. Eber, D. Heath. 1999. Coherent measures of risk. <i>Math. Finance</i> <b>9</b> 203--228.], and show that such risk measures, in conjunction with the support of the uncertain parameters, are equivalent to explicit uncertainty sets for robust optimization. We explore the structure of these sets in detail. In particular, we study a class of coherent risk measures, called <i>distortion risk measures</i>, which give rise to polyhedral uncertainty sets of a special structure that is tractable in the context of robust optimization. In the case of discrete distributions with rational probabilities, which is useful in practical settings when we are sampling from data, we show that the class of all distortion risk measures (and their corresponding polyhedral sets) are generated by a finite number of conditional value-at-risk (CVaR) measures. A subclass of the distortion risk measures corresponds to polyhedral uncertainty sets symmetric through the sample mean. We show that this subclass is also finitely generated and can be used to find inner approximations to arbitrary, polyhedral uncertainty sets.", "e:keyword": ["Robust optimization", "Uncertainty sets", "Coherent risk measures", "Distortion risk measures"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0647", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0648", "e:abstract": "In the United States, patients with end-stage liver disease must join a waiting list to be eligible for cadaveric liver transplantation. Due to privacy concerns, the details of the composition of this waiting list are not publicly available. This paper considers the benefits associated with creating a more transparent waiting list. We study these benefits by modeling the organ accept/reject decision faced by these patients as a Markov decision process in which the state of the process is described by patient health, quality of the offered liver, and a measure of the rank of the patient in the waiting list. We prove conditions under which there exist structured optimal solutions, such as monotone value functions and control-limit optimal policies. We define the concept of the patient's price of privacy, namely, the number of expected life days lost due to the lack of complete waiting list information. We conduct extensive numerical studies based on clinical data, which indicate that this price of privacy is typically on the order of 5% of the optimal solution value.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Markov", "Health care", "Treatment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0649", "e:abstract": "We present a strategic planning model in which the activities to be planned, such as production and distribution in a supply network, require technology to be installed before they can be performed. The technology improves over time, so that a decision maker has incentive to delay starting an activity to take advantage of better technology and lower operational costs. The model captures the fundamental trade-off between delaying the start time of an activity and the need for some activities to be performed now. Models of this type are used in the oil industry to plan the development of oil fields. This problem is naturally formulated as a mixed-integer program with a bilinear objective. We develop a series of progressively more compact mixed-integer linear formulations, along with classes of valid inequalities that make the formulations strong. We also present a specialized branch-and-cut algorithm to solve an extremely compact concave formulation. Computational results indicate that these formulations can be used to solve large-scale instances, whereas a straightforward linearization of the mixed-integer bilinear formulation fails to solve even small instances.", "e:keyword": ["Integer programming", "Theory and applications", "Facilities/equipment planning", "Capacity expansion", "Technology"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0650", "e:abstract": "We develop a robust optimization framework for dynamic empty repositioning problems modeled using time-space networks. In such problems, uncertainty arises primarily from forecasts of future supplies and demands for assets at different time epochs. The proposed approach models such uncertainty using intervals about nominal forecast values and a limit on the systemwide scaled deviation from the nominal forecast values. A robust repositioning plan is defined as one in which the typical flow balance constraints and flow bounds are satisfied for the nominal forecast values, and the plan is <i>recoverable</i> under a limited set of recovery actions. A plan is recoverable when feasibility can be reestablished for <i>any</i> outcome in a defined uncertainty set. We develop necessary and sufficient conditions for flows to be robust under this definition for three types of allowable recovery actions. When recovery actions allow only flow changes on inventory arcs, we show that the resulting problem is polynomially solvable. When recovery actions allow limited reactive repositioning flows, we develop feasibility conditions that are independent of the size of the uncertainty set. A computational study establishes the practical viability of the proposed framework.", "e:keyword": ["Transportation", "Network models", "Programming", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0651", "e:abstract": "Motivated by call center practice, we study asymptotically optimal staffing of many-server queues with abandonment. A call center is modelled as an M/M/<i>n</i> + G queue, which is characterized by Poisson arrivals, exponential service times, <i>n</i> servers, and generally distributed patience times of customers. Our asymptotic analysis is performed as the arrival rate, and hence the number of servers <i>n</i>, increases indefinitely. We consider a constraint satisfaction problem, where one chooses the minimal staffing level <i>n</i> that adheres to a given cost constraint. The cost can incorporate the fraction abandoning, average wait, and tail probabilities of wait. Depending on the cost, several operational regimes arise as asymptotically optimal: Efficiency-Driven (ED), Quality and Efficiency-Driven (QED), and also a new ED + QED operational regime that enables QED tuning of the ED regime. Numerical experiments demonstrate that, over a wide range of system parameters, our approximations provide useful insight as well as excellent fit to exact optimal solutions. It turns out that the QED regime is preferable either for small-to-moderate call centers or for large call centers with relatively tight performance constraints. The other two regimes are more appropriate for large call centers with loose constraints. We consider two versions of the constraint satisfaction problem. The first one is constraint satisfaction on a single time interval, say one hour, which is common in practice. Of special interest is a constraint on the tail probability, in which case our new ED + QED staffing turns out asymptotically optimal. We also address a global constraint problem, say over a full day. Here several time intervals, say 24 hours, are considered, with interval-dependent staffing levels allowed; one seeks to minimize staffing levels, or more generally costs, given the overall performance constraint. In this case, there is the added flexibility of trading service levels among time intervals, but we demonstrate that only little gain is associated with this flexibility if one is concerned with the fraction abandoning.", "e:keyword": ["Queues", "Abandonment", "Limit theorems", "Optimization", "Call centers", "Staffing", "Workforce management", "Halfin-Whitt", "ED + QED"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0652", "e:abstract": "A credit investor such as a bank granting loans to firms or an asset manager buying corporate bonds is exposed to correlated corporate default risk. A multiname credit derivative is a financial security that allows the investor to transfer this risk to the credit market. In this paper, we study the valuation and risk analysis of multiname derivatives. To capture the complex economic phenomena that drive the pricing of these securities, we introduce a time-changed birth process as a probabilistic model of correlated event timing. The self-exciting property of a time-changed birth process captures the feedback from events that is often observed in credit markets. The stochastic variation of arrival rates between events captures the exposure of firms to common economic risk factors. We derive a closed-form expression for the distribution of a time-changed birth process, and develop analytically tractable pricing relations for a range of multiname derivatives valuation problems. We illustrate our results by calibrating a tranche forward and option pricer to market rates of index and tranche swaps.", "e:keyword": ["Finance", "Asset pricing", "Portfolio credit derivatives", "Financial institutions", "Banks", "Risk management", "Hedging", "Probability", "Self-exciting point processes", "Birth process", "Time change"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0653", "e:abstract": "In the traffic assignment problem, first proposed by Wardrop in 1952, commuters select the shortest available path to travel from their origins to their destinations. We study a generalization of this problem in which competitors, who may control a nonnegligible fraction of the total flow, ship goods across a network. This type of games, usually referred to as atomic games, readily applies to situations in which the competing freight companies have market power. Other applications include intelligent transportation systems, competition among telecommunication network service providers, and scheduling with flexible machines. Our goal is to determine to what extent these systems can benefit from some form of coordination or regulation. We measure the quality of the outcome of the game without centralized control by computing the worst-case inefficiency of Nash equilibria. The main conclusion is that although self-interested competitors will not achieve a fully efficient solution from the system's point of view, the loss is not too severe. We show how to compute several bounds for the worst-case inefficiency that depend on the characteristics of cost functions and on the market structure in the game. In addition, building upon the work of Catoni and Pallotino, we show examples in which market aggregation (or collusion) adversely impacts the aggregated competitors, even though their market power increases. For example, Nash equilibria of atomic network games may be less efficient than the corresponding Wardrop equilibria. When competitors are completely symmetric, we provide a characterization of the Nash equilibrium using a potential function, and prove that this counterintuitive phenomenon does not arise. Finally, we study a pricing mechanism that elicits more coordination from the players by reducing the worst-case inefficiency of Nash equilibria.", "e:keyword": ["Networks/graphs", "Multicommodity", "Theory", "Games/group decisions", "Noncooperative", "Atomic", "Transportation", "Models", "Network", "Programming", "Complementarity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0654", "e:abstract": "In this paper, we consider the revenue management problem from the perspective of online algorithms. This approach eliminates the need for both demand forecasts and a risk-neutrality assumption. The competitive ratio of a policy relative to a given input sequence is the ratio of the policy's performance to the offline optimal. Under the online algorithm approach, revenue management policies are evaluated based on the highest competitive ratio they can guarantee. We are able to define lower bounds on the best-possible performance and describe policies that achieve these lower bounds. We address the two-fare problem in greatest detail, but also treat the general multifare problem and the bid-price control problem.", "e:keyword": ["Analysis of algorithms", "Suboptimal algorithms", "Inventory/production", "Policies", "Marketing/pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0657", "e:abstract": "We consider a manufacturer using a request-for-quotes (RFQ) reverse auction in combination with supplier qualification screening to determine which qualified supplier will be awarded a contract. Supplier qualification screening is costly for the manufacturer---for example, involving reference checks, financial audits, and on-site visits. The manufacturer seeks to minimize its total procurement costs, i.e., the contract payment plus qualification costs. Although suppliers can be qualified prior to the auction (prequalification), we allow the manufacturer to delay all or part of the qualification until after the auction (postqualification). Using an optimal mechanism analysis, we analytically explore the trade-offs between varying levels of pre- and postqualification. Although using postqualification causes the expected contract payment to increase (bids from unqualified suppliers are discarded), we find that standard industrial practices of prequalification can be improved upon by judicious use of postqualification, particularly when supplier qualification screening is moderately expensive relative to the value of the contract to the manufacturer.", "e:keyword": ["Bidding/auctions", "Procurement", "Supplier qualification", "Supplier screening", "Mechanism design"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0658", "e:keyword": ["Network optimization", "Multicommodity flow"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0659", "e:abstract": "Zhao showed that the log barrier associated with the recourse function of two-stage stochastic linear programs behaves as a strongly self-concordant barrier and forms a self-concordant family on the first-stage solutions. In this paper, we show that the recourse function is also strongly self-concordant and forms a self-concordant family for the two-stage stochastic convex quadratic programs with recourse. This allows us to develop Bender's decomposition based linearly convergent interior point algorithms. An analysis of such an algorithm is given in this paper.", "e:keyword": ["Two-stage stochastic programming", "Linear-quadratic programming", "Bender's decomposition", "Large-scale optimization", "Nondifferentiable convex optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0660", "e:abstract": "This paper considers the problem of scheduling a set of jobs on a single machine subject to stochastic breakdowns with incomplete information on the probability distributions involved in the decision process. We focus on the preemptive-repeat discipline, under which a machine breakdown leads to the loss of the work done on the job being processed. The breakdown process of the machine is allowed to depend on the job it is processing. The processing times required to complete the jobs, and the machine uptimes and downtimes, are random variables with incomplete information on their probability distributions characterized by unknown parameters. We establish the preemptive-repeat model with incomplete information and investigate its probabilistic characteristics. We show that optimal static policies can be obtained for a wide range of performance measures, which are determined by the prior distributions of the unknown parameters. We derive optimal dynamic policies via Gittins indices represented by the posterior distributions, which are updated adaptively based on processing histories. Under appropriate conditions, the optimal dynamic policies can be calculated by one-step reward rates in a closed form. As a by-product, we also show that our incomplete information model subsumes the traditional preemptive-repeat models with complete information as extreme cases.", "e:keyword": ["Production/scheduling", "Stochastic", "Learning", "Probability", "Stochastic model applications", "Dynamic programming/optimal control", "Semi-Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0661", "e:abstract": "Reliability-based design optimization is concerned with designing a product to optimize an objective function, given uncertainties about whether various design constraints will be satisfied. However, the widespread practice of formulating such problems as chance-constrained programs can lead to misleading solutions. While a decision-analytic approach would avoid this undesirable result, many engineers find it difficult to determine the utility functions required for a traditional decision analysis. This paper presents an alternative decision-analytic formulation that, although implicitly using utility functions, is more closely related to probability maximization formulations with which engineers are comfortable and skilled. This result combines the rigor of decision analysis with the convenience of existing optimization approaches.", "e:keyword": ["Decision analysis", "Stochastic programming", "Chance-constrained programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0662", "e:abstract": "In this paper, we analyze the worst-case performance of heuristics for the classical economic lot-sizing problem with time-invariant cost parameters. We consider a general class of online heuristics that is often applied in a rolling-horizon environment. We develop a procedure to systematically construct worst-case instances for a fixed time horizon and use it to derive worst-case problem instances for an infinite time horizon. Our analysis shows that any online heuristic has a worst-case ratio of at least 2.", "e:keyword": ["Analysis of algorithms", "Inventory/production", "Approximations/heuristics", "Production/scheduling", "Planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0663", "e:abstract": "A model is developed that allows the derivation of feedback Nash equilibrium advertising strategies for oligopolistic competitors. The model is an extension of a modified Vidale-Wolfe model that incorporates multiple brands per competitor. The resulting expressions of feedback advertising strategies are combined with those for sales dynamics in an empirical model that is applied to the carbonated soft drink market, which involves three primary competitors and five primary brands. The research provides the following contributions:", "e:keyword": ["Oligopoly", "Differential game", "Feedback Nash equilibrium", "Advertising competition", "Empirical application"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0664", "e:abstract": "The concept of chaining, or in more general terms, sparse process structure, has been extremely influential in the process flexibility area, with many large automakers already making this the cornerstone of their business strategies to remain competitive in the industry. The effectiveness of the process strategy, using chains or other sparse structures, has been validated in numerous empirical studies. However, to the best of our knowledge, there have been relatively few concrete analytical results on the performance of such strategies vis-á-vis the full flexibility system, especially when the system size is large or when the demand and supply are asymmetrical. This paper is an attempt to bridge this gap. We study the problem from two angles: (1) For the symmetrical system where the (mean) demand and plant capacity are balanced and identical, we utilize the concept of a generalized random walk to evaluate the asymptotic performance of the chaining structure in this environment. We show that a simple chaining structure performs surprisingly well for a variety of realistic demand distributions, even when the system size is large. (2) For the more general problem, we identify a class of conditions under which only a sparse flexible structure is needed so that the expected performance is already within (epsilon) optimality of the full flexibility system. Our approach provides a theoretical justification for the widely held maxim: In many practical situations, adding a small number of links to the process flexibility structure can significantly enhance the ability of the system to match (fixed) production capacity with (random) demand.", "e:keyword": ["Random walk", "Stochastic programming", "Production", "Flexible manufacturing", "Facility planning", "Design"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0665", "e:abstract": "Quantile assessments are commonly encountered in the elicitation of probability distributions in decision analysis, forecasting, and risk analysis. Scoring rules have been developed to provide ex ante incentives for careful and truthful assessments and ex post evaluation measures in the context of probability assessment. We show that these scoring rules designed for probability assessment provide inappropriate incentives if used for quantile assessment. We investigate the properties of a linear family of scoring rules that are intended specifically for quantile assessment (including the assessment of multiple quantiles) and can be related to a realistic decision-making problem. These rules provide proper incentives for quantile assessment and yield higher expected scores for distributions that are more informative in the sense of having less dispersion. We discuss the special case of interval forecasts and a generalization involving transformations, and we briefly mention other possible extensions.", "e:keyword": ["Probability", "Assessment", "Evaluation", "Decision analysis", "Expert information", "Forecasting", "Probability forecasts"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0666", "e:abstract": "Patients suffering from a chronic condition often require periodic treatment. For example, patients with End-Stage Renal Disease (ESRD) require dialysis three times a week. These patients are also frequently hospitalized for complications from their treatment, resulting in idle capacity at the clinic. These temporary patient absences make overbooking at the clinic attractive. This paper develops a semiclosed migration network to capture patient flow into the clinic and between the clinic and hospital. We consider a simple class of stationary control policies for patient admissions and provide algorithms for selecting one that maximizes long-run average earnings. Local diffusion approximations were constructed to provide square-root loading formulas for the optimal capacity level and patient overbooking level: as the total patient arrival rate increases, the deviation between the optimal and fluid-limit capacity and overbooking levels scale up with the square root of the total arrival rate. We find that high hospitalization rates and long inpatient stays allow for more overbooking. Numerical examples based on the typical dialysis clinic in the United States suggest an increase in earnings of 11%--14% over policies derived from traditional <i>M</i>/<i>M</i>/<i>N</i> models that do not account for hospitalizations and do not allow overbooking, while keeping the probability of capacity shortage arbitrarily small.", "e:keyword": ["Health care", "Chronic diseases", "Capacity planning", "Newsvendor", "Queues", "Statistical multiplexing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0667", "e:abstract": "We analyze a planning model for a firm or public organization that needs to cover uncertain demand for a given item by procuring supplies from multiple sources. The necessity to employ multiple suppliers arises from the fact that when an order is placed with any of the suppliers, only a random fraction of the order size is usable. The model considers a single demand season with a given demand distribution, where all supplies need to be ordered simultaneously before the start of the season. The suppliers differ from one another in terms of their yield distributions, their procurement costs, and capacity levels. The planning model determines which of the potential suppliers are to be retained and what size order is to be placed with each. We consider two versions of the planning model: in the first, the service constraint model (SCM), the orders must be such that the available supply of usable units covers the random demand during the season with (at least) a given probability. In the second version of the model, the total cost model (TCM), the orders are determined so as to minimize the aggregate of procurement costs and end-of-the-season inventory and shortage costs. In the classical inventory model with a single, fully reliable supplier, these two models are known to be equivalent, but the equivalency breaks down under multiple suppliers with unreliable yields. For both the service constraint and total cost models, we develop a highly efficient procedure that generates the optimal set of suppliers as well as the optimal orders to be assigned to each. Most importantly, these procedures generate a variety of important qualitative insights, for example, regarding which sets of suppliers allow for a feasible solution, both when they have ample supply and when they are capacitated, and how various model parameters influence the selected set of suppliers, the aggregate order size, and the optimal cost values.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Operating characteristics", "Reliability", "Failure models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0668", "e:abstract": "We consider a class of parallel server systems that are known as <i>N</i>-systems. In an <i>N</i>-system, there are two customer classes that are catered by servers in two pools. Servers in one of the pools are cross-trained and can serve customers from both classes, whereas all of the servers in the other pool can serve only one of the customer classes. A customer reneges from his queue if his waiting time in the queue exceeds his patience. Our objective is to minimize the total cost that includes a linear holding cost and a reneging cost. We prove that, when the service speed is pool dependent, but not class dependent, a <i>c(mu)</i>-type greedy policy is asymptotically optimal in many-server heavy traffic.", "e:keyword": ["N-systems", "Parallel server systems", "Asymptotic optimality", "Many-server limit", "Heavy-traffic limit", "Halfin-Whitt regime"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0669", "e:abstract": "As large-scale discrete-event stochastic simulation becomes a tool that is used routinely for the design and analysis of stochastic systems, the need for input-modeling support with the ability to represent complex interactions and interdependencies among the components of multivariate time-series input processes is more critical than ever. Motivated by the failure of independent and identically distributed random variables to represent such input processes, a comprehensive framework called Vector-Autoregressive-To-Anything (VARTA) has been introduced for multivariate time-series input modeling. Despite its flexibility in capturing a wide variety of distributional shapes, we show that VARTA falls short in representing dependence structures that arise in situations where extreme component realizations occur together. We demonstrate that it is possible to extend VARTA to work for such dependence structures via the use of the copula theory, which has been used primarily for random vectors in the simulation input-modeling literature, for multivariate time-series input modeling. We show that our copula-based multivariate time-series input model, which includes VARTA as a special case, allows the development of statistically valid fitting and fast sampling algorithms well suited for driving large-scale stochastic simulations.", "e:keyword": ["Correlation", "Estimation", "Sampling", "Time series"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0677", "e:abstract": "Classical measures of network connectivity are the number of disjoint paths between a pair of nodes and the size of a minimum cut. For standard graphs, these measures can be computed efficiently using network flow techniques. However, in the Internet on the level of autonomous systems (ASs), referred to as AS-level Internet, routing policies impose restrictions on the paths that traffic can take in the network. These restrictions can be captured by the valley-free path model, which assumes a special directed graph model in which edge types represent relationships between ASs. We consider the adaptation of the classical connectivity measures to the valley-free path model, where it is (N-script)(P-script)-hard to compute them. Our first main contribution consists of presenting algorithms for the computation of disjoint paths, and minimum cuts, in the valley-free path model. These algorithms are useful for ASs that want to evaluate different options for selecting upstream providers to improve the robustness of their connection to the Internet. Our second main contribution is an experimental evaluation of our algorithms on four types of directed graph models of the AS-level Internet produced by different inference algorithms. Most importantly, the evaluation shows that our algorithms are able to compute optimal solutions to instances of realistic size of the connectivity problems in the valley-free path model in reasonable time. Furthermore, our experimental results provide information about the characteristics of the directed graph models of the AS-level Internet produced by different inference algorithms. It turns out that (i) we can quantify the difference between the undirected AS-level topology and the directed graph models with respect to fundamental connectivity measures, and (ii) the different inference algorithms yield topologies that are similar with respect to connectivity and are different with respect to the types of paths that exist between pairs of ASs.", "e:keyword": ["Networks", "Computer network modeling", "Integer programming", "Analysis of algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0678", "e:abstract": "We describe a multistage, stochastic, mixed-integer programming model for planning capacity expansion of production facilities. A scenario tree represents uncertainty in the model; a general mixed-integer program defines the operational submodel at each scenario-tree node, and capacity-expansion decisions link the stages. We apply “variable splitting” to two model variants, and solve those variants using Dantzig-Wolfe decomposition. The Dantzig-Wolfe master problem can have a much stronger linear programming relaxation than is possible without variable splitting, over 700% stronger in one case. The master problem solves easily and tends to yield integer solutions, obviating the need for a full branch-and-price solution procedure. For each scenario-tree node, the decomposition defines a subproblem that may be viewed as a single-period, deterministic, capacity-planning problem. An effective solution procedure results as long as the subproblems solve efficiently, and the procedure incorporates a good “duals stabilization method.” We present computational results for a model to plan the capacity expansion of an electricity distribution network in New Zealand, given uncertain future demand. The largest problem we solve to optimality has six stages and 243 scenarios, and corresponds to a deterministic equivalent with a quarter of a million binary variables.", "e:keyword": ["Facilities/equipment planning", "Capacity expansion", "Industries", "Electric", "Networks/graphs", "Applications", "Stochastic", "Programming", "Integer", "Algorithms", "Benders decomposition", "Applications", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0680", "e:abstract": "We study a class of two-echelon serial systems with identical ordering/production capacities or limits for both echelons. Demands are assumed to be integer valued. For the case where the lead time to the upstream echelon is one period, the optimality of state-dependent modified echelon base-stock policies is proved using a decomposition approach. For the case where the upstream lead time is two periods, we introduce a new class of policies called “two-tier base-stock policies,” and prove their optimality. Some insight about the inventory control problem in <i>N</i> echelon serial systems with identical capacities at all stages and arbitrary lead times everywhere is also provided. We argue that a generalization of two-tier base-stock policies, which we call “multitier base-stock policies,” are optimal for these systems; we also provide a bound on the number of parameters required to specify the optimal policy.", "e:keyword": ["Inventory/production", "Policy", "Optimal policies", "Markov modulated demands", "Lead times", "Planning horizon", "Finite", "Infinite", "Discounted"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0682", "e:abstract": "Nonprofit firms sometimes engage in for-profit activities for the purpose of generating revenue to subsidize their mission activities. The organization is then confronted with a consumption versus investment trade-off, where investment corresponds to providing capacity for revenue customers, and consumption corresponds to serving mission customers. Exemplary of this approach are the Aravind Eye Hospitals in India, where profitable paying hospitals are used to subsidize care at free hospitals. We model this problem as a multiperiod stochastic dynamic program. In each period, the organization must decide how much of the current assets should be invested in revenue-customer service capacity, and at what price the service should be sold. We provide sufficient conditions under which the optimal capacity and pricing decisions are of threshold type. Similar results are derived when the selling price is fixed, but the banking of assets from one period to the next is allowed. We compare the performance of the optimal threshold policy with heuristics that may be more appealing to managers of nonprofit organizations, and we assess the value of banking and of dynamic pricing through numerical experiments.", "e:keyword": ["Capacity allocation", "Revenue management", "Dynamic pricing", "Nonprofit"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0683", "e:abstract": "We illustrate the correspondence between uncertainty sets in robust optimization and some popular risk measures in finance and show how robust optimization can be used to generalize the concepts of these risk measures. We also show that by using properly defined uncertainty sets in robust optimization models, one can construct coherent risk measures and address the issue of the computational tractability of the resulting formulations. Our results have implications for efficient portfolio optimization under different measures of risk.", "e:keyword": ["Finance", "Portfolio management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0684", "e:abstract": "This paper considers the worst-case Conditional Value-at-Risk (CVaR) in the situation where only partial information on the underlying probability distribution is available. The minimization of the worst-case CVaR under mixture distribution uncertainty, box uncertainty, and ellipsoidal uncertainty are investigated. The application of the worst-case CVaR to robust portfolio optimization is proposed, and the corresponding problems are cast as linear programs and second-order cone programs that can be solved efficiently. Market data simulation and Monte Carlo simulation examples are presented to illustrate the proposed approach.", "e:keyword": ["Conditional value-at-risk", "Portfolio management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0685", "e:abstract": "Markov decision processes are an effective tool in modeling decision making in uncertain dynamic environments. Because the parameters of these models typically are estimated from data or learned from experience, it is not surprising that the actual performance of a chosen strategy often differs significantly from the designer's initial expectations due to unavoidable modeling ambiguity. In this paper, we present a set of percentile criteria that are conceptually natural and representative of the trade-off between optimistic and pessimistic views of the question. We study the use of these criteria under different forms of uncertainty for both the rewards and the transitions. Some forms are shown to be efficiently solvable and others highly intractable. In each case, we outline solution concepts that take parametric uncertainty into account in the process of decision making.", "e:keyword": ["Markov decision processes", "Parameter uncertainty", "Finite state", "Stochastic model applications", "Stochastic programming", "Value at risk", "Chance-constrained optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0686", "e:abstract": "We study an oligopoly consisting of <i>M</i> leaders and <i>N</i> followers that supply a homogeneous product (or service) noncooperatively. Leaders choose their supply levels first, knowing the demand function only in distribution. Followers make their decisions after observing the leader supply levels and the realized demand function. We term the resulting equilibrium a <i>stochastic multiple-leader Stackelberg-Nash-Cournot</i> (SMS) equilibrium. We show the existence and uniqueness of SMS equilibrium under mild assumptions. We also propose a computational approach to find the equilibrium based on the sample average approximation method and analyze its rate of convergence. Finally, we apply this framework to model competition in the telecommunication industry.", "e:keyword": ["Programming", "Noncooperative games/group decisions", "Stackelberg game", "Equilibrium existence", "Uniqueness", "Sample average approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0687", "e:abstract": "We introduce the notion of a multiattribute utility copula that expresses any (i) continuous; (ii) bounded multiattribute utility function that is (iii) nondecreasing with each of its arguments, and (iv) strictly increasing with each argument for at least one reference value of the complement attributes, in terms of single-attribute utility assessments. This formulation provides a wealth of new functional forms that can be used to model preferences over utility-dependent attributes and enables sensitivity analyses to some of the widely used functional forms of utility independence. We introduce a class of utility copulas, called Archimedean utility copulas, and discuss the conditions under which it yields the additive and multiplicative forms. We also discuss linear and composite transformations of utility copulas that construct utility functions with partial utility independence. We conclude with the risk aversion functions that are induced by utility copula formulations and work through several examples to illustrate the approach.", "e:keyword": ["Utility elicitation", "Utility dependence", "Copula", "Utility copula", "Multiattribute utility"]}, {"@id": "http://dx.doi.org/10.1287/opre.1080.0688", "e:abstract": "In many applications involving make-to-order or time-sensitive (e.g., perishable, seasonal) products, finished orders are often delivered to customers immediately or shortly after the production. Consequently, there is little or no finished product inventory in the supply chain such that production and outbound distribution are very intimately linked and must be scheduled jointly to achieve a desired on-time delivery performance at minimum total cost. Research on integrated scheduling models of production and outbound distribution is relatively recent but is growing very rapidly. In this paper, we provide a survey of such existing models. We present a unified model representation scheme, classify existing models into several different classes, and for each class of the models give an overview of the optimality properties, computational tractability, and solution algorithms for the various problems studied in the literature. We clarify the tractability of some open problems left in the literature and some new problems by providing intractability proofs or polynomial-time exact algorithms. We also identify several problem areas and issues for future research.", "e:keyword": ["Production scheduling", "Outbound distribution", "Survey", "Representation scheme", "Optimality properties", "Solution algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0249", "e:keyword": ["OR/MS history", "Editorial comments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0692", "e:abstract": "We analyze comparative static effects under uncertainty when a decision maker has mean-variance preferences and faces a generic, quasi-linear decision problem with both an endogenous risk and a background risk. In terms of mean-variance preferences, we fully characterize the effects of changes in the location, scale, and concordance parameters of the stochastic environment on optimal risk taking. Presupposing compatibility between the mean-variance and the expected-utility approach, we then translate these mean-variance properties into their analogues for von Neumann-Morgenstern utility functions.", "e:keyword": ["Decision analysis", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0693", "e:abstract": "This paper introduces disjunctive decomposition for two-stage mixed 0-1 stochastic integer programs (SIPs) with random recourse. Disjunctive decomposition allows for cutting planes based on disjunctive programming to be generated for each scenario subproblem under a temporal decomposition setting of the SIP problem. A new class of valid inequalities for mixed 0-1 SIP with random recourse is presented. In particular, we derive valid inequalities that allow for scenario subproblems for SIP with random recourse but deterministic technology matrix and right-hand side vector to share cut coefficients. The valid inequalities are used to derive a disjunctive decomposition method whose derivation has been motivated by real-life stochastic server location problems with random recourse, which find many applications in operations research. Computational results with large-scale instances to demonstrate the potential of the method are reported.", "e:keyword": ["Stochastic programming", "Integer programming", "Disjunctive programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0694", "e:abstract": "We address a toll pricing problem in which the objective is to minimize the number of required toll facilities in a transportation network while inducing drivers to make the most efficient collective use of the network. We formulate the problem as a mixed-integer programming model and propose a solution method using combinatorial Benders cuts. Computational study of real networks as well as randomly generated networks indicates that our proposed method is efficient in obtaining provably optimal solutions for networks with small to medium sizes.", "e:keyword": ["Congestion pricing", "Traffic equilibrium", "Benders decomposition", "Branch and cut", "Mixed-integer program"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0695", "e:abstract": "This paper is the archival record of the INFORMS Philip McCord Morse Lecture delivered in 2008. It describes the author's research on four topics in homeland security and public health: preparedness and response to a bioterror anthrax attack, preparedness and response to a bioterror attack on the food supply, routes of transmission and infection control for pandemic influenza, and biometrics (e.g., fingerprint matching) to prevent terrorists from entering the country. The paper focuses on the modeling, policy recommendations, and implementation of these recommendations. The author draws lessons about policy implementation from these examples and from examples from his other homeland security work with colleagues, including a bioterror smallpox attack, preventing nuclear weapons from entering the country on a shipping container, preventing nuclear weapons from entering a city, and preventing terrorists from crossing the border between the United States and Mexico.", "e:keyword": ["Government", "Defense", "Health care", "Treatment", "Queues", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0696", "e:abstract": "Motivated by logistics practices, we consider a retailer that replenishes its inventory by making a delivery request without specifying a quantity, then deciding the quantity when the delivery vehicle arrives after one period. A fixed cost is incurred whenever a delivery request is made, regardless of the quantity ordered later. The new feature of this research relative to previous work is the separation of the delivery request and the quantity decision, or the postponement of ordering until one-period demand information is observed. Due to such separation, both the state space and the action space must be augmented in the model. We show that the optimal policy for delivery requests is of a threshold type: A delivery request is made if and only if the inventory on hand is below a threshold. The optimal decision on ordering is more complex, and there might be multiple order-up-to levels. Our numerical studies show, nonetheless, that the cost of an ordering policy that considers (at most) two order-up-to levels is close to the minimal when the planning horizon is not too short. We also identify conditions under which a base-stock policy is optimal for ordering. To understand the effects of ordering postponement, we compare our model with the traditional model in which the two decisions must be made at the same time. We show that postponement leads not only to a lower cost, but also a higher threshold for making delivery requests.", "e:keyword": ["Periodic-review inventory systems", "Optimal policy", "Postponement", "Quasi-K-convex", "Single crossing", "Dynamic programming", "Markov decision programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0697", "e:abstract": "This paper presents a novel framework for studying partially observable Markov decision processes (POMDPs) with finite state, action, observation sets, and discounted rewards. The new framework is solely based on future-reward vectors associated with future policies, which is more parsimonious than the traditional framework based on belief vectors. It reveals the connection between the POMDP problem and two computational geometry problems, i.e., finding the vertices of a convex hull and finding the Minkowski sum of convex polytopes, which can help solve the POMDP problem more efficiently. The new framework can clarify some existing algorithms over both finite and infinite horizons and shed new light on them. It also facilitates the comparison of POMDPs with respect to their degree of observability, as a useful structural result.", "e:keyword": ["Dynamic programming", "Markov", "Analysis of algorithms", "Computational complexity", "Mathematics", "Combinatorics", "Computers/computer science", "Artificial intelligence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0698", "e:abstract": "Working in collaboration with Spain-based retailer Zara, we address the problem of distributing, over time, a limited amount of inventory across all the stores in a fast-fashion retail network. Challenges specific to that environment include very short product life cycles, and store policies whereby an article is removed from display whenever one of its key sizes stocks out. To solve this problem, we first formulate and analyze a stochastic model predicting the sales of an article in a single store during a replenishment period as a function of demand forecasts, the inventory of each size initially available, and the store inventory management policy just stated. We then formulate a mixed-integer program embedding a piecewise-linear approximation of the first model applied to every store in the network, allowing us to compute store shipment quantities maximizing overall predicted sales, subject to inventory availability and other constraints. We report the implementation of this optimization model by Zara to support its inventory distribution process, and the ensuing controlled pilot experiment performed to assess the model's impact relative to the prior procedure used to determine weekly shipment quantities. The results of that experiment suggest that the new allocation process increases sales by 3% to 4%, which is equivalent to $275 M in additional revenues for 2007, reduces transshipments, and increases the proportion of time that Zara's products spend on display within their life cycle. Zara is currently using this process for all of its products worldwide.", "e:keyword": ["Industries", "Textiles/apparel", "Information systems", "Decision-support systems", "Inventory/production", "Applications", "Approximations", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0699", "e:abstract": "In this paper, we present a unified approach to study a class of cooperative games arising from inventory centralization. The optimization problems corresponding to the inventory games are formulated as stochastic programs. We observe that the strong duality of stochastic linear programming not only directly leads to a series of recent results concerning the nonemptiness of the core of such games, but also suggests a way to find an element in the core. The proposed approach is also applied to inventory games with concave ordering cost. In particular, we show that the newsvendor game with concave ordering cost has a nonempty core. Finally, we prove that it is NP-hard to determine whether a given allocation is in the core of the inventory games even in a very simple setting.", "e:keyword": ["Stochastic programming", "Inventory centralization", "Cooperative games"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0700", "e:abstract": "In insurgency situations, the government-organized force is confronted by a small guerrilla group that is dispersed in the general population with no or a very small signature. Effective counterinsurgency operations require good intelligence. Absent intelligence, not only might the insurgents escape unharmed and continue their violent actions, but collateral damage caused to the general population from poor targeting may generate adverse response against the government and create popular support for the insurgents, which may result in higher recruitment to the insurgency. We model the dynamic relations among intelligence, collateral casualties in the population, attrition, recruitment to the insurgency, and reinforcement to the government force. Even under best-case assumptions, we show that the government cannot totally eradicate the insurgency by force. The best it can do is contain it at a certain fixed level.", "e:keyword": ["Counterinsurgency", "Intelligence", "Lanchester models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0701", "e:abstract": "Prior work has investigated time- and inventory-level-dependent pricing of limited inventories with finite selling horizons. We consider a third dimension---in addition to time and inventory level---that the firms can use in setting their prices: the information that the firm has at the individual customer level. An arriving customer provides a <i>signal</i> to the firm, which is an imperfect indicator of the customer's willingness to pay, and the firm makes a <i>personalized</i> price offer depending on the signal, inventory level, and time. We consider two different models: <i>full personalization</i> and <i>partial personalization</i>. In the full personalization model, the firm charges any price it wishes given the customer signal, while in the partial personalization model, the firm can charge one of two prices. We find that a mere correlation between the signals and customers' willingness to pay is not sufficient to ensure intuitive relationships between the signal and the optimal prices. We determine a stronger condition, which leads to several structural properties, including the monotonicity of the optimal price with respect to the signal in the full personalization model. For the partial personalization model, we show that the optimal pricing policy is of threshold-type and that the threshold is monotonic in the inventory level and time.", "e:keyword": ["Inventory/production", "Policies", "Marketing", "Dynamic pricing", "Personalized pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0702", "e:abstract": "Data-mining techniques can be used not only to study collective behavior about customers, but also to discover private information about individuals. In this study, we demonstrate that decision trees, a popular classification technique for data mining, can be used to effectively reveal individuals' confidential data, even when the identities of the individuals are not present in the data. We propose a novel approach for organizations to protect confidential data from such a classification attack. The key components of this approach include a set of entropy-based measures to evaluate disclosure risks of individual records, an optimal pruning algorithm to identify high-risk records, and a pair of data-swapping procedures to reduce the disclosure risks. The proposed method provides the best trade-off between data utility and privacy protection against classification attacks. It can be applied to data with both numeric and categorical attributes. An experimental study on six real-world data sets shows that the proposed method is very effective in protecting privacy while enabling legitimate data mining and analysis.", "e:keyword": ["Computers", "Databases/artificial intelligence", "Data mining", "Decision trees", "Pruning", "Public sector", "Society", "Privacy", "Probability", "Entropy", "Relative entropy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0703", "e:abstract": "This note describes probabilistic properties of optimal price sample paths in a dynamic pricing model with a finite horizon and limited stock. We assume that customer arrivals follow a nonhomogeneous Poisson process. We show that if customers' willingness-to-pay increases rapidly over time, then the optimal price process follows a submartingale, which implies an upward price trend. Alternatively, if customers' willingness-to-pay decreases rapidly over time, then the optimal price process follows a supermartingale, which implies a downward price trend.", "e:keyword": ["Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0704", "e:abstract": "Retailing is a huge industry. In the United States, retail business represents about 40% of the economy and is the largest employer. Retail supply chain management is still more art than science, but this is changing rapidly as retailers begin to apply analytic models to the huge volume of data they are collecting on consumer purchases and preferences. This industry-wide movement resembles the transformation of Wall Street that occurred in the 1970s when physicists and other “rocket scientists” applied their analytic skills to investment decisions. The Consortium for Operational Excellence in Retailing (COER) (codirected by Ananth Raman, Harvard Business School, and myself) is a group of academics working with about 50 leading retailers to assess their progress towards rocket science retailing and to accelerate that progress through selected research projects. After some brief comments on the current state of industry practice in retail supply chain management, this paper will describe examples of COER research in four areas: assortment planning, pricing, inventory optimization, and store execution.", "e:keyword": ["Retailing", "Assortment optimization", "Forecasting", "Inventory management", "Store execution", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0705", "e:abstract": "We study the general approach to accelerating the convergence of the most widely used solution method of Markov decision processes (MDPs) with the total expected discounted reward. Inspired by the monotone behavior of the contraction mappings in the feasible set of the linear programming problem equivalent to the MDP, we establish a class of operators that can be used in combination with a contraction mapping operator in the standard value iteration algorithm and its variants. We then propose two such operators, which can be easily implemented as part of the value iteration algorithm and its variants. Numerical studies show that the computational savings can be significant especially when the discount factor approaches one and the transition probability matrix becomes dense, in which the standard value iteration algorithm and its variants suffer from slow convergence.", "e:keyword": ["Markov decision processes", "Value iteration", "Accelerated convergence", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0706", "e:abstract": "We study single and multistage inventory systems with stochastic lead times. We study a class of stochastic lead time processes, which we refer to as <i>exogenous</i> lead times. This class of lead time processes includes as special cases all lead time models from existing literature (such as Kaplan's lead times with no order crossing or independent and identically distributed lead times with order crossing, among others) but is a substantially broader class. For a system with an exogenous lead time process, we provide a method to determine base-stock levels and to compute the cost of a given base-stock policy. The method relies on relating the cost of a base-stock policy to the cost of a threshold policy in a related single-unit, single-customer problem. This single-unit method is exact for single-stage systems and for multistage systems under certain conditions. If the conditions are not satisfied, the method obtains near-optimal base-stock levels and accurate approximations of cost for multistage systems.", "e:keyword": ["Inventory/production", "Multiechelon", "Policies", "Review/lead times"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0707", "e:abstract": "We study a two-echelon supply chain with one warehouse and <i>N</i> (nonidentical) retailers facing stochastic demand. An easy-to-implement inventory policy, the so-called power-of-two (POT) policy, is proposed to manage inventory for the system. To maintain a certain service level, safety stocks are kept at the warehouse and each retailer outlet to buffer random demand. Our analysis highlights the important role of the warehouse safety stock level, which, in addition to the length of the warehouse order interval, significantly affects the lengths of the retailers' order intervals. By combining the length of the warehouse order interval with the warehouse safety stock level, we introduce a plane partition method and develop a polynomial time algorithm to find a POT policy for arbitrary target service levels. The long-run average cost of the proposed POT policy is guaranteed to be no more than 1.26 times the optimal POT policy cost. We also show that our proposed policy can be computed in <i>O</i>(<i>N</i><sup>3</sup>).", "e:keyword": ["One-warehouse multiretailer system", "Approximation algorithm", "Analysis of algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0710", "e:abstract": "Motivated by the cost and disruption involved in changing from a two-finger to a ten-finger biometric system for matching U.S. visitors to a watchlist of criminals and terrorists, we investigate whether any two-finger multistage biometric strategies would fix the inadequate matching performance of poor-quality prints that plagues the U.S. Government's original two-finger, single-stage biometric system. For several multistage strategies, we solve the Stackelberg game in which the U.S. Government chooses the biometric threshold levels to maximize the detection probability subject to constraints on the false positive probability and on the mean time per visitor to perform biometric screening, and the terrorist chooses the fingerprint image quality to minimize his detection probability. The first stage of all the strategies uses the current minutiae-based fingerprint matching system, but with thresholds that depend on image quality, which in isolation achieves a detection probability of 0.771. Using face recognition (based on 2002 data) in the second stage increases the detection probability to 0.841, whereas using a slower and more thorough texture-based fingerprint matcher in the second stage leads to a detection probability of at least 0.913 and perhaps significantly higher. (Data for the texture matcher is only available for the poorest-quality prints and we assume that this is its performance for all prints.) Adding face recognition as a third stage to this latter system does not improve performance. The two-finger, two-stage strategy may be comparable in performance to the ten-finger, single-stage strategy (which has a detection probability of 0.937), is robust against gaming and poor image acquisition, requires no additional hardware, and would generate no visible changes from the original two-finger, single-stage system from a visitor's viewpoint. The uncertainty in our performance estimates needs to be better quantified, ideally with raw data on similarity scores, before recommending this strategy for implementation.", "e:keyword": ["Probability", "Applications", "Military", "Defense systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0711", "e:abstract": "We study a single-item <i>(r, q)</i> inventory system, where <i>r</i> is the reorder point and <i>q</i> is the order quantity. The demand is a compound-Poisson process. We investigate the behavior of the optimal policy parameters and the long-run average cost of the system in response to stochastically shorter or less-variable lead times. We show that although some of the properties of the base-stock system can be extended to this more general model, some cannot. The same findings also apply when the comparison is conducted on the lead-time demand distributions.", "e:keyword": ["Inventory system", "Reorder-point/order quantity", "Stochastic lead time", "Variability", "Optimal policy", "Stochastic comparison"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0712", "e:abstract": "We review and develop different tractable approximations to individual chance-constrained problems in robust optimization on a variety of uncertainty sets and show their interesting connections with bounds on the conditional-value-at-risk (CVaR) measure. We extend the idea to joint chance-constrained problems and provide a new formulation that improves upon the standard approach. Our approach builds on a classical worst-case bound for order statistics problems and is applicable even if the constraints are correlated. We provide an application of the model on a network resource allocation problem with uncertain demand.", "e:keyword": ["Decision analysis", "Risk", "Probability", "Application", "Programming", "Stochastic", "Nonlinear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0713", "e:abstract": "This paper addresses the split-delivery vehicle routing problem with time windows (SDVRPTW) that consists of determining least-cost vehicle routes to service a set of customer demands while respecting vehicle capacity and customer time windows. The demand of each customer can be fulfilled by several vehicles. For solving this problem, we propose a new exact branch-and-price-and-cut method, where the column generation subproblem is a resource-constrained elementary shortest-path problem combined with the linear relaxation of a bounded knapsack problem. Each generated column is associated with a feasible route and a compatible delivery pattern. As opposed to existing branch-and-price methods for the SDVRPTW or its variant without time windows, integrality requirements in the integer master problem are not imposed on the variables generated dynamically, but rather on additional variables. An ad hoc label-setting algorithm is developed for solving the subproblem. Computational results show the effectiveness of the proposed method.", "e:keyword": ["Transportation", "Vehicle routing", "Integer programming", "Branch-and-price-and-cut algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0714", "e:abstract": "Motivated by emerging applications in workforce management, we consider a class of revenue management problems in systems with reusable resources. The corresponding applications are modeled using the well-studied <i>loss network systems</i>. We use an extremely simple linear program (LP) that provides an upper bound on the best achievable expected long-run revenue rate. The optimal solution of the LP is used to devise a conceptually simple control policy that we call the <i>class selection policy</i> (CSP). Moreover, the LP is used to analyze the performance of the CSP and show that it admits uniform performance guarantees. In particular, for the model with a single resource and uniform resource requirements, we prove that the CSP is guaranteed to have an expected long-run revenue rate that is at least half of the best achievable. Furthermore, as the capacity of the system grows to infinity, the CSP is asymptotically optimal, regardless of any other parameter of the problem. Finally, our techniques can be used to analyze the performance of the well-known class of trunk-reservation policies.", "e:keyword": ["Revenue management", "Algorithms", "Loss networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0715", "e:abstract": "In engineering design, an optimized solution often turns out to be suboptimal when errors are encountered. Although the theory of robust convex optimization has taken significant strides over the past decade, all approaches fail if the underlying cost function is not explicitly given; it is even worse if the cost function is nonconvex. In this work, we present a robust optimization method that is suited for unconstrained problems with a nonconvex cost function as well as for problems based on simulations, such as large partial differential equations (PDE) solver, response surface, and Kriging metamodels. Moreover, this technique can be employed for most real-world problems because it operates directly on the response surface and does not assume any specific structure of the problem. We present this algorithm along with the application to an actual engineering problem in electromagnetic multiple scattering of aperiodically arranged dielectrics, relevant to nanophotonic design. The corresponding objective function is highly nonconvex and resides in a 100-dimensional design space. Starting from an “optimized” design, we report a robust solution with a significantly lower worst-case cost, while maintaining optimality. We further generalize this algorithm to address a nonconvex optimization problem under both implementation errors and parameter uncertainties.", "e:keyword": ["Robust optimization", "Nonconvex optimization", "Robustness", "Implementation errors", "Data uncertainty", "Engineering optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0716", "e:abstract": "We study a periodically reviewed, serial inventory system in which excess demand from external customers is lost. We derive elementary properties of the vector of optimal order quantities in this system. In particular, we derive bounds on the sensitivity (or, more mathematically, the derivative) of the optimal order quantity at each stage to the vector of the current inventory levels. Our analysis uses the concept of <i>L</i>-natural-convexity, which was studied in discrete convex analysis and recently used in the study of single-stage inventory systems with lost sales. We also remark on how our analysis extends to models with capacity constraints and/or backordering.", "e:keyword": ["Inventory/production", "Lost sales", "Optimal policies", "Multiechelon"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0717", "e:abstract": "We study a dynamic lead-time quotation problem in a base-stock inventory system characterized by lead-time sensitive Poisson demand and exponentially distributed service times. We show that the optimal profit is unimodal in the base-stock level. We compare the base-stock system with a make-to-order (MTO) system and show that the lead-time quotes are lower in an MTO system and that increasing the base-stock level does not necessarily decrease the expected number of customers waiting. Numerical results show that when customers are less sensitive to lead-time quotes, the benefit of quoting lead times with a higher precision is significant, whereas when customers are more sensitive to lead-time quotes, the benefit of holding inventory is significant.", "e:keyword": ["Inventory/production policies", "Stochastic production systems", "Optimal control", "Order acceptance", "Due-date quotation", "Benefits of flexibility"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0718", "e:abstract": "In this paper, we study a make-to-stock manufacturing system where two firms compete through dynamic pricing and inventory control. Our goal is to address competition (in particular a duopoly setting) together with the presence of demand uncertainty. We consider a dynamic setting where multiple products share production capacity. We introduce a demand-based fluid model where the demand is a linear function of the price of the supplier and of her competitor, the inventory and production costs are quadratic, and all coefficients are time dependent. A key part of the model is that no backorders are allowed and the strategy of a supplier depends on her competitor's strategy. First, we reformulate the robust problem as a fluid model of similar form to the deterministic one and show existence of a Nash equilibrium in continuous time. We then discuss issues of uniqueness and address how to compute a particular Nash equilibrium, i.e., the normalized Nash equilibrium.", "e:keyword": ["Game theory", "Optimization under uncertainty", "Robust optimizations", "Normalized Nash equilibrium", "Dynamic pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0719", "e:abstract": "In critical energy infrastructure sectors (e.g., electric power generation, natural gas transportation, oil-refining capacity), maintaining a certain level of excess capacity is socially valuable (because it serves to protect against unexpected market conditions) but not necessarily compatible with the incentives for individual firms in the market. In this paper, we develop a dynamic oligopoly model with a stochastically growing demand to analyze the inherent tension in market-based incentives for capacity expansion where capacity additions take place over long time lags. Our results indicate that the market fails to induce the socially optimal level of capacity. However, the magnitude of this failure varies greatly as a function of entry costs and the relative profitability of investments in the market (as measured by the ratio of maximum markup over production costs and investment costs). In general, the likelihood of insufficient capacity in equilibrium increases with decreasing probability of demand growth, increasing discount and depreciation rates, and/or increasing investment and/or production costs. We discuss the public policy implications of our results.", "e:keyword": ["Capacity expansion", "Critical infrastructure", "Dynamic games", "Markov perfect equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0721", "e:abstract": "We study the modeling of nonconvex piecewise-linear functions as mixed-integer programming (MIP) problems. We review several new and existing MIP formulations for continuous piecewise-linear functions with special attention paid to multivariate nonseparable functions. We compare these formulations with respect to their theoretical properties and their relative computational performance. In addition, we study the extension of these formulations to lower semicontinuous piecewise-linear functions.", "e:keyword": ["Mixed-integer programming", "Piecewise-linear functions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0722", "e:abstract": "Data envelopment analysis (DEA) is known as a nonparametric mathematical programming approach to productive efficiency analysis. In this paper, we show that DEA can be alternatively interpreted as nonparametric least-squares regression subject to shape constraints on the frontier and sign constraints on residuals. This reinterpretation reveals the classic parametric programming model by Aigner and Chu [Aigner, D., S. Chu. 1968. On estimating the industry production function. <i>Amer. Econom. Rev.</i> <b>58</b> 826--839] as a constrained special case of DEA. Applying these insights, we develop a nonparametric variant of the corrected ordinary least-squares (COLS) method. We show that this new method, referred to as corrected concave nonparametric least squares (C<sup>2</sup>NLS), is consistent and asymptotically unbiased. The linkages established in this paper contribute to further integration of the econometric and axiomatic approaches to efficiency analysis.", "e:keyword": ["Frontier estimation", "Mathematical programming", "Nonparametric estimation", "Performance measurement", "Benchmarking"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0725", "e:abstract": "A retailer is endowed with a finite inventory of a nonperishable product. Demand for this product is driven by a price-sensitive Poisson process that depends on an unknown parameter that is a proxy for the market size. The retailer has a prior belief on the value of this parameter that he updates as time and available information (prices and sales) evolve. The retailer's objective is to maximize the discounted long-term average profits of his operation using dynamic pricing policies. We consider two cases. In the first case, the retailer is constrained to sell the entire initial stock of the nonperishable product before a different assortment is considered. In the second case, the retailer is able to stop selling the nonperishable product at any time and switch to a different menu of products. For both cases, we formulate the retailer's problem as a (Poisson) intensity control problem and derive structural properties of an optimal solution, and suggest a simple and efficient approximated solution. We use numerical computations, together with asymptotic analysis, to evaluate the performance of our proposed policy.", "e:keyword": ["Dynamic pricing", "Bayesian demand learning", "Approximations", "Intensity control", "Nonhomogeneous Poisson process", "Optimal stopping"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0726", "e:abstract": "In most retail environments, when inventory runs out, the unmet demand is lost and not observed. The sales data are effectively censored by the inventory level. Factoring this censored data effect into demand estimation and inventory control decision makes the problem difficult to solve. In this paper, we focus on developing bounds and heuristics for this problem. Specifically, we consider a finite-horizon inventory control problem for a nonperishable product with unobserved lost sales and a demand distribution having an unknown parameter. The parameter is estimated sequentially by the Bayesian updating method. We first derive a set of solution upper bounds that work for all prior and demand distributions. For a fairly general monotone likelihood-ratio distribution family, we derive relaxed but easily computable lower and upper bounds along an arbitrary sample path. We then propose two heuristics. The first heuristic is derived from the solution bound results. Computing this heuristic solution only requires the evaluation of the objective function in the observed lost-sales case. The second heuristic is based on the approximation of the first-order condition. We combine the first-order derivatives of the simpler observed lost-sales and perishable-inventory models to obtain the approximation. For the latter case, we obtain a recursive formula that simplifies the computation. Finally, we conduct an extensive numerical study to evaluate and compare the bounds and heuristics. The numerical results indicate that both heuristics perform very well. They outperform the myopic policies by a wide margin.", "e:keyword": ["Inventory/production", "Heuristics", "Unknown demand distribution", "Bayesian updating", "Unobserved lost sales", "Dynamic programming/optimal control", "Bounds"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0727", "e:abstract": "In this study, improved and new algorithms are developed for economic lot-sizing problems with integrated production and transportation operations. To model the economies of scale in production with the effect of shipment consolidation in transportation, we assume concave production costs and stepwise transportation costs. More specifically, we consider concave/fixed-charge/nonspeculative cost functions in production, and nonstationary/stationary delivery cost functions in transportation. The cost functions in production are always assumed to be nonstationary. To achieve a cost-effective production and shipment schedule over time, inventories are considered for carrying and backlogging items. Efficient solution procedures are provided for all the models with or without backlogging under assumed cost structures.", "e:keyword": ["Inventory/production", "Lot-sizing", "Transportation", "Shipment consolidation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0728", "e:abstract": "During counterinsurgency operations, government forces with superior firepower confront weaker low-signature insurgents. Under what conditions should government (Blue) forces attack insurgent (Red) strongholds? How should the government allocate its force across different strongholds when the insurgents' threat to the Blue civilian population must be taken into account? How should the government respond to “smart” insurgents who anticipate the government's optimal plan of attack and prepare accordingly? How do the results change when the government takes Red civilian casualties resulting from attacks on insurgent strongholds into account? This article addresses these questions. Using Lanchester models modified to account for imperfect intelligence, we formulate an optimal force allocation problem for the government and develop a knapsack approximation that has tight error bounds. We also model a sequential force allocation game between the insurgents and the government and solve for its equilibrium. When the government has perfect intelligence, in equilibrium the insurgents concentrate their force in a single stronghold that the government either attacks or not depending upon the resulting casualty count. Otherwise, under reasonable assumptions regarding the government's behavior and intelligence capabilities, it is optimal for the insurgents to “spread out” in a way that maximizes the number of soldiers required to win all battles. If the government worries about Red civilian casualties, the insurgents have a strong incentive to blend in with the Red civilian population, because this can prevent government attacks while allowing the insurgents to inflict casualties on Blue civilians. Such strategic behavior makes it harder for the government to protect its citizens from insurgent attacks.", "e:keyword": ["Counterinsurgency", "Optimal force allocation", "Lanchester models", "Value of intelligence", "Game theory", "Hezbollah/Israel conflict"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0729", "e:abstract": "We study a problem of dynamic pricing faced by a vendor with limited inventory, uncertain about demand, and aiming to maximize expected discounted revenue over an infinite time horizon. The vendor learns from purchase data, so his strategy must take into account the impact of price on both revenue and future observations. We focus on a model in which customers arrive according to a Poisson process of uncertain rate, each with an independent, identically distributed reservation price. Upon arrival, a customer purchases a unit of inventory if and only if his reservation price equals or exceeds the vendor's prevailing price. We propose a simple heuristic approach to pricing in this context, which we refer to as decay balancing. Computational results demonstrate that decay balancing offers significant revenue gains over recently studied certainty equivalent and greedy heuristics. We also establish that changes in inventory and uncertainty in the arrival rate bear appropriate directional impacts on decay balancing prices in contrast to these alternatives, and we derive worst-case bounds on performance loss. We extend the three aforementioned heuristics to address a model involving multiple customer segments and stores, and provide experimental results demonstrating similar relative merits in this context.", "e:keyword": ["Dynamic pricing", "Demand learning", "Revenue management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0730", "e:abstract": "Efficient management of water requires balancing environmental needs, externality considerations, and economic efficiency. Toward that end, this paper presents a deterministic linear program that could be used to operate a smart spot market for groundwater. The market design uses the existing hydrological programs MODFLOW and GWM along with standard linear programming methods. In principle, a market could be set up anywhere that a MODFLOW model is available. The market design has parallels to markets in the electricity and gas sectors, which we discuss. We present a case study with notional bids for Marlborough, New Zealand. Our approach would reduce transaction costs for a water market, reduce users' risk, and increase the reliability of environmental flows. We discuss a number of cautions and limitations to the model and recommend further work on introducing a stochastic framework to the model.", "e:keyword": ["Games/group decisions", "Bidding/auctions", "Environment", "Natural resources", "Water resources"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0731", "e:abstract": "We consider a novel variant of the perishable inventory profit management problem faced by a firm that sells a fixed inventory over a finite horizon in the presence of price-adjustment costs. In economics literature, such price-adjustment costs are widely studied and are typically assumed to include a fixed component (e.g., advertising costs), an inventory-dependent component (e.g., inventory relabeling costs), as well as a component that depends on the magnitude of the price adjustment (e.g., cognitive and coordination managerial costs). We formulate the firm's profit management problem as a finite-horizon dynamic program in which the state of the system is described by the inventory level as well as the current price level. We derive first-order properties of the optimal value function and give a complete characterization of optimal policies for the case of ample inventory. Through a set of examples we demonstrate the complex and counterintuitive nature of optimal price-adjustment policies. Consequently, we focus on developing easily computable and implementable heuristics with demonstrably good performance. To this end, we develop and solve a fluid model based on the original stochastic dynamics and propose three fluid-based heuristic policies. We derive expressions for the expected profit generated by each one of these heuristics when applied to the stochastic problem and derive sufficient conditions for the asymptotic optimality of the policies when the initial inventory levels and planning horizons are proportionally scaled up. We test the performance of the heuristics in a numerical study and demonstrate a robust, near-optimal performance of one of the heuristics (which we call the “Fluid Time” heuristic) for a wide range of problem parameters. Finally, we demonstrate the importance of proper accounting of price-adjustment costs in several alternative business settings.", "e:keyword": ["Revenue management", "Pricing", "Price adjustment costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0732", "e:abstract": "Traditional optimization models assume a central decision maker who optimizes a global system performance measure. However, problem data is often distributed among several agents, and agents make autonomous decisions. This gives incentives for strategic behavior of agents, possibly leading to suboptimal system performance. Furthermore, in dynamic environments, machines are locally dispersed and administratively independent. Examples are found both in business and engineering applications. We investigate such issues for a parallel machine scheduling model where jobs arrive online over time. Instead of centrally assigning jobs to machines, each machine implements a local sequencing rule and jobs decide for machines themselves. In this context, we introduce the concept of a myopic best-response equilibrium, a concept weaker than the classical dominant strategy equilibrium, but appropriate for online problems. Our main result is a polynomial time, online mechanism that---assuming rational behavior of jobs---results in an equilibrium schedule that is 3.281-competitive with respect to the maximal social welfare. This is only slightly worse than state-of-the-art algorithms with central coordination.", "e:keyword": ["Online machine scheduling", "Total weighted completion time", "Decentralization", "Mechanism design", "Competitive equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0733", "e:abstract": "One of the central trends in the optimization community over the past several years has been the steady improvement of general-purpose solvers. A logical next step in this evolution is to combine mixed-integer linear programming, constraint programming, and global optimization in a single system. Recent research in the area of integrated problem solving suggests that the right combination of different technologies can simplify modeling and speed up computation substantially. Nevertheless, integration often requires special-purpose coding, which is time consuming and error prone. We present a general-purpose solver, SIMPL, that allows its user to replicate (and sometimes improve on) the results of custom implementations with concise models written in a high-level language. We apply SIMPL to production planning, product configuration, machine scheduling, and truss structure design problems on which customized integrated methods have shown significant computational advantage. We obtain results that either match or surpass the original codes at a fraction of the implementation effort.", "e:keyword": ["Programming", "Linear", "Nonlinear", "Integer", "Constraint", "Modeling languages", "Global", "Optimization", "Integrated optimization", "Production", "Planning and product configuration", "Scheduling", "Parallel machines"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0734", "e:abstract": "This paper studies a periodic-review, serial inventory system in which echelon (<i>r</i>, <i>nQ</i>, <i>T</i>) policies are implemented. Under such a policy, each stage reviews its inventory in every <i>T</i> period and orders according to an echelon (<i>r</i>, <i>nQ</i>) policy. Two types of fixed costs are considered: one is associated with each order batch <i>Q</i>, and the other is incurred for each inventory review. The objective is to find the policy parameters such that the average total cost per period is minimized. This paper provides a method for obtaining heuristic and optimal policy parameters. The heuristic is based on minimizing lower and upper bounds on the total cost function. These total cost bounds, which are separable functions of the policy parameters, are obtained in two steps: First, we decompose the total cost into costs associated with each stage, which include a penalty cost for holding inadequate stock. Second, we construct lower and upper bounds for the penalty cost by regulating downstream policy parameters. To find the optimal solution, we further construct cost bounds for each echelon (a subsystem that includes a stage and all of its downstream stages) by regulating holding and backorder cost parameters. The echelon lower-bound cost functions, as well as the stage cost bounds, generate bounds for the optimal solution. In a numerical study, we find that the heuristic is near optimal when the ratio of the fixed cost to the holding cost at the most downstream stage is large. We also find that changing the optimal batch sizes may not affect the optimal reorder intervals or, equivalently, the delivery schedules under some conditions.", "e:keyword": ["Multiechelon inventory systems", "Optimal policy", "Heuristic", "Lower and upper bounds", "Batch ordering", "Replenishment interval"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0735", "e:abstract": "This paper develops a multiattribute competition model for procurement of short life-cycle products. In such an environment, the buyer installs dedicated production capacity at the suppliers before demand is realized. Final production orders are decided after demand materializes. Of course, the buyer is reluctant to bear all the capacity and inventory risk, and thus signs flexible contracts with several suppliers. We model the suppliers' offers as option contracts, where each supplier charges a reservation price per unit of capacity and an execution price per unit of delivered supply. These two parameters illustrate the trade-off between total price and flexibility of a contract, which are both important to the buyer. We model the interaction between suppliers and the buyer as a game in which the suppliers are the leaders and the buyer is the follower. Specifically, suppliers compete to provide supply capacity to the buyer, and the buyer optimizes its expected profit by selecting one or more suppliers. We characterize the suppliers' equilibria in pure strategies for a class of customer demand distributions. In particular, we show that this type of interaction gives rise to <i>cluster competition</i>. That is, in equilibrium suppliers tend to be clustered in small groups of two or three suppliers each, such that within the same group all suppliers use similar technologies and offer the same type of contract. Finally, we show that in equilibrium, supply chain inefficiencies---i.e., the loss of profit due to competition---are at most 25% of the profit of a centralized supply chain.", "e:keyword": ["Production/scheduling", "Capacity management", "Flexibility", "Games/group decisions", "Noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0736", "e:abstract": "Motivated by telephone call centers, we study large-scale service systems with multiple customer classes and multiple agent pools, each with many agents. To minimize staffing costs subject to service-level constraints, where we delicately balance the service levels (SLs) of the different classes, we propose a family of routing rules called <i>fixed-queue-ratio</i> (FQR) rules. With FQR, a newly available agent next serves the customer from the head of the queue of the class (from among those he is eligible to serve) whose queue length most exceeds a specified proportion of the total queue length. The proportions can be set to achieve desired SL targets. The FQR rule achieves an important <i>state-space collapse</i> (SSC) as the total arrival rate increases, in which the individual queue lengths evolve as fixed proportions of the total queue length. In the current paper we consider a variety of service-level types and exploit SSC to construct asymptotically optimal solutions for the staffing-and-routing problem. The key assumption in the current paper is that the service rates depend only on the agent pool.", "e:keyword": ["Queues", "Networks", "Multiple classes", "Server pools", "Queues", "Optimization", "Design", "Staffing", "Routing", "Queues", "Limit theorems", "Asymptotic optimality", "Many-server heavy-traffic limits"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0737", "e:abstract": "We consider a number of servers that may improve the efficiency of the system by pooling their service capacities to serve the union of the individual streams of customers. This economies-of-scope phenomenon is due to the reduction in the steady-state mean total number of customers in the system. The question we pose is how the servers should split among themselves the cost of the pooled system. When the individual incoming streams of customers form Poisson processes and individual service times are exponential, we define a transferable utility cooperative game in which the cost of a coalition is the mean number of customers (or jobs) in the pooled system. We show that, despite the characteristic function is neither monotone nor concave, the game and its subgames possess nonempty cores. In other words, for any subset of servers there exist cost-sharing allocations under which no partial subset can take advantage by breaking away and forming a separate coalition. We give an explicit expression for all (infinitely many) nonnegative core cost allocations of this game. Finally, we show that, except for the case where all individual servers have the same cost, there exist infinitely many core allocations with negative entries, and we show how to construct a convex subset of the core where at least one server is being paid to join the grand coalition.", "e:keyword": ["Queues", "Games/group decisions", "Bargaining", "Cooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0738", "e:abstract": "We present update formulas that allow us to express the stationary distribution of a continuous-time Markov process with denumerable state space having generator matrix <i>Q<sup>*</sup></i> through a continuous-time Markov process with generator matrix <i>Q</i>. Under suitable stability conditions, numerical approximations can be derived from the update formulas, and we show that the algorithms converge at a geometric rate. Applications to sensitivity analysis and bounds on perturbations are discussed as well. Numerical examples are presented to illustrate the efficiency of the proposed algorithm.", "e:keyword": ["Probability", "Markov processes", "Queues", "Algorithms", "Numerical methods", "Retrial queue", "Deviation matrix", "Sensitivity analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0739", "e:abstract": "A probability is the expectation of an indicator function. However, the standard pathwise sensitivity estimation approach, which interchanges the differentiation and expectation, cannot be directly applied because the indicator function is discontinuous. In this paper, we design a pathwise sensitivity estimator for probability functions based on a result of Hong [Hong, L. J. 2009. Estimating quantile sensitivities. <i>Oper. Res.</i> <b>57</b>(1) 118--130]. We show that the estimator is consistent and follows a central limit theorem for simulation outputs from both terminating and steady-state simulations, and the optimal rate of convergence of the estimator is <i>n</i><sup>-2/5</sup> where <i>n</i> is the sample size. We further demonstrate how to use importance sampling to accelerate the rate of convergence of the estimator to <i>n</i><sup>-1/2</sup>, which is the typical rate of convergence for statistical estimation. We illustrate the performances of our estimators and compare them to other well-known estimators through several examples.", "e:keyword": ["Stochastic simulation", "Gradient estimation", "Probability function", "Perturbation analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0740", "e:abstract": "Even in the face of deteriorating and highly volatile demand, firms often invest in, rather than discard, aging technologies. To study this phenomenon, we model the firm's profit stream as a Brownian motion with negative drift. At each point in time, the firm can continue operations, or it can stop and exit the project. In addition, there is a one-time option to make an investment that boosts the project's profit rate. Using stochastic analysis, we show that the optimal policy always exists and that it is characterized by three thresholds. There are investment and exit thresholds before investment, and there is a threshold for exit after investment. We also effect a comparative statics analysis of the thresholds with respect to the drift and the volatility of the Brownian motion. When the profit boost upon investment is sufficiently large, we find a novel result: the investment threshold decreases in volatility.", "e:keyword": ["Decision analysis", "Sequential", "Finance", "Investment criteria", "Probability", "Diffusion"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0741", "e:abstract": "Stochastic programming can effectively describe many decision-making problems in uncertain environments. Unfortunately, such programs are often computationally demanding to solve. In addition, their solution can be misleading when there is ambiguity in the choice of a distribution for the random parameters. In this paper, we propose a model that describes uncertainty in both the distribution form (discrete, Gaussian, exponential, etc.) and moments (mean and covariance matrix). We demonstrate that for a wide range of cost functions the associated distributionally robust (or min-max) stochastic program can be solved efficiently. Furthermore, by deriving a new confidence region for the mean and the covariance matrix of a random vector, we provide probabilistic arguments for using our model in problems that rely heavily on historical data. These arguments are confirmed in a practical example of portfolio selection, where our framework leads to better-performing policies on the “true” distribution underlying the daily returns of financial assets.", "e:keyword": ["Programming", "Stochastic", "Statistics", "Estimation", "Finance", "Portfolio"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0744", "e:abstract": "We consider a scheduling environment with <i>m (m</i> (ge) 1) identical machines in parallel and two agents. Agent <i>A</i> is responsible for <i>n</i><sub>1</sub> jobs and has a given objective function with regard to these jobs; agent <i>B</i> is responsible for <i>n</i><sub>2</sub> jobs and has an objective function that may be either the same or different from the one of agent <i>A</i>. The problem is to find a schedule for the <i>n</i><sub>1</sub> + <i>n</i><sub>2</sub> jobs that minimizes the objective of agent <i>A</i> (with regard to his <i>n</i><sub>1</sub> jobs) while keeping the objective of agent <i>B</i> (with regard to his <i>n</i><sub>2</sub> jobs) below or at a fixed level <i>Q</i>. The special case with a single machine has recently been considered in the literature, and a variety of results have been obtained for two-agent models with objectives such as <i>f<sub>max</sub>, (sum) w<sub>j</sub>C<sub>j</sub></i>, and (sum) <i>U<sub>j</sub></i>. In this paper, we generalize these results and solve one of the problems that had remained open. Furthermore, we enlarge the framework for the two-agent scheduling problem by including the total tardiness objective, allowing for preemptions, and considering jobs with different release dates; we consider also identical machines in parallel. We furthermore establish the relationships between two-agent scheduling problems and other areas within the scheduling field, namely rescheduling and scheduling subject to availability constraints.", "e:keyword": ["Production/scheduling", "Multiagent deterministic sequencing", "Games/group decisions", "Cooperative sequencing", "Single machine", "Parallel machines"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0745", "e:abstract": "Many companies have started segmenting customers to better match their products and services to the needs of the customers. We support this development by presenting a stochastic model of a rental system with two customer classes that was motivated by the operations of one of Europe's leading logistics companies. At the company, customers can choose between premium and classic service. Under premium service, customers provide advance demand information (ADI) by reserving cars ahead of the time when they need them, and they receive a service guarantee in return. Under classic service, customers do not make a reservation and do not receive a service guarantee. Because both demand classes access a common pool of cars, the company must decide which demands to fill and which to reject. The admission decision must be made without knowing the rental duration, which is an exponentially distributed random variable. We model the system as a multiserver loss system and prove that the optimal admission policy is a threshold policy. Because computing the parameters of the policy is computationally intractable, we propose an ADI policy that can be implemented and executed with moderate effort. We analyze the performance of our ADI policy by analytically deriving upper and lower bounds on the optimal expected profit and by performing numerical experiments using data from the logistics company that motivated our research. The numerical experiments indicate that the potential benefit of using ADI is significant and that our ADI policy performs close to optimal. Finally, we extend our model to a different cost structure and to multiple ADI classes.", "e:keyword": ["Dynamic programming", "Semi-Markov", "Finite state", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0746", "e:abstract": "We propose a robust optimization approach to address a multiperiod inventory control problem under ambiguous demands, that is, only limited information of the demand distributions such as mean, support, and some measures of deviations. Our framework extends to correlated demands and is developed around a factor-based model, which has the ability to incorporate business factors as well as time-series forecast effects of trend, seasonality, and cyclic variations. We can obtain the parameters of the replenishment policies by solving a tractable deterministic optimization problem in the form of a second-order cone optimization problem (SOCP), with solution time; unlike dynamic programming approaches, it is polynomial and independent on parameters such as replenishment lead time, demand variability, and correlations. The proposed truncated linear replenishment policy (TLRP), which is piecewise linear with respect to demand history, improves upon static and linear policies, and achieves objective values that are reasonably close to optimal.", "e:keyword": ["Robust optimization", "Inventory control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0749", "e:abstract": "We give a counterexample to show that the optimality proof by Kise et al. (1978) for an algorithm for a scheduling problem is incorrect.", "e:keyword": ["Production scheduling", "Algorithm", "Proof of optimality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0750", "e:abstract": "In this paper, we study the practice of forecast sharing and supply chain coordination with a game-theoretical model. We find that in a one-shot version of the game, forecasts are not shared truthfully by the customer. The supplier will rationally discount the forecast information in her capacity allocation. This results in Pareto suboptimality for both supply chain parties. However, we show that a more efficient, truth-sharing outcome can emerge as an equilibrium from a long-term relationship. In this equilibrium, forecast information is transmitted truthfully and trusted by the supplier, who in turn allocates the system-optimal capacity. This leaves both the customer and the supplier better-off, compared to the nontruthful-sharing equilibrium. We identify a multiperiod review strategy profile that supports the truthful-sharing equilibrium. The key element of this strategy is that the supplier computes a scoring index of the customer's behavior that is updated over time and used to evaluate if the customer has sufficient incentive to share his private information truthfully in each transaction of the repeated game. Compared to trigger strategies, review strategies are more tolerant but require diligence and more monitoring effort.", "e:keyword": ["Supply chain management", "Forecast sharing", "Long-term relationship"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0751", "e:abstract": "This paper considers scheduling problems where the processing of a set of jobs has been scheduled (i.e., planned) to minimize a classical cost objective, under the assumption that the jobs are all available at the start of the planning horizon. Before processing starts, however, the availability of a subset of the jobs is delayed. Therefore, the decision maker needs to adjust the existing schedule to allow for the initial unavailability of those jobs, but without causing excessive disruption to the schedule and expensive resource reallocations. The limit on allowable disruption is measured by the maximum time disruption to any job, between the original and adjusted schedules. For the classical sum of weighted completion times scheduling objective, we provide a computationally efficient optimal algorithm and an intractability proof showing that such an algorithm is the best possible type of result. Also, we provide a linear time approximate solution procedure, show that its worst-case performance ratio is a small constant, and demonstrate computationally that its average performance is very close to optimal. Finally, we provide a fully polynomial time approximation scheme. We also summarize analogous results for three other classical scheduling objectives. Our work is among the first to develop optimal algorithms, heuristics with guaranteed performance bounds, and approximation schemes, for rescheduling problems.", "e:keyword": ["Production/scheduling", "Sequencing", "Deterministic", "Single machine", "Manufacturing", "Performance/productivity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0752", "e:abstract": "This paper investigates the effect of using an end-of-period accounting scheme for inventory-related costs when costs actually accrue in continuous time. Using a simple model, we show that (i) the end-of-period scheme results in higher than optimal order-up-to levels and inventory cost if the cost and demand parameters are unchanged, and (ii) it is possible to replicate both the optimal base-stock level and its cost by selecting the values of the cost or demand parameters judiciously. The cost adjustments often require extreme values, and no systematic cost parameter adjustment scheme is robust. However, we find a systematic adjustment to the demand parameters that serves as a good approximation and is robust. We therefore conclude that end-of-period cost accounting without parameter adjustments is in general inappropriate when costs are incurred continuously, but there are adjustments that can make it work well.", "e:keyword": ["Accounting", "Inventory cost", "Inventory/production", "Periodic review", "Sensitivity analysis", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0753", "e:abstract": "Allaz and Vila made the seminal contribution that forward contracts mitigate market power on the spot market. This result is widely quoted and elaborated in studies of restructured power markets, where generators can potentially exploit the special characteristics of this industry in order to extract higher prices. Allaz-Vila established their result under the assumption that the production capacities of the players are infinite. We show that the Allaz-Vila result does not hold when capacities are endogenous and constraining generation. Specifically, a forward market can enhance or mitigate market power when capacities are endogenous and demand is unknown at the time of investment. We also show that forward markets do not mitigate market power when capacities are endogenous and demand is known at the time of investment. Our results complement other work that shows that forward markets systematically enhance market power in some symmetric capacity-constrained markets.", "e:keyword": ["Electric industries", "Capacity expansion", "Investment"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0754", "e:abstract": "We extend the basic theory of kriging, as applied to the design and analysis of deterministic computer experiments, to the stochastic simulation setting. Our goal is to provide flexible, interpolation-based metamodels of simulation output performance measures as functions of the controllable design or decision variables, or uncontrollable environmental variables. To accomplish this, we characterize both the intrinsic uncertainty inherent in a stochastic simulation and the extrinsic uncertainty about the unknown response surface. We use tractable examples to demonstrate why it is critical to characterize both types of uncertainty, derive general results for experiment design and analysis, and present a numerical example that illustrates the stochastic kriging method.", "e:keyword": ["Simulation", "Design of experiments", "Statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0755", "e:abstract": "We present a simplex-type algorithm---that is, an algorithm that moves from one extreme point of the infinite-dimensional feasible region to another, not necessarily adjacent, extreme point---for solving a class of linear programs with countably infinite variables and constraints. Each iteration of this method can be implemented in finite time, whereas the solution values converge to the optimal value as the number of iterations increases. This simplex-type algorithm moves to an adjacent extreme point and hence reduces to a true infinite-dimensional simplex method for the important special cases of nonstationary infinite-horizon deterministic and stochastic dynamic programs.", "e:keyword": ["Programming", "Infinite dimensional", "Dynamic programming/optimal control", "Markov", "Infinite state", "Network/graphs", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0756", "e:abstract": "In this paper, we outline the development of the core optimization technology used within a decision support tool to help providers and caretakers in constructing catch-up schedules for childhood immunization. These schedules ensure that a child continues to receive timely coverage against vaccine-preventable diseases in the likely event that one or more doses have been delayed. This project was undertaken as part of a collaborative effort between the Centers for Disease Control and Prevention (CDC) and Georgia Institute of Technology. Our aim is to develop a decision support tool that removes from the task of constructing catch-up schedules the tedious combinatorial aspects, while maintaining a level of generality that allows easy accommodation for changes in the existing rules and adding new vaccines to the schedule lineup. We show that the catch-up scheduling problem is NP-hard, and we develop a dynamic programming algorithm that exploits the typical size and structure of the problem to construct optimized schedules almost at the click of a button. In using an optimization-based algorithm, our approach is unique not only in methodology but also in the information, strategy, and advice we can offer to the user. The tool is being advocated by both the CDC and the American Academy of Pediatrics (AAP) as a means of encouraging caretakers and providers to take a more proactive role in ensuring timely vaccination coverage for children, as well as ensuring the accuracy and quality of a catch-up regime.", "e:keyword": ["Decision analysis", "Multiple criteria", "Dynamic programming/optimal control", "Applications", "Deterministic", "Health care", "Information systems", "Decision support systems", "Scheduling", "Applications", "Sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0757", "e:abstract": "We study the inventory replenishment of a product whose demand can be manipulated by restricting the supply. This research is motivated by a novel marketing tactic employed by manufacturers of fashion and luxury items. Such a tactic combines innovative marketing with deliberate understocking in an attempt to create shortages (i.e., waitlists) that add to the allure and sense of exclusivity of a product and stimulate its demand. We model the problem as a finite-horizon, periodic-review system where demand in each period is a decreasing function of the net ending inventory in the previous period. Although the optimal structure can be complex in general, under certain conditions we are able to characterize the optimal policy as a state-dependent, monotone, base-stock policy. We compare this policy with the optimal policy for the case in which demand is independent of the net inventory. We also show that <i>understocking</i> is optimal in various scenarios. We then propose a novel strategy, called the inventory-withholding strategy, to further explore the wait-list effect by making customers wait even when there is inventory on hand to satisfy them. Our numerical experiments study the impact of various model parameters in combination with the wait-list effect on the optimal policy and the corresponding expected profits.", "e:keyword": ["Inventory management", "Inventory withholding strategy", "Under-stocking"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0758", "e:abstract": "We study a stochastic network that consists of two servers shared by two classes of jobs. Class 1 jobs require a concurrent occupancy of both servers while class 2 jobs use only one server. The traffic intensity is such that both servers are bottlenecks, meaning the service capacity is equal to the offered workload. The real-time allocation of the service capacity among the job classes takes the form of a solution to an optimization problem that maximizes a utility function. We derive the diffusion limit of the network and establish its asymptotic optimality. In particular, we identify a cost objective associated with the utility function and show that it is minimized at the diffusion limit by the utility-maximizing allocation within a broad class of “fair” allocation schemes. The model also highlights the key issues involved in multiple bottlenecks.", "e:keyword": ["Stochastic processing network", "Utility-maximizing resource control", "Dynamic complementarity problem", "Diffusion limit", "Asymptotic optimality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0759", "e:abstract": "We consider a problem dealing with the efficient delivery of intensity modulated radiation therapy (IMRT) to individual patients. IMRT treatment planning is usually performed in three phases. The first phase determines a set of beam angles through which radiation is delivered, followed by a second phase that determines an optimal radiation intensity profile (or fluence map). This intensity profile is selected to ensure that certain targets receive a required amount of dose while functional organs are spared. To deliver these intensity profiles to the patient, a third phase must decompose them into a collection of apertures and corresponding intensities. In this paper, we investigate this last problem. Formally, an intensity profile is represented as a nonnegative integer matrix; an aperture is represented as a binary matrix whose ones appear consecutively in each row. A feasible decomposition is one in which the original desired intensity profile is equal to the sum of a number of feasible binary matrices multiplied by corresponding intensity values. To most efficiently treat a patient, we wish to minimize a measure of total treatment time, which is given as a weighted sum of the number of apertures and the sum of the aperture intensities used in the decomposition. We develop the first exact algorithm capable of solving real-world problem instances to optimality within practicable computational limits, using a combination of integer programming decomposition and combinatorial search techniques. We demonstrate the efficacy of our approach on a set of 25 test instances derived from actual clinical data and on 100 randomly generated instances.", "e:keyword": ["Health care", "Treatment", "Programming", "Integer", "Algorithms", "Applications", "Benders decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0760", "e:abstract": "We consider multicomponent maintenance systems with an <i>F</i>-failure group age-replacement policy: it keeps failed components idling until <i>F</i> components are failed and then replaces all failed components together with the nonfailed components whose age has passed the critical threshold age (theta)<sub><i>n</i></sub> for components of type <i>n</i>. With each maintenance action, costs are associated. We derive various unbiased gradient estimators based on the measure-valued differentiation approach for the gradient of the average cost. Each estimator has its own domain of applicability. We also compare the performance of our gradient estimators when applied to stochastic optimization with other general gradient-free methods.", "e:keyword": ["Maintenance", "Simulation", "Gradient estimation", "Measure-valued differentiation", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0761", "e:abstract": "The performance of storage systems and database systems depends significantly on the page replacement policies. Although many page replacement policies have been discussed in the literature, their performances are not fully understood. We introduce analytical techniques for evaluating the performances of page replacement policies including two queue (<monospace>2Q</monospace>), which manages two buffers to capture both the recency and frequency of requests. We derive an exact expression for the probability that a requested item is found (the hit probability) in a buffer managed by <monospace>2Q</monospace> in the fluid limit, where the number of items is scaled by <i>n</i>, the size of items is scaled by 1/<i>n</i>, and <i>n</i> approaches infinity. The hit probability in the fluid limit approximates the hit probability in the original system, and we find that the relative error in the approximation is typically within 1%. Our analysis also illuminates several fundamental properties of <monospace>2Q</monospace> useful for system designers.", "e:keyword": ["Two queue", "Least recently used", "Invalidation", "Probability", "Fluid limit", "Computers/computer science", "Cache", "Algorithms", "Analysis", "Page replacement policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0762", "e:abstract": "Recent epidemiologic studies have suggested that the prophylactic use of antiviral drugs could slow down the spread of an influenza epidemic. Because drug stockpiles are presently scattered in different countries, the outbreak of an epidemic gives rise to a game in which each country must make decisions about how best to allocate its own stockpile in order to protect its population. We develop a two-period multivariate Reed-Frost model to represent the spread of the epidemic within and across countries at its onset. We consider the first two periods only to mimic the exponential growth of an epidemic in its early stage, while keeping the model tractable. Preliminary numerical studies suggest that insights from the two-period model hold in general when considering the entire time horizon. Our model captures three critical sources of uncertainty: the number of initial infections, the spread of the disease, and drug efficacy. We show that for small probabilities of between-country infections, the underlying game is supermodular, Nash equilibrium exists, and there is a unique one that is Pareto optimal among all existing equilibria. Further, we identify sufficient conditions under which the optimal solution of a central planner (such as the World Health Organization) constitutes a Pareto improvement over the decentralized equilibrium, suggesting that countries should agree on an allocation scheme that would benefit everyone. By contrast, when the central planner's solution does not constitute a Pareto improvement, minimizing the total number of infected persons globally requires some countries to sacrifice part of their own population, which raises intriguing ethical issues.", "e:keyword": ["Reed-Frost model", "Epidemic control", "Influenza", "Supermodular games"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0763", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0766", "e:abstract": "We consider the problem of evaluating the quality of solution sets generated by heuristics for multiple-objective combinatorial optimization problems. We extend previous research on the integrated preference functional (IPF), which assigns a scalar value to a given discrete set of nondominated points so that the weighted Tchebycheff function can be used as the underlying implicit value function. This extension is useful because modeling the decision maker's value function with the weighted Tchebycheff function reflects the impact of unsupported points when evaluating sets of nondominated points. We present an exact calculation method for the IPF measure in this case for an arbitrary number of criteria. We show that every nondominated point has its optimal weight interval for the weighted Tchebycheff function. Accordingly, all nondominated points, and not only the supported points in a set, contribute to the value of the IPF measure when using the weighted Tchebycheff function. Two- and three-criteria numerical examples illustrate the desirable properties of the weighted Tchebycheff function, providing a richer measure than the original IPF based on a convex combination of objectives.", "e:keyword": ["Multiple objective combinatorial optimization", "Tchebycheff function", "Multiple criteria metaheuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0767", "e:abstract": "An optimum of a Markov decision process (MDP) is <i>myopic</i> if it can be obtained by solving a series of static problems. Myopic optima are desirable because they can be computed relatively easily. We identify new classes of MDPs with myopic optima and sequential games with myopic equilibrium points. In one of the classes, the single-period reward is homogeneous with respect to the state variable. We illustrate the results with models of revenue management and investment.", "e:keyword": ["Myopic", "Dynamic program", "Markov decision process", "Homogeneous", "Sequential game"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0768", "e:abstract": "The valuation of the real option to store natural gas is a practically important problem that entails dynamic optimization of inventory trading decisions with capacity constraints in the face of uncertain natural gas price dynamics. Stochastic dynamic programming is a natural approach to this valuation problem, but it does not seem to be widely used in practice because it is at odds with the high-dimensional natural gas price evolution models that are widespread among traders. According to the practice-based literature, practitioners typically value natural gas storage heuristically. The effectiveness of the heuristics discussed in this literature is currently unknown because good upper bounds on the value of storage are not available. We develop a novel and tractable approximate dynamic programming method that, coupled with Monte Carlo simulation, computes lower and upper bounds on the value of storage, which we use to benchmark these heuristics on a set of realistic instances. We find that these heuristics are extremely fast to execute but significantly suboptimal compared to our upper bound, which appears to be fairly tight and much tighter than a simpler perfect information upper bound; computing our lower bound takes more time than using these heuristics, but our lower bound substantially outperforms them in terms of valuation. Moreover, with periodic reoptimizations embedded in Monte Carlo simulation, the practice-based heuristics become nearly optimal, with one exception, at the expense of higher computational effort. Our lower bound with reoptimization is also nearly optimal, but exhibits a higher computational requirement than these heuristics. Besides natural gas storage, our results are potentially relevant for the valuation of the real option to store other commodities, such as metals, oil, and petroleum products.", "e:keyword": ["Finance", "Asset pricing", "Real options", "Storage valuation", "Dynamic programming", "Heuristics", "Markov", "Upper bounds", "Industries", "Petroleum/natural gas"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0769", "e:abstract": "We introduce and analyze an intertemporal choice model where customer valuations are uncertain and evolve over time. The model leads directly to the study of call options on capacity that are similar to partially refundable fares. We show that the capacity provider earns significantly higher revenues by selling real options on capacity than on low-to-high pricing. We also investigate the social implications and show that the use of options is both socially optimal and socially efficient.", "e:keyword": ["Pricing", "Revenue management", "Real options", "Stochastic", "Transportation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0770", "e:abstract": "The no-fit polygon is a geometric construct that can offer faster and more efficient handling of geometry between pairs of shapes than traditional line-by-line intersection. The detection of intersections is a critical operation within the irregular two-dimensional stock-cutting problem (also known as “nesting”), which aims to place shapes onto sheets of material so that the material is utilised as efficiently as possible and the waste (or trim loss) is reduced. The problem forms an important process within many real-world manufacturing industries such as metalworking, automotive production, aerospace, clothing and conservatory manufacture, and others. If manufacturers can reduce their costs by utilising raw materials more effectively, this can directly translate into increased profit margins or greater competitiveness within the marketplace. Moreover, there are significant environmental benefits to be gained. Several methods have been proposed to calculate no-fit polygons, but most, if not all, can only operate on geometry that consists of line segments. This paper extends the orbital sliding method of calculating no-fit polygons to enable it to handle arcs and then shows the resultant no-fit polygons being utilised successfully on the two-dimensional irregular packing problem. As far as the authors are aware, this is the first time that a no-fit polygon algorithm has been able to handle arcs robustly without decomposing to their line approximations. The modification of the authors' previously published packing algorithm to utilise the proposed no-fit polygon approach yields solutions of excellent quality (including several best-known) on well-established literature benchmark problems after only a few minutes. The authors believe that the success of the packing strategy and the line and arc no-fit polygon algorithm make this approach a serious candidate for use in real-world production environments.", "e:keyword": ["Search production/scheduling", "Cutting-stock/trim", "Production/scheduling", "Approximations/heuristic", "Computers/computer science", "Artificial intelligence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0771", "e:abstract": "Carbon dioxide allowance trading systems for electricity generators are in place in the European Union and in several U.S. states. An important question in the design of such systems is how allowances are to be initially allocated: by auction, by giving away fixed amounts (grandfathering), or by allocating based on present or recent output, investment, or other decisions. The latter system can bias investment, operations, and product pricing decisions and increase costs relative to the other systems. A nonlinear complementarity model is proposed for investigating the long-run equilibria that would result under alternative systems for power markets characterized by time-varying demand and multiple technology types. Existence of equilibria is shown under mild conditions. Solutions for simple systems show that allocating allowances to new capacity based on fuel use or generator type can yield large distortions in capacity investment, invert the operating order of power plants, and inflate consumer costs. The distortions can be smaller for tighter CO<sub>2</sub> restrictions and are somewhat mitigated if there is also a market for electricity capacity or minimum-run restrictions on coal plants. Distortions are also less if allowances are allocated to plants in proportion to sales rather than capacity.", "e:keyword": ["Equilibrium programming", "Model properties and applications", "Economics", "Electricity and emissions markets"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0772", "e:abstract": "We consider assemble-to-order inventory systems with identical component lead times. We use a stochastic program (SP) to develop an inventory strategy that allows preferential component allocation for minimizing total inventory cost. We prove that the solution of a relaxation of this SP provides a lower bound on total inventory cost for all feasible policies. We demonstrate and test our approach on the W system, which involves three components used to produce two products. (There are two unique parts and a common part. Each product uses the common part and its own unique part.) For the W system, we develop efficient solution procedures for the SP as well as the relaxed SP. We define a simple priority allocation policy that mimics the second-stage SP recourse solution and set base-stock levels according to the first-stage SP solution. We show that our policy achieves the lower bound and is, thus, optimal in two situations: when a certain symmetry condition in the cost parameters holds and when the SP solution satisfies a “balanced capacity” condition. For other cases, numerical results demonstrate that our policy works well and outperforms alternative approaches in many circumstances.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Multi-item", "Assemble-to-order", "Component commonality", "Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0773", "e:abstract": "The stochastic root-finding problem is that of finding a zero of a vector-valued function known only through a stochastic simulation. The simulation-optimization problem is that of locating a real-valued function's minimum, again with only a stochastic simulation that generates function estimates. Retrospective approximation (RA) is a sample-path technique for solving such problems, where the solution to the underlying problem is approached via solutions to a sequence of approximate deterministic problems, each of which is generated using a specified sample size, and solved to a specified error tolerance. Our primary focus, in this paper, is providing guidance on choosing the sequence of sample sizes and error tolerances in RA algorithms. We first present an overview of the conditions that guarantee the correct convergence of RA's iterates. Then we characterize a class of error-tolerance and sample-size sequences that are superior to others in a certain precisely defined sense. We also identify and recommend members of this class and provide a numerical example illustrating the key results.", "e:keyword": ["Simulation", "Efficiency", "Design of experiments", "Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0775", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0777", "e:abstract": "In a call center, there is a natural trade-off between minimizing customer wait time and fairly dividing the workload among agents of different skill levels. The relevant control is the routing policy, that is, the decision concerning which agent should handle an arriving call when more than one agent is available. We formulate an optimization problem for a call center with heterogeneous agent pools, in which each pool is distinguished by the speed at which agents in that pool handle calls. The objective is to minimize steady-state expected customer wait time subject to a “fairness” constraint on the workload division. We first solve the optimization problem by formulating it as a Markov decision process (MDP), and solving a related linear program. We note that this approach does not in general lead to an optimal policy that has a simple structure. Fortunately, the optimal policy does appear to have a simple structure as the system size grows large, in the Halfin-Whitt many-server heavy-traffic limit regime. Therefore, we solve the diffusion control problem that arises in this regime and interpret its solution as a policy for the original system. The resulting routing policy is a threshold policy that determines server pool priorities based on the total number of customers in the system. We prove that a continuous modification of our proposed threshold routing policy is asymptotically optimal in the Halfin-Whitt limit regime. We furthermore present simulation results to illustrate that our proposed threshold routing policy outperforms a common routing policy used in call centers (that routes to the agent that has been idle the longest).", "e:keyword": ["Probability", "Diffusion", "Stochastic model applications", "Queues", "Approximations", "Diffusion models", "Limit theorems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0778", "e:abstract": "This paper addresses a class of problems in which available resources need to be optimally allocated to a random number of jobs with stochastic parameters. Optimal policies are presented for variations of the sequential stochastic assignment problem and the dynamic stochastic knapsack problem, in which the number of arriving jobs is unknown until after the final arrival, and the job parameters are assumed to be independent but not identically distributed random variables.", "e:keyword": ["Sequential assignment", "Dynamic stochastic knapsack", "Policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0779", "e:abstract": "We study a periodically reviewed multiechelon serial inventory system with a capacity constraint on the order quantity at every stage. Under echelon base-stock policies, we demonstrate a simple sample-path result that maps the echelon shortfalls in the serial system to the shortfalls of suitably defined single-stage systems. Because the shortfall processes of single-stage systems are well understood, our result allows us to reinterpret results in the literature on the stability and regeneration times of such multiechelon systems in a simpler fashion with weaker assumptions.", "e:keyword": ["Stochastic inventory theory", "Capacitated systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0780", "e:abstract": "We propose a continuous-time stochastic model for the dynamics of a limit order book. The model strikes a balance between three desirable features: it can be estimated easily from data, it captures key empirical properties of order book dynamics, and its analytical tractability allows for fast computation of various quantities of interest without resorting to simulation. We describe a simple parameter estimation procedure based on high-frequency observations of the order book and illustrate the results on data from the Tokyo Stock Exchange. Using simple matrix computations and Laplace transform methods, we are able to efficiently compute probabilities of various events, conditional on the state of the order book: an increase in the midprice, execution of an order at the bid before the ask quote moves, and execution of both a buy and a sell order at the best quotes before the price moves. Using high-frequency data, we show that our model can effectively capture the short-term dynamics of a limit order book. We also evaluate the performance of a simple trading strategy based on our results.", "e:keyword": ["Limit order book", "Financial engineering", "Laplace transform inversion", "Queueing systems", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0781", "e:abstract": "Lifetime financial decisions often require a decision analyst to elicit a decision maker's preferences for consumption streams. In assessing such preferences, the analyst might look for a set of reasonable conditions to check when selecting a utility form. We provide such a set of conditions and show that they lead to the multiplicative-expo-power (MEP) utility form. Some of our conditions involve trade-offs under certainty and others relate to choices under uncertainty. In the deterministic setting, we invoke increasingness, continuous differentiability, mutual preferential independence, and preferential scale invariance. In the uncertainty setting, we invoke componentwise risk aversion, a utility independence condition, and correlation aversion. We apply the MEP utility in a life-cycle consumption-planning problem under mortality risk. In this context, we find that the correlation-averse MEP utility is more realistic than, and as tractable as, the popular correlation-neutral additive-power and additive-exponential utilities. We show that the correlation-averse decision maker prefers to consume more in life and leave less behind in death. In addition, we demonstrate that the correlation-averse decision maker is more averse to delaying consumption in the face of mortality risk.", "e:keyword": ["Decision analysis", "Multiattribute utility", "Life-cycle consumption", "Mutual preferential independence", "Utility independence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0783", "e:abstract": "We consider a manufacturer of mass-customized modular products who orders components under demand uncertainty, and sets prices, produces to order, and trades excess components in a secondary market after this uncertainty is resolved. The sequence of events reflects, in a parsimonious fashion, the considerable reduction in demand uncertainty between the procurement stage and the selling season, typical of industries with long supply lead times and short product life cycles.We prove that, in contrast to conventional wisdom, the value of production flexibility and expected profit increase with demand correlation if, and only if, commonality between the corresponding products does not exceed a threshold. We also prove that the value of flexibility and expected profit may each increase or decrease with demand variability, depending on demand correlations and component commonalities across the entire product line. Finally, we prove that when demand shocks are independent, the optimal product prices are positively correlated if, and only if, the degree of commonality between the corresponding products exceeds a threshold.", "e:keyword": ["Inventory/production", "Assemble-to-order", "Component commonality", "Flexibility", "Demand correlation", "Pricing", "Spot market"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0784", "e:abstract": "Reliable risk measurement is a key problem for financial institutions and regulatory authorities. The current industry standard Value-at-Risk has several deficiencies. Improved risk measures have been suggested and analyzed in the recent literature, but their computational implementation has largely been neglected so far. We propose and investigate stochastic approximation algorithms for the convex risk measure Utility-Based Shortfall Risk. Our approach combines stochastic root-finding schemes with importance sampling. We prove that the resulting Shortfall Risk estimators are consistent and asymptotically normal, and provide formulas for confidence intervals. The performance of the proposed algorithms is tested numerically. We finally apply our techniques to the Normal Copula Model, which is also known as the industry model CreditMetrics. This provides guidance for future implementations in practice.", "e:keyword": ["Convex risk measures", "Shortfall risk", "Stochastic approximation", "Stochastic root finding", "Importance sampling", "Exponential twisting", "Portfolio credit risk management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0785", "e:abstract": "This paper analyzes a class of common-component allocation rules, termed <i>no-holdback</i> (NHB) rules, in continuous-review assemble-to-order (ATO) systems with positive lead times. The inventory of each component is replenished following an independent base-stock policy. In contrast to the usually assumed first-come-first-served (FCFS) component allocation rule in the literature, an NHB rule allocates a component to a product demand only if it will yield immediate fulfillment of that demand. We identify metrics as well as cost and product structures under which NHB rules outperform all other component allocation rules. For systems with certain product structures, we obtain key performance expressions and compare them to those under FCFS. For general product structures, we present performance bounds and approximations. Finally, we discuss the applicability of these results to more general ATO systems.", "e:keyword": ["Stochastic multi-item inventory system", "Assemble-to-order", "Base-stock policy", "Common-component allocation rule", "Non-FCFS", "Sample-path analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0786", "e:abstract": "We present an update formula that allows the expression of the deviation matrix of a continuous-time Markov process with denumerable state space having generator matrix Q<sup>*</sup> through a continuous-time Markov process with generator matrix <i>Q</i>. We show that under suitable stability conditions the algorithm converges at a geometric rate. By applying the concept to three different examples, namely, the M/M/1 queue with vacations, the M/G/1 queue, and a tandem network, we illustrate the broad applicability of our approach. For a problem in admission control, we apply our approximation algorithm to Markov decision theory for computing the optimal control policy. Numerical examples are presented to highlight the efficiency of the proposed algorithm.", "e:keyword": ["Markov decision processes", "Deviation matrix", "Algorithm", "Probability", "Markov processes", "Queues", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0787", "e:abstract": "We analyze a short-term revenue optimization problem involving the targeting of customers for a promotion in which a finite number of perishable items are sold on a last-minute offer. The goal is to select the subset of customers to whom the offer will be made available in order to maximize the expected return. Each client replies with a certain probability and reports a specific value that might depend on the customer type, so that the selected subset has to balance the risk of not selling all items with the risk of assigning an item to a low value customer.We show that <i>threshold strategies</i>, which select all those clients with values above a certain optimal threshold, might fail to achieve the maximal revenue. However, using a linear programming relaxation, we prove that they attain a constant factor of the optimal value. Specifically, the achieved factor is 1/2 when a single item is to be sold and approaches 1 as the number of available items grows to infinity. Also, for the single item case, we propose an upper bound based on a sharper linear relaxation that allows us to obtain a threshold strategy achieving at least 2/3 of the optimal revenue. Moreover, although the complexity status of the problem is open, we provide a polynomial time approximation scheme for the single item case.", "e:keyword": ["Promotional sale", "Linear programming relaxations", "Approximation algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0788", "e:abstract": "Motivated by scheduling challenges of burn-in ovens in back-end semiconductor manufacturing, we propose a linear-programming-based algorithm, an integer-programming-based algorithm, and a heuristic-based algorithm to schedule nonhomogenous parallel batch machines with nonidentical job sizes and incompatible job families. We consider the common scheduling of consecutive steps that are linked together through secondary scarce resources. Our approach addresses the availability and compatibility of several resources required to make each process possible. The algorithms strive to meet short-term production targets expressed by product and step. The algorithms are shown to be effective and computationally efficient for this purpose. Taken together with previously developed methodology for the practical translation of target output schedules into short-term local production targets, this article suggests how a complex supply chain manufacturing system can be efficiently and effectively managed by decentralized local scheduling algorithms striving to meet short-term production targets that in turn ensure maintenance of an appropriate dynamic profile across production steps for work-in-process.", "e:keyword": ["Production/scheduling", "Parallel machine scheduling", "Batch processors"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0790", "e:abstract": "Oblivious equilibrium is a new solution concept for approximating Markov-perfect equilibrium in dynamic models of imperfect competition among heterogeneous firms. In this paper, we present algorithms for computing oblivious equilibrium and for bounding approximation error. We report results from computational case studies that serve to assess both efficiency of the algorithms and accuracy of oblivious equilibrium as an approximation to Markov-perfect equilibrium. We also extend the definition of oblivious equilibrium, originally proposed for models with only firm-specific idiosyncratic random shocks, and our algorithms to accommodate models with industry-wide aggregate shocks. Our results suggest that, by using oblivious equilibrium to approximate Markov-perfect equilibrium, it is possible to greatly increase the set of dynamic models of imperfect competition that can be analyzed computationally.", "e:keyword": ["Games/group decisions", "Stochastic", "Dynamic programming/optimal control", "Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0791", "e:abstract": "The allocation of surgeries to operating rooms (ORs) is a challenging combinatorial optimization problem. There is also significant uncertainty in the duration of surgical procedures, which further complicates assignment decisions. In this paper, we present stochastic optimization models for the assignment of surgeries to ORs on a given day of surgery. The objective includes a fixed cost of opening ORs and a variable cost of overtime relative to a fixed length-of-day. We describe two types of models. The first is a two-stage stochastic linear program with binary decisions in the first stage and simple recourse in the second stage. The second is its robust counterpart, in which the objective is to minimize the maximum cost associated with an uncertainty set for surgery durations. We describe the mathematical models, bounds on the optimal solution, and solution methodologies, including an easy-to-implement heuristic. Numerical experiments based on real data from a large health-care provider are used to contrast the results for the two models and illustrate the potential for impact in practice. Based on our numerical experimentation, we find that a fast and easy-to-implement heuristic works fairly well, on average, across many instances. We also find that the robust method performs approximately as well as the heuristic, is much faster than solving the stochastic recourse model, and has the benefit of limiting the worst-case outcome of the recourse problem.", "e:keyword": ["Optimization", "Stochastic programming", "Surgery"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0792", "e:abstract": "We develop and evaluate a two-level simulation procedure that produces a confidence interval for expected shortfall. The outer level of simulation generates financial scenarios, whereas the inner level estimates expected loss conditional on each scenario. Our procedure uses the statistical theory of empirical likelihood to construct a confidence interval. It also uses tools from the ranking-and-selection literature to make the simulation efficient.", "e:keyword": ["Conditional value-at-risk", "Worst conditional expectation", "Tail conditional expectation", "Conditional tail expectation", "Expected shortfall", "Empirical likelihood", "Two-level simulation", "Simulation", "Design of experiments", "Two-level simulation", "Simulation", "Efficiency", "Screening methods", "Finance", "Portfolio", "Risk management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0793", "e:abstract": "The proponents of airline passenger profiling claim that profiling will reduce the cost of security, improve the detection of attackers, increase the reliability of signals from screening devices, and reduce the inconvenience to normal passengers. In this paper we show that if the Transportation Security Administration (TSA) manually inspects all those passengers classified as likely attackers and sends others through a screening system, as it did when it deployed the Computer Assisted Passenger Prescreening System (CAPPS), then it is superior to no profiling on all four performance measures if and only if the quality of the profiler vis-à-vis that of the screening system is sufficiently high. If the quality of the screening device is sufficiently high, profiling could be detrimental on all four performance measures. On the other hand, if the TSA deploys two screening devices along with the profiler---each screening device optimally configured for each of the two groups of passengers---then profiling improves the reliability of screening device signals, reduces the inconvenience caused to normal passengers, and improves the social welfare even when quality of the screening device is high. One of the implications of our findings is that the security architecture used by the TSA when it deployed CAPPS could provide a strong support to the arguments by some against the use of profiling; however, if the TSA deploys a two-screening device architecture, it might not only blunt the criticism that profiling is discriminatory but also benefit normal passengers and overall society economically.", "e:keyword": ["Aviation security", "Homeland and transportation security", "Screening systems", "Passenger profiling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0794", "e:abstract": "The implicit definition and nondifferentiability of efficient frontiers used in data envelopment analysis are two major obstacles to obtaining their differential characteristics, including various elasticity measures and marginal rates of substitution. In this paper we invoke the theorem of the directional derivative of the optimal value function and show how this can be used to define and calculate the required elasticities without any simplifying assumptions. This approach allows us to extend the known elasticity measures and introduce new ones to the entire efficient frontier, including all its extreme points, in one single development. We also construct linear programs that are required for the calculation of elasticity measures. Our main development is undertaken in the variable returns-to-scale technology but extends to other polyhedral technologies of efficiency analysis.", "e:keyword": ["Decision analysis", "Data envelopment analysis", "Economics", "Input-output analysis", "Elasticity measures", "Programming", "Nondifferentiable"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0795", "e:abstract": "In this paper we focus on a linear optimization problem with uncertainties, having expectations in the objective and in the set of constraints. We present a modular framework to obtain an approximate solution to the problem that is distributionally robust and more flexible than the standard technique of using linear rules. Our framework begins by first affinely extending the set of primitive uncertainties to generate new linear decision rules of larger dimensions and is therefore more flexible. Next, we develop new piecewise-linear decision rules that allow a more flexible reformulation of the original problem. The reformulated problem will generally contain terms with expectations on the positive parts of the recourse variables. Finally, we convert the uncertain linear program into a deterministic convex program by constructing distributionally robust bounds on these expectations. These bounds are constructed by first using different pieces of information on the distribution of the underlying uncertainties to develop separate bounds and next integrating them into a combined bound that is better than each of the individual bounds.", "e:keyword": ["Programming", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0796", "e:abstract": "We describe a general technique for determining upper bounds on maximal values (or lower bounds on minimal costs) in stochastic dynamic programs. In this approach, we relax the nonanticipativity constraints that require decisions to depend only on the information available at the time a decision is made and impose a “penalty” that punishes violations of nonanticipativity. In applications, the hope is that this relaxed version of the problem will be simpler to solve than the original dynamic program. The upper bounds provided by this dual approach complement lower bounds on values that may be found by simulating with heuristic policies. We describe the theory underlying this dual approach and establish weak duality, strong duality, and complementary slackness results that are analogous to the duality results of linear programming. We also study properties of good penalties. Finally, we demonstrate the use of this dual approach in an adaptive inventory control problem with an unknown and changing demand distribution and in valuing options with stochastic volatilities and interest rates. These are complex problems of significant practical interest that are quite difficult to solve to optimality. In these examples, our dual approach requires relatively little additional computation and leads to tight bounds on the optimal values.", "e:keyword": ["Dynamic programming", "Duality", "Inventory control", "Option pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0797", "e:abstract": "We study a dynamic pricing problem for a class of products with stable consumption patterns (e.g., household items, staple foods). Consumers may stock up the product at current prices for future consumption, but they incur inventory holding costs. We model this situation as a dynamic game over an infinite time horizon: in each period, the seller sets a price, and each consumer chooses how many units to buy. We develop a solution methodology based on rational expectations. By endowing each player with beliefs, we decouple the dynamic game into individual dynamic programs for each player. We solve for the rational expectations equilibrium, where all players make optimal dynamic decisions given correct beliefs about others' behavior. In equilibrium, the seller may either charge a constant fixed price or offer periodic price promotions at predictable time intervals. We show that promotions are useful when frequent shoppers are willing to pay more for the product than are occasional shoppers. We also develop several model extensions to study the impact of consumer stockpiling on the seller's inventory, production, and rationing strategies.", "e:keyword": ["Dynamic pricing", "Stockpiling", "Consumer inventory", "Promotions", "Rational expectations", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0799", "e:abstract": "We study an inventory system under periodic review in the presence of two suppliers (or delivery modes). The emergency supplier has a shorter lead-time than the regular supplier, but the unit price he offers is higher. Excess demand is backlogged. We generalize the recently studied class of dual index policies [Veeraraghavan, S., A. Scheller-Wolf. 2008. Now or later: Dual index policies for capacitated dual sourcing systems. <i>Oper. Res.</i> <b>56</b>(4) 850--864] by proposing two classes of policies. The first class consists of policies that have an order-up-to structure for the emergency supplier. We provide analytical results that are useful for determining optimal or near-optimal policies within this class. This analysis and the policies we propose leverage our observation that the classical “lost sales inventory problem” is a special case of this problem. The second class consists of policies that have an order-up-to structure for the regular supplier. Here, we derive bounds on the optimal order quantity from the emergency supplier, in any period, and use these bounds for finding effective policies within this class. Finally, we undertake an elaborate computational investigation to compare the performance of the policies we propose with that of dual index policies. One of our policies provides an average cost-saving of 1.1% over the best dual index policy and has the same computational requirements. Another policy that we propose has a cost performance similar to the best dual index policy, but its computational requirements are lower.", "e:keyword": ["Inventory/production", "Lead times", "Dual sourcing", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0800", "e:abstract": "We discuss a stochastic-programming-based method for scheduling electric power generation subject to uncertainty. Such uncertainty may arise from either imperfect forecasting or moment-to-moment fluctuations, and on either the supply or the demand side. The method gives a system of locational marginal prices that reflect the uncertainty, and these may be used in a market settlement scheme in which payment is for energy only. We show that this scheme is revenue adequate in expectation.", "e:keyword": ["Electricity market", "Stochastic programming", "Locational pricing", "Wind power", "Regulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0801", "e:abstract": "Reliable facility location models consider unexpected failures with site-dependent probabilities, as well as possible customer reassignment. This paper proposes a compact mixed integer program (MIP) formulation and a continuum approximation (CA) model to study the reliable uncapacitated fixed charge location problem (RUFL), which seeks to minimize initial setup costs and expected transportation costs in normal and failure scenarios.The MIP determines the optimal facility locations as well as the optimal customer assignments and is solved using a custom-designed Lagrangian relaxation (LR) algorithm. The CA model predicts the total system cost without details about facility locations and customer assignments, and it provides a fast heuristic to find near-optimum solutions. Our computational results show that the LR algorithm is efficient for mid-sized RUFL problems and that the CA solutions are close to optimal in most of the test instances. For large-scale problems, the CA method is a good alternative to the LR algorithm that avoids prohibitively long running times.", "e:keyword": ["Facility location", "Reliability", "Mixed integer program", "Lagrangian relaxation", "Heuristics", "Continuum approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0802", "e:abstract": "In this paper, we establish a new preservation property of quasi-<i>K</i>-concavity under certain optimization operations. One important application of the result is to analyze joint inventory-pricing models for single-product periodic-review inventory systems with concave ordering costs. At each period, an ordering quantity and a selling price of the product are determined simultaneously. Demand is random but sensitive to the price. The objective is to maximize the total expected discounted profit over a finite planning horizon. Assuming that demand is a deterministic function of the selling price plus a random perturbation with a positive Pólya or uniform distribution, we show that a generalized (<i>s</i>, <i>S</i>, <i>p</i>) policy is optimal.", "e:keyword": ["Inventory control", "Pricing", "Concave ordering cost", "Quasi-K-concavity", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0803", "e:abstract": "We present a modification to Dantzig-Wolfe decomposition of variational inequality (VI) problems that allows for approximation of the VI mapping in the subproblem. The approximation is parameterized by the most recent master problem solution, and it must satisfy two simple requirements. In an electronic companion (online appendix), we show that the proofs of convergence and other important properties go through with subproblem approximation. The approximation procedure is illustrated by an application to a class of multicommodity economic equilibrium models (MCEEMs): the standard Dantzig-Wolfe decomposition by commodity does not allow the subproblem to be decomposed into separate subproblems for each commodity, but we show two ways to approximate the subproblem's inverse demand function, and both ways allow the subproblem to be broken into separate single-commodity problems. A further approximation is combined with each of the inverse demand approximations; in effect, an approximate supply or demand curve is introduced into each commodity's subproblem for transfers of commodities between different subproblems, thus allowing the subproblems to produce better proposals. An illustration is included for an MCEEM that represents energy markets in Canada.", "e:keyword": ["Dantzig-Wolfe decomposition", "Market equilibrium computation", "Variational inequality problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0804", "e:abstract": "An approach based on semilocal approximation is introduced for the solution of a general class of operations research problems, such as Markovian decision problems, multistage optimal control, and maximum-likelihood estimation. Because it is extremely hard to derive analytical solutions that minimize the cost in most instances of the problem, we must look for approximate solutions. Here, it is shown that good solutions can be obtained with a moderate computational effort by exploiting properties of semilocal approximation through kernel models and efficient sampling of the state space. The convergence of the proposed method, called <i>semilocal approximate minimization</i> (SLAM), is discussed, and the consistency of the solution is derived. Simulation results show the efficiency of SLAM, also through its application to a classic operations research problem, i.e., inventory forecasting.", "e:keyword": ["Functional optimization", "Kernel methods", "Semilocal approximation", "Low discrepancy sequences", "Inventory forecasting"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0805", "e:abstract": "We present an efficient dynamic programming algorithm to determine the optimal assortment and inventory levels in a single-period problem with stockout-based substitution. In our model, total customer demand is random and comprises <i>fixed proportion</i> of customers of different types. Customer preferences are modeled through the definition of these types. Each customer type corresponds to a specific preference ordering among products. A customer purchases the highest-ranked product, according to his type (if any), that is available at the time of his visit to the store (stockout-based substitution). We solve the optimal assortment problem using a dynamic programming formulation. We establish structural properties of the value function of the dynamic program that, in particular, help to characterize multiple local maxima. We use the properties of the optima to solve the problem in pseudopolynomial time. Our algorithm also gives a heuristic for the general case, i.e., when the proportion of customers of each type is random. In numerical tests, this heuristic performs better and faster than previously known methods, especially when the mean demand is large, the degree of substitutability is high, the population is homogeneous, or prices and/or costs vary across products.", "e:keyword": ["Assortment planning", "Inventory", "Stockout-based substitution"]}, {"@id": "http://dx.doi.org/10.1287/opre.1090.0806", "e:abstract": "We consider a multiple product supply chain where a manufacturer receives orders from several distributors. If the orders cannot all be met from available production capacity, then the manufacturer allocates that capacity and a set of resubmittable orders among the distributors. The distributors may share their allocated capacity among themselves before submitting revised orders. Finally, the manufacturer schedules the revised orders to minimize its cost. We consider three practical coordination issues. First, we estimate the benefit to the manufacturer from considering scheduling costs and constraints in making capacity and order allocation decisions. Second, we estimate the additional profit that the distributors achieve when they share their allocated capacity. Third, we estimate the value of coordination between the manufacturer and the distributors. Our work is among the first to consider all three issues simultaneously. We model scheduling costs and constraints within the manufacturer's capacity allocation problem. We model the distributors' capacity sharing problem as a cooperative game that has properties that are unique within cooperative game theory. Finally, we develop optimal algorithms for all the models defined by the three coordination issues. Our exact evaluation of decisions about the appropriate coordination level improves managers' ability to make those decisions.", "e:keyword": ["Supply chain", "Capacity allocation", "Scheduling", "Cooperative game", "Value of coordination", "Production/scheduling", "Sequencing", "Deterministic", "Single machine", "Games/group decisions", "Cooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0789", "e:abstract": "It is shown how infinite sequences of densities with defined properties can be used to evaluate the expected performance of mathematical aggregation rules for elicited densities. The performance of these rules is measured through the average variance, calibration, and average Brier score of the aggregates. A general result for the calibration of the arithmetic average of densities from well-calibrated independent experts is given. Arithmetic and geometric aggregation rules are compared using sequences of normal densities. Sequences are developed that exhibit dependence among experts and lack of calibration. The impact of correlation, number of experts, and degree of calibration on the performance of the aggregation is demonstrated.", "e:keyword": ["Linear opinion pools", "Geometric average", "Expert judgment", "Subjective probability", "Scoring rules", "Expert combination", "Dependence", "Overconfidence", "Consensus probability", "Calibration", "Elicitation", "Brier score"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0798", "e:abstract": "This paper investigates a capacity planning strategy that collects commitments to purchase before the capacity decision and uses the acquired advance sales information to decide on the capacity. In particular, we study a profit-maximization model in which a manufacturer collects advance sales information periodically prior to the regular sales season for a capacity decision. Customer demand is stochastic and price sensitive. Once the capacity is set, the manufacturer produces and satisfies customer demand (to the extent possible) from the installed capacity during the regular sales period. We study scenarios in which the advance sales and regular sales season prices are set exogenously and optimally. For both scenarios, we establish the optimality of a <i>control band</i> or a <i>threshold</i> policy that determines when to stop acquiring advance sales information and how much capacity to build. We show that advance selling can improve the manufacturer's profit significantly. We generate insights into how operating conditions (such as the capacity building cost) and market characteristics (such as demand variability) affect the value of information acquired through advance selling. From this analysis, we identify the conditions under which advance selling for capacity planning is most valuable. Finally, we study the joint benefits of acquiring information for capacity planning through advance selling <i>and</i> revenue management of installed capacity through dynamic pricing.", "e:keyword": ["Advance selling", "Capacity", "Stochastic demand", "Dynamic pricing", "Optimal stopping", "Demand learning"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0809", "e:abstract": "We consider a serial supply chain with one supplier and one retailer. Each obtains some demand forecast information, which may be shared or not. We investigate the members' benefits from sharing information. The forecasts follow a variant of the Martingale model of forecast evolution (MMFE). We construct a simple transfer-payment scheme to align the players' incentives with that of the overall system. The main finding is that, unless the players' incentives are aligned in this way, sharing information makes little sense. It might hurt one or the other player and the system as a whole.", "e:keyword": ["Inventory/production", "Multi-item", "Multiechelon", "Multistage", "Policies", "Review/lead times", "Games/group decisions", "Noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0810", "e:abstract": "In this paper, we examine a supply chain in which a single supplier sells to a downstream retailer. We consider a multiperiod model with the following sequence of events. In period <i>t</i> the supplier offers a contract to the retailer, and the retailer makes her purchasing decision in anticipation of the random demand. The demand then unravels, and the retailer carries over any excess inventory to the next period (unmet demand is lost). In period <i>t</i>+1 the supplier designs a new contract based on his belief of the retailer's inventory, and the game is played dynamically. We assume that <i>short-term</i> contracts are used, i.e., the contracting is dynamically conducted at the beginning of each period. We also assume that the retailer's inventory before ordering is not observed by the supplier. This setting describes scenarios in which the downstream retailer does not share inventory/sales information with the supplier. For instance, it captures the phenomenon of retailers distorting past sales information to secure better contracting terms from their suppliers. We cast our problem as a dynamic adverse-selection problem and show that, given relatively high production and holding costs, the optimal contract can take the form of a batch-order contract, which minimizes the retailer's information advantage. We then analyze the performance of this type of contract with respect to some useful benchmarks and quantify the value of prudent contract design and the value of inventory information to the supply chain. Markovian adverse-selection models, in which the state and action in a period affect the state in the subsequent period, are recognized as theoretically challenging and are relatively less understood. We take a nontrivial step towards a better understanding of such models under short-term contracting.", "e:keyword": ["Games", "Stochastic", "Adverse selection", "Short-term contracting", "Inventory", "Uncertainty", "Stochastic", "Dynamic programming", "Markov", "Infinite state"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0811", "e:abstract": "The aim of this note is to correct an error in the formulation of Theorem 1 by Savin and Terwiesch [Savin, S., C. Terwiesch. 2005. Optimal product launch times in a duopoly: Balancing life-cycle revenues with product cost. <i>Oper. Res.</i> <b>53</b>(1) 26--47].", "e:keyword": ["New products", "Competitive diffusion dynamics", "Differential equations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0814", "e:abstract": "Many decision problems exhibit structural properties in the sense that the objective function is a composition of different component functions that can be identified using empirical data. We consider the approximation of such objective functions, subject to general monotonicity constraints on the component functions. Using a constrained B-spline approximation, we provide a data-driven robust optimization method for environments that can be sample-sparse. The method, which simultaneously identifies and solves the decision problem, is illustrated for the problem of optimal debt settlement in the credit-card industry.", "e:keyword": ["B-splines", "Monotone approximation", "Nonparametric/semiparametric methods", "Robust optimization", "Sample-sparse environments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0815", "e:abstract": "We consider queueing systems in which customers arrive according to a Poisson process and have exponentially distributed service requirements. The customers are impatient and may abandon the system while waiting for service after a generally distributed amount of time. The system incurs customer-related costs that consist of waiting and abandonment penalty costs. We study capacity sizing in such systems to minimize the sum of the long-term average customer-related costs and capacity costs. We use fluid models to derive prescriptions that are asymptotically optimal for large customer arrival rates. Although these prescriptions are easy to characterize, they depend intricately upon the distribution of the customers' time to abandon and may prescribe operating in a regime with offered load (the ratio of the arrival rate to the capacity) greater than 1. In such cases, we demonstrate that the fluid prescription is optimal up to <i>O</i>(1). That is, as the customer arrival rate increases, the optimality gap of the prescription remains bounded.", "e:keyword": ["Queues", "Balking and reneging", "Approximations", "Limit theorems", "Capacity sizing", "Impatient customers", "Order-1 accuracy", "Fluid approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0818", "e:abstract": "We explicitly calculate the aggregate diffusion dynamics in one-dimensional agent-based models of adoption of new products, without using the mean-field approximation. We then introduce a clusters-dynamics approach, and use it to derive an analytic approximation of the aggregate diffusion dynamics in multidimensional agent-based models. The clusters-dynamics approximation shows that the aggregate diffusion dynamics does not depend on the average distance between individuals, but rather on the expansion rate of clusters of adopters. Therefore, the grid dimension has a large effect on the aggregate adoption dynamics, but a small-world structure and heterogeneity among individuals have only a minor effect. Our results suggest that the one-dimensional model and the Bass model provide a lower bound and an upper bound, respectively, for the aggregate diffusion dynamics in agent-based models with “any” spatial structure.", "e:keyword": ["Agent-based model", "Cellular automata", "Bass model", "Diffusion", "New products", "Small-world", "Mean-field approximation", "Heterogeneity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0819", "e:abstract": "Multiplicity of equilibria is a prevalent problem in many economic models. Often equilibria are characterized as solutions to a system of polynomial equations. This paper gives an introduction to the application of Gröbner bases for finding all solutions of a polynomial system. The Shape Lemma, a key result from algebraic geometry, states under mild assumptions that a given equilibrium system has the same solution set as a much simpler triangular system. Essentially, the computation of all solutions then reduces to finding all roots of a single polynomial in a single unknown. The software package Singular computes the equivalent simple system. If all coefficients in the original equilibrium equations are rational numbers or parameters, then the Gröbner basis computations of Singular are exact. Thus, Gröbner basis methods cannot only be used for a numerical approximation of equilibria, but in fact may allow the proof of theoretical results for the underlying economic model. Three economic applications illustrate that without much prior knowledge of algebraic geometry, Gröbner basis methods can be easily applied to gain interesting insights into many modern economic models.", "e:keyword": ["Multiple equilibria", "Polynomial equations", "Groebner bases"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0820", "e:abstract": "We consider a single-item, periodic-review inventory control problem in which discrete stochastic demand must be satisfied. When shortages occur, the unmet demand must be filled by some form of expediting; we allow a very general form for the cost structure of expediting. We explicitly consider the case where expedited production is allowed to produce up to a positive inventory level. We also consider the case where expedited production beyond the deficit is not permitted; an alternate application for this model is an inventory system with general lost sales costs. For the infinite-horizon discounted problem, we characterize the structure of the optimal stationary expediting policy and show that an (<i>s</i>, <i>S</i>) policy is optimal for regular production. For the special cases where the expediting cost function is concave or consists of a fixed and linear per-unit cost, we show that the optimal stationary expediting policy is generalized (<i>s</i>, <i>S</i>) or order-up-to, respectively. A numerical study allows us to gain insight into when expediting above and beyond the deficit is cost-effective.", "e:keyword": ["Inventory/production policies", "Markov decision policies", "Overtime production"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0821", "e:abstract": "In this paper, we propose a framework for robust optimization that relaxes the standard notion of robustness by allowing the decision maker to vary the protection level in a smooth way across the uncertainty set. We apply our approach to the problem of maximizing the expected value of a payoff function when the underlying distribution is ambiguous and therefore robustness is relevant. Our primary objective is to develop this framework and relate it to the standard notion of robustness, which deals with only a single guarantee across one uncertainty set. First, we show that our approach connects closely to the theory of convex risk measures. We show that the complexity of this approach is equivalent to that of solving a small number of standard robust problems. We then investigate the conservatism benefits and downside probability guarantees implied by this approach and compare to the standard robust approach. Finally, we illustrate the methodology on an asset allocation example consisting of historical market data over a 25-year investment horizon and find in every case we explore that relaxing standard robustness with soft robustness yields a seemingly favorable risk-return trade-off: each case results in a higher out-of-sample expected return for a relatively minor degradation of out-of-sample downside performance.", "e:keyword": ["Robust optimization", "Ambiguity", "Convex risk measures", "Divergence measures", "Optimized certainty equivalent", "Portfolio optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0822", "e:abstract": "In this paper, we propose a partial differential equation formulation for the value of an option when the underlying asset price is described by a discrete-time GARCH process. Our numerical approach involves a spectral Fourier-Chebyshev interpolation. Numerical illustrations are provided, and the results are compared with other available valuation methods. Our numerical procedure converges exponentially fast and allows for the efficient computation of option prices, achieving a high level of precision in a few seconds of computing time.", "e:keyword": ["Asset pricing", "Algorithms", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0823", "e:abstract": "Capacity addition and withdrawal decisions are among the most important strategic decisions made by firms in oligopolistic industries. In this paper, we develop and analyze a fully dynamic model of an oligopolistic industry with lumpy capacity and lumpy investment/disinvestment. We use our model to suggest answers to two questions: First, what economic factors facilitate preemption races? Second, what economic factors facilitate capacity coordination? With a series of examples we show that low product differentiation, low investment sunkness, and high depreciation tend to promote preemption races. The same examples also show that low product differentiation and low investment sunkness tend to promote capacity coordination. Although depreciation removes capacity, it might impede capacity coordination. Finally, our examples show that multiple equilibria arise over at least some range of parameter values. The distinct structures of these equilibria suggest that firms' expectations play a key role in determining whether or not industry dynamics are characterized by preemption races and capacity coordination. Taken together, our results suggest that preemption races and excess capacity in the short run often go hand-in-hand with capacity coordination in the long run.", "e:keyword": ["Capacity", "Investment and disinvestment", "Industry evolution", "Preemption races", "Capacity coordination", "Dynamic stochastic game", "Markov perfect equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0824", "e:abstract": "Multidimensional mechanism design problems have proven difficult to solve by extending techniques from the one-dimensional case. This paper considers mechanism design problems with multidimensional types when the seller's cost function is not separable across buyers. By adapting results obtained by Border [Border, K. 1991. Implementation of reduced form auctions: A geometric approach. <i>Econometrica</i> <b>59</b> 1175--1187], we transform the seller's problem into a representation that only involves “interim” variables and eliminates the dimensionality dependence on the number of buyers. We show that the associated infinite-dimensional optimization problem posed by the theoretical model can be approximated arbitrarily well by a sequence of finite-dimensional linear programming problems.We provide an efficient---i.e., terminating in polynomial time in the problem size---method to compute the separation oracle associated with the Border constraints and incentive compatibility constraints. This implies that our finite-dimensional approximation is solvable in polynomial time.Finally, we illustrate how the numerical solutions of the finite-dimensional approximations can provide insights into the nature of optimal solutions to the infinite-dimensional problem in particular cases.", "e:keyword": ["Linear programming", "Large-scale systems", "Algorithms", "Infinite dimensional", "Bidding", "Auctions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0825", "e:abstract": "A minimum volume set of a probability density is a region of minimum size among the regions covering a given probability mass of the density. Effective methods for finding the minimum volume sets are very useful for detecting failures or anomalies in commercial and security applications---a problem known as <i>novelty detection</i>. One theoretical approach of estimating the minimum volume set is to use a density level set where a kernel density estimator is plugged into the optimization problem that yields the appropriate level. Such a plug-in estimator is not of practical use because solving the corresponding minimization problem is usually intractable. A modified plug-in estimator was proposed by Hyndman in 1996 to overcome the computation difficulty of the theoretical approach but is not well studied in the literature. In this paper, we provide theoretical support to this estimator by showing its asymptotic consistency. We also show that this estimator is very competitive to other existing novelty detection methods through an extensive empirical study.", "e:keyword": ["Density level sets", "Minimum volume sets", "Novelty detection", "Generalized statistical control chart", "Plug-in estimator", "Asymptotic consistency"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0826", "e:abstract": "We consider a multiclass queueing system with multiple homogeneous servers and customer abandonment. For each customer class <i>i</i>, the holding cost per unit time, the service rate, and the abandonment rate are denoted by <i>c</i><sub><i>i</i></sub>, <i>(mu)</i><sub><i>i</i></sub>, and <i>(theta)</i><sub><i>i</i></sub>, respectively. We prove that under a many-server fluid scaling and overload conditions, a server-scheduling policy that assigns priority to classes according to their index <i>c</i><sub><i>i</i></sub><i>(mu)</i><sub><i>i</i></sub>/<i>(theta)</i><sub><i>i</i></sub> is asymptotically optimal for minimizing the overall long-run average holding cost. An additional penalty on customer abandonment is easily incorporated into this model and leads to a similar index rule.", "e:keyword": ["Multiclass queue", "Customer abandonment", "Fluid limits"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0827", "e:abstract": "We analyze investment incentives and market structure under oligopoly competition in industries with congestion effects. Our results are particularly focused on models inspired by modern technology-based services such as telecommunications and computing services. We consider situations where firms compete by simultaneously choosing prices and investments; increasing investment reduces the congestion disutility experienced by consumers. We define a notion of returns to investment, according to which congestion models inspired by delay exhibit increasing returns, whereas loss models exhibit nonincreasing returns. For a broad range of models with nonincreasing returns to investment, we characterize and establish uniqueness of pure-strategy Nash equilibrium. We also provide conditions for existence of pure-strategy Nash equilibrium. We extend our analysis to a model in which firms must additionally decide whether to enter the industry. Our theoretical results contribute to the basic understanding of competition in service industries and yield insight into business and policy considerations.", "e:keyword": ["Competition", "Game theory", "Services", "Congestion"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0828", "e:abstract": "The two major paradigms in the theoretical agency literature are moral hazard (i.e., hidden action) and adverse selection (i.e., hidden information). Prior research typically solves these problems in isolation, as opposed to simultaneously incorporating both adverse selection and moral hazard features. We formulate two complementary generalized principal-agent models that incorporate features observed in real-world contracting environments (e.g., agents with power utility and limited liability, lognormal stock price distributions, and stock options) as mathematical programs with equilibrium constraints (MPEC). We use state-of-the-art numerical algorithms to solve the resulting models. We find that many of the standard results no longer obtain when wealth effects are present. We also develop a new measure of incentives calculated as the change in the agent's certainty equivalent under the optimal contract for a change in action evaluated at the optimal action. This measure facilitates interpretation of the resulting contracts and allows us to compare contracts across different contracting environments.", "e:keyword": ["Generalized principal-agent model", "Executive compensation", "Mathematical programs with equilibrium constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0829", "e:abstract": "We apply the average cost optimality equation to zero-sum Markov games by considering a simple game with one-sided incomplete information that generalizes an example of Aumann and Maschler [Aumann, R. J., M. B. Maschler. 1995. <i>Repeated Games with Incomplete Information</i>. MIT Press, Cambridge, MA]. We determine the value and identify the optimal strategies for a range of parameters.", "e:keyword": ["Games/group decisions", "Noncooperative", "Stochastic", "Dynamic programming/optimal control", "Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0830", "e:abstract": "We consider electricity pool markets in radial transmission networks in which the lines have capacities. At each node there is a strategic generator injecting generation quantities into the pool. Prices are determined by a linear competitive fringe at each node (or equivalently a linear demand function) through a convex dispatch optimization. We derive a set of linear inequalities satisfied by the line capacities that gives necessary and sufficient conditions for the unconstrained one-shot Cournot equilibrium to remain an equilibrium in the constrained network. We discuss the extension of this model to general networks and to lines with transmission losses, and we conclude by discussing the application of this methodology to the New Zealand electricity transmission network.", "e:keyword": ["Electricity markets", "Transmission", "Game theory", "Cournot"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0831", "e:abstract": "This article presents the first models developed specifically for understanding the infiltration and interdiction of ongoing terror plots by undercover intelligence agents, and does so via novel application of ideas from queueing theory and Markov population processes. The resulting “terror queue” models predict the number of undetected terror threats in an area from agent activity/utilization data, and also estimate the rate with which such threats can be detected and interdicted. The models treat terror plots as customers and intelligence agents as servers. Agents spend all of their time either detecting and infiltrating new terror plots (in which case they are “available”), or interdicting already detected terror plots (in which case they are “busy”). Initially we examine a Markov model assuming that intelligence agents, while unable to detect all plots, never err by falsely detecting fake plots. While this model can be solved numerically, a simpler Ornstein-Uhlenbeck diffusion approximation yields some results in closed form while providing nearly identical numerical performance. The transient behavior of the terror queue model is discussed briefly along with a sample sensitivity analysis to study how model predictions compare to simulated results when using estimated versus known terror plot arrival rates. The diffusion model is then extended to allow for the false detection of fake plots. Such false detection is a real feature of counterterror intelligence given that intelligence agents or informants can make mistakes, as well as the proclivity of terrorists to deliberately broadcast false information. The false detection model is illustrated using suicide bombing data from Israel.", "e:keyword": ["Intelligence", "Counterterrorism", "Informants", "Markov models", "Diffusion models", "Queues", "Suicide bombings"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0832", "e:abstract": "We introduce a decision-support framework for the research and development (R&D) portfolio selection problem faced by a major U.S. semiconductor manufacturer. R&D portfolio selection is of critical importance to high-tech operations such as semiconductors and pharmaceuticals, because it determines the blend of technological development the firm must invest in its R&D resources. This R&D investment leads to differentiating technologies that drive the firm's market position. We developed a general, three-phase decision-support structure for the R&D portfolio selection problem. First is the <i>scenario generation phase</i>, where we transform qualitative assessment and market foresight from senior executives and market analysts into quantitative data. This is combined with the company's financial data (e.g., revenue projections) to generate scenarios of potential project revenue outcomes. This is followed by the <i>optimization phase</i>, where a multistage stochastic program (SP) is solved to maximize expected operating income (OI) subject to risk, product interdependency, capacity, and resource allocation constraints. The optimization procedure generates an efficient frontier of portfolios at different OI (return) and risk levels. The <i>refinement phase</i> offers managerial insights through a variety of analysis tools that utilize the optimization results. For example, the robustness of the optimal portfolio with respect to the risk level, the variability of a portfolio's OI, and the resource level usage as a function of the optimal portfolio can be analyzed and compared to any qualitatively suggested portfolio of projects. The decision-support structure is implemented, tested, and validated with various real-world cases and managerial recommendations. We discuss our implementation experience using a case example, and we explain how the system is incorporated into the corporate R&D investment decisions.", "e:keyword": ["R&D/project selection", "R&D project interdependency", "Multiperiod horizon", "Programming/stochastic", "Scenario generation", "Organizational studies/strategy", "Semiconductor industry"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0833", "e:abstract": "This paper considers the two-dimensional strip-packing problem (2SP) in which a set of rectangular items have to be orthogonally packed, without overlapping, into a strip of a given width and infinite height by minimizing the overall height of the packing. The 2SP is NP-hard in the strong sense and finds many practical applications. We propose reduction procedures, lower and upper bounds, and an exact algorithm for the 2SP. The new lower bounds are both combinatorial bounds and bounds derived from different relaxations of mathematical formulations of the 2SP. The new upper bounds are obtained by constructive heuristics based on different strategies to place the items into the strip. The new exact method is based on a branch-and-bound approach. Computational results on different sets of test problems derived from the literature show the effectiveness of the new lower and upper bounds and of the new exact algorithm.", "e:keyword": ["Production/scheduling", "Cutting stock/trim", "Programming", "Integer", "Algorithms", "Branch and bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0834", "e:abstract": "I present a self-contained introduction to multigrid methods with an emphasis on techniques relevant to dynamic programming and related problems. A probabilistic interpretation of the numerical principles is highlighted. Multigrid solvers are shown to be naturally matched to the challenges posed by intractable structural dynamic models routinely encountered in applied economics. I argue that multigrid techniques have potential to substantially extend the scale and complexity of models under consideration. Multigrid also provides a unified computational framework to extend model solvers to perform sensitivity analysis, calibration, estimation, and counterfactual policy experiments.", "e:keyword": ["Computational economics", "Dynamic programming", "Multigrid"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0835", "e:abstract": "Motivated by empirical observations, we assume that the inventory level of a company follows a mean-reverting process. The objective of the management is to keep this inventory level as close as possible to a given target; there is a running cost associated with the difference between the actual inventory level and the target. If inventory deviates too much from the target, management may perform an intervention in the form of either a purchase or a sale of an amount of the goods. There are fixed and proportional costs associated with each intervention. The objective of this paper is to find the optimal inventory levels at which interventions should be performed as well as the magnitudes of the interventions to minimize the total cost. We solve this problem by applying the theory of stochastic impulse control. Our analysis yields the optimal policy, which at times exhibits a behavior that is not intuitive.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0836", "e:abstract": "We analyze firms' investment incentives in markets where demand at spot markets is fluctuating and storability of the output is limited. Firms will then find it optimal to invest in a differentiated portfolio of technologies in order to serve fluctuating demand. For optimal behavior of firms, this has been analyzed in the so-called peak load pricing literature---cf. Crew and Kleindorfer [Crew, M., P. Kleindorfer. 1986. <i>The Economics of Public Utility Regulation</i>. MIT Press, Cambridge, MA]. We analyze the case of strategically behaved firms. We derive the equilibrium of the investment game and compare it to the benchmark case of optimal investment. We find that strategic firms have an incentive to overinvest in base load technologies but choose total capacities, which are too low from a welfare point of view.", "e:keyword": ["Electric utilities", "Technology choice", "Facilities and equipment planning", "Energy", "Strategic decisions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0837", "e:abstract": "The likelihood ratio method (LRM) is a technique for estimating derivatives of expectations through simulation. LRM estimators are constructed from the derivatives of probability densities of inputs to a simulation. We investigate the application of the likelihood ratio method for sensitivity estimation when the relevant densities for the underlying model are known only through their characteristic functions or Laplace transforms. This problem arises in financial applications, where sensitivities are used for managing risk and where a substantial class of models have transition densities known only through their transforms. We quantify various sources of errors arising when numerical transform inversion is used to sample through the characteristic function and to evaluate the density and its derivative, as required in LRM. This analysis provides guidance for setting parameters in the method to accelerate convergence.", "e:keyword": ["Simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0838", "e:abstract": "We present a risk-group oriented chronic disease progression model embedded within a metaheuristic-based optimization of the policy variables. Policy-makers are provided with Pareto-optimal screening schedules for risk groups by considering cost and effectiveness outcomes as well as budget constraints. The quality of the screening technology depends on risk group, disease stage, and time. As the metaheuristic solution technique, we use the Pareto ant colony optimization (P-ACO) algorithm for multiobjective combinatorial optimization problems, which is based on the ant colony optimization paradigm. Our approach is illustrated by a numerical example for breast cancer. For a 10-year time horizon, we provide cost-effective screening schedules for selected annual and total budgets. We then discuss policy implications of 16 mammography screening scenarios varying the screening schedule (annual, biennial, triennial, quadrennial) and the rate of women tested (25%, 50%, 75%, 100%). Due to the model's flexible structure, interventions for multiple chronic diseases can be considered simultaneously.", "e:keyword": ["Decision analysis", "Dynamic resource allocation", "Multicriteria optimization", "Metaheuristics", "Health care", "Chronic disease policy analysis", "Prevention"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0841", "e:abstract": "We study cooperative games with supermodular costs. We show that supermodular costs arise in a variety of situations; in particular, we show that the problem of minimizing a linear function over a supermodular polyhedron---a problem that often arises in combinatorial optimization---has supermodular optimal costs. In addition, we examine the computational complexity of the least core and least core value of supermodular cost cooperative games. We show that the problem of computing the least core value of these games is strongly NP-hard and, in fact, is inapproximable within a factor strictly less than 17/16 unless P = NP. For a particular class of supermodular cost cooperative games that arises from a scheduling problem, we show that the Shapley value---which, in this case, is computable in polynomial time---is in the least core, while computing the least core value is NP-hard.", "e:keyword": ["Games/group decisions", "Cooperative", "Analysis of algorithms", "Computational complexity", "Mathematics", "Combinatorics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0842", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0843", "e:abstract": "This paper provides a step-by-step guide to solving dynamic stochastic games using the homotopy method. The homotopy method facilitates exploring the equilibrium correspondence in a systematic fashion; it is especially useful in games that have multiple equilibria. We discuss the theory of the homotopy method and its implementation and present two detailed examples of dynamic stochastic games that are solved using this method.", "e:keyword": ["Homotopy method", "Dynamic stochastic games", "Markov-perfect equilibrium", "Equilibrium correspondence", "Game theory", "Industrial organization", "Numerical methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0844", "e:abstract": "The Greeks are the derivatives (also known as sensitivities) of the option prices with respect to market parameters. They play an important role in financial risk management. Among many Monte Carlo methods of estimating the Greeks, the classical pathwise method requires only the pathwise information that is directly observable from simulation and is generally easier to implement than many other methods. However, the classical pathwise method is generally not applicable to the Greeks of options with discontinuous payoffs and the second-order Greeks. In this paper, we generalize the classical pathwise method to allow discontinuity in the payoffs. We show how to apply the new pathwise method to the first- and second-order Greeks and propose kernel estimators that require little analytical efforts and are very easy to implement. The numerical results show that our estimators work well for practical problems.", "e:keyword": ["Finance", "Securities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0845", "e:abstract": "An important problem in the theory of dynamic programming is that of characterizing sufficient conditions under which the optimal policies for Markov decision processes (MDPs) under the infinite-horizon discounted cost criterion converge to an optimal policy under the average cost criterion as the discount factor approaches 1. In this paper, we provide, for stochastic inventory models, a set of such sufficient conditions. These conditions, unlike many others in the dynamic programming literature, hold when the action space is noncompact and the underlying transition law is weakly continuous. Moreover, we verify that these conditions hold for almost all conceivable single-stage inventory models with few assumptions on cost and demand parameters. As a consequence of our analysis, we partially characterize, for the first time, optimal policies for the following inventory systems under the infinite-horizon average-cost criterion, which have thus far been a challenge: (a) capacitated systems with setup costs, (b) uncapacitated systems with convex ordering costs plus a setup cost, and (c) systems with lost sales and lead times.", "e:keyword": ["Inventory", "Dynamic programming", "Infinite horizon", "Optimal policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0846", "e:abstract": "The objective of the classical minimum cost flow problem is to send units of a good that reside at one or more points in a network (sources or supply nodes) with arc capacities to one or more other points in the network (sinks or demand nodes), incurring minimum cost. We develop fast algorithms for previously unstudied specially structured minimum cost flow problems that have applications in many areas, such as locomotive and airline scheduling, repositioning of empty rail freight cars, highway and river transportation, congestion pricing, shop loading, and production planning. First, we consider the case where the <i>n</i><sub>1</sub> supply and <i>n</i><sub>2</sub> demand nodes lie on a circle (or line) (<i>n</i> = <i>n</i><sub>1</sub> + <i>n</i><sub>2</sub>) and flow is allowed only in one direction; our algorithm solves this problem in <i>O</i>(<i>n</i>) time. Next, we consider a constrained version of this problem and show that it can be solved in <i>O</i>(<i>n</i> log <i>n</i><sub>2</sub>) time. Finally, we consider the version where the nodes lie on a circle (or line), flow is allowed in both directions, and the costs of flow between two nodes in the clockwise and the counterclockwise direction are different; our algorithm solves this problem in <i>O</i>(<i>n</i> log <i>n</i>) time. Our algorithms are based on the successive shortest-path algorithm for the minimum cost flow problem. We exploit the special structure of the problem and use advanced data structures, when required, to achieve short run times.", "e:keyword": ["Networks", "Flow algorithms", "Analysis of algorithms", "Computational complexity", "Networks", "Minimum cost flow problem", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0847", "e:abstract": "We formulate a risk-averse two-stage stochastic linear programming problem in which unresolved uncertainty remains after the second stage. The objective function is formulated as a composition of conditional risk measures. We analyze properties of the problem and derive necessary and sufficient optimality conditions. Next, we construct a new decomposition method for solving the problem that exploits the composite structure of the objective function. We illustrate its performance on a portfolio optimization problem.", "e:keyword": ["Stochastic programming", "Risk", "Two-stage models", "Decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0848", "e:abstract": "Many real-world systems operate in a decentralized manner, where individual operators interact with varying degrees of cooperation and self motive. In this paper, we study transportation networks that operate as an alliance among different carriers. In particular, we study alliance formation among carriers in liner shipping. We address tactical problems such as the design of large-scale networks (that result from integrating the service networks of different carriers in an alliance) and operational problems such as the allocation of limited capacity on a transportation network among the carriers in the alliance. We utilize concepts from mathematical programming and game theory and design a mechanism to guide the carriers in an alliance to pursue an optimal collaborative strategy. The mechanism provides side payments to the carriers, as an added incentive, to motivate them to act in the best interest of the alliance while maximizing their own profits. Our computational results suggest that the mechanism can be used to help carriers form sustainable alliances.", "e:keyword": ["Transportation", "Liner shipping", "Alliances", "Resource allocation", "Programming", "Inverse optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0851", "e:abstract": "This paper introduces and studies the <i>maximum k-plex problem</i>, which arises in social network analysis and has wider applicability in several important areas employing graph-based data mining. After establishing NP-completeness of the decision version of the problem on arbitrary graphs, an integer programming formulation is presented, followed by a polyhedral study to identify combinatorial valid inequalities and facets. A branch-and-cut algorithm is implemented and tested on proposed benchmark instances. An algorithmic approach is developed exploiting the graph-theoretic properties of a <i>k</i>-plex that is effective in solving the problem to optimality on very large, sparse graphs such as the <i>power law graphs</i> frequently encountered in the applications of interest.", "e:keyword": ["Maximum k-plex problem", "Maximum clique problem", "Social network analysis", "Clique relaxations", "Cohesive subgroups", "Scale-free graphs", "Power law graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0852", "e:abstract": "Dynamic in-game advertising is a new form of advertising in which ads are served to video game consoles in real time over the Internet. We present a model for the in-game ad-scheduling problem faced by Massive Inc., a wholly owned subsidiary of Microsoft, and a leading global network provider of in-game ad space. Our model has two components: (1) a linear program (solved periodically) establishes target service rates, and (2) a real-time packing heuristic (run whenever a player enters a new level) tracks these service rates. We benchmark our model against Massive's legacy algorithm: When tested on historical data, we observe (1) an 80%--87% reduction in make-good costs (depending on forecast accuracy), and (2) a shift in the age distribution of served ad space, leaving more premium inventory open for future sales. As a result of our work, Massive has increased the number of unique individuals that see each campaign by, on average, 26% per week and achieved 33% smoother campaign delivery as measured by standard deviation of hourly impressions served.", "e:keyword": ["Dynamic in-game advertising", "Video game advertising", "Display advertising", "Revenue management", "Linear programming", "Goal programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0853", "e:abstract": "Quasi-Monte Carlo (QMC) methods are playing an increasingly important role in the pricing of complex financial derivatives. For models in which the prices of the underlying assets are driven by Brownian motions, the performance of QMC methods is known to depend crucially on the construction of Brownian motions. This paper focuses on the impact of various constructions. Although the Brownian bridge (BB) construction often yields very good results, as Papageorgiou pointed out, there are financial derivatives for which the BB construction performs badly [Papageorgiou, A. 2002. The Brownian bridge does not offer a consistent advantage in quasi-Monte Carlo integration. <i>J. Complexity</i> <b>18</b>(1) 171--186]. In this paper we first extend Papageorgiou's analysis to establish an equivalence principle: if the BB construction (or any other construction) is the preferred construction for a particular financial derivative, then for any other construction, there is another financial derivative for which the latter construction is the preferred one. In this sense, all methods of construction are equivalent and no method is consistently superior to others; it all depends on the particular financial derivative. We then show how to find a good construction for a particular class of financial derivatives. In practice, our strategy is to find a good construction for an “easy” problem and then apply it to more complicated problems related to the easy one. This strategy is applied to the arithmetic Asian options (including Bermudan Asian options) based on the weighted average of the stock prices. We do this by studying a simpler problem, namely, the geometric Asian option, for which the best construction is easily available, and applying it to the arithmetic Asian option. Numerical experiments confirm the success of this strategy: whereas in QMC all the common methods (the standard method, BB, and principal component analysis) may lose their power in some situations, the new method behaves very well in all cases. Further large variance reduction can be achieved in combination with a control variate. The new method can be interpreted as a practical way of reducing the effective dimension for some class of functions.", "e:keyword": ["Finance", "Financial engineering", "Asset pricing", "Simulation", "Quasi-Monte Carlo methods", "Dimension reduction", "Brownian bridge", "Principal component analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0854", "e:abstract": "In the interest of deriving classifiers that are robust to outlier observations, we present integer programming formulations of Vapnik's support vector machine (SVM) with the ramp loss and hard margin loss. The ramp loss allows a maximum error of 2 for each training observation, while the hard margin loss calculates error by counting the number of training observations that are in the margin or misclassified outside of the margin. SVM with these loss functions is shown to be a consistent estimator when used with certain kernel functions. In computational studies with simulated and real-world data, SVM with the robust loss functions ignores outlier observations effectively, providing an advantage over SVM with the traditional hinge loss when using the linear kernel. Despite the fact that training SVM with the robust loss functions requires the solution of a quadratic mixed-integer program (QMIP) and is NP-hard, while traditional SVM requires only the solution of a continuous quadratic program (QP), we are able to find good solutions and prove optimality for instances with up to 500 observations. Solution methods are presented for the new formulations that improve computational performance over industry-standard integer programming solvers alone.", "e:keyword": ["Statistics", "Pattern analysis", "Support vector machines", "Programming", "Integer applications", "Quadratic integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0855", "e:abstract": "A multiname credit derivative is a security that is tied to an underlying portfolio of corporate bonds and has payoffs that depend on the loss due to default in the portfolio. The value of a multiname derivative depends on the distribution of portfolio loss at multiple horizons. Intensity-based models of the loss point process that are specified without reference to the portfolio constituents determine this distribution in terms of few economically meaningful parameters and lead to computationally tractable derivatives valuation problems. However, these models are silent about the portfolio constituent risks. They cannot be used to address applications that are based on the relationship between portfolio and component risks, for example, constituent risk hedging. This paper develops a method that extends these models to the constituents. We use random thinning to decompose the portfolio intensity into a sum of constituent intensities. We show that a thinning process, which allocates the portfolio intensity to constituents, uniquely exists, and is a probabilistic model for the next-to-default. We derive a formula for the constituent default probability in terms of the thinning process and the portfolio intensity, and develop a semi-analytical transform approach to evaluate it. The formula leads to a calibration scheme for the thinning processes and an estimation scheme for constituent hedge sensitivities. An empirical analysis for September 2008 shows that the constituent hedges generated by our method outperform the hedges prescribed by the Gaussian copula model, which is widely used in practice.", "e:keyword": ["Finance", "Securities", "Financial institutions", "Banks", "Probability", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0856", "e:abstract": "This paper examines consensus building in AHP-group decision making from a Bayesian perspective. In accordance with the <i>multicriteria procedural rationality paradigm</i>, the methodology employed in this study permits the automatic identification, in a local context, of “agreement” and “disagreement” zones among the actors involved. This approach is based on the analysis of the pairwise comparison matrices provided by the actors themselves. In addition, the study integrates the attitudes of the actors implicated in the decision-making process and puts forward a number of semiautomatic initiatives for establishing consensus. This information is given to the actors as the first step in the negotiation processes. The knowledge obtained will be incorporated into the system via the learning process developed during the resolution of the problem. The proposed methodology, valid for the analysis of incomplete or imprecise pairwise comparison matrices, is illustrated by an example.", "e:keyword": ["Analytic hierarchy process", "Group decision making", "Bayesian analysis", "Negotiation attitude", "Consensus building", "Multiple criteria", "Decision analysis", "Decision making", "Bayesian"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0857", "e:abstract": "Given a set of items with associated deterministic weights and random rewards, the <i>adaptive</i> stochastic knapsack problem (adaptive SKP) maximizes the probability of reaching a predetermined target reward level when items are inserted <i>sequentially</i> into a capacitated knapsack before the reward of each item is realized. This model arises in resource allocation problems that permit or require sequential allocation decisions in a probabilistic setting. One particular application is in obsolescence inventory management. In this paper, the adaptive SKP is formulated as a dynamic programming (DP) problem for discrete random rewards. The paper also presents a heuristic that mixes adaptive and static policies to overcome the “curse of dimensionality” in the DP. The proposed heuristic is extended to problems with normally distributed random rewards. The heuristic can solve large problems quickly, and its solution always outperforms a static policy. The numerical study indicates that a near-optimal solution can be obtained by using an algorithm with limited look-ahead capabilities.", "e:keyword": ["Dynamic programming", "Sequential decision analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0858", "e:abstract": "We generalize analysis of competition among newsvendors to a setting in which competitors possess asymmetric information about future demand realizations, and this information is limited to knowledge of the support of demand distribution. In such a setting, traditional expectation-based optimization criteria are not adequate, and therefore we focus on the alternative criterion used in the robust optimization literature: the absolute regret minimization. We show existence and derive closed-form expressions for the robust optimization Nash equilibrium solution for a game with an arbitrary number of players. This solution allows us to gain insight into the nature of robust asymmetric newsvendor competition. We show that the competitive solution in the presence of information asymmetry is an intuitive extension of the robust solution for the monopolistic newsvendor problem, which allows us to distill the impact of both competition and information asymmetry. In addition, we show that, contrary to the intuition, a competing newsvendor does not necessarily benefit from having better information about its own demand distribution than its competitor has.", "e:keyword": ["Robust optimization", "Newsvendor competition", "Absolute regret", "Asymmetric information", "Robust optimization equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0861", "e:abstract": "Many intensive care units (ICUs) face overcrowding. One response to this overcrowding is to bump ICU patients to other departments of the hospital to make room for new patient arrivals. Such bumping clearly has the potential to reduce quality of care. In this paper we develop a stochastic model of a single ICU with patient bumping. The purpose of this model is to enable planners to predict performance, in terms of bumping, under differing arrival patterns and capacity. We develop a Markov chain model and a new aggregation-disaggregation algorithm for this problem that enables us to keep track of the time in system for each patient despite the high dimensionality of the problem. Our approach allows for more accurate modeling of the system than previous work that assumed an exponential distribution for length of stay (LOS). We also demonstrate the superior computational efficiency of our approach over the Gauss-Seidel iterative method for solving the Markov chain. Finally, we use the model to explore how different surgery schedules influence bumping rates.", "e:keyword": ["Health care", "Markov chain", "ICU", "Bumping", "Aggregation-disaggregation methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0862", "e:abstract": "In this paper we consider the problem of a firm that faces a stochastic (Poisson) demand and must replenish from a market in which prices fluctuate, such as a commodity market. We describe the price evolution as a continuous stochastic process and we focus on commonly used processes suggested by the financial literature, such as the geometric Brownian motion and the Ornstein-Uhlenbeck process. It is well known that under variable purchase price, a price-dependent base-stock policy is optimal. Using the single-unit decomposition approach, we explicitly characterize the optimal base-stock level using a series of threshold prices. We show that the base-stock level is first increasing and then decreasing in the current purchase price. We provide a procedure for calculating the thresholds, which yields closed-form solutions when price follows a geometric Brownian motion and implicit solutions under the Ornstein-Uhlenbeck price model. In addition, our numerical study shows that the optimal policy performs much better than inventory policies that ignore future price evolution, because it tends to place larger orders when prices are expected to increase.", "e:keyword": ["Production/inventory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0863", "e:abstract": "We study competition in a supply chain where multiple manufacturers compete in quantities to supply a set of products to multiple risk-averse retailers who compete in quantities to satisfy the uncertain consumer demand. For the symmetric supply chain, we give closed-form expressions for the unique equilibrium. We find that, provided there is a sufficiently large number of manufacturers and retailers, the supply chain efficiency (the ratio of the aggregate utility in the decentralized and centralized chains) can be raised to 1 by inducing the right degree of retailer differentiation. Also, risk aversion results in triple marginalization: retailers require a strictly positive margin to distribute even when they are perfectly competitive, because otherwise they are unwilling to undertake the risk associated with the uncertainty in demand. For the asymmetric supply chain, we show how numerical optimization can be used to compute the equilibria, and we find that the supply chain efficiency may drop sharply with the asymmetry of either manufacturers or retailers. We also find that the introduction of asymmetric product assortment reduces the degree of competition among retailers and thus has an effect similar to that of reducing the number of retailers. We show that, unlike in the symmetric chain, the asymmetric chain efficiency depends on product differentiation and risk aversion because of the interaction between these features and the asymmetry of manufacturers and retailers.", "e:keyword": ["Supply chain", "Competition", "Efficiency"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0864", "e:abstract": "Collateralized debt obligations, which are securities with payoffs that are tied to the cash flows in a portfolio of defaultable assets such as corporate bonds, play a significant role in the financial crisis that has spread throughout the world. Insufficient capital provisioning due to flawed and overly optimistic risk assessments is at the center of the problem. This paper develops stochastic methods to measure the risk of positions in collateralized debt obligations and related instruments tied to an underlying portfolio of defaultable assets. It proposes an adaptive point process model of portfolio default timing, a maximum likelihood method for estimating point process models that is based on an acceptance/rejection resampling scheme, and statistical tests for model validation. To illustrate these tools, they are used to estimate the distribution of the profit or loss generated by positions in multiple tranches of a collateralized debt obligation that references the CDX High Yield portfolio and the risk capital required to support these positions.", "e:keyword": ["Correlated default risk", "Collateralized debt obligation", "Portfolio credit derivative", "Actual measure", "Point process", "Intensity", "Resampling", "Thinning", "Acceptance/rejection sampling", "Exact simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0865", "e:abstract": "In this paper we study resource allocation problems that involve multiple self-interested parties or players and a central decision maker. We introduce and study the price of fairness, which is the relative system efficiency loss under a “fair” allocation assuming that a fully efficient allocation is one that maximizes the sum of player utilities. We focus on two well-accepted, axiomatically justified notions of fairness, viz., proportional fairness and max-min fairness. For these notions we provide a tight characterization of the price of fairness for a broad family of problems.", "e:keyword": ["Analysis of algorithms", "Games/group decisions", "Bargaining", "Programming", "Multiple criteria", "Nonlinear", "Applications", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0866", "e:abstract": "We consider an assortment optimization problem where a retailer chooses an assortment of products that maximizes the profit subject to a capacity constraint. The demand is represented by a multinomial logit choice model. We consider both the static and dynamic optimization problems. In the static problem, we assume that the parameters of the logit model are known in advance; we then develop a simple algorithm for computing a profit-maximizing assortment based on the geometry of lines in the plane and derive structural properties of the optimal assortment. For the dynamic problem, the parameters of the logit model are unknown and must be estimated from data. By exploiting the structural properties found for the static problem, we develop an adaptive policy that learns the unknown parameters from past data and at the same time optimizes the profit. Numerical experiments based on sales data from an online retailer indicate that our policy performs well.", "e:keyword": ["Assortment optimization", "Multinomial logit choice model", "Capacity constraint"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0867", "e:abstract": "We consider a pricing problem in an environment where the customers' willingness-to-pay (WtP) distribution may change at some point over the selling horizon. Customers arrive sequentially and make purchase decisions based on a quoted price and their private reservation price. The seller knows the WtP distribution pre- and postchange but does not know the time at which this change occurs. The performance of a pricing policy is measured in terms of regret: the loss in revenues relative to an oracle that knows the time of change prior to the start of the selling season. We derive lower bounds on the worst-case regret and develop pricing strategies that achieve the order of these bounds, thus establishing the complexity of the pricing problem. Our results shed light on the role of price experimentation and its necessity for optimal detection of changes in market response/WtP. Our formulation allows for essentially arbitrary consumer WtP distributions and purchase request patterns.", "e:keyword": ["Pricing", "Nonstationary demand", "Estimation", "Detection", "Change-point", "Price experimentation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0868", "e:abstract": "We model the problem of managing capacity in a build-to-order environment as a Brownian drift control problem and seek a policy that minimizes the long-term average cost. We assume the controller can, at some cost, shift the processing rate among a finite set of alternatives, for example by adding or removing staff, increasing or reducing the number of shifts, or opening or closing production lines. The controller incurs a cost for capacity per unit time and a delay cost that reflects the opportunity cost of revenue waiting to be recognized or the customer service impacts of delaying delivery of orders. Furthermore, he incurs a cost per unit to reject orders or idle resources as necessary to keep the workload of waiting orders within a prescribed range. We introduce a practical restriction on this problem, called the (S-script)-restricted Brownian control problem, and show how to model it via a structured linear program. We demonstrate that an optimal solution to the (S-script)-restricted problem can be found among a special class of policies called deterministic nonoverlapping control band policies. These results exploit apparently new relationships between complementary dual solutions and relative value functions that allow us to obtain a lower bound on the average cost of any nonanticipating policy for the problem, even without the (S-script) restriction. Under mild assumptions on the cost parameters, we show that our linear programming approach is asymptotically optimal for the unrestricted Brownian control problem in the sense that by appropriately selecting the (S-script)-restricted problem, we can ensure its solution is within an arbitrary finite tolerance of a lower bound on the average cost of any nonanticipating policy for the unrestricted Brownian control problem.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Probability", "Stochastic model applications", "Programming", "Linear"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0869", "e:abstract": "The efficient operation of airports, and runways in particular, is critical to the throughput of the air transportation system as a whole. Scheduling arrivals and departures at runways is a complex problem that needs to address diverse and often competing considerations of efficiency, safety, and equity among airlines. One approach to runway scheduling that arises from operational and fairness considerations is that of constrained position shifting (CPS), which requires that an aircraft's position in the optimized sequence not deviate significantly from its position in the first-come-first-served sequence. This paper presents a class of scalable dynamic programming algorithms for runway scheduling under constrained position shifting and other system constraints. The results from a prototype implementation, which is fast enough to be used in real time, are also presented.", "e:keyword": ["Transportation", "Runway scheduling under constrained position shifting", "Dynamic programming/optimal control", "Deterministic polynomial-time scheduling algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0870", "e:abstract": "We consider an assemble-to-order system where one of the components faces uncertainty in the supply process in which the actual available quantity is equal to some random fraction of the production quantity. Demand is assumed to be price-dependent. We analyze how the supply uncertainty of one component affects the product pricing and production quantities of all the components under the assembly structure. We show that it is profitable for the firm to assemble the product only if the product price exceeds a certain threshold. This price threshold increases as the unit cost of each component or the degree of variability of the supply reliability distribution increases, but it is independent of the underlying demand function and demand distribution. Also, the optimal product price decreases as supply uncertainty decreases. We further show that under deterministic demand, the components can be managed independently such that the production quantity or unit cost of the component with supply uncertainty does not affect the optimal production quantity of the other components, as long as it is profitable to assemble the product. However, when demand is stochastic, the optimal production quantity of each component depends on the supply reliability distribution as well as the unit costs of the other components. For a fixed product price, the optimal production quantities of the components are smaller when the unit product price is low, and they are higher when the unit product price is high as compared to the case with no supply uncertainty.", "e:keyword": ["Assembly systems", "Inventory management", "Supply uncertainty", "Random yields", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0871", "e:abstract": "Based on our work with ConAgra Foods (http://www.conagrafoods.com), a leading U.S. food manufacturer, we study a large-scale production-planning problem. The problem incorporates several distinguishing characteristics of production in the processed-food industry, including (i) production patterns that define specific combinations of weeks in which products can be produced, (ii) food groups that classify products based on the allergens they contain, (iii) sequence-dependent setup times, and (iv) manufacture of a large number of products (typically, around 200--250) on multiple production lines (typically, around 15--20) in the presence of significant inventory holding costs and production setup costs. The objective is to obtain a minimum-cost four-week cyclic schedule to resolve three basic decisions: (a) the assignment of products to each line, (b) the partitioning of the demand of each product over the lines to which it is assigned, and (c) the sequence of production on each line.We show that the general problem is strongly NP-hard. To develop intuition via theoretical analysis, we first obtain a polynomially solvable special case by sacrificing as little of its structure as possible and then analyzing the impact of imposing production patterns. A mixed-integer programming model of the general problem allows us to assess the average impact of production patterns and production capacities on the cost of an optimal schedule. Next, to solve practical instances of the problem, we develop an easy-to-implement heuristic. We first demonstrate the effectiveness of the heuristic on a comprehensive test bed of instances; the average percentage gap of the heuristic solution from the optimum is about 3%. Then, we show savings of about 28% on a real-world instance (283 products, 17 production lines) by comparing the schedule obtained from the heuristic to one that was in use (at ConAgra) based on an earlier consultant's work. Finally, we discuss the IT infrastructure implemented to enable the incorporation of optimized (or near-optimized) solutions for ongoing use.", "e:keyword": ["Production planning", "Food industry", "Integer programming algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0872", "e:abstract": "We investigate the (<i>S</i> - 1, <i>S</i>) inventory policy under stuttering Poisson demand and generally distributed lead time when the excess demand is lost. We correct results presented in Feeney and Sherbrooke's seminal paper [Feeney, G. J., C. C. Sherbrooke. 1966. The (<i>S</i> - 1, <i>S</i>) inventory policy under compound Poisson demand. <i>Management Sci.</i> <b>12</b>(5) 391--411] and note that the stationary distribution of units on order for the general compound Poisson demand case is still an open question.", "e:keyword": ["Lost sales", "Stuttering Poisson process", "Reversible Markov chain"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0873", "e:abstract": "We derive a knowledge gradient policy for an optimal learning problem on a graph, in which we use sequential measurements to refine Bayesian estimates of individual edge values in order to learn about the best path. This problem differs from traditional ranking and selection in that the implementation decision (the path we choose) is distinct from the measurement decision (the edge we measure). Our decision rule is easy to compute and performs competitively against other learning policies, including a Monte Carlo adaptation of the knowledge gradient policy for ranking and selection.", "e:keyword": ["Optimal learning", "Knowledge gradient", "Bayesian learning", "Stochastic shortest paths", "Ranking and selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0874", "e:abstract": "Although the impact of layout on the productivity of manufacturing systems is well recognized, a quantification of this impact is an issue that is often ignored or crudely approximated in practice. When evaluating competing layouts for a manufacturing system, the trade-off between their relative benefits and their relative costs underlines the need for a reasonably accurate comparison of the productivity offered by these potential layouts. In this paper, we argue for this approach by comparing the productivity of two well-known layouts in robotic-cell manufacturing: circular and linear.We consider the problem of optimizing throughput in single-gripper, bufferless robotic cells that produce identical parts under the free-pickup criterion and the additive-travel-time metric. For cells with a circular layout, we show that the problem of finding an optimal 1-unit cycle is NP-hard. Our main algorithmic result is a polynomial-time 5/3-approximation algorithm for this problem. We then demonstrate that our algorithm provides near-optimal solutions by compiling its performance on an extensive test bed of practically-relevant instances. Finally, we use the algorithm to assess the increase in throughput for cells with a circular layout over those with a linear layout. We show that a circular layout offers a significant improvement in productivity and demonstrate the robustness of this improvement by examining the sensitivity with respect to changes in the design parameters of the robotic cell. Thus, our work provides operations managers with a tool to trade off the resulting increase in revenue with the additional cost of acquiring and maintaining a robot that can exploit a circular layout.", "e:keyword": ["Productivity", "Manufacturing systems", "Robotic cells", "Linear and circular layouts", "Throughput optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0875", "e:abstract": "This paper presents an exact algorithm for solving strategic and tactical multiperiod vehicle routing problems that can be modeled as period vehicle routing problems (PVRPs). The PVRP is defined on a time horizon of several days and consists of assigning appropriate combinations of delivery to customers and designing a set of delivery routes for every day of the planning period. The objective is to service all customers assigned to each day minimizing the overall routing cost. This paper describes an integer programming formulation of the PVRP that is used to derive different lower bounds and an exact solution method. Computational results on test instances from the literature and on new sets of test instances show the effectiveness of the proposed method.", "e:keyword": ["Period vehicle routing", "Set partitioning", "Dual ascent", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0876", "e:abstract": "We study the effect of downstream competition on incentives for demand forecast investments in supply chains. We show that with common pricing schemes, such as wholesale price or two-part tariffs, downstream firms under Cournot competition overinvest in demand forecasting. Analyzing the determinants of overinvestment, we demonstrate that under wholesale price contracts and two-part tariffs, total demand forecast investment can be very significant, and as a result, the supply chain can suffer substantial losses. We show that an increased number of competing retailers and uncertainty in consumer demand tend to increase inefficiency, whereas increased consumer market size and demand forecast costs reduce the loss in supply chain surplus. We identify the causes of inefficiency, and to coordinate the channel with forecast investments, we explore contracts in the general class of market-based contracts used in practice. When retailers' forecast investments are not observable, such a contract that employs an index-price can fully coordinate the supply chain. When forecast investments are observable to others, however, the retailers engage in an “arms race” for forecast investment, which can result in a significant increase in overinvestment and reduction in supply chain surplus. Furthermore, in that case, simple market-based contracts cannot coordinate the supply chain. To solve this problem, we propose a uniform-price divisible-good auction-based contracting scheme, which can achieve full coordination when forecast investments are observable. We also demonstrate the desirable properties for implementability of our proposed coordinating contracting schemes, including incentive-compatible and reliable demand forecast information revelation by the retailers, and being regret-free.", "e:keyword": ["Supply chain management", "Forecasting", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0877", "e:abstract": "Breast cancer is the most common non-skin cancer affecting women in the United States, where every year more than 20 million mammograms are performed. Breast biopsy is commonly performed on the suspicious findings on mammograms to confirm the presence of cancer. Currently, 700,000 biopsies are performed annually in the U.S.; 55%--85% of these biopsies ultimately are found to be benign breast lesions, resulting in unnecessary treatments, patient anxiety, and expenditures. This paper addresses the decision problem faced by radiologists: When should a woman be sent for biopsy based on her mammographic features and demographic factors? This problem is formulated as a finite-horizon discrete-time Markov decision process. The optimal policy of our model shows that the decision to biopsy should take the age of patient into account; particularly, an older patient's risk threshold for biopsy should be higher than that of a younger patient. When applied to the clinical data, our model outperforms radiologists in the biopsy decision-making problem. This study also derives structural properties of the model, including sufficiency conditions that ensure the existence of a control-limit type policy and nondecreasing control-limits with age.", "e:keyword": ["Markov decision processes", "Dynamic programming", "Control-limit policy", "Service operations", "Breast cancer", "Mammography", "Breast biopsy", "Medical decision-making"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0878", "e:abstract": "Motivated by call centers, we study large-scale service systems with homogeneous impatient customers and heterogeneous servers; the servers differ with respect to their speed of service. For this model, we propose staffing and routing rules that are jointly asymptotically optimal in the heavy-traffic many-server QED, ED, and ED + QED regimes, respectively. For the QED regime, our proposed routing rule is FSF, that assigns customers to the fastest server available first. In the ED and ED + QED regimes, all work-conserving policies perform (asymptotically) equally well. In all these regimes, the form of the asymptotically optimal staffing is consistent with the asymptotically optimal staffing in the same regimes in the single-pool case, respectively. In particular, the total service capacity is (asymptotically) equal to a term that is proportional to the arrival rate plus, possibly, a term that is proportional to the square-root of the arrival rate, with both terms being regime dependent. Our specific proposed approximation for the optimal staffing vector is obtained via a straightforward solution to a deterministic optimization problem subject to a linear feasible region.", "e:keyword": ["Queues", "Applications", "Balking and reneging", "Diffusion models", "Limit theorems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0879", "e:abstract": "Performance analysis of queueing networks is one of the most challenging areas of queueing theory. Barring very specialized models such as product-form type queueing networks, there exist very few results that provide provable nonasymptotic upper and lower bounds on key performance measures.In this paper we propose a new performance analysis method, which is based on the robust optimization. The basic premise of our approach is as follows: rather than assuming that the stochastic primitives of a queueing model satisfy certain probability laws---such as i.i.d. interarrival and service times distributions---we assume that the underlying primitives are deterministic and satisfy the <i>implications</i> of such probability laws. These implications take the form of simple linear constraints, namely, those motivated by the law of the iterated logarithm (LIL). Using this approach we are able to obtain performance bounds on some key performance measures. Furthermore, these performance bounds imply similar bounds in the underlying stochastic queueing models.We demonstrate our approach on two types of queueing networks: (a) tandem single-class queueing network and (b) multiclass single-server queueing network. In both cases, using the proposed robust optimization approach, we are able to obtain <i>explicit</i> upper bounds on some steady-state performance measures. For example, for the case of TSC system we obtain a bound of the form <i>C</i>(1 - (rho))<sup>-1</sup> ln ln((1 - (rho))<sup>-1</sup>) on the expected steady-state sojourn time, where <i>C</i> is an explicit constant and (rho) is the bottleneck traffic intensity. This qualitatively agrees with the correct heavy traffic scaling of this performance measure up to the ln ln((1 - (rho))<sup>-1</sup>) correction factor.", "e:keyword": ["Queues", "Tandem", "Optimization", "Probability", "Applications", "Programming", "Linear applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0880", "e:abstract": "We describe a research project to design a distributed optimization environment in which solvers, modeling languages, registries, analyzers, and simulation engines can be implemented as services and utilities under a unified framework. Our work, which we call optimization services or OS, defines standards for all activities necessary to support decentralized optimization on the Internet: representation of optimization instances, results, and solver options; communication between clients and solvers; and discovery and registration of optimization-related software using the concept of Web services. In this paper we place emphasis on issues in distributed computing that are posed by the special character of optimization. We also describe a reference implementation that is freely available as an open-source project of COIN-OR.", "e:keyword": ["Optimization", "Modeling languages", "Distributed computing", "XML", "Web services", "COIN-OR"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0881", "e:abstract": "The pickup and delivery problem with time windows (PDPTW) is a generalization of the vehicle routing problem with time windows. In the PDPTW, a set of identical vehicles located at a central depot must be optimally routed to service a set of transportation requests subject to capacity, time window, pairing, and precedence constraints. In this paper, we present a new exact algorithm for the PDPTW based on a set-partitioning--like integer formulation, and we describe a bounding procedure that finds a near-optimal dual solution of the LP-relaxation of the formulation by combining two dual ascent heuristics and a cut-and-column generation procedure. The final dual solution is used to generate a reduced problem containing only the routes whose reduced costs are smaller than the gap between a known upper bound and the lower bound achieved. If the resulting problem has moderate size, it is solved by an integer programming solver; otherwise, a branch-and-cut-and-price algorithm is used to close the integrality gap. Extensive computational results over the main instances from the literature show the effectiveness of the proposed exact method.", "e:keyword": ["Pickup and delivery", "Set partitioning", "Dual ascent", "Column generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0882", "e:abstract": "In this paper, we give a finite disjunctive programming procedure to obtain the convex hull of general mixed-integer linear programs (MILP) with bounded integer variables. We propose a finitely convergent <i>convex hull tree algorithm</i> that constructs a linear program that has the same optimal solution as the associated MILP. In addition, we combine the standard notion of sequential cutting planes with ideas underlying the convex hull tree algorithm to help guide the choice of disjunctions to use within a cutting plane method. This algorithm, which we refer to as the <i>cutting plane tree algorithm</i>, is shown to converge to an integral optimal solution in finitely many iterations. Finally, we illustrate the proposed algorithm on three well-known examples in the literature that require an infinite number of elementary or split disjunctions in a rudimentary cutting plane algorithm.", "e:keyword": ["Mixed-integer programming", "Disjunctive programming", "Convex hull", "Finite convergence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0887", "e:abstract": "This paper has two aims. First, to present cases in which scientists developed a defensive system for their homeland: Blackett and the air defense of Britain in WWII, Forrester and the SAGE system for North America in the Cold War, and Archimedes' work defending Syracuse during the Second Punic War. In each case the historical context and the individual's other achievements are outlined, and a description of the contribution's relationship to OR/MS is given.The second aim is to consider some of the features the cases share and examine them in terms of contemporary OR/MS methodology. Particular reference is made to a recent analysis of the field's strengths and weaknesses. This allows both a critical appraisal of the field and a set of potential responses for strengthening it. Although a mixed set of lessons arise, the overall conclusion is that the cases are examples to build on and that OR/MS retains the ability to do high stakes work.", "e:keyword": ["OR/MS implementation", "Applications", "OR/MS philosophy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0888", "e:abstract": "Consider selling bundles of indivisible goods to buyers with concave utilities that are additively separable in money and goods. We propose an ascending auction for the case when the seller is constrained to sell bundles whose elements form a basis of a matroid. It extends easily to polymatroids. Applications include scheduling, allocation of homogeneous goods, and spatially distributed markets, among others. Our ascending auction induces buyers to bid truthfully and returns the economically efficient basis. Unlike other ascending auctions for this environment, ours runs in pseudopolynomial or polynomial time. Furthermore, we prove the impossibility of an ascending auction for nonmatroidal independence <i>set</i>-systems.", "e:keyword": ["Matroid", "Vickrey", "Multi-item", "Combinatorial", "Auction", "Polymatroid"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0889", "e:abstract": "We consider an assembly system with multiple stages, multiple items, and multiple customer classes. The system consists of <i>m</i> production facilities, each producing a different item. Items are produced in variable batch sizes, one batch at a time, with exponentially distributed batch production times. Demand from each class takes place continuously over time according to a compound Poisson process. At each decision epoch, we must determine whether or not to produce an item and, should demand from a particular class arise, whether or not to satisfy it from existing inventory, if any is available. We formulate the problem as a Markov decision process and use it to characterize the structure of the optimal policy. In contrast to systems with exogenous and deterministic production lead times, we show that the optimal production policy for each item is a state-dependent base-stock policy with the base-stock level nonincreasing in the inventory level of items that are downstream and nondecreasing in the inventory level of all other items. For inventory allocation, we show that the optimal policy is a multilevel state-dependent rationing policy with the rationing level for each demand class nonincreasing in the inventory level of all nonend items. We also show how the optimal control problem can be reformulated in terms of echelon inventory and how the essential features of the optimal policy can be reinterpreted in terms of echelon inventory.", "e:keyword": ["Production and inventory control", "Systems with multiple echelons", "Inventory rationing", "Markov decision processes", "Make-to-stock queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0890", "e:abstract": "We introduce a measure of elasticity of stochastic demand, called the elasticity of the lost-sales rate, which offers a unifying perspective on the well-known newsvendor with pricing problem. This new concept provides a framework to characterize structural results for coordinated and uncoordinated pricing and inventory strategies. Concavity and submodularity of the profit function, as well as sensitivity properties of the optimal inventory and price policies, are characterized by monotonicity conditions, or bounds, on the elasticity of the lost-sales rate. These elasticity conditions are satisfied by most relevant demand models in the marketing and operations literature. Our results unify and complement previous work on price-setting newsvendor models and provide a new tool for researchers modeling stochastic price-sensitive demand in other contexts.", "e:keyword": ["Inventory/production", "Newsvendor problem with pricing", "Coordination", "Marketing", "Pricing", "Economics", "Monopoly under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0891", "e:abstract": "This paper considers the multiarmed bandit problem with multiple simultaneous arm pulls and the additional restriction that we do not allow recourse to arms that were pulled at some point in the past but then discarded. This additional restriction is highly desirable from an operational perspective, and we refer to this problem as the “irrevocable multiarmed bandit” problem. We observe that natural modifications to well-known heuristics for multiarmed bandit problems that satisfy this irrevocability constraint have unsatisfactory performance and, thus motivated, introduce a new heuristic: the “packing” heuristic. We establish through numerical experiments that the packing heuristic offers excellent performance, even relative to heuristics that are not constrained to be irrevocable. We also provide a theoretical analysis that studies the “price” of irrevocability, i.e., the performance loss incurred in imposing the constraint we propose on the multiarmed bandit model. We show that this performance loss is uniformly bounded for a general class of multiarmed bandit problems and indicate its dependence on various problem parameters. Finally, we obtain a computationally fast algorithm to implement the packing heuristic; the algorithm renders the packing heuristic computationally cheaper than methods that rely on the computation of Gittins indices.", "e:keyword": ["Dynamic programming/optimal control", "Multiarmed bandit problem", "Production/scheduling", "Learning", "Sequencing", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0892", "e:abstract": "We develop multidimensional balancing algorithms to compute provably near-optimal capacity-expansion policies. Our approach is computationally efficient and guaranteed to produce a policy with total expected cost of no more than twice that of an optimal policy. We overcome the curse of dimensionality by introducing novel <i>cost-separation schemes</i> to separate the lost-sales cost of the system into exact monotonic subparts. This is the first approximation technique for multimachine, multiproduct systems facing stochastic, nonstationary, and correlated demands. To show the generality of this separation technique, we apply it to the capacity-expansion problem under two different production planning models: monotone production and revenue-maximizing production. We make the assumptions of minimal inventory and lost sales.", "e:keyword": ["Analysis of algorithms", "Facilities/equipment planning", "Capacity expansion", "Dynamic programming", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0893", "e:abstract": "In two-moment decomposition approximations of queueing networks, the arrival process is modeled as a renewal process, and each station is approximated as a GI/G/1 queue whose mean waiting time is approximated based on the first two moments of the interarrival times and the service times. The departure process is also approximated as a renewal process even though the autocorrelation of this process may significantly affect the performance of the subsequent queue depending on the traffic intensity. When the departure process is split into substreams by Markovian random routing, the split processes typically are modeled as independent renewal processes even though they are correlated with each other. This cross correlation might also have a serious impact on the queueing performance. In this paper, we propose an approach for modeling both the cross correlation and the autocorrelation by a three-moment four-parameter decomposition approximation of queueing networks. The arrival process is modeled as a nonrenewal process by a two-state Markov-modulated Poisson process, viz., MMPP(2). The cross correlation between randomly split streams is accounted for in the second and third moments of the merged process by the innovations method. The main contribution of the present research is that both the cross correlation and the autocorrelation can be modeled in parametric decomposition approximations of queueing networks by integrating the MMPP(2) approximation of the arrival/departure process and the innovations method. We also present numerical results that strongly support our refinements.", "e:keyword": ["Parametric decomposition", "Queueing networks", "Splitting", "Superposition", "Autocorrelation", "Cross correlation", "Variability function", "Innovations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0894", "e:abstract": "This article describes numerical methods that exploit fixed-point equations equivalent to the first-order condition for Bertrand-Nash equilibrium prices in a class of differentiated product market models based on the mixed-logit model of demand. One fixed-point equation is already prevalent in the literature, and one is novel. Equilibrium prices are computed for the calendar year 2005 new-vehicle market under two mixed-logit models using (i) a state-of-the-art variant of Newton's method applied to the first-order conditions as well as the two fixed-point equations and (ii) a fixed-point iteration generated by our novel fixed-point equation. A comparison of the performance of these methods for a simple model with multiple equilibria is also provided. The analysis and trials illustrate the importance of using fixed-point forms of the first-order conditions for efficient and reliable computations of equilibrium prices.", "e:keyword": ["Bertrand-Nash equilibrium prices", "Differentiated product markets", "Mixed logit", "Newton's method", "GMRES-Newton hookstep", "Fixed-point iteration"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0895", "e:abstract": "Static game-theoretic models of bilateral bargaining assume that the seller knows his valuation for the item that is up for sale; that is, how the seller may determine this quantity is exogenous to these models. In this paper, we develop and analyze a stylized Markov decision process that endogenizes the seller's computation of his marginal inventory valuation in an infinite-horizon revenue management setting when each sale occurs according to a given bilateral bargaining mechanism. We use this model to compare, both analytically and numerically, the seller's performance under four basic bilateral bargaining mechanisms with a tractable information structure. These comparisons provide insights into the seller's performance under the following trading arrangements: buyer and seller posted pricing, negotiated pricing, and rule-based pricing.", "e:keyword": ["Dynamic programming", "Markov", "Finite state", "Games/group decisions", "Bargaining", "Incomplete information", "Mechanisms", "Neutral bargaining solution", "Marketing", "Pricing", "Revenue management", "Dynamic pricing", "Negotiated pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0896", "e:abstract": "We consider a multiproduct risk-averse newsvendor under the law-invariant coherent measures of risk. We first establish several fundamental properties of the model regarding the convexity of the problem, the symmetry of the solution, and the impact of risk aversion. Specifically, we show that for identical products with independent demands, increased risk aversion leads to decreased orders. For a large but finite number of heterogeneous products with independent demands, we derive closed-form approximations for the optimal order quantities. The approximations are as simple to compute as the classical risk-neutral solutions. We also show that the risk-neutral solution is asymptotically optimal as the number of products tends to be infinity, and thus risk aversion has no impact in the limit. For a risk-averse newsvendor with dependent demands, we show that positively (negatively) dependent demands lead to lower (higher) optimal order quantities than independent demands. Using a numerical study, we examine the convergence rates of the approximations and develop additional insights into the interplay between dependent demands and risk aversion.", "e:keyword": ["Multiple products", "Newsvendor", "Risk aversion", "Coherent measures of risk", "Diversification", "Portfolio"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0897", "e:abstract": "Wireless local area networks (WLANs) are widely used for cable replacement and wireless Internet access. Because the medium access control (MAC) scheme of WLANs has a strong influence on network performance, it should be accounted for in WLAN design. This paper presents AP location models that optimize a network performance measure specifically for the MAC scheme of WLANs that represents the efficiency in sharing the wireless medium. For these models, we propose a solution framework based on an effective integer-linear programming Dantzig--Wolfe reformulation. This framework is applicable to any nonlinear covering problem where the objective function is a sum of contributions over the groundset elements (users in WLANs). Extensive computational results show that our solution strategy quickly yields optimal or near-optimal solutions for WLAN design instances of realistic size.", "e:keyword": ["Integer programming", "Networks", "Telecommunications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0898", "e:abstract": "Acquisition of used products (cores) is central to the success of remanufacturing programs for companies. At the same time, dynamic pricing strategies have been adopted in various industries to better balance supply and customer demand. In this paper, we study the integration of these two aspects of operations together with inventory management for a production/remanufacturing firm. We develop a periodic-review single-product inventory system with price-dependent customer demand. The product return in each period is random but can be actively controlled by the firm's acquisition effort. The firm aims to maximize its total discounted profit over a finite planning horizon by implementing optimal production, remanufacturing, product acquisition, and pricing strategies. We first show that with an exogenous selling price, the optimal production-remanufacturing-disposal policy is simple and characterized by three state-independent parameters. The optimal acquisition effort is decreasing in the aggregate inventory level of serviceable product and cores. Nevertheless, when pricing is an endogenous decision, we find that the optimal policy becomes much more complicated, and its control parameters are state dependent. The optimal selling price is decreasing, whereas the optimal acquisition effort is increasing in the serviceable product inventory level, and both decisions decrease with the aggregate inventory level.", "e:keyword": ["Inventory control", "Optimal policy", "Pricing", "Product acquisition", "Remanufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0899", "e:abstract": "This paper presents a new integer programming (IP) model for large-scale instances of the air traffic flow management (ATFM) problem. The model covers all the phases of each flight---i.e., takeoff, en route cruising, and landing---and solves for an optimal combination of flow management actions, including ground-holding, rerouting, speed control, and airborne holding on a flight-by-flight basis. A distinguishing feature of the model is that it allows for rerouting decisions. This is achieved through the imposition of sets of “local” conditions that make it possible to represent rerouting options in a compact way by only introducing some new constraints. Moreover, three classes of valid inequalities are incorporated into the model to strengthen the polyhedral structure of the underlying relaxation.Computational times are short and reasonable for practical application on problem instances of size comparable to that of the entire U.S. air traffic management system. Thus, the proposed model has the potential of serving as the main engine for the preliminary identification, on a daily basis, of promising air traffic flow management interventions on a national scale in the United States or on a continental scale in Europe.", "e:keyword": ["Transportation", "Air traffic", "Programming", "Integer", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0900", "e:abstract": "We compare equilibrium profits of Bertrand (price) and Cournot (quantity) competition in oligopolies with an arbitrary number of nonsymmetric firms offering differentiated substitutable products under an affine demand function. We provide a precise characterization of the profit relationship in terms of (1) the number of firms, (2) their relative quality and cost differences, and (3) the competition intensity, defined as the maxiumum absolute value of total change in competitors' demand over change in own demand due to a unit change in own price. We first examine the case where firms have the same demand sensitivity to own price and the same demand sensitivity to competitor prices but different cost and quality parameters. For this case, we prove that the total profit of the industry under Cournot competition is at least as high as the total profit under Bertrand competition if the number of firms is less than 28 or if the competition intensity is less than 0.909 or if the differences in quality and cost competitiveness between firms are small. We also prove that for each firm, the profit achieved under Cournot competition is at least as high as the profit achieved under Bertrand competition if the number of firms is less than eight or if the competition intensity is less than 0.739. We then provide numerical and analytical results that qualitatively support the same conclusions for general affine demand functions with variable demand sensitivities to prices.", "e:keyword": ["Oligopoly", "Competition", "Bertrand", "Cournot"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0903", "e:abstract": "Although take-it-or-leave-it pricing is the main mode of operation for many retailers, a number of retailers discreetly allow price negotiation when some haggle-prone customers ask for a bargain. At these retailers, the posted price, which itself is subject to dynamic adjustments in response to the pace of sales during the selling season, serves two important roles: (i) it is the take-it-or-leave-it price to many customers who do not bargain, and (ii) it is the price from which haggle-prone customers negotiate down. To effectively measure the benefit of dynamic pricing and negotiation in such a retail environment, one must take into account the interactions among inventory, dynamic pricing, and negotiation. The outcome of the negotiation (and the final price a customer pays) depends on the inventory level, the remaining selling season, the retailer's bargaining power, and the posted price. We model the retailer's dynamic pricing problem as a dynamic program, where the revenues from both negotiation and posted pricing are embedded in each period. We characterize the optimal posted price and the resulting negotiation outcome as a function of inventory and time. We also show that negotiation is an effective tool to achieve price discrimination, particularly when the inventory level is high and/or the remaining selling season is short, even when implementing negotiation is costly.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Inventory/production", "Policies", "Marketing/pricing", "Dynamic programming", "Models", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0905", "e:abstract": "We analyze the structure and pricing of option contracts for an industrial good in the presence of spot trading. We combine the analysis of spot trading and buyers' disparate private valuations for different suppliers' products, and we jointly endogenize the determination of three major dimensions in contract design: (i) sales contracts versus options contracts, (ii) flat-price versus volume-dependent contracts, and (iii) volume discounts versus volume premia. We build a model in which a supplier of an industrial good transacts with a manufacturer who uses the supplier's product to produce an end good with an uncertain demand. We show that, consistent with industry observations, volume-dependent optimal sales contracts always demonstrate volume discounts (i.e., involve concave pricing). However, options are more complex agreements, and optimal option contracts can involve both volume discounts and volume premia. Three major contract structures commonly emerge in optimality. First, if the seller has a high discount rate relative to the buyer and the seller's production costs or the production capacity is low, the optimal contracts tend to be flat-price sales contracts. Second, when the seller has a relatively high discount rate compared to the buyer but production costs or production capacity are high, the optimal contracts are sales contracts with volume discounts. Third, if the buyer's discount rate is high relative to the seller's, then the optimal contracts tend to be volume-dependent options contracts and can involve both volume discounts and volume premia. However, when the seller's production capacity is sufficiently low, it is possible to observe flat-price option contracts. Furthermore, we provide links between production and spot market characteristics, contract design, and efficiency.", "e:keyword": ["Supply chain management", "Procurement", "Contract pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0906", "e:abstract": "Using the well-known product-limit form of the Kaplan-Meier estimator from statistics, we propose a new class of nonparametric adaptive data-driven policies for stochastic inventory control problems. We focus on the distribution-free newsvendor model with censored demands. The assumption is that the demand distribution is not known and there are only <i>sales data</i> available. We study the theoretical performance of the new policies and show that for discrete demand distributions they converge almost surely to the set of optimal solutions. Computational experiments suggest that the new policies converge for general demand distributions, not necessarily discrete, and demonstrate that they are significantly more robust than previously known policies. As a by-product of the theoretical analysis, we obtain new results on the asymptotic consistency of the Kaplan-Meier estimator for discrete random variables that extend existing work in statistics. To the best of our knowledge, this is the first application of the Kaplan-Meier estimator within an adaptive optimization algorithm, in particular, the first application to stochastic inventory control models. We believe that this work will lead to additional applications in other domains.", "e:keyword": ["Inventory control", "Data-driven algorithms", "Statistics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0907", "e:abstract": "We consider a single server queueing system in which service shuts down when there are no customers present and is resumed only when the queue length reaches a given critical length. We analyze the strategic response of customers to this mechanism and compare it to the overall optimal behavior, with and without information on delay. The results are significantly different from those obtained when the server is continuously available. We show that there may exist multiple equilibria in such a system and the optimal arrival rate may be greater or smaller than that of the decentralized equilibrium. Finally, the critical length is taken as a decision variable, and the optimal operations policy is discussed by taking strategic customers into consideration.", "e:keyword": ["Vacation queue", "Strategic customers", "Balking queue", "Equilibrium analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0908", "e:abstract": "Allocating resources in grid computing requires local and external schedulers to communicate in order to achieve an efficient management of the resources themselves. To this end, some economic/market-based models have been introduced in the literature, where users, external schedulers, and local schedulers negotiate to optimize their objectives. In this paper, we propose a tender/contract-net model for the grid resource allocation problem, showing the interactions among the involved actors. The performance of the proposed market-based approach is experimentally compared with a round-robin allocation protocol, a system-centric least-cost allocation approach, and also a market-based approach available from the literature.", "e:keyword": ["Grid computing", "Resource management", "Economic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0909", "e:abstract": "The valuation of the real option to store liquefied natural gas (LNG) at the downstream terminal of an LNG value chain is an important problem in practice. Because the exact valuation of this real option is computationally intractable, we develop a novel and tractable heuristic model for its strategic valuation that integrates models of LNG shipping, natural gas price evolution, and inventory control and sale into the wholesale natural gas market. We incorporate real and estimated data to quantify the value of this real option and its dependence on the throughput of an LNG chain, the type of price variability, the type of inventory control policy employed, and the level of stochastic variability in both the shipping model and the natural gas price model used. In addition, we develop an imperfect information dual upper bound to assess the effectiveness of our heuristic and find that our method is near optimal. Our approach also has potential relevance to value the real option to store other commodities in facilities located downstream from a commodity production or transportation stage, such as petroleum and agricultural products, chemicals, and metals, or the real option to store the input used in the production of a commodity such as electricity.", "e:keyword": ["Finance", "Asset pricing", "Real options", "Storage valuation", "Dynamic programming", "Heuristics", "Markov", "Upper bounds", "Industries", "Petroleum/natural gas"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0910", "e:abstract": "When there is parameter uncertainty in the constraints of a convex optimization problem, it is natural to formulate the problem as a joint chance constrained program (JCCP), which requires that all constraints be satisfied simultaneously with a given large probability. In this paper, we propose to solve the JCCP by a sequence of convex approximations. We show that the solutions of the sequence of approximations converge to a Karush-Kuhn-Tucker (KKT) point of the JCCP under a certain asymptotic regime. Furthermore, we propose to use a gradient-based Monte Carlo method to solve the sequence of convex approximations.", "e:keyword": ["Programming", "Stochastic", "Chance constrained program"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0911", "e:abstract": "Assessment of multiattribute utility functions is significantly simplified if it is possible to decompose the function into more manageable pieces. Utility independence is a powerful property that serves well for this purpose, but if it is not appropriate in a given situation, what options does the analyst have? We review some possibilities and propose a new independence assumption based on the one-switch property. We argue that it is a natural generalization of utility independence and show how it leads to tractable multiattribute utility functions.", "e:keyword": ["One-switch", "Utility independence", "Multiattribute utility"]}, {"@id": "http://dx.doi.org/10.1287/opre.1100.0913", "e:abstract": "We present two significant enhancements to the guaranteed-service (GS) model for multiechelon safety stock placement. First, we let each stage's expected inventory cost be a generalized nonconcave non-closed-form function of its incoming and outgoing service time. This allows the GS model to incorporate important phenomena such as variable stage times and nonnested review periods, which previous GS literature has not allowed. Second, we optimize the generalized cost GS model for directed acyclic networks, rather than assembly/distribution networks or trees. For the resulting NP-hard optimization problem, we present a provably optimal algorithm that runs within minutes for 29 chains from a data set of 38 real-world supply chains ranging from 8 to 2,025 stages. We also present two significantly faster yet near-optimal heuristics. One heuristic is motivated by the structure of the formulation's dual space, whereas the other heuristic simply terminates the optimization algorithm after a fixed number of iterations. As a performance benchmark, on the 38 chains, the first heuristic has an average optimality gap of approximately 1.1% and average run time of 88 seconds, whereas the second heuristic has an average optimality gap of 2.8% and an average run time of 5.9 seconds.", "e:keyword": ["Multiechelon inventory system", "Safety stock optimization", "Dynamic programming application", "General acyclic networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0912", "e:abstract": "Failure of many engineering systems usually results from a gradual and irreversible accumulation of damage, a degradation process. Most degradation processes can be monitored using sensor technology. The resulting degradation signals are usually correlated with the degradation process. A system is considered to have failed once its degradation signal reaches a prespecified failure threshold. This paper considers a replacement problem for components whose degradation process can be monitored using dedicated sensors. First, we present a stochastic degradation modeling framework that characterizes, in real time, the path of a component's degradation signal. These signals are used to predict the evolution of the component's degradation state. Next, we formulate a single-unit replacement problem as a Markov decision process and utilize the real-time signal observations to determine a replacement policy. We focus on exponentially increasing degradation signals and show that the optimal replacement policy for this class of problems is a monotonically nondecreasing control limit policy. Finally, the model is used to determine an optimal replacement policy by utilizing vibration-based degradation signals from a rotating machinery application.", "e:keyword": ["Markov decision processes", "Control limit policies", "Single-unit replacement model", "Degradation modeling", "Nonstationary degradation", "Sensors"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0914", "e:abstract": "We analyze a generalization of the discriminatory processor-sharing (DPS) queue in a heavy-traffic setting. Customers present in the system are served simultaneously at rates controlled by a vector of weights. We assume that customers have phase-type distributed service requirements and allow that customers have different weights in various phases of their service.In our main result we establish a state-space collapse for the queue-length vector in heavy traffic. The result shows that in the limit, the queue-length vector is the product of an exponentially distributed random variable and a deterministic vector. This generalizes a previous result by Rege and Sengupta [Rege, K. M., B. Sengupta. 1996. Queue length distribution for the discriminatory processor-sharing queue. <i>Oper. Res.</i> <b>44</b>(4) 653--657], who considered a DPS queue with exponentially distributed service requirements. Their analysis was based on obtaining all moments of the queue-length distributions by solving systems of linear equations. We undertake a more direct approach by showing that the probability-generating function satisfies a partial differential equation that allows a closed-form solution after passing to the heavy-traffic limit.Making use of the state-space collapse result, we derive interesting properties in heavy traffic: (i) For the DPS queue, we obtain that, conditioned on the number of customers in the system, the residual service requirements are asymptotically independent and distributed according to the forward recurrence times. (ii) We then investigate how the choice for the weights influences the asymptotic performance of the system. In particular, for the DPS queue we show that the scaled holding cost reduces as classes with a higher value for <i>d</i><sub><i>k</i></sub>/<i>E</i>(<i>B</i><sub><i>k</i></sub><sup><i>fwd</i></sup>) obtain a larger share of the capacity, where <i>d</i><sub><i>k</i></sub> is the cost associated to class <i>k</i>, and <i>E</i>(<i>B</i><sub><i>k</i></sub><sup><i>fwd</i></sup>) is the forward recurrence time of the class-<i>k</i> service requirement. The applicability of this result for a moderately loaded system is investigated by numerical experiments.", "e:keyword": ["Discriminatory processor sharing", "Heavy traffic", "Phase-type service requirements", "Residual service requirements", "Scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0915", "e:abstract": "This paper considers large-scale stochastic simulations with correlated inputs having normal-to-anything (NORTA) distributions with arbitrary continuous marginal distributions. Examples of correlated inputs include processing times of workpieces across several workcenters in manufacturing facilities and product demands and exchange rates in global supply chains. Our goal is to obtain mean performance measures and confidence intervals for simulations with such correlated inputs by accounting for the uncertainty around the NORTA distribution parameters estimated from finite historical input data. This type of uncertainty is known as the parameter uncertainty in the discrete-event stochastic simulation literature. We demonstrate how to capture parameter uncertainty with a Bayesian model that uses Sklar's marginal-copula representation and Cooke's copula-vine specification for sampling the parameters of the NORTA distribution. The development of such a Bayesian model well suited for handling many correlated inputs is the primary contribution of this paper. We incorporate the Bayesian model into the simulation replication algorithm for the joint representation of stochastic uncertainty and parameter uncertainty in the mean performance estimate and the confidence interval. We show that our model improves both the consistency of the mean line-item fill-rate estimates and the coverage of the confidence intervals in multiproduct inventory simulations with correlated demands.", "e:keyword": ["Bayesian", "Correlation", "Design of experiments", "Sampling", "Statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0916", "e:abstract": "Recently it has been shown that minimal inequalities for a continuous relaxation of mixed-integer linear programs are associated with maximal lattice-free convex sets. In this paper, we show how to lift these inequalities for integral nonbasic variables by considering maximal lattice-free convex sets in a higher dimensional space. We apply this approach to several examples. In particular, we identify cases in which the lifting is unique.", "e:keyword": ["Programming/integer/cutting plane"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0917", "e:abstract": "In response to Assembly Bill 32, the state of California considered three types of carbon emissions trading programs for the electric power sector: load-based, source-based, and first-seller. They differed in terms of their point of regulation and in whether in-state-to-out-of-state and out-of-state-to-in-state electricity sales are regulated. In this paper, we formulate a market equilibrium model for each of the three approaches, considering power markets, transmission limitations, and emissions trading, and making the simplifying assumption of pure bilateral markets. We analyze the properties of their solutions and show the equivalence of load-based, first-seller, and source-based approaches when in-state-to-out-of-state sales are regulated under the cap. A numeric example illustrates the emissions and economic implications of the models. In the simulated cases, “leakage” eliminates most of the emissions reductions that the regulations attempt to impose. Furthermore, “contract reshuffling” occurs to such an extent that all the apparent emissions reductions resulting from changes in sources of imported power are illusory.In reality, the three systems would not be equivalent because there will also be pool-type markets, and the three systems provide different incentives for participating in those markets. However, the equivalence results under our simplifying assumptions show that load-based trading has no inherent advantage compared to other systems in terms of costs to consumers, contrary to claims elsewhere.", "e:keyword": ["Emissions trading", "Electric market", "CO2 emissions", "Load-based", "First-seller", "Source-based"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0918", "e:abstract": "In this paper, we analyze mixed 0-1 linear programs under objective uncertainty. The mean vector and the second-moment matrix of the nonnegative objective coefficients are assumed to be known, but the exact form of the distribution is unknown. Our main result shows that computing a tight upper bound on the expected value of a mixed 0-1 linear program in maximization form with random objective is a completely positive program. This naturally leads to semidefinite programming relaxations that are solvable in polynomial time but provide weaker bounds. The result can be extended to deal with uncertainty in the moments and more complicated objective functions. Examples from order statistics and project networks highlight the applications of the model. Our belief is that the model will open an interesting direction for future research in discrete and linear optimization under uncertainty.", "e:keyword": ["Mixed 0-1 linear program", "Moments", "Completely positive program"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0919", "e:abstract": "Latency problems are characterized by their focus on minimizing the waiting time for all clients. We study <i>periodic</i> latency problems, a nontrivial extension of standard latency problems. In a periodic latency problem each client has to be visited regularly: there is a server traveling at unit speed, and there is a set of <i>n</i> clients with given positions. The server must visit the clients over and over again, subject to the constraint that successive visits to client <i>i</i> are at most <i>q</i><sub><i>i</i></sub> time units away from each other.We investigate two main problems. In problem PLPP the goal is to find a repeatable route for the server visiting as many clients as possible without violating their <i>q</i><sub><i>i</i></sub>s. In problem PLP the goal is to minimize the number of servers needed to serve all clients. Depending on the topology of the underlying network, we derive polynomial-time algorithms or hardness results for these two problems. Our results draw sharp separation lines between easy and hard cases.", "e:keyword": ["Latency problem", "Periodicity", "Complexity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0920", "e:abstract": "Resource allocation decisions are crucial for the success of an organization. This paper proposes an integrated approach to resource allocation problems, in which decision makers have one observation of the multiple input-output criteria of candidates. We offer important improvements over existing approaches based on the widely used data envelopment analysis (DEA), which has two major limitations in its application to resource allocation. First, traditional DEA models compute efficiency scores by optimizing firm-specific shadow prices of inputs and outputs. This could be problematic, because in practice stakeholders would usually require unanimously agreed-upon trade-offs among evaluation criteria. Second, previous allocation approaches based on DEA do not allow for controlling the risk exposure of allocation portfolios. To tackle these problems, we propose an efficiency measure based on equilibrium shadow prices of different criteria, and we use the bootstrap efficiency distributions to gather information regarding efficiency variations and correlations. Through our methodology, decision makers can obtain the risk-minimizing allocation portfolio. We illustrate the proposed approach through an empirical R&D project budgeting problem in which we allocate funding according to the projects efficiency distributions.", "e:keyword": ["Organizational studies", "Productivity", "Data envelopment analysis", "Decision making", "Programming", "Linear application", "Statistics", "Nonparametric", "Sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0921", "e:abstract": "We consider a system in which an order is placed every <i>T</i> periods to bring the inventory position up to the base stock <i>S</i>. We accept demand until the inventory position reaches a sales rejection threshold <i>M</i>. Our objective is to find the optimal values of <i>S</i> and <i>M</i> that minimize the long-run average cost per period. We establish the stationary distribution of our system and develop structural properties of the optimal solution that facilitate computation. In particular, we show that in an optimal solution, the optimal value of <i>M</i> is nonnegative under some reasonable conditions. Hence, in our model a mixture of backorders and lost sales may occur. Additionally, we compare our system against traditional systems in which demand during stockouts is either fully backordered or lost.", "e:keyword": ["Stochastic inventory models", "(R", "T) systems", "Base stock systems", "Backorders", "Lost sales"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0922", "e:abstract": "Recently, coinciding with and perhaps driving the increased popularity of prediction markets, several novel pari-mutuel mechanisms have been developed such as the logarithmic market-scoring rule (LMSR), the cost-function formulation of market makers, utility-based markets, and the sequential convex pari-mutuel mechanism (SCPM). In this work, we present a convex optimization framework that unifies these seemingly unrelated models for centrally organizing contingent claims markets. The existing mechanisms can be expressed in our unified framework by varying the choice of a concave value function. We show that this framework is equivalent to a convex risk minimization model for the market maker. This facilitates a better understanding of the risk attitudes adopted by various mechanisms. The unified framework also leads to easy implementation because we can now find the cost function of a market maker in polynomial time by solving a simple convex optimization problem.In addition to unifying and explaining the existing mechanisms, we use the generalized framework to derive necessary and sufficient conditions for many desirable properties of a prediction market mechanism such as proper scoring, truthful bidding (in a myopic sense), efficient computation, controllable risk measure, and guarantees on the worst-case loss. As a result, we develop the <i>first</i> proper, truthful, risk-controlled, loss-bounded (independent of the number of states) mechanism; none of the previously proposed mechanisms possessed all these properties simultaneously. Thus, our work provides an effective tool for designing new prediction market mechanisms. We also discuss possible applications of our framework to dynamic resource pricing and allocation in general trading markets.", "e:keyword": ["Programming", "Convex", "Applications", "Games/group decisions", "Bidding/auctions", "Gambling", "Risk", "Decision analysis", "Risk", "Finance", "Asset pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0923", "e:abstract": "In this paper we conduct a quantitative analysis for a strategic risk management problem that involves allocating certain available failure-mitigating and consequence-alleviating resources to reduce the failure probabilities of system safety components and subsequent losses, respectively, together with selecting optimal strategic decision alternatives, to minimize the risk or expected loss in the event of a hazardous occurrence. Using a novel decision tree optimization approach to represent the cascading sequences of probabilistic events as controlled by key decisions and investment alternatives, the problem is modeled as a nonconvex mixed-integer 0-1 factorable program. We develop a specialized branch-and-bound algorithm in which lower bounds are computed via tight linear relaxations of the original problem that are constructed by utilizing a polyhedral outer-approximation mechanism in concert with two alternative linearization schemes having different levels of tightness and complexity. We also suggest three alternative branching schemes, each of which is proven to guarantee convergence to a global optimum for the underlying problem. Extensive computational results and sensitivity analyses are presented to provide insights and to demonstrate the efficacy of the proposed algorithm.", "e:keyword": ["Decision analysis", "Risk", "Programming", "Integer", "Algorithms", "Branch-and-bound", "Mixed-integer nonlinear program", "Factorable program", "Polyhedral analysis", "Branch-and-bound", "Decision analysis", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0926", "e:abstract": "We develop a sequential sampling procedure for a class of stochastic programs. We assume that a sequence of feasible solutions with an optimal limit point is given as input to our procedure. Such a sequence can be generated by solving a series of sampling problems with increasing sample size, or it can be found by any other viable method. Our procedure estimates the optimality gap of a candidate solution from this sequence. If the point estimate of the optimality gap is sufficiently small according to our termination criterion, then we stop. Otherwise, we repeat with the next candidate solution from the sequence under an increased sample size. We provide conditions under which this procedure (i) terminates with probability one and (ii) terminates with a solution that has a small optimality gap with a prespecified probability.", "e:keyword": ["Programming", "Stochastic", "Simulation", "Efficiency", "Statistics", "Sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0927", "e:abstract": "We consider a two-stage serial supply chain with capacity limits, where each installation is operated by managers attempting to minimize their own costs. A multiple-period model is necessitated by the multiple stages, capacity limits, stochastic demand, and the explicit consideration of inventories. With appropriate salvage value functions, a Markov equilibrium policy is found. Intuitive profit dominance allows for existence of a unique equilibrium solution, which is shown to be a modified echelon base-stock policy. This equilibrium policy structure is sustained in the infinite horizon. A numerical study compares the behavior of the decentralized system with the first-best integrated capacitated system. The performance of this decentralized system relative to the integrated system across other parameters can be very good over a broad range of values. This implies that an acceptable system performance may be attained without the imposition of a contract or other coordinating mechanism, which themselves may encounter difficulties in implementation in the form of negotiation, execution, or enforcement of these agreements. We find instances where tighter capacities may actually enhance channel efficiency. We also examine the effect of capacity utilization on the system suboptimality.", "e:keyword": ["Inventory", "Capacity", "Supply chain", "Competition", "Markov games"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0928", "e:abstract": "We study a supply chain consisting of one supplier and one OEM (original equipment manufacturer). The OEM faces stochastic demand for a final product that requires assembly of two major components, one of which is procured exclusively from the supplier. In the absence of competition, the supplier is able to make a take-it-or-leave-it offer to the OEM in the form of a menu of price-quantity contracts. The OEM possesses private information across two dimensions: (1) demand forecasts about the final product, and (2) production cost of the in-house component. Both pieces of information are relevant to the total supply chain profit, thus affecting the supplier's optimal offer. By initially assuming an exogenous information structure, we characterize the supplier's optimal contract menu for a simple case and demonstrate that more dimensions of asymmetric information are not always preferable for the OEM but could be beneficial for the supply chain. We subsequently examine whether this preference for one less dimension of private information implies disclosure of private information to the supplier when the information structure is endogenized. Our results indicate that if OEMs that are indifferent between disclosing and keeping information private choose to disclose it, disclosure of any verifiable information from all OEMs is always an equilibrium, whereas nondisclosure might fail to be an equilibrium. We also consider the possibility of the OEM and the supplier contracting at the ex-ante stage, i.e., before the OEM observes his private information. When both dimensions of the OEM's private information are verifiable and the cost of disclosing information is small enough, an ex-ante agreement on information disclosure is always possible; otherwise its feasibility depends on the problem parameters.", "e:keyword": ["Purchasing", "Asymmetric information", "Private information disclosure"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0929", "e:abstract": "We consider the problem faced by an airline that is flying both passengers and cargo over a network of locations on a fixed periodic schedule. Bookings for many classes of cargo shipments between origin-destination pairs in this network are made in advance, but the weight and volume of aircraft capacity available for cargo as well as the exact weight and volume of each shipment are not known at the time of booking. The problem is to control cargo accept/reject decisions to maximize expected profits while ensuring effective dispatch of accepted shipments through the network. This network stochastic dynamic control problem has very high computational complexity. We propose a linear programming and stochastic simulation-based computational method for learning approximate control policies and discuss their structural properties. The proposed method is flexible and can utilize historical booking data as well as decisions generated by default control policies.", "e:keyword": ["Transportation", "Freight", "Dynamic programming", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0930", "e:abstract": "The <i>perspective relaxation</i> (PR) is a general approach for constructing tight approximations to mixed-integer nonlinear programs (MINLP) with semicontinuous variables. The PR of a MINLP can be formulated either as a mixed-integer second-order cone program (MI-SOCP), provided that the original objective function is SOCP-representable, or as a semi-infinite MINLP. In this paper, we show that under some further assumptions (rather restrictive, but satisfied in several practical applications), the PR of a mixed-integer quadratic program (MIQP) can also be reformulated as a piecewise-quadratic program (QP), ultimately yielding a QP relaxation of roughly the same size of the standard continuous relaxation. Furthermore, if the original problem has some exploitable structure, then this structure is typically preserved in the reformulation, thus allowing the construction of specialized approaches for solving the PR. We report on implementing these ideas on two MIQPs with appropriate structure: a sensor placement problem and a quadratic-cost (single-commodity) network design problem.", "e:keyword": ["Mixed-integer nonlinear programming problems", "Semicontinuous variables", "Perspective relaxation", "Sensor placement problem", "Network design problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0931", "e:abstract": "This paper presents a robust optimization model for <i>n</i>-person finite state/action stochastic games with incomplete information. We consider nonzero sum discounted stochastic games in which none of the players knows the true data of a game, and each player adopts a robust optimization approach to address the uncertainty. We call these games <i>discounted robust stochastic games</i>. Such games allow us to use simple uncertainty sets for the unknown data and eliminate the need to have an a-priori probability distribution over a set of games. We prove the existence of equilibrium points and propose an explicit mathematical programming formulation for an equilibrium calculation. We illustrate the use of discounted robust stochastic games in a single server queueing control problem.", "e:keyword": ["Games", "Stochastic", "Dynamic programming/optimal control", "Markov", "Finite state", "Queues", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0932", "e:abstract": "In a two-level nested simulation, an outer level of simulation samples scenarios, while the inner level uses simulation to estimate a conditional expectation given the scenario. Applications include financial risk management, assessing the effects of simulation input uncertainty, and computing the expected value of gathering more information in decision theory. We show that an ANOVA-like estimator of the variance of the conditional expectation is unbiased under mild conditions, and we discuss the optimal number of inner-level samples to minimize this estimator's variance given a fixed computational budget. We show that as the computational budget increases, the optimal number of inner-level samples remains bounded. This finding contrasts with previous work on two-level simulation problems in which the inner- and outer-level sample sizes must both grow without bound for the estimation error to approach zero. The finding implies that the variance of a conditional expectation can be estimated to arbitrarily high precision by a simulation experiment with a fixed inner-level computational effort per scenario, which we call a one-and-a-half-level simulation. Because the optimal number of inner-level samples is often quite small, a one-and-a-half-level simulation can avoid the heavy computational burden typically associated with two-level simulation.", "e:keyword": ["Nested simulation", "Analysis of variance", "ANOVA", "Variance components"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0933", "e:abstract": "We study a single-stage inventory system with a generalized shortage penalty cost that includes the following three components: (i) a cost that is an increasing function of the number of backordered units in a period, (ii) a fixed cost incurred for each period in which there is a backorder irrespective of how many units are backordered, and finally (iii) a cost that is an increasing function of the number of periods a customer is backordered. We show the problem can be transformed into one in which the backorder cost depends on the inventory position only. Then we present two sets of conditions; the first one restricts our attention to a special case of the generalized penalty cost model while the second one restricts our attention to stationary demand models with some distributional assumptions. Under the first (resp. second) set of conditions, we show that the expected cost in a period can be expressed as a convex (resp. quasiconvex) function of the after-ordering inventory position. We use this property to prove the optimality of order-up-to policies under both sets of conditions and discuss extensions to the cases where either a fixed ordering cost or a batch ordering constraint is present.", "e:keyword": ["Inventory/production systems", "Backordering cost", "Base-stock policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0934", "e:abstract": "We consider tandem lines with finite buffers and flexible, heterogeneous servers that are synergistic in that they work more effectively in teams than on their own. Our objective is to determine how the servers should be assigned dynamically to tasks in order to maximize the long-run average throughput. In particular, we investigate when it is better to take advantage of synergy among servers, rather than exploiting the servers' special skills, to achieve the best possible system throughput. We show that when there is no trade-off between server synergy and servers' special skills (because the servers are generalists who are equally skilled at all tasks), the optimal policy has servers working in teams of two or more at all times. Moreover, for Markovian systems with two stations and two servers, we provide a complete characterization of the optimal policy and show that, depending on how well the servers work together, the optimal policy either takes full advantage of servers' special skills, or full advantage of server synergy (and hence there is no middle ground in this case). Finally, for a class of larger Markovian systems, we provide sufficient conditions that guarantee that the optimal policy should take full advantage of server synergy at all times.", "e:keyword": ["Production/scheduling", "Flexible manufacturing/line balancing", "Dynamic programming/optimal control", "Applications", "Queues", "Tandem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0935", "e:abstract": "We propose and analyze a general periodic-review model in which the firm has access to a set of potential suppliers, each with specific yield and price characteristics. Assuming that unsatisfied demand is backlogged, the firm incurs three types of costs: (i) procurement costs, (ii) inventory-carrying costs for units carried over from one period to the next, and (iii) backlogging costs. A procurement strategy requires the specification, in each period, of (i) the <i>set</i> of suppliers to be retained, (ii) their respective <i>shares</i> in this period's replenishments, as well as (iii) the traditional <i>aggregate</i> order placed (among the various suppliers). We show how the optimal procurement strategy can be obtained with an efficient algorithm. A base-stock policy is no longer optimal, but in each period there exists a maximum inventory level, such that orders are placed if and only if the starting inventory is below this threshold. In each period it is optimal to retain a given number of suppliers that are cheapest in terms of that period's effective cost rates, i.e., the expected cost per usable unit. The optimal <i>number</i> of suppliers to be retained in a given period depends on all current and future parameters and distributions, but this dependence can be aggregated into a single so-called benchmark cost measure. Under Normal yield and demand distributions, the suppliers' market shares are determined by a single aggregate score, itself the product of a simple reliability score and a cost score.", "e:keyword": ["Supplier selections", "Unreliable yields", "Multiperiod procurement strategies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0936", "e:abstract": "We consider a group of players who perform tasks repeatedly. The players are nodes of a communication network and observe their neighbors' actions. Players have partial knowledge of the network and only know their set of neighbors. We study the existence of protocols for fault reporting: whenever a player chooses a faulty action, the communication protocol starts and the output publicly reveals the identity of the faulty player. We consider two setups. In the first one, players do not share authentication keys. We show that existence of a protocol for fault reporting is equivalent to the 2-vertex-connectedness of the network: no single vertex deletion disconnects the graph. In the second setup, we allow players to share authentication keys. We show that existence of a distribution of the keys and of a protocol for fault reporting is equivalent to the 2-edge-connectedness of the network: no single edge deletion disconnects the graph. We give applications to the implementation of socially optimal outcomes in repeated games.", "e:keyword": ["Networks/graphs", "Communication networks", "Communications", "Communication protocols", "Games/group decisions", "Noncooperative", "Repeated games"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0937", "e:abstract": "Data envelopment analysis (DEA) provides an optimization methodology for deriving an efficiency score for each member of a set of peer decision-making units. Under the original DEA model it was assumed that there is constant returns to scale (CRS). This idea was later extended to the more general case that allowed for variable returns to scale (VRS). In both of these structures, it is assumed that the returns to scale (RTS) classification, consistent with the classical definition, applies to the entire (input, output) bundle. In many settings it can be the case that the output bundle can be separated into distinct subsets or business units wherein an RTS-type behavior may be different for one subgroup than for another. We refer to such situations as involving multiple variable proportionality (MVP). Examples of MVP can occur when there are different product subgroupings in a company, different wards in hospitals, different programs in a university, and so on. Identification of such differential behavior can provide management with important insights regarding the most productive proportionality size (MPPS) in each of those subgroups. In the current paper we introduce DEA-based tools that address those situations where MVP exists.", "e:keyword": ["DEA", "Returns to scale", "Multiple business units", "Multiple variable proportionality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0940", "e:abstract": "Fifty years ago, the author published a paper in <i>Operations Research</i> with the title, “A proof for the queuing formula: <i>L = (lambda)W</i>” [Little, J. D. C. 1961. A proof for the queuing formula: <i>L = (lambda)W</i>. <i>Oper. Res.</i> <b>9</b>(3) 383--387]. Over the years, <i>L = (lambda)W</i> has become widely known as “Little's Law.” Basically, it is a theorem in queuing theory. It has become well known because of its theoretical and practical importance. We report key developments in both areas with the emphasis on practice. In the latter, we collect new material and search for insights on the use of Little's Law within the fields of operations management and computer architecture.", "e:keyword": ["Little's Law", "Queuing theory", "Operations management", "Computer engineering", "Computer architecture", "Operations research"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0941", "e:keyword": ["OR history"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0942", "e:abstract": "To describe the congestion in large-scale service systems, we introduce and analyze a non-Markovian open network of many-server fluid queues with customer abandonment, proportional routing, and time-varying model elements. Proportions of the fluid completing service from each queue are immediately routed to the other queues, with the fluid not routed to one of the queues being immediately routed out of the network. The fluid queue network serves as an approximation for the corresponding non-Markovian open network of many-server queues with Markovian routing, where all model elements may be time varying. We establish the existence of a unique vector of (net) arrival rate functions at each queue and the associated time-varying performance. In doing so, we provide the basis for an efficient algorithm, even for networks with many queues.", "e:keyword": ["Queues", "Time-varying arrivals", "Queueing networks", "Many-server queues", "Deterministic fluid model", "Customer abandonment", "Non-Markovian queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0943", "e:abstract": "Nursing care is arguably the single biggest factor in both the cost of hospital care and patient satisfaction. Inadequate inpatient nursing levels have also been cited as a significant factor in medical errors and emergency room overcrowding. Yet, there is widespread dissatisfaction with the current methods of determining nurse staffing levels, including the most common one of using minimum nurse-to-patient ratios. In this paper, we represent the nursing system as a variable finite-source queuing model. We develop a reliable, tractable, easily parameterized two-dimensional model to approximate the actual interdependent dynamics of bed occupancy levels and demands for nursing. We use this model to show how unit size, nursing intensity, occupancy levels, and unit length-of-stay affect the impact of nursing levels on performance and thus how inflexible nurse-to-patient ratios can lead to either understaffing or overstaffing. The model is also useful for estimating the impact of nurse staffing levels on emergency department overcrowding.", "e:keyword": ["Finite capacity model", "Queueing model", "Nurse staffing", "Emergency room overcrowding", "Hospital applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0944", "e:abstract": "We introduce ROME, an algebraic modeling toolbox for a class of robust optimization problems. ROME serves as an intermediate layer between the modeler and optimization solver engines, allowing modelers to express robust optimization problems in a mathematically meaningful way. In this paper, we discuss how ROME can be used to model (1) a service-constrained robust inventory management problem, (2) a project-crashing problem, and (3) a robust portfolio optimization problem. Through these modeling examples, we highlight the key features of ROME that allow it to expedite the modeling and subsequent numerical analysis of robust optimization problems. ROME is freely distributed for academic use at http://www.robustopt.com.", "e:keyword": ["Robust optimization", "Algebraic modeling toolbox", "MATLAB", "Stochastic programming", "Decision rules", "Inventory control", "PERT", "Project management", "Portfolio optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0945", "e:abstract": "The problem of pricing an American option written on an underlying asset with constant price volatility has been studied extensively in literature. Real-world data, however, demonstrate that volatility is not constant, and stochastic volatility models are used to account for dynamic volatility changes. Option pricing methods that have been developed in literature for pricing under stochastic volatility focus mostly on European options. We consider the problem of pricing American options under stochastic volatility, which has had relatively much less attention from literature. First, we develop a transformation procedure to compute the optimal-exercise policy and option price and provide theoretical guarantees for convergence. Second, using this computational tool, we explore a variety of questions that seek insights into the dependence of option prices, exercise policies, and implied volatilities on the market price of volatility risk and correlation between the asset and stochastic volatility. The speed and accuracy of the procedure are compared against existing methods as well.", "e:keyword": ["American option", "Stochastic volatility", "Free boundary"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0946", "e:abstract": "Motivated by the widespread adoption of dynamic pricing in industry and the empirical evidence of costly price adjustments, in this paper we consider a periodic-review inventory model with price adjustment costs that consist of both fixed and variable components. In each period, demand is stochastic and price-dependent. The firm needs to coordinate the pricing and inventory replenishment decisions in each period to maximize its total discounted profit over a finite planning horizon. We develop the general model and characterize the optimal policies for two special scenarios, namely, <i>a model with inventory carryover and no fixed price-change costs</i> and <i>a model with fixed price-change costs and no inventory carryover</i>. Finally, we propose an intuitive heuristic policy to tackle the general system whose optimal policy is expected to be very complicated. Our numerical studies show that this heuristic policy performs well.", "e:keyword": ["Inventory system", "Pricing", "Price adjustment costs", "Fixed costs", "Base-stock"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0947", "e:abstract": "We consider a particular instance of a stochastic multi-leader multi-follower equilibrium problem in which players compete in the forward and spot markets in successive periods. Proving the existence of such equilibria has proved difficult, as has the construction of globally convergent algorithms for obtaining such points. By conjecturing a relationship between forward and spot decisions, we consider a variant of the original game and relate the equilibria of this game to a related simultaneous stochastic Nash game where forward and spot decisions are made simultaneously. We characterize the complementarity problem corresponding to the simultaneous Nash game and prove that it is indeed solvable. Moreover, we show that an equilibrium to this Nash game is a local Nash equilibrium of the conjectured variant of the multi-leader multi-follower game of interest. Numerical tests reveal that the difference between equilibrium profits between the original and constrained games are small. Under uncertainty, the equilibrium point of interest is obtainable as the solution to a stochastic mixed-complementarity problem. Based on matrix-splitting methods, a globally convergent decomposition method is suggested for such a class of problems. Computational tests show that the effort grows linearly with the number of scenarios. Further tests show that the method can address larger networks as well. Finally, some policy-based insights are drawn from utilizing the framework to model a two-settlement six-node electricity market.", "e:keyword": ["Nash equilibrium", "Complimentarity", "Stochastic programming", "Decomposition methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0948", "e:abstract": "There are many situations in supply chain scheduling when the supplier finds it impossible to meet the promised due dates for some orders. We present a model for the rescheduling of orders with simultaneous assignment of attainable revised due dates to minimize due date escalation and tardiness penalties for the supplier. We show that the problem is equivalent to minimizing the total tardiness with rejection with respect to the original due dates. We prove that the problem is (N-script)(P-script)-hard and present a pseudopolynomial algorithm for it. We also present a fully polynomial time approximation scheme for the problem. Finally, we discuss the implications of our solution for setting fair tardiness penalties when due dates have to be renegotiated because of the delays.", "e:keyword": ["Production/scheduling", "Sequencing", "Deterministic", "Approximations/heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0949", "e:abstract": "We study the impact of learning on the optimal policy and the time-to-decision in an infinite-horizon Bayesian sequential decision model with two irreversible alternatives: exit and expansion. In our model, a firm undertakes a small-scale pilot project to learn, via Bayesian updating, about the project's profitability, which is known to be in one of two possible states. The firm continuously observes the project's cumulative profit, but the true state of the profitability is not immediately revealed because of the inherent noise in the profit stream. The firm bases its exit or expansion decision on the posterior probability distribution of the profitability. The optimal policy is characterized by a pair of thresholds for the posterior probability. We find that the time-to-decision does not necessarily have a monotonic relation with the arrival rate of new information.", "e:keyword": ["Decision analysis", "Sequential", "Finance", "Investment criteria", "Probability", "Diffusion", "Bayesian sequential decision", "Project-specific investment", "Time to decision", "Brownian motion"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0950", "e:abstract": "In this paper we develop tight bounds on the expected values of several risk measures that are of interest to us. This work is motivated by the robust optimization models arising from portfolio selection problems. Indeed, the whole paper is centered around robust portfolio models and solutions. The basic setting is to find a portfolio that maximizes (respectively, minimizes) the expected utility (respectively, disutility) values in the midst of infinitely many possible ambiguous distributions of the investment returns fitting the given mean and variance estimations. First, we show that the single-stage portfolio selection problem within this framework, whenever the disutility function is in the form of <i>lower partial moments</i> (LPM), or <i>conditional value-at-risk</i> (CVaR), or <i>value-at-risk</i> (VaR), can be solved analytically. The results lead to the solutions for single-stage robust portfolio selection models. Furthermore, the results also lead to a multistage <i>adjustable robust optimization</i> (ARO) solution when the disutility function is the second-order LPM. Exploring beyond the confines of convex optimization, we also consider the so-called <i>S</i>-shaped value function, which plays a key role in the prospect theory of Kahneman and Tversky. The nonrobust version of the problem is shown to be NP-hard in general. However, we present an efficient procedure for solving the robust counterpart of the same portfolio selection problem. In this particular case, the consideration of the robustness actually helps to reduce the computational complexity. Finally, we consider the situation whereby we have some additional information about the chance that a quadratic function of the random distribution reaches a certain threshold. That information helps to further reduce the ambiguity in the robust model. We show that the robust optimization problem in that case can be solved by means of semidefinite programming (SDP), if no more than two additional chance inequalities are to be incorporated.", "e:keyword": ["Portfolio selection", "Robust optimization", "S-shaped function", "Chebyshev inequality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0951", "e:abstract": "We propose an interior point constraint generation (IPCG) algorithm for semi-infinite linear optimization (SILO) and prove that the algorithm converges to an (epsilon)-solution of SILO after a finite number of constraints is generated. We derive a complexity bound on the number of Newton steps needed to approach the updated (mu)-center after adding multiple violated constraints and a complexity bound on the total number of constraints that is required for the overall algorithm to converge.We implement our algorithm to solve the <i>sector duration optimization</i> problem arising in Leksell Gamma Knife® Perfexion™ (Elekta, Stockholm Sweden) treatment planning, a highly specialized treatment for brain tumors. Using real patient data provided by the Department of Radiation Oncology at Princess Margaret Hospital in Toronto, Ontario, Canada, we show that our algorithm can efficiently handle problems in real-life health-care applications by providing a quality treatment plan in a timely manner.Comparing our computational results with MOSEK, a commercial software package, we show that the IPCG algorithm outperforms the classical primal-dual interior point methods on sector duration optimization problem arising in Perfexion™ treatment planning. We also compare our results with that of a projected gradient method. In both cases we show that IPCG algorithm obtains a more accurate solution substantially faster.", "e:keyword": ["Semi-infinite linear optimization", "Second-order cone optimization", "Sector duration optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0952", "e:abstract": "We study the dynamic pricing implications of a new, behaviorally motivated reference price mechanism based on the peak-end memory mode. This model suggests that consumers anchor on a reference price that is a weighted average of the lowest and most recent prices. Loss-averse consumers are more sensitive to perceived losses than gains relative to this reference price. We find that a range of constant pricing policies is optimal for the corresponding dynamic pricing problem. This range is wider the more consumers anchor on lowest prices, and it persists when buyers are loss neutral, in contrast with previous literature. In a transient regime, the optimal pricing policy is monotone and converges to a steady-state price, which is lower the more extreme and salient the low-price anchor is. Our results suggest that behavioral regularities, such as peak-end anchoring and loss aversion, limit the benefits of varying prices, and caution that the adverse effects of deep discounts on the firm's optimal prices and profits might be more enduring than previous models predict.", "e:keyword": ["Dynamic pricing", "Dynamic programming", "Behavioral pricing", "Peak-end rule", "Prospect theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0953", "e:abstract": "We study the information asymmetry issues in a decentralized inventory-sharing system consisting of a manufacturer and two independent retailers, who privately hold demand information, noncooperatively place their orders, but cooperatively share inventories with each other. We find that although the manufacturer needs retailers' mean demand and standard deviation for her wholesale price decision, each retailer only needs to know the other retailer's demand standard deviation for his order quantity decision. However, an incentive compatibility analysis shows that retailers have incentives to share their demand information untruthfully. Although a truth-inducing scheme can be developed for a system with symmetric retailers who share information between themselves, no such scheme can be developed to ensure truth-telling to the manufacturer. Further, we develop a coordination mechanism (CIS) for the decentralized inventory-sharing system, <i>considering information asymmetry</i>. We show that CIS coordinates the manufacturer-retailers system and leads to an all-win situation under complete information. More importantly, CIS minimizes the value of information such that each party can obtain expected profits very close to their first-best profits even under asymmetric information and hence indirectly solves the information asymmetry problem. To our knowledge, this work is the first to study decentralized inventory sharing and its coordination <i>considering asymmetric information</i>.", "e:keyword": ["Decentralized inventory sharing", "Asymmetric information", "Information sharing", "Incentive compatibility", "Coordination"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0956", "e:abstract": "We present a mixed integer programming (MIP) model to solve the problems of (i) selecting an airport's optimal sequence of runway configurations and (ii) determining the optimal balance of arrivals and departures to be served at any moment. These problems, the runway configuration management (RCM) problem and the arrival/departure runway balancing (ADRB) problem, respectively, are of critical importance in minimizing the delay of both in-flight and on-the-ground aircraft along with their associated costs. We show that under mild assumptions on the time required to change between configurations, large realistic problem instances can be solved within several seconds. Furthermore, as assumptions are relaxed, optimal solutions are still found within several minutes. Comparison with a sophisticated baseline heuristic reveals that in many cases the potential reduction in cost from using the method is significant and could be expected to be of the order of at least 10%. Finally, we present an extension of the MIP model to solve these two problems for a group of airports in a metropolitan area such as New York (<i>metroplex</i>), where operations at each airport within the metroplex might have an impact on operations at some of the other airports due to limitations in shared airspace.", "e:keyword": ["Transportation", "Air traffic", "Programming", "Integer", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0957", "e:abstract": "This paper proves a long-standing conjecture regarding the optimal design of the <i>M</i>/<i>M</i>/<i>s</i> queue. The classical Erlang delay formula is shown to be a convex function of the number of servers when the server utilization is held constant. This means that when the server utilization is held constant, the marginal decrease in the probability that all servers are busy in the <i>M</i>/<i>M</i>/<i>s</i> queue brought about by the addition of two extra servers is always less than twice the decrease brought about by the addition of one extra server. As a consequence, a method of marginal analysis yields the optimal number of servers that minimize the waiting and service costs when the server utilization is held constant. In addition, it is shown that the expected number of customers in the queue and in the system, as well as the expected waiting time and sojourn in the <i>M</i>/<i>M</i>/<i>s</i> queue, are convex in the number of servers when the server utilization is held constant. These results are useful in design studies involving capacity planning in service operations. The classical Erlang loss formula is also shown to be a convex function of the number of servers when the server utilization is held constant.", "e:keyword": ["Queues", "Optimization", "Mathematics", "Convexity", "Probability", "Stochastic model applications", "Erlang delay formula", "Erlang loss formula", "M/M/s queue", "M/G/s/s queue", "Convexity", "Design of queues", "Marginal analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0958", "e:abstract": "Workers in a bucket brigade production system perform unproductive travel when they walk to get more work from their colleagues. We introduce a new design of bucket brigades to reduce unproductive travel. Under the new design, each worker works on one side of an aisle when he proceeds in one direction and works on the other side when he proceeds in the reverse direction. We propose simple rules for workers to share work under the new design and find a sufficient condition for the system to self-balance. Numerical examples suggest that the improvement in throughput by the new design can be as large as 30%. Even with a 20% reduction in labor, the new design can still increase throughput by 7%.", "e:keyword": ["Bucket brigades", "Self-balancing assembly lines", "Work-sharing", "Production", "Order-picking", "Dynamical systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0959", "e:abstract": "We consider the integrated optimization problem of procurement, processing, and trade of commodities in a multiperiod setting. Motivated by the operations of a prominent commodity processing firm, we model a firm that procures an input commodity and has processing capacity to convert the input into a processed commodity. The processed commodity is sold using forward contracts, while the input itself can be traded at the end of the horizon. We solve this problem optimally and derive closed-form expressions for the marginal value of input and output inventory. We find that the optimal procurement and processing decisions are governed by price-dependent inventory thresholds. We use commodity markets data for the soybean complex to conduct numerical studies and find that approximating the joint price processes of multiple output commodities using a single, composite output product and using the approximate price process to determine procurement and processing decisions is near optimal. Compared to a myopic spread-option-based heuristic, the optimization-based dynamic programming policy provides significant benefits under conditions of tight processing capacities and high price volatilities. Finally, we propose an approximation procedure to compute heuristic policies and an upper bound to compare the heuristic against, when commodity prices follow multifactor processes.", "e:keyword": ["Inventory", "Procurement", "Processing", "Commodities", "Dynamic programming", "Real options", "Agricultural commodities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0960", "e:abstract": "We consider a general parallel server system model with multiple customer classes and several flexible multiserver pools, in the many-server asymptotic regime where the input rates and server pool sizes are scaled up linearly to infinity. Service of a customer brings a constant reward, which depends on its class. The objective is to maximize the long-run reward rate. Our primary focus is on overloaded systems. Unlike in the case when the system is not overloaded, where the main decision is how to allocate resources to incoming customers, in this case it is also crucial to determine which customers will be admitted to the system. We propose a real-time, parsimonious, robust routing policy, SHADOW-RM, which does not require the knowledge of customer input rates and does not solve any optimization problem explicitly, and we prove its asymptotic optimality. Then, by combining SHADOW-RM with another policy, SHADOW-LB, proposed in our previous work for systems that are not overloaded, we suggest policy SHADOW-TANDEM, which automatically and seamlessly detects overload and reduces to one of the schemes, SHADOW-RM or SHADOW-LB, accordingly. Extensive simulations demonstrate a remarkably good performance of proposed policies.", "e:keyword": ["Queueing networks", "Large flexible server pools", "Routing and scheduling", "Revenue maximization", "Shadow routing", "Many-server asymptotics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0961", "e:abstract": "Urban growth compromises open space and ecosystem functions. To mitigate the negative effects, some agencies use reserve selection models to identify conservation sites for purchase or retention. Existing models assume that conservation has no impact on nearby land prices. We propose a new integer program that relaxes this assumption via adaptive cost coefficients. Our model accounts for the two key land price feedbacks that arise in markets where conservation competes with development: the amenity premium and price increases driven by shifts in market equilibriums. We illustrate the mechanics of the proposed model in a real land retention context. The results suggest that in competitive land markets, the optimal conservation strategy during the initial phase of the retention effort might be to use available dollars to buy fewer parcels with smaller total area that are under high risk of development. We show that failure to capture the land-price feedbacks can lead to significant losses in biological conservation. The present study is the first to create a reserve selection model that captures the economic theory of competitive land markets in a dynamic framework, produces tangible, parcel-level conservation recommendations, and works on problems with thousands of potential site selection decisions and several planning periods.", "e:keyword": ["Integer programming", "Applications", "Natural resources", "Land development"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0962", "e:abstract": "Point processes with stochastic arrival intensities are ubiquitous in many areas, including finance, insurance, reliability, health care, and queuing. They can be simulated from a Poisson process by time scaling with the cumulative intensity. The paths of the cumulative intensity are often generated with a discretization method. However, discretization introduces bias into the simulation results. The magnitude of the bias is difficult to quantify. This paper develops a sampling method that eliminates the need to discretize the cumulative intensity. The method is based on a projection argument and leads to unbiased simulation estimators. It is exemplified for a point process whose intensity is a function of a jump-diffusion process and the point process itself. In this setting, the method facilitates the exact sampling of both the point process and the driving jump-diffusion process. Numerical experiments demonstrate the effectiveness of the method.", "e:keyword": ["Point process", "Intensity projection", "Filtering", "Exact sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0963", "e:abstract": "Databases are a significant source of information in organizations and play a major role in managerial decision-making. This study considers how to process commercial data on customer purchasing timing to provide insights on the rate of new product adoption by the company's consumers. Specifically, we show how to use the separation-deviation model (SD-model) to rate customers according to their proclivity for adopting products for a given line of high-tech products. We provide a novel interpretation of the SD-model as a unidimensional scaling technique and show that, in this context, it outperforms several dimension-reduction and scaling techniques. We analyze the results with respect to various dimensions of the customer base and report on the generated insights.", "e:keyword": ["Decision analysis", "Applications", "Theory", "Networks/graphs", "Applications", "Marketing", "Buyer behavior", "New products", "Unidimensional scaling methodology"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0964", "e:abstract": "We explore the computational complexity of computing pure Nash equilibria for a new class of strategic games called integer programming games, with differences of piecewise-linear convex functions as payoffs. Integer programming games are games where players' action sets are integer points inside of polytopes. Using recent results from the study of short rational generating functions for encoding sets of integer points pioneered by Alexander Barvinok, we present efficient algorithms for enumerating all pure Nash equilibria, and other computations of interest, such as the pure price of anarchy and pure threat point, when the dimension and number of “convex” linear pieces in the payoff functions are fixed. Sequential games where a leader is followed by competing followers (a Stackelberg--Nash setting) are also considered.", "e:keyword": ["Algorithmic game theory", "Integer programming", "Barvinok's generating functions", "Pure Nash equilibria", "Stackelberg--Nash games"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0965", "e:abstract": "This paper describes an exact algorithm capable of solving large-scale instances of the well-known <i>uncapacitated hub location problem with multiple assignments</i>. The algorithm applies Benders decomposition to a strong path-based formulation of the problem. The standard decomposition algorithm is enhanced through the inclusion of several features such as the use of a multicut reformulation, the generation of strong optimality cuts, the integration of reduction tests, and the execution of a heuristic procedure. Extensive computational experiments were performed to evaluate the efficiency and robustness of the algorithm. Computational results obtained on classical benchmark instances (with up to 200 nodes) and on a new and more difficult set of instances (with up to 500 nodes) confirm the efficiency of the algorithm.", "e:keyword": ["Hub location", "Benders decomposition", "Pareto-optimal cuts", "Elimination tests"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0966", "e:abstract": "We introduce a new type of search game called the “find-and-fetch” game <i>F</i>(<i>Q</i>, <i>O</i>). The Hider simply picks any point <i>H</i> in the network <i>Q</i>. The Searcher starts at time zero at a given point <i>O</i> of <i>Q</i>, moving at unit speed until he reaches <i>H</i> (finds the Hider). Then he returns at a given speed (rho) along the shortest path back to <i>O</i>, arriving at time <i>R</i>, the payoff. This models the problem faced in many types of search, including search-and-rescue problems and foraging problems of animals (where food must be found and returned to the lair). When <i>Q</i> is a binary tree, we derive optimal probabilities for the Searcher to branch at the nodes. These probabilities give a positive bias towards searching longer branches first. We show that the minimax value of the return time <i>R</i> (the game value of <i>F</i>(<i>Q</i>, <i>O</i>)) is (mu) + <i>D</i>/(rho), where (mu) is the total length of <i>Q</i> and <i>D</i> is the mean distance from the root <i>O</i> to the leaves (terminal nodes) of <i>Q</i>, where the mean is taken with respect to what is known as the equal branch density distribution. As (rho) goes to infinity, our problem reduces to the search game model where the payoff is simply the time to reach the Hider, and our results tend to those obtained by Gal [Gal, S. 1979. Search games with mobile and immobile hider. <i>SIAM J. Control Optim.</i> <b>17</b>(1) 99--122] and Anderson and Gal [Anderson, E. J., S. Gal. 1990. Search in a maze. <i>Probab. Engrg. Inform. Sci.</i> <b>4</b>(3) 311--318] for that model. We also apply our return time formula (mu) + <i>D</i>/(rho) to determine the ideal location for the root (lair or rescue center) <i>O</i>, assuming it can be moved. In the traditional “find only” model, the location of <i>O</i> does not matter.", "e:keyword": ["Search and surveillance", "Networks/graphs", "Tree algorithms", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0967", "e:abstract": "We consider the <i>vehicle routing problem with stochastic demands</i> (VRPSD). We give randomized approximation algorithms achieving approximation guarantees of 1 + (alpha) for <i>split-delivery</i> VRPSD, and 2 + (alpha) for <i>unsplit-delivery</i> VRPSD; here (alpha) is the best approximation guarantee for the traveling salesman problem. These bounds match the best known for even the respective deterministic problems [Altinkemer, K., B. Gavish. 1987. Heuristics for unequal weight delivery problems with a fixed error guarantee. <i>Oper. Res. Lett.</i> <b>6</b>(4) 149--158; Altinkemer, K., B. Gavish. 1990. Heuristics for delivery problems with constant error guarantees. <i>Transportation Res.</i> <b>24</b>(4) 294--297]. We also show that the “cyclic heuristic” for split-delivery VRPSD achieves a constant approximation ratio, as conjectured in Bertsimas [Bertsimas, D. J. 1992. A vehicle routing problem with stochastic demand. <i>Oper. Res.</i> <b>40</b>(3) 574--585].", "e:keyword": ["Analysis of algorithms", "Suboptimal algorithms", "Transportation", "Vehicle routing", "Networks/graphs", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0968", "e:abstract": "In this paper, we present a closed queueing model to determine efficient nurse staffing policies. We explicitly model the workload experienced by <i>s</i> nurses within a single medical unit with <i>n</i> homogeneous patients as a closed <i>M</i>/<i>M</i>/<i>s</i>//<i>n</i> queueing system, where each patient alternates between requiring assistance and not. The performance of the medical unit is based on the probability of excessive delay, the relative frequency with which the delay between the onset of patient neediness and the provision of care from a nurse exceeds a given time threshold. Using new many-server asymptotic results, we find that effective staffing policies should deviate from threshold-specific nurse-to-patient ratios by factors that take into account the total number of patients present in the unit. In particular, our staffing rule significantly differs from California Bill AB 394, legislation that mandates fixed nurse-to-patient staffing ratios. Simulations show that our results are robust to delay-dependent service times, generally distributed service times, and nonhomogeneous patients, i.e., those with different acuity levels.", "e:keyword": ["Queueing system", "Health care", "Public policy", "Nursing", "Staffing", "Many-server limit theorems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0969", "e:abstract": "For a dynamic joint price and lead-time quotation problem with a fairly general demand function, we show that the policy consisting of a threshold and a reward-maximizing lead-time is optimal. This policy offers some interesting managerial insights. Under this policy, finding the exact optimal quotation can be accomplished by single-variable policy iterations of unimodal value functions.", "e:keyword": ["Dynamic pricing", "Lead-time quotation", "Threshold policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0970", "e:abstract": "We consider the Kiefer-Wolfowitz (KW) stochastic approximation algorithm and derive general upper bounds on its mean-squared error. The bounds are established using an elementary induction argument and phrased directly in the terms of tuning sequences of the algorithm. From this we deduce the nonnecessity of one of the main assumptions imposed on the tuning sequences by Kiefer and Wolfowitz [Kiefer, J., J. Wolfowitz. 1952. Stochastic estimation of the maximum of a regression function. <i>Ann. Math. Statist.</i> <b>23</b>(3) 462--466] and essentially all subsequent literature. The optimal choice of sequences is derived for various cases of interest, and an adaptive version of the KW algorithm, scaled-and-shifted KW (or SSKW), is proposed with the aim of improving its finite-time behavior. The key idea is to dynamically scale and shift the tuning sequences to better match them with characteristics of the unknown function and noise level, and thus improve algorithm performance. Numerical results are provided that illustrate that the proposed algorithm retains the convergence properties of the original KW algorithm while dramatically improving its performance in some cases.", "e:keyword": ["Stochastic optimization", "Stochastic approximation", "The Kiefer-Wolfowitz algorithm", "Mean-squared-error convergence", "Finite-time improvement"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0971", "e:abstract": "We formulate and solve the problem of making advance energy commitments for wind farms in the presence of a storage device with conversion losses, mean-reverting price process, and an autoregressive energy generation process from wind. We derive an optimal commitment policy under the assumption that wind energy is uniformly distributed. Then, the stationary distribution of the storage level corresponding to the optimal policy is obtained, from which the economic value of the storage as the relative increase in the expected revenue due to the existence of storage is obtained.", "e:keyword": ["Markov decision process", "Dynamic programming", "Energy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0972", "e:abstract": "Many service providers use delay announcements to inform customers of anticipated delays. However, this information is usually not provided immediately but after a short period of time (spent either waiting or occupied by the system). The focus of this paper is on the impact of this postponement on the ability of the firm to influence customer behavior by communicating nonverifiable congestion information to its customers, as well as on the profits and utilities for the firm and the customers, respectively. We show that this postponement can actually help the firm create credibility and augment the resulting equilibrium. However, in other settings this delay can also detract from the resulting equilibrium. Furthermore, we show that whenever credibility is created it improves not only the profit for the firm but also the customers' overall utility under certain settings.", "e:keyword": ["Noncooperative game", "Queues", "Tandem queues", "Admission control", "Access control", "Real-time information", "Strategic consumer"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0973", "e:abstract": "Risk managers are often confronted with the evaluation of operational policies in which two or more system components are simultaneously affected by a change. In these instances, the decision-making process should be informed by the relevance of interactions. However, because of system and model complexity, a rigorous study for determining whether and how interactions quantitatively impact operational choices has not been developed yet. In light of the central role played by the multilinearity of the decision support models, we investigate the presence of interactions in multilinear functions first. We identify interactions that can be a priori excluded from the analysis. We introduce sensitivity measures that apportion the model output change to individual factors and interaction contributions in an exact fashion. The sensitivity measures are linked to graphical representation methods as tornado diagrams and Pareto charts, and a systematic way of inferring managerial insights is presented. We then specialize the findings to reliability and probabilistic safety assessment (PSA) problems. We set forth a procedure for determining the magnitude of changes that make interactions relevant in the analysis. Quantitative results are discussed by application to a PSA model developed at NASA to support decision making in space mission planning and design. Numerical findings show that suboptimal decisions concerning the components on which to focus managerial attention can be made, if the decision-making process is not informed by the relevance of interactions.", "e:keyword": ["Risk analysis", "Sensitivity analysis", "Multilinearity", "Interactions", "System risk", "Safety", "Probabilistic safety assessment", "Importance measures", "Operational decision making"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0974", "e:abstract": "We develop new, improved real-time delay predictors for many-server service systems with a time-varying arrival rate, a time-varying number of servers, and customer abandonment. We develop four new predictors, two of which exploit an established deterministic fluid approximation for a many-server queueing model with those features. These delay predictors can be used to make delay announcements. We use computer simulation to show that the proposed predictors outperform previous predictors.", "e:keyword": ["Delay prediction", "Delay announcements", "Simulation", "Time-varying arrival rates", "Time-varying number of servers", "Nonstationary queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0975", "e:abstract": "In this paper, we describe an effective exact method for solving both the capacitated vehicle routing problem (<sc>cvrp</sc>) and the vehicle routing problem with time windows (<sc>vrptw</sc>) that improves the method proposed by Baldacci et al. [Baldacci, R., N. Christofides, A. Mingozzi. 2008. An exact algorithm for the vehicle routing problem based on the set partitioning formulation with additional cuts. <i>Math. Programming</i> <b>115</b>(2) 351--385] for the <sc>cvrp</sc>. The proposed algorithm is based on the set partitioning (SP) formulation of the problem. We introduce a new route relaxation called <i>ng</i>-route, used by different dual ascent heuristics to find near-optimal dual solutions of the LP-relaxation of the SP model. We describe a column-and-cut generation algorithm strengthened by valid inequalities that uses a new strategy for solving the pricing problem. The new <i>ng</i>-route relaxation and the different dual solutions achieved allow us to generate a reduced SP problem containing all routes of any optimal solution that is finally solved by an integer programming solver. The proposed method solves four of the five open Solomon's <sc>vrptw</sc> instances and significantly improves the running times of state-of-the-art algorithms for both <sc>vrptw</sc> and <sc>cvrp</sc>.", "e:keyword": ["Vehicle routing", "Time windows", "Dual ascent heuristic", "Column-and-cut generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0976", "e:abstract": "Delay announcements informing customers about anticipated service delays are prevalent in service-oriented systems. How delay announcements can influence customers in service systems is a complex problem that depends on both the dynamics of the underlying queueing system and on the customers' strategic behavior. We examine this problem of information communication by considering a model in which both the firm and the customers act strategically: the firm in choosing its delay announcement while anticipating customer response, and the customers in interpreting these announcements and in making the decision about when to join the system and when to balk. We characterize the equilibrium language that emerges between the service provider and her customers. The analysis of the emerging equilibria provides new and interesting insights into customer-firm information sharing. We show that even though the information provided to customers is nonverifiable, it improves the profits of the firm and the expected utility of the customers. The robustness of the results is illustrated via various extensions of the model. In particular, studying models with incomplete information on the system parameters allows us also to highlight the role of information provision in managing customer expectations regarding the congestion in the system. Further, the information could be as simple as “high congestion”/“low congestion” announcements, or it could be as detailed as the true state of the system. We also show that firms may choose to shade some of the truth by using <i>intentional vagueness</i> to lure customers.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Games/group decisions", "Noncooperative", "Probability", "Stochastic model applications", "Queues", "Applications", "Balking and reneging"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0979", "e:abstract": "Resource allocation problems play a key role in many applications, including traffic networks, telecommunication networks, and economics. In most applications, the allocation of resources is determined by a finite number of independent players, each optimizing an individual objective function. An important question in all these applications is the degree of suboptimality caused by selfish resource allocation. We consider the worst-case efficiency of cost sharing methods in resource allocation games in terms of the ratio of the minimum guaranteed surplus of a Nash equilibrium and the maximal surplus. Our main technical result is an upper bound on the efficiency loss that depends on the class of allowable cost functions and the class of allowable cost sharing methods. We demonstrate the power of this bound by evaluating the worst-case efficiency loss for three well-known cost sharing methods: incremental cost sharing, marginal cost pricing, and average cost sharing.", "e:keyword": ["Resource allocation games", "Cost sharing methods", "Network routing games", "Marginal cost pricing", "Inefficiency of equilibria"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0980", "e:abstract": "We consider a model where a finite number of producers compete to meet an infinitely divisible but <i>inelastic</i> demand for a product. Each firm is characterized by a production cost that is convex in the output produced, and firms act as profit maximizers. We consider a uniform price market design that uses <i>supply function bidding</i>: firms declare the amount they would supply at any positive price, and a single price is chosen to clear the market. We are interested in evaluating the impact of price-anticipating behavior both on the allocative efficiency of the market and on the prices seen at equilibrium. We show that by <i>restricting</i> the strategy space of the firms to parameterized supply functions, we can provide upper bounds on both the inflation of aggregate cost at the Nash equilibrium relative to the socially optimal level, as well as the markup of the Nash equilibrium price above the competitive level: as long as <i>N</i> > 2 firms are competing, these quantities are both upper bounded by 1 + 1/(<i>N</i> - 2). This result holds even in the presence of asymmetric cost structure across firms. We also discuss several extensions, generalizations, and related issues.", "e:keyword": ["Supply function equilibrium", "Resource allocation", "Efficiency loss"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0983", "e:abstract": "A key operational problem for those charged with the security of vulnerable facilities (such as airports or art galleries) is the scheduling and deployment of patrols. Motivated by the problem of optimizing randomized, and thus unpredictable, patrols, we present a class of patrolling games. The facility to be patrolled can be thought of as a network or graph <i>Q</i> of interconnected nodes (e.g., rooms, terminals), and the Attacker can choose to attack any node of <i>Q</i> within a given time <i>T</i>. He requires <i>m</i> consecutive periods there, uninterrupted by the Patroller, to commit his nefarious act (and win). The Patroller can follow any path on the graph. Thus, the patrolling game is a win-lose game, where the Value is the probability that the Patroller successfully intercepts an attack, given best play on both sides. We determine analytically either the Value of the game, or bounds on the Value, for various classes of graphs, and we discuss possible extensions and generalizations.", "e:keyword": ["Games", "Noncooperative", "Military", "Search/surveillance", "Decision analysis", "Risk", "Networks/graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0984", "e:abstract": "This note considers a joint inventory-pricing control problem in an infinite-horizon periodic-review system. Demand in a period is random and depends on the posted price. Besides the holding and shortage costs, the system incurs inventory-replenishment costs that consist of both variable and fixed components. At the beginning of each period, a joint inventory and pricing decision is made. Under the long-run average profit criterion, we show that an optimal policy exists within the class of so-called (<i>s</i>, <i>S</i>, <i>p</i>) policies. This is established based on our algorithmic development, which also results in an algorithm for finding an optimal (<i>s</i>, <i>S</i>, <i>p</i>) policy.", "e:keyword": ["Joint pricing and inventory control", "Setup cost", "Price dependent demand", "Stochastic inventory model"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0985", "e:abstract": "In a recent paper we considered two networked service systems, each having its own customers and designated service pool with many agents, where all agents are able to serve the other customers, although they may do so inefficiently. Usually the agents should serve only their own customers, but we want an automatic control that activates serving some of the other customers when an unexpected overload occurs. Assuming that the identity of the class that will experience the overload or the timing and extent of the overload are unknown, we proposed a queue-ratio control with thresholds: When a weighted difference of the queue lengths crosses a prespecified threshold, with the weight and the threshold depending on the class to be helped, serving the other customers is activated so that a certain queue ratio is maintained. We then developed a simple deterministic steady-state fluid approximation, based on flow balance, under which this control was shown to be optimal, and we showed how to calculate the control parameters. In this sequel we focus on the fluid approximation itself and describe its transient behavior, which depends on a heavy-traffic averaging principle. The new fluid model developed here is an ordinary differential equation driven by the instantaneous steady-state probabilities of a fast-time-scale stochastic process. The averaging principle also provides the basis for an effective Gaussian approximation for the steady-state queue lengths. Effectiveness of the approximations is confirmed by simulation experiments.", "e:keyword": ["Large-scale service systems", "Overload control", "Many-server queues", "Fluid approximation", "Averaging principle", "Separation of time scales", "Differential equation", "Heavy traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0986", "e:abstract": "We give examples to show that the necessary conditions of Theorem 2.1 and Theorem 3.4 in Cheng and Wu [Cheng, T. C. E., Y. N. Wu. 2006. A multiproduct, multicriterion supply-demand network equilibrium model. <i>Oper. Res</i>. <b>54</b>(3) 544--554] for a multiproduct network equilibrium model may not hold.", "e:keyword": ["Network equilibrium", "Variational inequality", "Wardrop's equilibrium principle"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0987", "e:abstract": "We examine how to design a flexible process structure for a production system to match supply with demand more effectively. We argue that good flexible process structures are essentially highly connected graphs, and we use the concept of graph expansion (a measure of graph connectivity) to achieve various insights into this design problem. Whereas existing literature on process flexibility has focused on the <i>expected</i> performance of process structure, we analyze in this paper the <i>worst-case</i> performance of the flexible structure design problem under a more general setting, which encompasses a large class of objective functions. Chou et al. [Chou, M. C., G. Chua, C. P. Teo, H. Zheng. 2010. Design for process flexibility: Efficiency of the long chain and sparse structure. <i>Oper. Res.</i> <b>58</b>(1) 43--58] showed the existence of a sparse process structure that performs nearly as well as the fully flexible system on average, but the approach using random sampling yields few insights into the nature of the process structure. We show that the (psi)-expander structure, a variant of the graph expander structure (a highly connected but sparse graph) often used in communication networks, is within (epsilon)-optimality of the fully flexible system <i>for all demand scenarios</i>. Furthermore, the same expander structure works uniformly well for all objective functions in our class. Based on this insight, we derive design guidelines for general nonsymmetrical systems and develop a simple and easy-to-implement heuristic to design flexible process structures. Numerical results show that this simple heuristic performs well for a variety of numerical examples previously studied in the literature and compares favourably even with the best solutions obtained via extensive simulation and known demand distribution.", "e:keyword": ["Facilities planning", "Design", "Production", "Flexible manufacturing", "Networks/graphs", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0988", "e:abstract": "Seasonal influenza is a major public health concern, and the first line of defense is the flu shot. Antigenic drifts and the high rate of influenza transmission require annual updates to the flu shot composition. The World Health Organization recommends which flu strains to include in the annual vaccine, based on surveillance and epidemiological analysis. There are two critical decisions regarding the flu shot design. One is its composition; currently, three strains constitute the flu shot, and they influence vaccine effectiveness. Another critical decision is the timing of the composition decisions, which affects the flu shot production. Both of these decisions have to be made under uncertainty many months before the flu season starts. We quantify the trade-offs involved through a multistage stochastic mixed-integer program that determines the optimal flu shot composition and its timing in a stochastic and dynamic environment.We incorporate risk sensitivity through mean-risk models. Our results provide valuable insights for pressing policy issues.", "e:keyword": ["Influenza vaccine", "Multistage stochastic integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0989", "e:abstract": "The capacitated location-routing problem (LRP) consists of opening one or more depots on a given set of a-priori defined depot locations, and designing, for each opened depot, a number of routes in order to supply the demands of a given set of customers. With each depot are associated a fixed cost for opening it and a capacity that limits the quantity that can be delivered to the customers. The objective is to minimize the sum of the fixed costs for opening the depots and the costs of the routes operated from the depots. This paper describes a new exact method for solving the LRP based on a set-partitioning-like formulation of the problem. The lower bounds produced by different bounding procedures, based on dynamic programming and dual ascent methods, are used by an algorithm that decomposes the LRP into a limited set of multicapacitated depot vehicle-routing problems (MCDVRPs). Computational results on benchmark instances from the literature show that the proposed method outperforms the current best-known exact methods, both for the quality of the lower bounds achieved and the number and the dimensions of the instances solved to optimality.", "e:keyword": ["Location routing", "Set partitioning", "Dual ascent", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0990", "e:abstract": "We consider an inbound call center with a fixed reward per call and communication and agent costs. By controlling the number of lines and the number of agents, we can maximize the profit. Abandonments are included in our performance model. Monotonicity results for the maximization problem are obtained, which lead to an efficient optimization procedure. We give a counterexample to the concavity in the number of agents, which is equivalent to saying that the law of diminishing returns does not hold. Numerical results are given.", "e:keyword": ["Call centers", "Monotonicity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0991", "e:abstract": "We apply a new corrected diffusion approximation for the Erlang C formula to determine staffing levels in cost minimization and constraint satisfaction problems. These problems are motivated by large customer contact centers that are modeled as an <i>M</i>/<i>M</i>/<i>s</i> queue with <i>s</i> the number of servers or agents. The proposed staffing levels are refinements of the celebrated square-root safety-staffing rule and have the appealing property that they are as simple as the conventional square-root safety-staffing rule. In addition, we provide theoretical support for the empirical fact that square-root safety-staffing works well for moderate-sized systems.", "e:keyword": ["Call centers", "QED regime", "Corrected diffusion approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0992", "e:abstract": "We cast models of the generation capacity expansion type formally developed for the monopoly regime into equilibrium models better adapted for a competitive environment. We focus on some of the risks faced today by investors in generation capacity and thus pose the problem as a stochastic equilibrium model. We illustrate the approach on the problem of the incentive to invest. Agents can be risk neutral or risk averse. We model risk aversion through the CVaR of plants' profit. The CVaR induces risk-adjusted probabilities according to which investors value their plants. The model is formulated as a complementarity problem (including the CVaR valuation of investments). An illustration is provided on a small problem that captures several features of today's electricity world: a choice often restricted to coal and gas units, a peaky load curve because of wind penetration, uncertain fuel prices, and an evolving carbon market. We assess the potential of the approach by comparing energy-only and capacity market organizations in this risky environment. Our results can be summarized as follows: a deterministic analysis overlooks some changes of capacity structure induced by risk, whether in the capacity market or energy-only organizations. The risk-neutral analysis also misses a shift towards less capital-intensive technologies that may result from risk aversion. Last, risk aversion also increases the shortage of capacity compared to the risk-neutral view in the energy-only market when the price cap is low. This may have a dramatic impact on the bill to the final consumer. The approach relies on mathematical programming techniques and can be extended to full-size problems. The results are illustrative and may deserve more investigation.", "e:keyword": ["Capacity adequacy", "Risk functions", "Stochastic equilibrium models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0993", "e:abstract": "Simulation of small probabilities has important applications in many disciplines. The probabilities considered in value-at-risk (VaR) are moderately small. However, the variance reduction techniques developed in the literature for VaR computation are based on large-deviations methods, which are good for very small probabilities. Modeling heavy-tailed risk factors using multivariate <i>t</i> distributions, we develop a new method for VaR computation. We show that the proposed method minimizes the variance of the importance-sampling estimator exactly, whereas previous methods produce approximations to the exact solution. Thus, the proposed method consistently outperforms existing methods derived from large deviations theory under various settings. The results are confirmed by a simulation study.", "e:keyword": ["Importance sampling", "Moderate deviation", "Multivariate t distribution", "Quadratic approximation", "Component VaR"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0996", "e:abstract": "As targeted advertising becomes prevalent in a wide variety of media vehicles, planning models become increasingly important to ad networks that need to match ads to appropriate audience segments, provide a high quality of service (meet advertisers' goals), and ensure that ad serving opportunities are not wasted. We define <i>Guaranteed Targeted Display Advertising</i> (GTDA) as a class of media vehicles that include webpage banner ads, video games, electronic outdoor billboards, and the next generation of digital TV, and formulate the GTDA planning problem as a transportation problem with quadratic objective. By modeling audience uncertainty, forecast errors, and the ad server's execution of the plan, we derive sufficient conditions that state when our quadratic objective is a good surrogate for several ad delivery performance metrics. Moreover, our quadratic objective allows us to construct duality-based bounds for evaluating aggregations of the audience space, leading to two efficient algorithms for solving large problems: the first intelligently refines the audience space into successively smaller blocks, and the second uses scaling to find a feasible solution given a fixed audience space partition. Near-optimal schedules can often be produced despite significant aggregation.", "e:keyword": ["Guaranteed targeted display advertising", "Advertising", "Planning", "Aggregation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0997", "e:abstract": "A branch-price-and-cut algorithm is developed for a complex maritime inventory-routing problem with varying storage capacities and production/consumption rates at facilities. The resulting mixed-integer pricing problem is solved exactly and efficiently using a dynamic program that exploits certain “extremal” characteristics of the pricing problem. The formulation is tightened by using the problem's boundary conditions in preprocessing and to restrict the set of columns that are produced by the pricing problem. Branching schemes and cuts are introduced that can be implemented efficiently and that preserve the structure of the pricing problem. Some of the cuts are inspired by the capacity cuts known for the vehicle-routing problem, whereas others specifically target fractional solutions brought about by individual vessels “competing” for limited inventory at load ports and limited storage capacity at discharge ports. The branch-price-and-cut approach solves practically sized problems motivated by the operations of an oil company to optimality, and it provides reasonable bounds for larger instances.", "e:keyword": ["Maritime inventory routing", "Integer programming", "Column generation", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0998", "e:abstract": "Consider a system with <i>K</i> parallel servers, each with its own waiting room. Upon arrival, a job is routed to the queue of one of the servers. Finding a routing policy that minimizes the total workload in the system is a known difficult problem in general. Even if the optimal policy is identified, the policy would require the full queue length information at the arrival of each job; for example, the join-the-shortest-queue policy (which is known to be optimal for identical servers with exponentially distributed service times) would require comparing the queue lengths of all the servers. In this paper, we consider a balanced routing policy that examines only a subset of <i>c</i> servers, with 1 (le) <i>c</i> (le) <i>K</i>: specifically, upon the arrival of a job, choose a subset of <i>c</i> servers with a probability proportional to their service rates, and then route the job to the one with the shortest queue among the <i>c</i> chosen servers. Under such a balanced policy, we derive the diffusion limits of the queue length processes and the workload processes. We note that the diffusion limits are the same for these processes regardless the choice of <i>c</i>, as long as <i>c</i> (ge) 2. We further show that the proposed balanced routing policy for any fixed <i>c</i> (ge) 2 is asymptotically optimal in the sense that it minimizes the workload over all time in the diffusion limit. In addition, the policy helps to distribute work among all the servers evenly.", "e:keyword": ["Balanced routing", "Join-the-shortest-queue", "Fluid limit", "Diffusion limit", "Asymptotic optimality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.0999", "e:abstract": "We derive a one-period look-ahead policy for finite- and infinite-horizon online optimal learning problems with Gaussian rewards. Our approach is able to handle the case where our prior beliefs about the rewards are correlated, which is not handled by traditional multiarmed bandit methods. Experiments show that our KG policy performs competitively against the best-known approximation to the optimal policy in the classic bandit problem, and it outperforms many learning policies in the correlated case.", "e:keyword": ["Multiarmed bandit", "Optimal learning", "Online learning", "Knowledge gradient", "Gittins index", "Index policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1000", "e:abstract": "In this paper we consider the problem of maximizing a separable concave function over a polymatroid. More specifically, we study the submodularity of its optimal objective value in the parameters of the objective function. This question is interesting in its own right and is encountered in many applications. But our research has been motivated mainly by a cooperative game associated with the well-known joint replenishment model. By applying our general results on polymatroid optimization, we prove that this cooperative game is submodular (i.e., its characteristic cost function is submodular) if the joint setup cost is a normalized and nondecreasing submodular function. Furthermore, the same result holds true for a more general one-warehouse multiple retailer game, which affirmatively answers an open question posed by Anily and Haviv [Anily, S., M. Haviv. 2007. The cost allocation problem for the first order interaction joint replenishment model. <i>Oper. Res.</i> <b>55</b>(2) 292--302].", "e:keyword": ["Polymatroid optimization", "Separable concave function", "Cooperative games", "Joint replenishment problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1001", "e:abstract": "Problems dealing with the design and operations of gas transmission networks are challenging. The standard approaches lead to a difficult nonlinear nonconvex optimization problem. To get around this difficulty, we use a minimum energy principle to define stationary flows in the network. This solution minimizes the total energy dissipated in the system. We extend the minimization process to the choice of suitable diameters on the reinforcing arcs and add a constraint that limits the monetary cost of investment and of purchase and delivery of gas. Under a suitable and acceptable approximation of the structure of the investment cost function, the new problem turns out to be convex and tractable even for very large networks.", "e:keyword": ["Gas transmission networks", "Reinforcement", "Convex optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1002", "e:abstract": "A military arms race is characterized by an iterative development of measures and countermeasures. An attacker attempts to introduce new weapons in order to gain some advantage, whereas a defender attempts to develop countermeasures that can mitigate or even eliminate the effects of the weapons. This paper addresses the defender's decision problem: given limited resources, which countermeasures should be developed and how much should be invested in their development to minimize the damage caused by the attacker's weapons over a certain time horizon. We formulate several optimization models, corresponding to different operational settings, as constrained shortest-path problems and variants thereof. We then demonstrate the potential applicability and robustness of this approach with respect to various scenarios.", "e:keyword": ["Arms race", "Network optimization", "Constrained shortest path"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1003", "e:abstract": "We present a methodology for long-term mine planning based on a general capacitated multicommodity network flow formulation. It considers underground and open-pit ore deposits sharing multiple downstream processing plants over a long horizon. The purpose of the model is to optimize several mines in an integrated fashion, but real size instances are hard to solve due to the combinatorial nature of the problem. We tackle this by solving the relaxation of a tight linear formulation, and we round the resulting near-integer solution with a customized procedure. The model has been implemented at Codelco, the largest copper producer in the world. Since 2001, the system has been used on a regular basis and has increased the net present value of the production plan for a single mine by 5%. Moreover, integrating multiple mines provided an additional increase of 3%. The system has allowed planners to evaluate more scenarios. In particular, the model was used to study the option of delaying by four years the conversion of Chiquicamata, Codelco's largest open-pit mine, to underground operations.", "e:keyword": ["Industries", "Mining/metals", "Networks/graphs", "Multicommodity", "Linear programming", "Large-scale systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1004", "e:abstract": "This paper presents a general framework based on copulas for modeling dependent multivariate uncertainties through the use of a decision tree. The proposed dependent decision tree model allows multiple dependent uncertainties with arbitrary marginal distributions to be represented in a decision tree with a sequence of conditional probability distributions. This general framework could be naturally applied in decision analysis and real options valuations, as well as in more general applications of dependent probability trees. While this approach to modeling dependencies can be based on several popular copula families as we illustrate, we focus on the use of the normal copula and present an efficient computational method for multivariate decision and risk analysis that can be standardized for convenient application.", "e:keyword": ["Correlation", "Copulas", "Multivariate decision and risk analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1005", "e:abstract": "Given a combinatorial optimization problem with an arbitrary partition of the set of random objective coefficients, we evaluate the tightest-possible bound on the expected optimal value for joint distributions consistent with the given multivariate marginals of the subsets in the partition. For univariate marginals, this bound was first proposed by Meilijson and Nadas [Meilijson, I., A. Nadas. 1979. Convex majorization with an application to the length of critical path. <i>J. Appl. Probab.</i> <b>16</b>(3) 671--677]. We generalize the bound to nonoverlapping multivariate marginals using multiple-choice integer programming. New instances of polynomial-time computable bounds are identified for discrete distributions. For the problem of selecting up to <i>M</i> items out of a set of <i>N</i> items of maximum total weight, the multivariate marginal bound is shown to be computable in polynomial time, when the size of each subset in the partition is <i>O</i>(log <i>N</i>). For an activity-on-arc PERT network, the partition is naturally defined by subsets of incoming arcs into nodes. The multivariate marginal bound on expected project duration is shown to be computable in time polynomial in the maximum number of scenarios for any subset and the size of the network. As an application, a polynomial-time solvable two-stage stochastic program for project crashing is identified. An important feature of the bound developed in this paper is that it is exactly achievable by a joint distribution, unlike many of the existing bounds.", "e:keyword": ["Probability bounds", "Integer programming", "PERT"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1006", "e:abstract": "We obtain a closed-form solution for the double-Laplace transform of Asian options under the hyper-exponential jump diffusion model. Similar results were available previously only in the special case of the Black-Scholes model (BSM). Even in the case of the BSM, our approach is simpler as we essentially use only Itô's formula and do not need more advanced results such as those of Bessel processes and Lamperti's representation. As a by-product we also show that a well-known recursion relating to Asian options has a unique solution in a probabilistic sense. The double-Laplace transform can be inverted numerically via a two-sided Euler inversion algorithm. Numerical results indicate that our pricing method is fast, stable, and accurate; and it performs well even in the case of low volatilities.", "e:keyword": ["Finance", "Asset pricing", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1007", "e:abstract": "Convex regression is concerned with computing the best fit of a convex function to a data set of <i>n</i> observations in which the independent variable is (possibly) multidimensional. Such regression problems arise in operations research, economics, and other disciplines in which imposing a convexity constraint on the regression function is natural. This paper studies a least-squares estimator that is computable as the solution of a quadratic program and establishes that it converges almost surely to the “true” function as <i>n</i> (rightarrow) (infinity) under modest technical assumptions. In addition to this multidimensional consistency result, we identify the behavior of the estimator when the model is misspecified (so that the “true” function is nonconvex), and we extend the consistency result to settings in which the function must be both convex and nondecreasing (as is needed for consumer preference utility functions).", "e:keyword": ["Nonparametric regression", "Multidimensional convex functions", "Asymptotic properties", "Consistency"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1008", "e:abstract": "We provide a sequential Monte Carlo method for estimating rare-event probabilities in dynamic, intensity-based point process models of portfolio credit risk. The method is based on a change of measure and involves a resampling mechanism. We propose resampling weights that lead, under technical conditions, to a logarithmically efficient simulation estimator of the probability of large portfolio losses. A numerical analysis illustrates the features of the method and contrasts it with other rare-event schemes recently developed for portfolio credit risk, including an interacting particle scheme and an importance sampling scheme.", "e:keyword": ["Event timing models", "Portfolio credit risk", "Rare-event simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1009", "e:abstract": "Assume that <i>m</i> periods with stochastic demand remain until the next replenishment arrives at a central warehouse. How should the available inventory be allocated among <i>N</i> retailers? This paper presents a new policy and a new lower bound for the expected cost of this problem. The lower bound becomes tight as <i>N</i> (rightarrow) (infinity) . The infinite horizon problem then decomposes into <i>N</i> independent <i>m</i>-period problems with optimal retailer ship-up-to levels that decrease over the <i>m</i> periods, and the warehouse is optimally replenished by an order-up-to level that renders zero (local) warehouse safety stock at the end of each replenishment cycle. Based on the lower bound solution, we suggest a heuristic for finite <i>N</i>. In a numerical study it outperforms the heuristic by Jackson [Jackson, P. L. 1988. Stock allocation in a two-echelon distribution system or what to do until your ship comes in. <i>Management Sci.</i> <b>34</b>(7) 880--895], and the new lower bound improves on Clark and Scarf's [Clark, A. J., H. Scarf. 1960. Optimal policies for a multi-echelon inventory problem. <i>Management Sci.</i> <b>6</b>(4) 475--490] bound when <i>N</i> is not too small. Moreover, the warehouse zero-safety-stock heuristic is comparable to Clark and Scarf's warehouse policy for lead times that are not too long. The suggested approach is quite general and may be applied to other logistical problems. In the present application it retains some of the risk-pooling benefits of holding central warehouse stock.", "e:keyword": ["Inventory/production", "Distribution systems", "Programming", "Stochastic", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1010", "e:abstract": "In many situations, such as trade in stock exchanges, agents have many opportunities to act within a short interval of time. The agents in such situations can often coordinate their actions in advance, but coordination during the game consumes too much time. An equilibrium in such situations has to be sequential in order to handle mistakes made by players. In this paper, we present a new solution concept for infinite-horizon dynamic games, which is appropriate for such situations: a sequential normal-form correlated approximate equilibrium. Under additional assumptions, we show that every such game admits this kind of equilibrium.", "e:keyword": ["Games/group decisions", "Stochastic", "Financial institutions", "Trading"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1011", "e:abstract": "When decisions are made in the presence of high-dimensional stochastic data, handling joint distribution of correlated random variables can present a formidable task, both in terms of sampling and estimation as well as algorithmic complexity. A common heuristic is to estimate only marginal distributions and substitute joint distribution by independent (product) distribution. In this paper, we study possible loss incurred on ignoring correlations through a distributionally robust stochastic programming model, and we quantify that loss as <i>price of correlations</i> (POC). Using techniques of cost sharing from game theory, we identify a wide class of problems for which POC has a small upper bound. To our interest, this class will include many stochastic convex programs, uncapacitated facility location, Steiner tree, and submodular functions, suggesting that the intuitive approach of assuming independent distribution acceptably approximates the robust model for these stochastic optimization problems. Additionally, we demonstrate hardness of bounding POC via examples of subadditive and supermodular functions that have large POC. We find that our results are also useful for solving many deterministic optimization problems like welfare maximization, <i>k</i>-dimensional matching, and transportation problems, under certain conditions.", "e:keyword": ["Stochastic optimization", "Submodularity", "Cost sharing", "Correlation gap", "Joint distributions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1012", "e:abstract": "We propose a method for estimating substitute and lost demand when only sales and product availability data are observable, not all products are displayed in all periods (e.g., due to stockouts or availability controls), and the seller knows its aggregate market share. The model combines a multinomial logit (MNL) choice model with a nonhomogeneous Poisson model of arrivals over multiple periods. Our key idea is to view the problem in terms of primary (or first-choice) demand; that is, the demand that would have been observed if all products had been available in all periods. We then apply the expectation-maximization (EM) method to this model, and we treat the observed demand as an incomplete observation of primary demand. This leads to an efficient, iterative procedure for estimating the parameters of the model. All limit points of the procedure are provably stationary points of the incomplete data log-likelihood function. Every iteration of the algorithm consists of simple, closed-form calculations. We illustrate the effectiveness of the procedure on simulated data and two industry data sets.", "e:keyword": ["Demand estimation", "Demand untruncation", "Choice behavior", "Multinomial logit model", "EM method"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1013", "e:abstract": "David Simchi-Levi, <i>Operations Research</i> editor-in-chief, reflects on the last six years.", "e:keyword": ["Editorial comments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1014", "e:abstract": "In this paper we study piecewise linear multicriteria programs, that is, multicriteria programs with either a continuous or discontinuous piecewise linear objective function and a polyhedron set constraint. We obtain an algebraic representation of a semi-closed polyhedron and apply it to show that the image of a semi-closed polyhedron under a continuous linear function is always one semi-closed polyhedron. We establish that the (weak) Pareto solution/point set of a piecewise linear multicriteria program is the union of finitely many semi-closed polyhedra. We propose an algorithm for finding the Pareto point set of a continuous piecewise linear bi-criteria program and generalize it to the discontinuous case. We apply our algorithm to solve the discontinuous bi-criteria portfolio selection problem with an <i>l</i><sub>(infinity)</sub> risk measure and transaction costs and show that this algorithm can be improved by using an ideal point strategy.", "e:keyword": ["Multicriteria program", "Piecewise linear function", "The structure of  Pareto solution set", "Bi-criteria program", "Algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1015", "e:abstract": "Inventory routing problems combine the optimization of product deliveries (or pickups) with inventory control at customer sites. The application that motivates this paper concerns the planning of single-product pickups over time; each site accumulates stock at a deterministic rate; the stock is emptied on each visit. At the tactical planning stage considered here, the objective is to minimize a surrogate measure of routing cost while achieving some form of regional clustering by partitioning the sites between the vehicles. The fleet size is given but can potentially be reduced. Planning consists of assigning customers to vehicles in each time period, but the routing, i.e., the actual sequence in which vehicles visit customers, is considered an “operational” decision. The planning is due to be repeated over the time horizon with constrained periodicity. We develop a truncated branch-and-price-and-cut algorithm combined with rounding and local search heuristics that yield both primal solutions and dual bounds. On a large-scale industrial test problem (with close to 6,000 customer visits to schedule), we obtain a solution within 6.25% deviation from the optimal to our model. A rough comparison between an operational routing resulting from our tactical solution and the industrial practice shows a 10% decrease in the number of vehicles as well as in the travel distance. The key to the success of the approach is the use of a state-space relaxation technique in formulating the master program to avoid the symmetry in time.", "e:keyword": ["Inventory routing", "Branch-and-price-and-cut", "Primal heuristic", "Symmetry"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1016", "e:abstract": "In call centers it is crucial to staff the right number of agents so that the targeted service levels are met. These staffing problems typically lead to constraint satisfaction problems that are hard to solve. During the last decade, a beautiful many-server asymptotic theory has been developed to solve such problems for large call centers, and optimal staffing rules are known to obey the square-root staffing principle. This paper presents refinements to many-server asymptotics and this staffing principle for a Markovian queueing model with impatient customers.", "e:keyword": ["Call centers", "Impatient customers", "Square-root staffing", "Quality-and-efficiency-driven regime", "Erlang A queueing model"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1017", "e:abstract": "We study a generalized model of quantity (Cournot) oligopolistic competition. The impact of competition on social surplus and firms' profit is analyzed. Firms produce multiple differentiated products and face production constraints. We compare the social surplus produced by competing firms and by colluding firms with the maximum achievable total surplus in the market. Similarly, we quantify the loss of profit that firms incur by competing instead of colluding. Our goal is to understand how the presence of competition affects the firms and society as a whole, but also to determine what are the key drivers of the inefficiencies that arise due to competition.", "e:keyword": ["Noncooperative games", "Bidding", "Cournot competition"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1018", "e:abstract": "Motivated by infinitesimal perturbation analysis (IPA) and the likelihood ratio (LR) method, we derive a new unbiased stochastic derivative estimator for a class of discontinuous payoff functions that arise in many options pricing settings from finance. Our method includes IPA and the LR method as special cases and can be applied to functions of more general forms containing indicator functions. This new estimator can be computed from a <i>single</i> sample path or simulation, whereas existing estimators generally require additional simulations for the class of discontinuous payoff functions considered here. We apply this method to sensitivity analysis for European call options and American-style call options, and numerical experiments indicate that the estimator is computationally more efficient than other estimators.", "e:keyword": ["Price sensitivity", "Infinitesimal perturbation analysis", "Simulation", "Derivative estimation", "Likelihood ratio", "Option pricing", "Stochastic approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1019", "e:abstract": "Breast cancer is the most common nonskin cancer and the second leading cause of cancer death in U.S. women. Although mammography is the most effective modality for breast cancer screening, it has several potential risks, including high false-positive rates. Therefore, the balance of benefits and risks, which depend on personal characteristics, is critical in designing a mammography screening schedule. In contrast to prior research and existing guidelines that consider population-based screening recommendations, we propose a personalized mammography screening policy based on the prior screening history and personal risk characteristics of women. We formulate a finite-horizon, partially observable Markov decision process (POMDP) model for this problem. Our POMDP model incorporates two methods of detection (self or screen), age-specific unobservable disease progression, and age-specific mammography test characteristics. We solve this POMDP optimally after setting transition probabilities to values estimated from a validated microsimulation model. Additional published data is used to specify other model inputs such as sensitivity and specificity of test results. Our results show that our proposed personalized screening schedules outperform the existing guidelines with respect to the total expected quality-adjusted life years, while significantly decreasing the number of mammograms and false-positives. We also report the lifetime risk of developing undetected invasive cancer associated with each screening scenario.", "e:keyword": ["Partially observable Markov decision processes", "Dynamic programming", "Decision analysis", "Medical decision making", "Breast cancer", "Mammography screening", "Personalized screening"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1023", "e:abstract": "We consider a problem faced by an airline that operates a number of parallel flights to transport cargo between a particular origin to destination pair. The airline can sell its cargo capacity either through allotment contracts or on the spot market, where customers exhibit choice behavior between different flights. The goal is to simultaneously select allotment contracts among available bids and find a booking control policy for the spot market to maximize the sum of the profit from the allotments and the total expected profit from the spot market. We formulate the booking control problem on the spot market as a dynamic program and construct approximations to its value functions, which can be used to estimate the total expected profit from the spot market. We show that our value function approximations provide upper bounds on the optimal total expected profit from the spot market, and they allow us to solve the allotment selection problem through a sequence of linear mixed-integer programs with a special structure. Furthermore, the value function approximations are useful for constructing a booking control policy for the spot market with desirable monotonic properties. Computational experiments show that the proposed approach can be scaled to realistic problems and provides well-performing allotment allocation and booking control decisions.", "e:keyword": ["Transportation", "Freight", "Dynamic programming", "Applications", "Programming", "Integer/applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1024", "e:abstract": "We report on the use of a quadratic programming technique in recent and upcoming spectrum auctions in Europe. Specifically, we compute a unique point in the core that minimizes the sum of squared deviations from a reference point, for example, from the Vickrey-Clarke-Groves payments. Analyzing the Karush-Kuhn-Tucker conditions, we demonstrate that the resulting payments can be decomposed into a series of economically meaningful and equitable penalties. Furthermore, we discuss the benefits of this combinatorial auction, explore the use of alternative reserve pricing approaches in this context, and indicate the results of several hundred computational runs using CATS data.", "e:keyword": ["Games", "Group decisions", "Bidding", "Auctions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1025", "e:abstract": "Distributed, iterative algorithms operating with minimal data structure while performing little computation per iteration are popularly known as <i>message passing</i> in the recent literature. Belief propagation (BP), a prototypical message-passing algorithm, has gained a lot of attention across disciplines, including communications, statistics, signal processing, and machine learning as an attractive, scalable, general-purpose heuristic for a wide class of optimization and statistical inference problems. Despite its empirical success, the theoretical understanding of BP is far from complete.With the goal of advancing the state of art of our understanding of BP, we study the performance of BP in the context of the capacitated minimum-cost network flow problem---a cornerstone in the development of the theory of polynomial-time algorithms for optimization problems and widely used in the practice of operations research. As the main result of this paper, we prove that BP converges to the optimal solution in pseudopolynomial time, provided that the optimal solution of the underlying network flow problem instance is unique and the problem parameters are integral. We further provide a simple modification of the BP to obtain a fully polynomial-time randomized approximation scheme (FPRAS) without requiring uniqueness of the optimal solution. This is the first instance where BP is proved to have fully polynomial running time. Our results thus provide a theoretical justification for the viability of BP as an attractive method to solve an important class of optimization problems.", "e:keyword": ["Belief propagation", "Network flow", "Graphical model"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1026", "e:abstract": "This paper describes a methodology for setting long-term care capacity levels over a multiyear planning horizon to achieve target wait time service levels. Our approach integrates demographic and survival analysis, discrete event simulation, and optimization. Based on this methodology, we developed a decision support system for use in practice. We illustrate this approach through two case studies: one for a regional health authority in British Columbia, Canada, and the other for a long-term care facility. We also compare our approach to the fixed ratio approach used in practice and the SIPP (stationary, independent, period by period) and MOL (modified offered load) approaches developed in the call center literature. Our results suggest that our approach is preferable. The fixed ratio approach lacks a rigorous foundation, and the SIPP and MOL approaches do not perform reliably mainly because of long service times. We conclude the paper with policy recommendations.", "e:keyword": ["Long-term care", "Capacity planning", "Survival analysis", "Simulation", "Optimization", "Service level"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1027", "e:abstract": "Motivated by queues with multitype servers and multitype customers, we consider an infinite sequence of items of types (C-script) = {<i>c</i><sub>1</sub>,...,<i>c<sub>I</sub></i>}, and another infinite sequence of items of types (S-script) = {<i>s</i><sub>1</sub>,...,<i>s<sub>J</sub></i>}, and a bipartite graph <i>G</i> of allowable matches between the types. We assume that the types of items in the two sequences are independent and identically distributed (i.i.d.) with given probability vectors (alpha), (beta). Matching the two sequences on a first-come, first-served basis defines a unique infinite matching between the sequences. For (<i>c<sub>i</sub></i>,<i>s<sub>j</sub></i>) (in) <i>G</i> we define the matching rate <i>r<sub>c<sub>i</sub>, s<sub>j</sub></sub></i> as the long-term fraction of (<i>c<sub>i</sub></i>, <i>s<sub>j</sub></i>) matches in the infinite matching, if it exists. We describe this system by a multidimensional countable Markov chain, obtain conditions for ergodicity, and derive its stationary distribution, which is, most surprisingly, of product form. We show that if the chain is ergodic, then the matching rates exist almost surely, and we give a closed-form formula to calculate them. We point out the connection of this model to some queueing models.", "e:keyword": ["Service system", "First-come", "First-served policy", "Multitype customers and servers", "Infinite bipartite matching", "Infinite bipartite matching rates", "Markov chains", "Product-form solution"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1028", "e:abstract": "Dike height optimization is of major importance to the Netherlands  because a large part of the country lies below sea level, and high water  levels in rivers can cause floods. Recently impovements have been made on the cost-benefit model introduced by van Dantzig after the devastating flood in  the Netherlands in 1953. We consider the extension of this model to  nonhomogeneous dike rings, which may also be applicable to other  deltas in the world. A nonhomogeneous dike ring consists of  different segments with different characteristics with  respect to flooding and investment costs. The individual segments can be heightened independently at different moments in time and by different amounts, making the problem considerably  more complex than the homogeneous case. We show how the problem can  be modeled as a mixed-integer nonlinear programming problem, and we present an iterative algorithm  that can be used to solve the problem. Moreover, we consider a  robust optimization approach to deal with uncertainty in the model  parameters. The method has been implemented and integrated in  software, which is used by the government to determine how the safety  standards in the Dutch Water Act should be changed.", "e:keyword": ["Flood prevention", "MINLP", "Cost-benefit analysis", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1029", "e:abstract": "Stefanos Zenios, editor in chief of <i>Operations Research</i>, provides his vision and goals for the journal's next three years.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1030", "e:abstract": "We study a diffusion regime---earlier considered by Gurvich, Mandelbaum, Shaikhet, and Whitt in the case of the M/M/N queue which may be regarded in a sense that we make precise, as a midpoint between two well-known heavy traffic diffusion regimes, the <i>conventional</i> and the <i>quality and efficiency driven</i> regimes. Unlike the other two, this regime, which we call the <i>nondegenerate slowdown</i> regime, enjoys the property that delay and service time are of the same order of magnitude, a property that is often desirable from a modeling viewpoint. Our main result is that in the case of heterogeneous exponential multiserver systems, this regime gives rise to new limit processes for the sojourn time. In particular, the joint limit law of the delay and service time processes is identified as a reflected Brownian motion and an independent process, whose marginal is a size-biased mixture of exponentials. Our results also motivate the formulation and study of new diffusion control problems based on sojourn time cost.", "e:keyword": ["Diffusion limits", "Many-server queue", "Heavy traffic", "Conventional diffusion regime", "ED and QED regimes", "Nondegenerate slowdown regime"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1031", "e:abstract": "The single-item stochastic lot-sizing problem is to find an inventory replenishment policy in the presence of discrete stochastic demands under periodic review and finite time horizon. A closely related problem is the single-period newsvendor model. It is well known that the newsvendor problem admits a closed formula for the optimal order quantity whenever the revenue and salvage values are linear increasing functions and the procurement (ordering) cost is fixed plus linear. The optimal policy for the single-item lot-sizing model is also well known under similar assumptions.In this paper we show that the classical (single-period) newsvendor model with fixed plus linear ordering cost cannot be approximated to any degree of accuracy when either the demand distribution or the cost functions are given by an oracle. We provide a fully polynomial time approximation scheme for the nonlinear single-item stochastic lot-sizing problem, when demand distribution is given by an oracle, procurement costs are provided as nondecreasing oracles, holding/backlogging/disposal costs are linear, and lead time is positive. Similar results exist for the nonlinear newsvendor problem. These approximation schemes are designed by extending the technique of <i>K</i>-approximation sets and functions.", "e:keyword": ["Stochastic inventory control", "Hardness results", "Fully polynomial time approximation schemes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1032", "e:abstract": "A comprehensive framework for diver detection by a hydrophone network in an urban harbor is presented. It includes a signal processing algorithm and a diver detection test and formulates optimal hydrophone placement as a two-stage stochastic optimization problem with respect to different scenarios of underwater noise. The signal processing algorithm identifies sound intensity peaks associated with diver breathing and outputs a diver number measuring the likelihood of diver presence, whereas the diver detection test aggregates the diver numbers obtained from the hydrophones in a linear statistic and optimizes the statistic's coefficients and a detection threshold for each noise scenario. The serial dependence of the diver numbers on a short time scale (several detection periods) is modeled by a hidden Markov chain, and finding the worst-case diver's trajectory for each hydrophone placement and noise scenario is reduced to a linear programming problem. The framework is tested in numerical experiments with real-life data for circular and elliptic hydrophone placements and is shown to be superior to a deterministic energy-based approach.", "e:keyword": ["Cost effectiveness", "Defense systems", "Surveillance", "Stochastic programming", "Statistical pattern analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1033", "e:abstract": "We consider a stochastic periodic-review inventory control system in which the fixed cost depends on the order quantity. In particular, we investigate the optimal ordering policies under three fixed cost structures. The first structure is motivated by transportation and production contracts and considers two fixed costs: if the order size is within a specified limit <i>C</i>, then the fixed cost is <i>K</i><sub>1</sub>; otherwise, it is <i>K</i><sub>2</sub>, where <i>K</i><sub>1</sub> ≼ <i>K</i><sub>2</sub>. The second structure contains multiple fixed costs in which the same incremental fixed cost <i>K</i> is incurred for any additional order quantity up to a given identical batch capacity <i>C</i>. In the third structure, in addition to the <i>K</i> incurred as in the previous case, a common fixed cost is charged for any nonzero order size. An example of the former case arises when an order is shipped with a homogeneous fleet of trucks with per-truck fixed costs. A situation in which a fixed administrative cost plus a quantity-dependent trucking cost is incurred for each shipment exemplifies the latter case. For the first cost structure, we separate the analysis according to the conditions (1) <i>K</i><sub>1</sub> ≼ <i>K</i><sub>2</sub> ≼ 2<i>K</i><sub>1</sub> and (2) <i>K</i><sub>1</sub> ≼ <i>K</i><sub>2</sub>. Under condition (1), we introduce a new concept called <i>C</i>-(<i>K</i><sub>1</sub>, <i>K</i><sub>2</sub>)-convexity, which enables us to almost completely characterize the optimal ordering policy. Under the general condition (2), we utilize a modified notion to provide a partial characterization of the optimal policy and propose a heuristic policy that performs well under a wide variety of model parameters. For the second cost structure, we show that it is optimal to order an integer multiple of the batch capacity to raise the inventory level to a specified range or band of length <i>C</i>, and then to order an additional full or partial batch size depending on the cost function, with no ordering required above the band. We also characterize a similar optimal policy for the third cost structure. Using different techniques, our study extends or redevelops several existing results in the literature.", "e:keyword": ["Periodic-review inventory systems", "(s", "S) policies", "C--convexity", "Strong K-convexity", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1034", "e:abstract": "We show that under a set of conditions, both the maximal profit function and the objective function in several lost-sales inventory models with fixed costs are quasiconcave. Not only is the quasiconcavity property useful computationally, it also leads to a sharper characterization of the optimal policies. Neither the proof of the quasiconcavity property itself nor the proof of the optimal policies by using the property requires the machinery of <i>K</i>-concavity or any of its <i>K</i>-related extensions, and hence they are intuitively appealing.", "e:keyword": ["Periodic-review inventory systems", "Fixed costs", "Optimal policies", "Quasiconcavity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1035", "e:abstract": "In this paper we study the impact of uncertainty about future innovations in quality and costs on consumers' technology adoption decisions. We model the uncertainty in the technology's quality and costs as a Markov process and consider three models of the adoption decision. The first model assumes that consumers do a simple net present value (NPV) analysis that compares the NPV of adopting to that of not adopting, without considering the possibility of waiting. The second model is a stochastic dynamic program that considers the possibility of waiting and views the adoption decision as a one-time event, i.e., the consumer will only make a single purchase, the only question is when. The third model allows repeat purchases so the consumer may “upgrade” by purchasing new versions of the technology whenever it suits her.We study structural properties of these models, e.g., the following: What changes in qualities and costs will make the consumer better off? What changes will encourage adoption? We will see that the simple NPV and single-purchase model have many intuitive properties: with the right notion of improvements and reasonable assumptions about the technology changes, we find that improvements in the technology make the consumer better off and encourage adoption. Here improvements are defined using a partial order on quality and cost pairs. The results are more complicated in the repeat-purchase model. Under the same conditions on technology changes, technology improvements will make the consumer better off. However, except for special cases of transitions, these improvements may make the consumer better off and <i>discourage</i> adoption.", "e:keyword": ["Dynamic programming", "Decision analysis", "Sequential"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1036", "e:abstract": "This paper investigates a Lagrangian dual problem for solving the optimal power flow problem in rectangular form that arises from power system analysis. If strong duality does not hold for the dual, we propose two classes of branch-and-bound algorithms that guarantee to solve the problem to optimality. The lower bound for the objective function is obtained by the Lagrangian duality, whereas the feasible set subdivision is based on the rectangular or ellipsoidal bisection. The numerical experiments are reported to demonstrate the effectiveness of the proposed algorithms. We note that no duality gap is observed for any of our test problems.", "e:keyword": ["Optimal power flow", "Branch-and-bound", "Lagrangian duality", "Quadratic programming", "Global optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1110.1037", "e:abstract": "We study several joint facility location and inventory management problems with stochastic retailer demand. In particular, we consider cases with uncapacitated facilities, capacitated facilities, correlated retailer demand, stochastic lead times, and multicommodities. We show how to formulate these problems as conic quadratic mixed-integer problems. Valid inequalities, including extended polymatroid and extended cover cuts, are added to strengthen the formulations and improve the computational results. Compared to the existing modeling and solution methods, the new conic integer programming approach not only provides a more general modeling framework but also leads to fast solution times in general.", "e:keyword": ["Integrated supply chain", "Risk pooling", "Conic mixed-integer program", "Polymatroids", "Covers"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1038", "e:abstract": "Plug-in hybrid electric vehicles (PHEVs) have been touted as a transportation technology with lower fuel costs and emissions impacts than other vehicle types. Most analyses of PHEVs assume that the power system operator can either directly or indirectly control PHEV charging to coordinate it with power system operations. This paper examines the incentives of individual drivers making charging decisions with different electricity tariffs, and it compares the cost and emissions impacts of these charging patterns to the ideal case of charging controlled by the system operator. Our results show that real-time pricing performs worst among all of the tariffs we consider, because linear prices are inherently limited in signaling efficient use of resources in a system with nonconvexities. We also show that controlling overnight PHEV charging is significantly more important than limiting midday vehicle charging.", "e:keyword": ["Plug-in hybrid electric vehicles", "Environment", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1039", "e:abstract": "Market-makers have the obligation to trade any given amount of assets at quoted bid or ask prices, and their inventories are exposed to the potential loss when the market price moves in an undesirable direction. One approach to reduce the risk brought by price uncertainty is to adjust the inventory at the price of losing potential spread gain. Using stochastic dynamic programming, we show that a threshold inventory control policy is optimal with respect to an exponential utility criterion and a mean-variance trade-off model. Symmetric and monotone properties of the threshold levels are also established.", "e:keyword": ["Market-making", "Inventory control", "Risk aversion", "Dynamic programming", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1040", "e:abstract": "We consider a supply chain with a retailer and a supplier: A newsvendor-like retailer has a single opportunity to order a product from a supplier to satisfy future uncertain demand. Both the retailer and supplier are capital constrained and in need of short-term financing. In the presence of bankruptcy risks for both the retailer and supplier, we model their strategic interaction as a Stackelberg game with the supplier as the leader. We use the <i>supplier early payment discount</i> scheme as a decision framework to analyze all decisions involved in optimally structuring the trade credit contract (discounted wholesale price if paying early, financing rate if delaying payment) from the supplier's perspective. Under mild assumptions we conclude that a risk-neutral supplier should always finance the retailer at rates less than or equal to the risk-free rate. The retailer, if offered an optimally structured trade credit contract, will always prefer supplier financing to bank financing. Furthermore, under optimal trade credit contracts, both the supplier's profit and supply chain efficiency improve, and the retailer might improve his profits relative to under bank financing (or equivalently, a rich retailer under wholesale price contracts), depending on his current “wealth” (working capital and collateral).", "e:keyword": ["Supplier financing", "Newsvendor", "Supply contract", "Trade credit", "Early payment discount"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1041", "e:abstract": "In this paper we consider the well-known fixed-charge transportation problem. To send any flow from source <i>s<sub>i</sub></i> to destination <i>t<sub>j</sub></i>, we incur a unit variable shipping cost of <i>c<sub>ij</sub></i> and a fixed cost <i>f<sub>ij</sub></i>. Here we study the structure of the projection polyhedron of this problem, in the space of 0-1 variables associated with fixed charges, and we develop several classes of valid inequalities and derive conditions under which they are facet defining. In some cases, if the conditions are not satisfied, we show how they can be lifted to define facets. Several heuristics for generating and adding these facets are presented. Using these results, we develop a computationally effective algorithm for solving the problem. The computational results clearly indicate the usefulness of this approach.", "e:keyword": ["Fixed charge", "Transportation problem", "Integer programming", "Branch and cut"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1044", "e:abstract": "We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP have typically relied on a natural “projection” of a well-studied linear program for exact dynamic programming. Such programs restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program---the “smoothed approximate linear program”---is distinct from such approaches and relaxes the restriction to lower bounding approximations in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First, we demonstrate bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach. These bounds are, in general, no worse than those available for extant LP approaches and for specific problem instances can be shown to be arbitrarily stronger. Second, experiments with our approach on a pair of challenging problems (the game of Tetris and a queueing network control problem) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by a substantial margin.", "e:keyword": ["Optimization", "Linear programming", "Stochastic control", "Markov decision processes", "Approximate dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1046", "e:abstract": "Making plans about how much to consume and how much to invest in risky assets over an uncertain lifetime is a fundamental economic challenge. The leading models of this planning problem use either additive or habit-forming preferences. For the most part, these models assume an individual is either correlation neutral or correlation seeking in consumption, respectively. In this paper, we introduce two habit-forming, correlation-averse preference models. With these preferences, we find closed-form solutions to the classic consumption and portfolio planning problem. Our solutions recommend that a correlation-averse decision maker follow a habit in their consumption plans. While such habits traditionally have been associated with correlation-seeking preferences, our model leads to consumption habits from correlation-averse preferences.", "e:keyword": ["Decision analysis", "Multiattribute utility", "Stochastic dynamic programming", "Consumption and portfolio planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1047", "e:abstract": "We study a multiclass stochastic processing network operating under the so-called proportional fair allocation scheme, and following the head-of-the-line processor-sharing discipline. Specifically, each server's capacity is shared among the job classes that require its service, and it is allocated, in every state of the network, among the first waiting job of each class to maximize a log-utility function. We establish the limiting regime of the network under diffusion scaling, allowing multiple bottlenecks in the network, and relaxing some of the conditions required in prior studies. We also identify the class of allocation schemes among which the proportional fair allocation minimizes a quadratic cost objective function of the diffusion-scaled queue lengths, and we illustrate the limitation of this asymptotic optimality through a counterexample.", "e:keyword": ["Stochastic processing network", "Proportional fair allocation", "Fluid limit", "Diffusion limit", "Asymptotic optimality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1048", "e:abstract": "We propose an algorithmic framework that successfully addresses three vehicle routing problems: the multidepot VRP, the periodic VRP, and the multidepot periodic VRP with capacitated vehicles and constrained route duration. The metaheuristic combines the exploration breadth of population-based evolutionary search, the aggressive-improvement capabilities of neighborhood-based metaheuristics, and advanced population-diversity management schemes. Extensive computational experiments show that the method performs impressively in terms of computational efficiency and solution quality, identifying either the best known solutions, including the optimal ones, or new best solutions for all currently available benchmark instances for the three problem classes. The proposed method also proves extremely competitive for the capacitated VRP.", "e:keyword": ["Multidepot", "Multiperiod vehicle routing problems", "Hybrid population-based metaheuristics", "Adaptive population diversity management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1049", "e:abstract": "This paper presents a robust optimization approach to the network design problem under traffic demand uncertainty. We consider the specific case of the network design problem in which there are several alternatives in edge capacity installations and the traffic cannot be split over several paths. A new decomposition approach is proposed that yields a strong LP relaxation and enables traffic demand uncertainty to be addressed efficiently through localization of the uncertainty to each edge of the underlying network. A branch-and-price-and-cut algorithm is subsequently developed and tested on a set of benchmark instances.", "e:keyword": ["Network design problem", "Robust optimization", "Integer programming", "Branch-and-price-and-cut", "Robust knapsack problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1050", "e:abstract": "For the purpose of production scheduling, open-pit mines are discretized into three-dimensional arrays known as block models. Production scheduling consists of deciding which blocks should be extracted, when they should be extracted, and what to do with the blocks once they are extracted. Blocks that are close to the surface should be extracted first, and capacity constraints limit the production in each time period. Since the 1960s, it has been known that this problem can be cast as an integer programming model. However, the large size of some real instances (3--10 million blocks, 15--20 time periods) has made these models impractical for use in real planning applications, thus leading to the use of numerous heuristic methods. In this article we study a well-known integer programming formulation of the problem that we refer to as C-PIT. We propose a new decomposition method for solving the linear programming relaxation (LP) of C-PIT when there is a single capacity constraint per time period. This algorithm is based on exploiting the structure of the precedence-constrained knapsack problem and runs in <i>O</i> (<i>mn log n</i>) in which <i>n</i> is the number of blocks and <i>m</i> a function of the precedence relationships in the mine. Our computations show that we can solve, in minutes, the LP relaxation of real-sized mine-planning applications with up to five million blocks and 20 time periods. Combining this with a quick rounding algorithm based on topological sorting, we obtain integer feasible solutions to the more general problem where multiple capacity constraints per time period are considered. Our implementation obtains solutions within 6% of optimality in seconds. A second heuristic step, based on local search, allows us to find solutions within 3% in one hour on all instances considered. For most instances, we obtain solutions within 1--2% of optimality if we let this heuristic run longer. Previous methods have been able to tackle only instances with up to 150,000 blocks and 15 time periods.", "e:keyword": ["Open-pit mining", "Optimization", "Mixed integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1051", "e:abstract": "We study strategic resource allocation settings, where jobs correspond to self-interested players who choose resources with the objective of minimizing their individual cost. Our framework departs from the existing game-theoretic models mainly in assuming conflicting congestion effects, but also in assuming an unlimited supply of resources. In our model, a job's cost is composed of both its resource's load (which increases with congestion) and its share in the resource's activation cost (which decreases with congestion). We provide results for a job-scheduling setting with heterogeneous jobs and identical machines.We show that if the resource's activation cost is shared equally among its users, a pure Nash equilibrium (NE) might not exist. In contrast, the <i>proportional</i> sharing rule induces a game that admits a pure NE, which can also be computed in polynomial time. As part of the algorithm's analysis, we establish a new, nontrivial property of schedules obtained by the longest processing time algorithm. We also observe that, unlike in congestion games, best-response dynamics (BRD) are not guaranteed to converge to a Nash equilibrium. Finally, we measure the inefficiency of equilibria with respect to the minimax objective function, and prove that there is no universal bound for the worst-case inefficiency (as quantified by the “price of anarchy” measure). However, the best-case inefficiency (quantified by the “price of stability” measure) is bounded by 5/4, and this is tight. These results add another layer to the growing literature on the price of anarchy and stability, which studies the extent to which selfish behavior affects system efficiency.", "e:keyword": ["Congestion games", "Cost sharing games", "Job scheduling", "Potential games", "Price of anarchy", "Price of stability", "Equilibrium existence"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1052", "e:abstract": "We consider a joint inventory-pricing control problem for a periodic-review, single-stage inventory system with a positive order leadtime and a linear order cost. Demands in consecutive periods are independent, but their distributions depend on the price in accordance with a stochastic demand function of additive form. Pricing and ordering decisions are made simultaneously at the beginning of each period. The objective is to maximize the total expected discounted profit over a finite horizon. We partially characterize the structure of the optimal joint ordering and pricing policies. We also show that our structural analysis can be extended to a multistage (or serial) inventory system with constant or stochastic leadtimes and an assemble-to-order system with price-sensitive demand.", "e:keyword": ["Inventory control", "Dynamic pricing", "Lead times", "L-concavity."]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1053", "e:abstract": "We consider the problem of appointment scheduling with discrete random durations but under the more realistic assumption that the duration probability distributions are not known and only a set of independent samples is available, e.g., historical data. For a given sequence of appointments (jobs, tasks), the goal is to determine the planned starting time of each appointment such that the expected total underage and overage costs due to the mismatch between allocated and realized durations is minimized. We use the convexity and subdifferential of the objective function of the appointment scheduling problem to determine bounds on the number of independent samples required to obtain a provably near-optimal solution with high probability.", "e:keyword": ["Appointment scheduling", "Project management", "Surgery scheduling", "Discrete random durations", "Optimization", "Sample average approximation", "Nonparametric sampling approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1054", "e:abstract": "Chance constraints are an important modeling tool in stochastic optimization, providing probabilistic guarantees that a solution “succeeds” in satisfying a given constraint. Although they control the probability of “success,” they provide no control whatsoever in the event of a “failure.” That is, they do not distinguish between a slight overshoot or undershoot of the bounds and more catastrophic violation. In short, they do not capture the magnitude of violation of the bounds. This paper addresses precisely this topic, focusing on linear constraints and ellipsoidal (Gaussian-like) uncertainties. We show that the problem of requiring different probabilistic guarantees at each level of constraint violation can be reformulated as a semi-infinite optimization problem. We provide conditions that guarantee polynomial-time solvability of the resulting semi-infinite formulation. We show further that this resulting problem is what has been called a <i>comprehensive robust optimization</i> problem in the literature. As a byproduct, we provide tight probabilistic bounds for comprehensive robust optimization. Thus, analogously to the connection between chance constraints and robust optimization, we provide a broader connection between probabilistic envelope constraints and comprehensive robust optimization.", "e:keyword": ["Programming: stochastic", "Statistics: nonparametric"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1055", "e:abstract": "The paper considers a race among multiple firms that compete over the development of a product. The first firm to complete the development gains a reward, whereas the other firms gain nothing. Each firm decides how much to invest in developing the product, and the time it completes the development is a random variable that depends on the investment level. The paper provides a method for explicitly computing a unique Nash equilibrium, parametrically in the interest rate; for a given interest rate, the Nash equilibrium is determined in time that is linear in the number of firms. The structure of the solution yields insights about the behavior of the participants. Furthermore, an explicit expression for a unique globally optimal solution is obtained and compared to the unique Nash equilibrium.", "e:keyword": ["R&D race", "Nash equilibria", "Global optimality", "Resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1056", "e:abstract": "This paper studies an infinite horizon adverse selection model with an underlying Markov information process. It introduces a graphic representation of continuation contracts and continuation payoff frontiers, namely finite policy graph, and provides an algorithm to approximate the optimal policy graph through iterations. The algorithm performs an additional step after each value iteration---replacing dominated points on the previous continuation payoff frontier by points on the new frontier and reevaluating the new frontier. This dominance-free reevaluation step accelerates the convergence of the continuation payoff frontiers. Numerical examples demonstrate the effectiveness of this algorithm and properties of the optimal contracts.", "e:keyword": ["Stochastic games", "Dynamic principal-agent model", "Adverse selection", "Dynamic programming", "Graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1057", "e:abstract": "We consider a stylized dynamic pricing model in which a monopolist prices a product to a sequence of <i>T</i> customers who independently make purchasing decisions based on the price offered according to a general parametric choice model. The parameters of the model are unknown to the seller, whose objective is to determine a pricing policy that minimizes the <i>regret</i>, which is the expected difference between the seller's revenue and the revenue of a clairvoyant seller who knows the values of the parameters in advance and always offers the revenue-maximizing price. We show that the regret of the optimal pricing policy in this model is <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\Theta(\\sqrt T)$\\end{document}</tex-math></inline-formula>, by establishing an <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\Omega(\\sqrt T)$\\end{document}</tex-math></inline-formula> lower bound on the worst-case regret under an arbitrary policy, and presenting a pricing policy based on maximum-likelihood estimation whose regret is <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\mathcal{O}(\\sqrt T)$\\end{document}</tex-math></inline-formula> across all problem instances. Furthermore, we show that when the demand curves satisfy a “well-separated” condition, the <i>T</i>-period regret of the optimal policy is (Theta)(log <i>T</i>). Numerical experiments show that our policies perform well.", "e:keyword": ["Dynamic pricing", "Customer choice model"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1058", "e:abstract": "In this paper, we study a multiechelon uncapacitated lot-sizing problem in series (<i>m</i>-ULS), where the output of the intermediate echelons has its own external demand and is also an input to the next echelon. We propose a polynomial-time dynamic programming algorithm, which gives a tight, compact extended formulation for the two-echelon case (2-ULS). Next, we present a family of valid inequalities for <i>m</i>-ULS, show its strength, and give a polynomial-time separation algorithm. We establish a hierarchy between the alternative formulations for 2-ULS. In particular, we show that our valid inequalities can be obtained from the projection of the multicommodity formulation. Our computational results show that this extended formulation is very effective in solving our uncapacitated multi-item two-echelon test problems. In addition, for capacitated multi-item, multiechelon problems, we demonstrate the effectiveness of a branch-and-cut algorithm using the proposed inequalities.", "e:keyword": ["Lot sizing", "Multiechelon", "Facets", "Extended formulation", "Fixed-charge networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1059", "e:abstract": "This paper is the archival record of the INFORMS Philip McCord Morse Lecture delivered in 2010. It considers applications of operations research to intelligence problems in national security and counterterrorism. The phrase “intelligence operations research” can be interpreted in two different ways: as <i>intelligence operations</i>} research, meaning studies to characterize and improve the operations of intelligence agencies themselves, and as intelligence <i>operations research</i>, meaning the application of operations research methods to specific substantive intelligence problems. After defining intelligence, I review the intelligence production process (or the <i>intelligence cycle</i>) with reference to the intelligence community of the United States. I then consider the extent to which operations research has been deployed inside this intelligence community and summarize previous attempts to apply operations research methods to intelligence problems. I close with some suggestions for future intelligence operations research studies.", "e:keyword": ["Intelligence", "National security", "Counterterrorism", "Intelligence operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1060", "e:abstract": "We study a periodically reviewed multiechelon inventory system in series such that order quantities at every stage have to be multiples of a given stage-specific batch size. The batch sizes are nested in the sense that the batch size for every stage is an integer multiple of the batch size for its downstream stage. The problem is that of determining the policy that minimizes the expected discounted sum of costs over a finite horizon. The result is that an echelon (<i>R, nQ</i>) policy is optimal when demands are independent across periods or, more generally, Markov-modulated. We also comment on algorithmic implications of our result and on extensions.", "e:keyword": ["Inventory/production systems", "Batch ordering", "Optimal policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1061", "e:abstract": "Comment on “Modeling the Impacts of Electricity Tariffs on Plug-In Hybrid Electric Vehicle Charging, Costs, and Emissions” by Ramteen Sieshansi.", "e:keyword": ["Plug-in hybrid electric vehicles", "Energy", "Environment", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1062", "e:abstract": "Make-to-stock queues are typically investigated in the <i>M/M</i>/1 settings. For centralized single-item systems with backlogs, the multilevel rationing (MR) policy is established as optimal and the strict priority (SP) policy is a practical compromise, balancing cost and ease of implementation. However, the optimal policy is unknown when service time is general, i.e., for <i>M/G</i>/1 queues. Dynamic programming, the tool commonly used to investigate the MR policy in make-to-stock queues, is less practical when service time is general. In this paper we focus on <i>customer composition</i>: the proportion of customers of each class to the total number of customers in the queue. We do so because the number of customers in <i>M/G</i>/1 queues is invariant for any nonidling and nonanticipating policy. To characterize customer composition, we consider a series of two-priority <i>M/G</i>/1 queues where the first service time in each busy period is different from standard service times, i.e., this first service time is exceptional. We characterize the required exceptional first service times and the exact solution of such queues. From our results, we derive the optimal cost and control for the MR and SP policies for <i>M/G</i>/1 make-to-stock queues.", "e:keyword": ["Make-to-stock", "M/G/1 queue", "Priority classes", "Customer composition", "Multilevel rationing", "Strict priority"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1063", "e:abstract": "We study robust formulations of assortment optimization problems under the multinomial logit choice model. The novel aspect of our formulations is that the true parameters of the logit model are assumed to be unknown, and we represent the set of likely parameter values by a compact uncertainty set. The objective is to find an assortment that maximizes the worst-case expected revenue over all parameter values in the uncertainty set. We consider both static and dynamic settings. The static setting ignores inventory consideration, whereas in the dynamic setting, there is a limited initial inventory that must be allocated over time. We give a complete characterization of the optimal policy in both settings, show that it can be computed efficiently, and derive operational insights. We also propose a family of uncertainty sets that enables the decision maker to control the trade-off between increasing the average revenue and protecting against the worst-case scenario. Numerical experiments show that our robust approach, combined with our proposed family of uncertainty sets, is especially beneficial when there is significant uncertainty in the parameter values. When compared to other methods, our robust approach yields over 10% improvement in the worst-case performance, but it can also maintain comparable average revenue if average revenue is the performance measure of interest.", "e:keyword": ["Robust optimization", "Assortment planning", "Customer choice model", "Multinomial logit"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1064", "e:abstract": "We consider the problem of designing an efficient system for allocating donated livers to patients waiting for transplantation. The trade-off between medical urgency and efficiency is at the heart of the liver allocation problem. We model the transplant waiting list as a multiclass fluid model of overloaded queues, which captures the disease evolution by allowing the patients to switch between classes, i.e., health levels. We consider the bicriteria objective of minimizing total number of patient deaths while waiting for transplantation (NPDWT) and maximizing total quality-adjusted life years (QALYs) through a weighted combination. On one hand, under the objective of minimizing NPDWT, the current policy of United Network for Organ Sharing (UNOS) emerges as the optimal policy, providing a theoretical justification for the current practice. On the other hand, under the metric of maximizing QALYs, the optimal policy is an intuitive dynamic index policy that ranks patients based on their marginal benefit from transplantation, i.e., the difference in benefit with versus without transplantation. Finally, we perform a detailed simulation study to compare the performances of our proposed policies and the current UNOS policy along the following metrics: total QALYs, NPDWT, number of patient deaths after transplantation, number of total patient deaths, and number of wasted livers. Numerical experiments show that our proposed policy for maximizing QALYs outperforms the current UNOS policy along all metrics except the NPDWT.", "e:keyword": ["Organ transplantation", "Liver allocation system", "Healthcare operations", "Health policy design", "Fluid models", "Simulation", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1065", "e:abstract": "Inspired by a real-life problem faced by one of the largest ground-based cargo companies of Turkey, the current study introduces a new facet to the hub location literature. The release time scheduling and hub location problem aims to select a specified number of hubs from a fixed set of demand centers, to allocate each demand center to a hub, and to decide on the release times of trucks from each demand center in such a way that the total amount of cargo guaranteed to be delivered to every potential destination by the next day is not below a threshold and the total routing cost is minimized. The paper introduces integer programming models to solve this problem in the special cases when the cargo uniformly arrives to each demand center during the day and the more realistic pattern of when the cargo arrivals exhibit a piecewise linear form. Several classes of valid inequalities are proposed to strengthen the formulations. Extensions with multiple service levels and discrete sets for release times are also discussed. Computational results show the computational viability of the models under realistic scenarios as well as the validity of the proposed problems in answering several interesting questions from the cargo sector's perspective.", "e:keyword": ["Hub location", "Cargo delivery", "Time definite delivery", "Release times", "Valid inequalities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1066", "e:abstract": "We study the optimal pricing strategies of a monopolist selling a divisible good (service) to consumers who are embedded in a social network. A key feature of our model is that consumers experience a (positive) <i>local network effect</i>. In particular, each consumer's usage level depends directly on the usage of her <i>neighbors</i> in the social network structure. Thus, the monopolist's optimal pricing strategy may involve offering discounts to certain agents who have a <i>central</i> position in the underlying network. Our results can be summarized as follows. First, we consider a setting where the monopolist can offer individualized prices and derive a characterization of the optimal price for each consumer as a function of her network position. In particular, we show that it is optimal for the monopolist to charge each agent a price that consists of three components: (i) a nominal term that is independent of the network structure, (ii) a discount term proportional to the influence that this agent exerts over the rest of the social network (quantified by the agent's <i>Bonacich centrality</i>), and (iii) a markup term proportional to the influence that the network exerts on the agent. In the second part of the paper, we discuss the optimal strategy of a monopolist who can only choose a single uniform price for the good and derive an algorithm polynomial in the number of agents to compute such a price. Third, we assume that the monopolist can offer the good in two prices, full and discounted, and we study the problem of determining which set of consumers should be given the discount. We show that the problem is NP-hard; however, we provide an explicit characterization of the set of agents who should be offered the discounted price. Next, we describe an approximation algorithm for finding the optimal set of agents. We show that if the profit is nonnegative under any feasible price allocation, the algorithm guarantees at least 88% of the optimal profit. Finally, we highlight the value of network information by comparing the profits of a monopolist who does not take into account the network effects when choosing her pricing policy to those of a monopolist who uses this information optimally.", "e:keyword": ["Optimal pricing", "Social networks", "Externalities"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1067", "e:abstract": "How should a firm modify its product assortment over time when learning about consumer tastes? In this paper, we study dynamic assortment decisions in a horizontally differentiated product category for which consumers' diverse tastes can be represented as locations on a Hotelling line. We presume that the firm knows all possible consumer locations, comprising a finite set, but does not know their probability distribution. We model this problem as a discrete-time dynamic program; each period, the firm chooses an assortment and sets prices to maximize the total expected profit over a finite horizon, given its subjective beliefs over consumer tastes. The consumers then choose a product from the assortment that maximizes their own utility. The firm observes sales, which provide censored information on consumer tastes, and it updates beliefs in a Bayesian fashion. There is a recurring trade-off between the immediate profits from sales in the current period (exploitation) and the informational gains to be exploited in all future periods (exploration). We show that one can (partially) order assortments based on their information content and that in any given period the optimal assortment cannot be less informative than the myopically optimal assortment. This result is akin to the well-known “stock more” result in censored newsvendor problems with the newsvendor learning about demand through sales when lost sales are not observable. We demonstrate that it can be optimal for the firm to alternate between exploration and exploitation, and even offer assortments that lead to losses in the current period in order to gain information on consumer tastes. We also develop a Bayesian conjugate model that reduces the state space of the dynamic program and study value of learning using this conjugate model.", "e:keyword": ["Product assortment", "Product variety", "Dynamic programming", "Bayesian learning"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1068", "e:abstract": "In physics, at the beginning of the twentieth century it was recognized that some experiments could not be explained by the conventional classical mechanics, but the same could be explained by the newly discovered quantum theory. It resulted in a new mechanics called quantum mechanics that revolutionized scientific and technological developments. Again, at the beginning of the twenty-first century, it is being recognized that some experiments related to the human decision-making processes could not be explained by the conventional classical decision theory but the same could be explained by the models based on quantum mechanics. It is now recognized that we need quantum mechanics in psychology as well as in economics and finance. In this paper we attempt to advance and explain the present understanding of applicability of quantum mechanics to the human decision-making processes. Using the postulates analogous to the postulates of quantum mechanics, we show the derivation of the quantum interference equation to illustrate the quantum approach. The explanation of disjunction effect experiments of Tversky and Shafir (Tversky A, Shafir E (1992) The disjunction effect in choice under uncertainty. <i>Psych. Sci.</i> 3(5):305--309) has been chosen to demonstrate the necessity of a quantum model. Further, to suggest the possibility of application of the quantum theory to the business-related decisions, some terms such as price operator, state of mind of the acquiring firm, etc., are introduced and discussed in context of the merger/acquisition of business firms. The possibility of the development in areas such as quantum finance, quantum management, application of quantum mechanics to the human dynamics related to healthcare management, etc., is also indicated.", "e:keyword": ["Decision analysis", "Theory", "Quantum decision model", "Quantum information processing", "Human decision making", "Quantum interference"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1069", "e:abstract": "We obtain a heavy traffic limit for the <i>GI/M/n</i> + <i>GI</i> queue, which includes the entire patience time distribution. Our main approach is to scale the hazard rate function of the patience time distribution in such a way that our resulting diffusion approximation contains the entire hazard rate function. We then show through numerical studies that for various performance measures, our approximations tend to outperform those commonly used in practice. The robustness of our results is also demonstrated by applying them to solving constraint satisfaction problems arising in the context of telephone call centers.", "e:keyword": ["Heavy traffic", "Many servers", "Abandonment", "Hazard rate"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1070", "e:abstract": "Motivated by call center cosourcing problems, we consider a service network operated under an overflow mechanism. Calls are first routed to an in-house (or dedicated) service station that has a finite waiting room. If the waiting room is full, the call is overflowed to an outside provider (an overflow station) that might also be serving overflows from other stations. We establish approximations for overflow networks with many servers under a resource-pooling assumption that stipulates, in our context, that the fraction of overflowed calls is nonnegligible. Our two main results are (i) an approximation for the overflow processes via limit theorems and (ii) asymptotic independence between each of the in-house stations and the overflow station. In particular, we show that, as the system becomes large, the dependency between each in-house station and the overflow station becomes negligible. Independence between stations in overflow networks is assumed in the literature on call centers, and we provide a rigorous support for those useful heuristics.", "e:keyword": ["Overflow networks", "Cosourcing", "Heavy-traffic approximations", "Separation of time scales"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1071", "e:abstract": "We introduce and study a family of models for multiexpert multiobjective/criteria decision making. These models use a concept of weight robustness to generate a risk-averse decision. In particular, the multiexpert multicriteria robust weighted sum approach (McRow) introduced in this paper identifies a (robust) Pareto decision that minimizes the worst-case weighted sum of objectives over a given weight region. The corresponding objective value, called the robust value of a decision, is shown to be increasing and concave in the weight set. We study compact reformulations of the McRow model with polyhedral and conic descriptions of the weight regions. The McRow model is developed further for stochastic multiexpert multicriteria decision making by allowing ambiguity or randomness in the weight region as well as the objective functions. The properties of the proposed approach are illustrated with a few textbook examples. The usefulness of the stochastic McRow model is demonstrated using a disaster planning example and an agriculture revenue management example.", "e:keyword": ["Pareto optimality", "Multicriterion optimization", "Multiexpert optimization", "Robust optimization", "Weighted sum method", "McRow"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1072", "e:abstract": "We discuss linear regression approaches to the estimation of law-invariant conditional risk measures. Two estimation procedures are considered and compared; one is based on residual analysis of the standard least-squares method, and the other is in the spirit of the <i>M</i>-estimation approach used in robust statistics. In particular, value-at-risk and average value-at-risk measures are discussed in detail. Large sample statistical inference of the estimators is derived. Furthermore, finite sample properties of the proposed estimators are investigated and compared with theoretical derivations in an extensive Monte Carlo study. Empirical results on the real data (different financial asset classes) are also provided to illustrate the performance of the estimators.", "e:keyword": ["Value-at-risk", "Average value-at-risk", "Linear regression", "Least-squares residuals", "M-estimators", "Quantile regression", "Conditional risk measures", "Law-invariant risk measures", "Statistical inference"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1073", "e:abstract": "We address the exact solution of the famous <monospace>esc</monospace> instances of the quadratic assignment problem. These are extremely hard instances that remained unsolved---even allowing for a tremendous computing power---by using all previous techniques from the literature. During this challenging task we found that three ideas were particularly useful and qualified as a breakthrough for our approach. The present paper is about describing these ideas and their impact in solving <monospace>esc</monospace> instances. Our method was able to solve, in a matter of seconds or minutes on a single PC, all easy cases (all <monospace>esc16*</monospace> plus <monospace>esc32e</monospace> and <monospace>esc32g</monospace>). The three very hard instances <monospace>esc32c, esc32d</monospace>, and <monospace>esc64a</monospace> were solved in less than half an hour, in total, on a single PC. We also report the solution, in about five hours, of <monospace>tai64c</monospace>. By using a facility-flow splitting procedure, we were also able to solve to proven optimality, for the first time, <monospace>esc32h</monospace> (in about two hours) as well as “the big fish” <monospace>esc128</monospace>. (To our great surprise, the solution of the latter required just a few seconds on a single PC.)", "e:keyword": ["Integer programming", "Quadratic assignment problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1074", "e:abstract": "The bullwhip effect, or demand information distortion, has been a subject of both theoretical and empirical studies in the operations management literature. In this paper, we develop a simple set of formulas that describe the traditional bullwhip measure as a combined outcome of several important drivers, such as finite capacity, batch-ordering, and seasonality. Our modeling framework is descriptive in nature as it features certain plausible approximations that are commonly employed in practical inventory systems. The results are nonetheless compelling and can be used to explain various conflicting observations in previous empirical studies. Building on the theoretical framework, we discuss the managerial implications of the bullwhip measurement. We show that the measurement can be completely noninformative about the underlying supply chain cost performance if it is not linked to the operational details (such as decision intervals and leadtimes). Specifically, we show that an aggregated measurement over relatively long time periods can mask the operational-level bullwhip. In addition, we show that masking also exists under product or location aggregation in some illustrative cases.", "e:keyword": ["Bullwhip effect", "Finite capacity", "Batch-ordering", "Seasonality", "Supply chain management", "Data aggregation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1075", "e:abstract": "In the aftermath of mass-casualty events, key resources (such as ambulances and operating rooms) can be overwhelmed by the sudden jump in patient demand. To ration these resources, patients are assigned different priority levels, a process that is called triage. According to triage protocols in place, each patient's priority level is determined based on that patient's injuries only. However, recent work from the emergency medicine literature suggests that when determining priorities, resource limitations and the scale of the event should also be taken into account in order to do <i>the greatest good for the greatest number</i>. This article investigates how this can be done and what the potential benefits would be. We formulate the problem as a priority assignment problem in a clearing system with multiple classes of impatient jobs. Jobs are classified based on their lifetime (i.e., their tolerance for wait), service time, and reward distributions. Our objective is to maximize the expected total reward, e.g., the expected total number of survivors. Using sample-path methods and stochastic dynamic programming, we identify conditions under which the state information is not needed for prioritization decisions. In the absence of these conditions, we partially characterize the optimal policy, which is possibly state dependent, and we propose a number of heuristic policies. By means of a numerical study, we demonstrate that simple state-dependent policies that prioritize less urgent jobs when the total number of jobs is large perform well, especially when jobs are time-critical.", "e:keyword": ["Triage", "Emergency response", "Stochastic scheduling", "Stochastic orders", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1078", "e:abstract": "In this paper, we discuss the replenishment decision of seasonal products in a two-echelon distribution system consisting of a supplier and multiple retailers. Because of long manufacturing lead time, the supplier orders its entire stock for the season well in advance. The retailers, on the other hand, can replenish their inventory from the supplier throughout the season as demand realizes. Demand at each retailer follows a Poisson process. Each retailer order incurs a fixed cost, and the usual understocking and overstocking costs occur. When retailer lead time is negligible, we show that it is optimal for the retailer to follow a time-based, order-up-to policy and order only when inventory is depleted. We also characterize the structure of the optimal policy and propose a number of heuristics for easier computation. For the supplier, we express the distribution of total demand. This allows the supplier to solve a classic newsvendor problem to determine the total stock for the season. We find that the optimal retailer policy can sometimes cause large demand variation for the supplier, resulting in lower supplier profit. In centralized settings, this may even result in lower system profit than some naïve retailer heuristics, creating inefficiency in the supply chain. We offer insights on potential causes and managerial implications.", "e:keyword": ["Inventory/production", "Multiechelon", "Policies", "Operating characteristics", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1079", "e:abstract": "This paper presents the first full-fledged branch-and-price (bap) algorithm for the capacitated arc-routing problem (CARP). Prior exact solution techniques either rely on cutting planes or the transformation of the CARP into a node-routing problem. The drawbacks are either models with inherent symmetry, dense underlying networks, or a formulation where edge flows in a potential solution do not allow the reconstruction of unique CARP tours. The proposed algorithm circumvents all these drawbacks by taking the beneficial ingredients from existing CARP methods and combining them in a new way. The first step is the solution of the one-index formulation of the CARP in order to produce strong cuts and an excellent lower bound. It is known that this bound is typically stronger than relaxations of a pure set-partitioning CARP model. Such a set-partitioning master program results from a Dantzig-Wolfe decomposition. In the second phase, the master program is initialized with the strong cuts, CARP tours are iteratively generated by a pricing procedure, and branching is required to produce integer solutions. This is a cut-first bap-second algorithm and its main function is, in fact, the splitting of edge flows into unique CARP tours.", "e:keyword": ["Transportation", "Vehicle routing", "Integer programming", "Cutting-plane and branch-and-price algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1081", "e:abstract": "The long chain has been an important concept in the design of flexible processes. This design concept, as well as other sparse designs, have been applied by the automotive and other industries as a way to increase flexibility in order to better match available capacities with variable demands. Numerous empirical studies have validated the effectiveness of these designs. However, there is little theory that explains the effectiveness of the long chain, except when the system size is large, i.e., by applying an asymptotic analysis.Our attempt in this paper is to develop a theory that explains the effectiveness of long chain designs for finite size systems. First, we uncover a fundamental property of long chains, supermodularity, that serves as an important building block in our analysis. This property is used to show that the marginal benefit, i.e., the increase in expected sales, increases as the long chain is constructed, and the largest benefit is always achieved when the chain is closed by adding the last arc to the system. Then, supermodularity is used to show that the performance of the long chain is characterized by the difference between the performances of two open chains. This characterization immediately leads to the optimality of the long chain among 2-flexibility designs. Finally, under independent and identically distributed (i.i.d.) demand, this characterization gives rise to three developments: (i) an effective algorithm to compute the performances of long chains using only matrix multiplications; (ii) a result that the gap between the fill rate of full flexibility and that of the long chain increases with system size, thus implying that the effectiveness of the long chain relative to full flexibility increases as the number of products decreases; (iii) a risk-pooling result implying that the fill rate of a long chain increases with the number of products, but this increase converges to zero exponentially fast.", "e:keyword": ["Flexible manufacturing", "Capacity pooling", "Facility planning", "Design", "Network flow", "Supermodular function", "Random walk"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1082", "e:abstract": "Sensor systems as critical components of a transportation network provide a variety of real-time traffic surveillance information for traffic management and control. The deployment of sensors significantly affects their overall surveillance effectiveness. This paper proposes a reliable sensor location model to optimize surveillance effectiveness when sensors are subject to site-dependent probabilistic failures, and a general effectiveness measure is proposed to encompass most existing measures needed for engineering practice (e.g., flow volume coverage, vehicle-mile coverage, and squared error reduction). The problem is first formulated into a compact mixed-integer program, and we develop a variety of solution algorithms (including a custom-designed Lagrangian relaxation algorithm) and analyze their properties. We also propose alternative formulations including a continuum approximation model for single corridor problems and reliable fixed-charge sensor location models. Numerical case studies are conducted to test the performance of the proposed algorithms and draw managerial insights on how different parameter settings (e.g., failure probability and spatial heterogeneity) affect overall surveillance effectiveness and the optimal sensor deployment.", "e:keyword": ["Traffic sensor deployment", "Reliability", "Mixed-integer program", "Lagrangian relaxation", "Heuristics", "Continuum approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1083", "e:abstract": "Political districting is an intractable problem with significant ramifications for political representation. Districts often are required to satisfy some legal constraints, but these typically are not very restrictive, allowing decision makers to influence the composition of these districts without violating relevant laws. For example, while districts must often comprise a single contiguous area, a vast collection of acceptable solutions (i.e., sets of districts) remains. Choosing the best set of districts from this collection can be treated as a (planar) graph partitioning problem. When districts must be contiguous, successfully solving this problem requires an efficient computational method for evaluating contiguity constraints; common methods for assessing contiguity can require significant computation as the problem size grows. This paper introduces the <i>geo-graph</i>, a new graph model that ameliorates the computational burdens associated with enforcing contiguity constraints in planar graph partitioning when each vertex corresponds to a particular region of the plane. Through planar graph duality, the geo-graph provides a scale-invariant method for enforcing contiguity constraints in local search. Furthermore, geo-graphs allow district holes (which typically are considered undesirable) to be rigorously and efficiently integrated into the partitioning process.", "e:keyword": ["Graphs/theory", "Plane graph partitioning", "Government/elections", "Political districting", "Graphs/heuristics", "Local search"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1084", "e:abstract": "This paper analyzes decentralized assembly systems under asymmetric demand information and sequential contracting. We reveal new insights on the value of contract type (price-only versus complex), demand information (complete versus asymmetric), and contract sequence (first mover versus second mover) to different players. Our results for the basic model show the following: (1) Complex contracts increase the suppliers' aggregate profit; however, individual suppliers do not necessarily benefit from a complex contracting equilibrium. We identify the conditions under which each supplier benefits from such an equilibrium. (2) Eliminating information asymmetry is not always beneficial for the suppliers because obtaining information might bring only marginal value and hence might not be realistically justified. Furthermore, a downstream supplier might prefer information asymmetry to complete information, especially when demand variability is moderate. (3) Unless there is a high demand risk, the first-mover advantage is prevalent when the assembler is a price-taker.We extend our basic model to analyze two additional scenarios. First, we study cases where the suppliers may offer contracts of different complexity. Beyond enriching our understanding of contract choice in decentralized assembly systems, such variations enhance the analysis beyond the standard methodology of principal-agent models and utilize solution techniques from optimal control. Second, we analyze the situation where the suppliers may possess different levels of information on demand under complex contracts. We show that an upstream supplier always benefits from a downstream supplier's superior information. However, the additional information might decrease the downstream supplier's profit, especially when the forecast variability is low compared to the total demand variability in the system. Our results for the basic model and its extensions confirm that studying interactions between suppliers, specifically under different contract types and information structures, in assembly systems presents rich opportunities for future research.", "e:keyword": ["Common agency framework", "Price-only contracts", "Complex contracts", "Assembly system", "Asymmetric demand information", "Procurement contracts"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1085", "e:abstract": "The United States Coast Guard (USCG), a part of the U.S. Department of Homeland Security, is the nation's leading agency in maritime security, safety, and stewardship. One of the primary USCG resources is a fleet of boats (maritime vessels less than 65 feet in length) of various types that must be allocated to USCG stations nationwide. This paper describes the academic-industry collaboration between the authors and the USCG, which resulted in the development of an integer linear programming model that optimally matches supplies of various types of boats to station demands. The paper also introduces a model for the optimal sharing of scarce boat resources. In addition, we generalize our model, using value-at-risk and robust optimization ideas, to manage the risk of boat shortages. The paper reports on the USCG implementation process and discusses internal resistance issues and eventual adoption. We describe USCG modifications to the model recommendations due to practicalities not captured by our model. Finally, we present the significant improvements to USCG quantitative performance metrics that resulted from our model's recommendations. These include a considerable reduction of excess capacity and boat shortages at the stations, a decrease in the overall fleet size with a simultaneous increase in boat utilization, and overall reduction of the fleet operating cost. We also discuss in depth how our model effected these improvements.", "e:keyword": ["Resource allocation", "Applications", "Military", "Risk management"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1086", "e:abstract": "This paper focuses on the competitive analysis of scheduling disciplines in a large deviations setting. Although there are policies that are known to optimize the sojourn time tail under a large class of heavy-tailed job sizes (e.g., processor sharing and shortest remaining processing time) and there are policies known to optimize the sojourn time tail in the case of light-tailed job sizes (e.g., first come first served), no policies are known that can optimize the sojourn time tail across both light- and heavy-tailed job size distributions. We prove that no such work-conserving, nonanticipatory, nonlearning policy exists, and thus that a policy must learn (or know) the job size distribution in order to optimize the sojourn time tail.", "e:keyword": ["Scheduling", "Queueing", "Large deviations", "Competitive analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1087", "e:abstract": "We present a framework to analyze the process location and product distribution problem with uncertain yields for a large multinational food processing company. This problem consists of selecting the location of processes, the assignment of products, and the distribution of production quantities to markets in order to minimize total expected costs. It differs from the traditional facility location problem due to characteristics that are inherent to process industry sectors. These include significant economies of scale at high volumes, large switchover times, and production yield uncertainty. We model the problem as a nonlinear mixed-integer program. A challenging aspect of this problem is that the objective function is neither convex nor concave. We develop an exact approach to linearize the objective function. We present heuristics to solve the problem and also construct lower bounds based on a reduction of the constraint set to evaluate the quality of the solutions. This framework has been used to make process choice and product allocation decisions at the food processing company, and the estimated annual cost savings are around 10%, or $50 million. In addition, the insights from the model have had a significant strategic and organizational impact at this company. Our framework and conclusions are relevant to other industrial sectors with similar characteristics, such as pharmaceuticals and specialty chemical manufacturers.", "e:keyword": ["Processed food industry", "Facility location", "Uncertain yields", "Model linearization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1088", "e:abstract": "We introduce a variety of new independence conditions for multiattribute utility functions that permit preference dependencies among the attributes of a decision problem. The hierarchy of new conditions varies in the degree to which it specifies the functional form, ranging from more general solutions with weaker constraints, to more specific solutions with stronger constraints. This formulation provides a wealth of new functional forms that a decision maker may use in a multiattribute decision problem. In addition, it may be used to tailor the utility elicitation process to the comfort level of the decision maker. The new conditions, and the corresponding functional forms, are based on the idea of limiting the number of switches that a decision maker may make between two decision alternatives as a parameter of the problem varies. We show how this formulation also relates many widely used concepts in single and multiattribute utility theory.", "e:keyword": ["One-switch", "Utility independence", "Risk aversion", "Multiattribute utility"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1089", "e:abstract": "We introduce a new approach to modelling queueing systems where the priority or the routing of customers depends on the time the first customer has waited in the queue. This past waiting time of the first customer in line, <i>W</i><sup>FIL</sup>, is used as the primary variable for our approach. A Markov chain is used for modelling the system where the states represent both the number of free servers and a discrete approximation to <i>W</i><sup>FIL</sup>. This approach allows us to obtain waiting time distributions for complex systems, such as the N-design routing scheme widely used, e.g., in call centers and systems with dynamic priorities.", "e:keyword": ["Waiting time distribution", "Call centers", "Priority queues", "Deterministic threshold", "Erlang distribution", "Dynamic priority", "Due date"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1090", "e:abstract": "We consider Markov decision processes with unknown transition probabilities and unknown single-period expected cost functions, and we study a method for estimating these quantities from historical or simulated data. The method requires knowledge of the system equations that govern state transitions as well as the single-period cost functions (but not the single-period expected cost functions). The estimation procedure is based upon taking expectations with respect to the empirical distribution functions of such data. Once the estimates are in place, the method computes a policy by solving the obtained “empirical” Markov decision process as if the estimates were correct. For MDPs that satisfy some conditions, we provide explicit, easily computed expressions for the probability that the procedure will produce a policy whose true expected cost is within any specified absolute distance of the actual optimal expected cost of the true Markov decision process. We also provide expressions for the number of historical or simulated data values that is sufficient for the procedure to produce a policy whose true expected cost is, with a prescribed probability, within a prescribed absolute distance of the actual optimal expected cost of the true Markov decision process. We apply our results to multiperiod inventory models. In addition, we provide a specialized analysis of such inventory models that also yields relative, rather than absolute, accuracy guarantees. We make comparisons with related results that have recently appeared, and we provide numerical examples.", "e:keyword": ["Dynamic programming/optimal control", "Markov", "Inventory/production", "Statistics", "Estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1091", "e:abstract": "It is common for rewards to be given on the basis of a rank ordering, so that relative performance amongst a cohort is the criterion. In this paper we formulate an equilibrium model in which an agent makes successive decisions on whether or not to gamble and is rewarded on the basis of a rank ordering of the final position amongst competing players. One application of this model is to the behavior of mutual fund managers who are paid depending on funds under management, which in turn are greatly influenced by annual or quarterly rank orderings. Our model deals with a situation in which fund managers can elect either to pick stocks or to use a market-tracking strategy. In equilibrium the distribution of the final position will have a negative skew. We explore how this distribution depends on the number of players, the probability of success when gambling, the structure of the rewards, and on information regarding the performance of other players.", "e:keyword": ["Ranking games", "Tournaments", "Negative skew", "Prize structure"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1092", "e:abstract": "Arrow and Pratt introduced a measure of risk aversion---the negative ratio of the second to the first derivative of the utility function. This measure has found widespread use in the valuation of uncertain lotteries and in the calculation of the risk premium of an investment. This paper introduces two new measures for characterizing changes in the valuation of uncertain lotteries when their outcomes are modified by a monotone transformation. The first is a characteristic transformation of a utility function, <i>U</i>, and a monotone transformation, <i>g</i>. The shape of the characteristic transformation determines an upper bound, lower bound, or equality on the magnitude of the certainty equivalent of the modified lottery. The second is a measure of change in certainty equivalent, (eta)<i><sub>g</sub></i>, whose sign also determines upper or lower bounds, and whose magnitude determines the change in value of a “small-risk” lottery when its outcomes are modified by a monotone transformation. For shift (and scale) transformations on the lottery outcomes, both the characteristic transformation and the measure of change, (eta)<i><sub>g</sub></i>, provide new characterizations for the notions of decreasing absolute (and relative) risk aversion with wealth.", "e:keyword": ["Utility theory", "Risk attitude", "Valuation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1093", "e:abstract": "In this paper, we present a general framework for designing approximation schemes for combinatorial optimization problems in which the objective function is a combination of more than one function. Examples of such problems include those in which the objective function is a product or ratio of two linear functions, parallel machine scheduling problems with the makespan objective, robust versions of weighted multiobjective optimization problems, and assortment optimization problems with logit choice models. The main idea behind our approximation schemes is the construction of an approximate Pareto-optimal frontier of the functions that constitute the given objective. Using this idea, we give the first fully polynomial-time approximation schemes for the max-min resource allocation problem with a fixed number of agents, combinatorial optimization problems in which the objective function is the sum of a fixed number of ratios of linear functions, or the product of a fixed number of linear functions, and assortment optimization problems with logit choice model.", "e:keyword": ["Analysis of algorithms", "Combinatorial optimization", "Networks/graphs", "Production/scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1094", "e:abstract": "Growing social concerns over the environmental externalities associated with business activities are pushing firms to identify activities that create economic value with less environmental impact and to become more eco-efficient. Over the past two decades, researchers have increasingly used frontier efficiency models to evaluate productive efficiency in the presence of undesirable outputs, such as greenhouse gas emissions or toxic emissions. In this paper, we identify critical flaws in existing frontier models and show that these models can identify eco-inefficient firms as eco-efficient. We develop a new eco-inefficiency frontier model that rectifies these problems. Our model calculates an eco-inefficiency score for each firm and improvements in outputs necessary to attain eco-efficiency. We demonstrate through a Monte Carlo experiment that our eco-inefficiency model provides a more reliable measurement of corporate eco-inefficiency than the existing frontier models. We also extend the single-output Cobb-Douglas production function to multiple desirable and undesirable outputs. This extension allows for greater flexibility in the simulation analysis of frontier models.", "e:keyword": ["Environmental performance", "Eco-efficiency", "Nonparametric frontier models", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1095", "e:abstract": "In this paper we consider a single-server queue fed by <i>K</i> independent renewal arrival streams, each representing a different job class. Jobs are processed in a FIFO fashion, regardless of class. The total amount of work arriving to the system exceeds the server's capacity. That is, the nominal traffic intensity of the system is assumed to be greater than one. Jobs arriving to the system grow impatient and abandon the queue after a random amount of time if service has not yet begun. Interarrival, service, and abandonment times are assumed to be generally distributed and class specific.We approximate this system using both fluid and diffusion limits. To this end, we consider a sequence of systems indexed by <i>n</i> in which the arrival and service rates are proportional to <i>n</i>; the abandonment distribution remains fixed across the sequence. In our first main result, we show that in the limit as <i>n</i> tends to infinity, the virtual waiting time process converges to a limiting deterministic process. This limit may be characterized as the solution to a first-order ordinary differential equation (ODE). Specific examples are then presented for which the ODE may be explicitly solved. In our second main result, we refine the deterministic fluid approximation by showing that the fluid-centered and diffusion-scaled virtual waiting time process weakly converges to an Ornstein-Uhlenbeck process whose drift and infinitesimal variance both vary over time. This process may also be solved for explicitly, thus yielding approximations to the transient as well as steady-state behavior of the virtual waiting time process.", "e:keyword": ["Queueing", "Abandonment", "Supercritical loading", "Overloaded queue", "Ornstein--Uhlenbeck process", "Diffusion limit", "Virtual waiting time process"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1096", "e:abstract": "Crisis-level overcrowding conditions in emergency departments (EDs) have led hospitals to seek out new patient-flow designs to improve both responsiveness and safety. One approach that has attracted attention and experimentation in the emergency medicine community is a system in which ED beds and care teams are segregated and patients are “streamed” based on predictions of whether they will be discharged or admitted to the hospital. In this paper, we use a combination of analytic and simulation models to determine whether such a streaming policy can improve ED performance, where it is most likely to be effective, and how it should be implemented for maximum performance. Our results suggest that the concept of streaming can indeed improve patient flow, but only in some situations. First, ED resources must be shared across streams rather than physically separated. This leads us to propose a new “virtual-streaming” patient flow design for EDs. Second, this type of streaming is most effective in EDs with (1) a high percentage of admitted patients, (2) longer care times for admitted patients than discharged patients, (3) a high day-to-day variation in the percentage of admitted patients, (4) long patient boarding times (e.g., caused by hospital “bed-block”), and (5) high average physician utilization. Finally, to take full advantage of streaming, physicians assigned to admit patients should prioritize upstream (new) patients, whereas physicians assigned to discharge patients should prioritize downstream (old) patients.", "e:keyword": ["Healthcare operations management", "Emergency department", "Patient flow", "Patient sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1097", "e:abstract": "We propose a novel analytic approach for the comparative statics analysis of multiproduct multiresource newsvendor networks under responsive pricing. Our approach involves exploiting the properties of the primal mathematical programming formulation and of the dual variables and linking those properties to the concept of convex orders and to properties of the underlying demand function. The use of convex orders allows us to establish our main results without restriction to a specific demand distribution. A major strength of our approach is that it is “scalable,” i.e., it applies to newsvendor networks with any number of “nonindependent” (i.e., demand or resource sharing) products and resources, without an exponential increase in effort as problem size increases. This is unlike the current approaches commonly used in the operations management literature, which typically involve a parametric analysis of the recourse problem, followed by the use of Jacobians and the implicit function theorem. Providing a rigorous framework for comparative statics analysis, which can be applied to other problems that are not amenable to traditional parametric analysis, is our main contribution.We demonstrate this approach on the optimal capacity decision problem in multiproduct newsvendor networks under responsive pricing, formulated as a two-stage stochastic programming problem with recourse: The firm determines the resource capacities ex ante, in the first stage, when demand intercepts are uncertain, and makes the pricing and production decisions ex post, in the second stage, when demand intercepts (e.g., market conditions) are fully observed. This particular problem and its variants are well studied in the operations management literature. A comparative statics analysis is integral to the study of the capacity investment decision, as it allows answers to important questions such as the following: “Does the firm acquire more or less of the different resources available as demand uncertainty increases? Does the firm benefit from an increase in demand uncertainty?” Using our proposed approach, we establish comparative statics results on how the newsvendor's expected profit and optimal capacity decision change with demand risk in multiproduct multiresource newsvendor networks. We also extend our analysis to the study of demand dependence in two-product networks.", "e:keyword": ["Comparative statics analysis", "Capacity planning", "Responsive pricing", "Resource flexibility", "Substitutable products", "Stochastic programming with recourse", "Convex orders"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1101", "e:abstract": "Comment on “A POMDP Approach to Personalize Mammography Screening Decisions” by Turgay Ayer, Oguzhan Alagoz, and Natasha K. Stout.", "e:keyword": ["Decision analysis", "Medical decision making", "Breast cancer", "Mammography screening", "Personalized screening"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1102", "e:abstract": "Fast-fashion retailers such as Zara offer continuously changing assortments and use minimal in-season promotions. Their clearance pricing problem is thus challenging because it involves comparatively more different articles of unsold inventory with less historical price data points. Until 2007, Zara used a manual and informal decision-making process for determining price markdowns. In collaboration with their pricing team, we since designed and implemented an alternative process relying on a formal forecasting model feeding a price optimization model. As part of a controlled field experiment conducted in all Belgian and Irish stores during the 2008 fall-winter season, this new process increased clearance revenues by approximately 6%. Zara is currently using this process worldwide for its markdown decisions during clearance sales.", "e:keyword": ["Retailing", "Markdown pricing", "Clearance sales", "Fast-fashion", "Revenue management", "Forecasting", "Field test", "Model implementation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1103", "e:abstract": "We consider a general class of network revenue management problems, where mean demand at each point in time is determined by a vector of prices, and the objective is to dynamically adjust these prices so as to maximize expected revenues over a finite sales horizon. A salient feature of our problem is that the decision maker can only observe <i>realized demand</i> over time but does not know the underlying <i>demand function</i> that maps prices into instantaneous demand rate. We introduce a family of “blind” pricing policies that are designed to balance trade-offs between exploration (demand learning) and exploitation (pricing to optimize revenues). We derive bounds on the revenue loss incurred by said policies in comparison to the <i>optimal</i> dynamic pricing policy that <i>knows</i> the demand function a priori, and we prove that asymptotically, as the volume of sales increases, this gap shrinks to zero.", "e:keyword": ["Revenue management", "Network", "Pricing", "Nonparametric estimation", "Minimax", "Learning", "Asymptotic optimality", "Curse of dimensionality"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1104", "e:abstract": "An algorithm is developed to determine time-dependent staffing levels to stabilize the time-dependent abandonment probabilities and expected delays at positive target values in the <i>M</i><sub>t</sub>/<i>GI</i>/<i>s<sub>t</sub></i> + <i>GI</i> many-server queueing model, which has a nonhomogeneous Poisson arrival process (the <i>M<sub>t</sub></i>), has general service times (the first <i>GI</i>), and allows customer abandonment according to a general patience distribution (the +<i>GI</i>). New offered-load and modified-offered-load approximations involving infinite-server models are developed for that purpose. Simulations show that the approximations are effective. A many-server heavy-traffic limit in the efficiency-driven regime shows that (i) the proposed approximations achieve the goal asymptotically as the scale increases, and (ii) it is not possible to simultaneously stabilize the mean queue length in the same asymptotic regime.", "e:keyword": ["Staffing", "Capacity planning", "Many-server queues", "Queues with time-varying arrivals", "Queues with abandonment", "Infinite-server queues", "Offered-load approximations", "Service systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1105", "e:abstract": "This work examines the impact of discharge decisions under uncertainty in a capacity-constrained high-risk setting: the intensive care unit (ICU). New arrivals to an ICU are typically very high-priority patients and, should the ICU be full upon their arrival, discharging a patient currently residing in the ICU may be required to accommodate a newly admitted patient. Patients so discharged risk physiologic deterioration, which might ultimately require readmission; models of these risks are currently unavailable to providers. These readmissions in turn impose an additional load on the capacity-limited ICU resources.We study the impact of several different ICU discharge strategies on patient mortality and total readmission load. We focus on discharge rules that prioritize patients based on some measure of criticality assuming the availability of a model of readmission risk. We use empirical data from over 5,000 actual ICU patient flows to calibrate our model. The empirical study suggests that a predictive model of the readmission risks associated with discharge decisions, in tandem with simple index policies of the type proposed, can provide very meaningful throughput gains in actual ICUs while at the same time maintaining, or even improving upon, mortality rates. We explicitly provide a discharge policy that accomplishes this. In addition to our empirical work, we conduct a rigorous performance analysis for the family of discharge policies we consider. We show that our policy is optimal in certain regimes, and is otherwise guaranteed to incur readmission related costs no larger than a factor of <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$(\\hat{\\rho}+1)$\\end{document}</tex-math></inline-formula> of an optimal discharge strategy, where <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\hat{\\rho}$\\end{document}</tex-math></inline-formula> is a certain natural measure of system utilization.", "e:keyword": ["Dynamic programming", "Healthcare", "Approximation algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1106", "e:abstract": "This paper presents an approach for representing functions of discrete variables, and their products, using logarithmic numbers of binary variables. Given a univariate function whose domain consists of <i>n</i> distinct values, it begins by employing a base-2 expansion to express the function in terms of the ceiling of log<sub>2</sub><i>n</i> binary and <i>n</i> continuous variables, using linear restrictions to equate the functional values with the possible binary realizations. The representation of the product of such a function with a nonnegative variable is handled via an appropriate scaling of the linear restrictions. Products of <i>m</i> functions are treated in an inductive manner from <i>i</i> = 2 to <i>m</i>, where each step <i>i</i> uses such a scaling to express the product of function <i>i</i> and a nonnegative variable denoting a translated version of the product of functions 1 through <i>i</i> - 1 as a newly defined variable. The resulting representations, both in terms of one function and many, are important for reformulating general discrete variables as binary, and also for linearizing mixed-integer generalized geometric and discrete nonlinear programs, where it is desired to economize on the number of binary variables. The approach provides insight into, improves upon, and subsumes related linearization methods for products of functions of discrete variables.", "e:keyword": ["Programming", "Integer", "Nonlinear", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1107", "e:abstract": "We analytically study optimal capacity and flexible technology selection in parallel queuing systems. We consider <i>N</i> stochastic arrival streams that may wait in <i>N</i> queues before being processed by one of many resources (technologies) that differ in their flexibility. A resource's ability to process <i>k</i> different arrival types or classes is referred to as level-<i>k</i> flexibility. We determine the capacity portfolio (consisting of <i>all</i> resources at <i>all</i> levels of flexibility) that minimizes linear capacity and linear holding costs in high-volume systems where the arrival rate (lambda) (rightarrow) (infinity). We prove that “a little flexibility is all you need”: the optimal portfolio invests <i>O</i>((lambda)) in specialized resources and only <i>O</i>((sqrt)(lambda)) in flexible resources and these optimal capacity choices bring the system into heavy traffic. Further, considering symmetric systems (with type-independent parameters), a novel “folding” methodology allows the specification of the asymptotic queue count process for any capacity portfolio under longest-queue scheduling in closed form that is amenable to optimization. This allows us to sharpen “a little flexibility is all you need”: the asymptotically optimal flexibility configuration for symmetric systems with mild economies of scope invests a lot in specialized resources but only a little in flexible resources and only in level-2 flexibility, but effectively nothing (<i>o</i>((sqrt)(lambda))) in level-<i>k</i> > 2 flexibility. We characterize “tailored pairing” as the theoretical benchmark configuration that maximizes the value of flexibility when demand and service uncertainty are the main concerns.", "e:keyword": ["Flexibility", "Capacity optimization", "Queueing network", "Diffusion approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1108", "e:abstract": "Lately, the problem of cell formation (CF) has gained a lot of attention in the industrial engineering literature. Since it was formulated (more than 50 years ago), the problem has incorporated additional industrial factors and constraints while its solution methods have been constantly improving in terms of the solution quality and CPU times. However, despite all the efforts made, the available solution methods (including those for a popular model based on the <i>p</i>-median problem, PMP) are prone to two major types of errors. The first error (the modeling one) occurs when the intended objective function of the CF (as a rule, verbally formulated) is substituted by the objective function of the PMP. The second error (the algorithmic one) occurs as a direct result of applying a heuristic for solving the PMP. In this paper we show that for instances that make sense in practice, the modeling error induced by the PMP is negligible. We exclude the algorithmic error completely by solving the adjusted pseudo-Boolean formulation of the PMP exactly, which takes less than one second on a general-purpose PC and software. Our experimental study shows that the PMP-based model produces high-quality cells and in most cases outperforms several contemporary approaches.", "e:keyword": ["Cell formation", "p-median problem", "Pseudo-Boolean polynomial", "Group technology"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1109", "e:abstract": "Given a set of identical capacitated bins, a set of weighted items, and a set of precedences among such items, we are interested in determining the minimum number of bins that can accommodate all items and can be ordered in such a way that all precedences are satisfied. The problem, denoted as the bin packing problem with precedence constraints (BPP-P), has a very intriguing combinatorial structure and models many assembly and scheduling issues.According to our knowledge, the BPP-P has received little attention in the literature, and in this paper we address it for the first time with exact solution methods. In particular, we develop reduction criteria, a large set of lower bounds, a variable neighborhood search upper bounding technique, and a branch-and-bound algorithm. We show the effectiveness of the proposed algorithms by means of extensive computational tests on benchmark instances and comparison with standard integer linear programming techniques.", "e:keyword": ["Bin packing problem", "Precedence constraints", "Branch-and-bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1110", "e:abstract": "We consider an outsourcing problem where a group of manufacturers outsource jobs to a single third party who owns a specialized facility needed to process these jobs. The third party announces the time slots available on her facility, and the associated prices. Manufacturers reserve, on a first-come-first-book basis, time slots that they desire to utilize. Booking of overtime is possible, at a higher cost. A job completed after its due date incurs a tardiness cost. Each manufacturer books chunks of facility time and sequences his jobs over the time slots booked to minimize his booking, overtime, and tardiness costs. This model captures the main features of outsourcing operations in industries such as semiconductor manufacturing, biotechnology, and drug R&D. In current practice, the third party executes all outsourced jobs without performing optimization and coordination. We investigate the issue of the third party serving as a coordinator to create a win--win solution for all. We propose a model based on a cooperative game as follows: (i) Upon receiving the booking requests from the manufacturers, the third party derives an optimal solution if manufacturers cooperate, and computes the savings achieved. (ii) She devises a savings sharing scheme so that, in monetary terms, every manufacturer is better off to coordinate than to act independently or coalesce with a subgroup of manufacturers. (iii) For her work, the third party withholds a portion (rho) of the booking revenue paid by the manufacturers for time slots that are released after coordination. We further design a truth-telling mechanism that can prevent any self-interested manufacturer from purposely reporting false job data to take advantage of the coordination scheme. Finally, we perform a computational experiment to assess the value of coordination to the various parties involved.", "e:keyword": ["Outsourcing", "Scheduling and planning", "Cooperative game", "Truth-telling mechanism"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1111", "e:abstract": "Every year, companies that produce commercial tax preparation software struggle with thousands of state and federal changes to tax laws and forms. Three competitors dominate the market with its short selling season, and release delays slash profits. Tax authorities issue updates August--December, and all changes must be processed and incorporated before year end. Systematic resource allocation and process management are crucial yet problematic due to the volume and complexity of changes, brief production time frame, and feedback loops for bug resolution. A leading tax software provider asked us to propose systematic approaches for managing process flow and staffing development stages with the goal of releasing the new version on time at minimum cost. To that end, we developed deterministic models that partitioned tax forms into development groups and determined staffing levels for each group. Partitioning forms into groups simplified workflow management and staffing decisions. To provide a range of resource configurations, we used two modeling approaches. Numerical experiments showed that our models capture the salient features of the process and that our heuristics perform well. Implementing our models reduced company overtime hours by 31% and total workforce cost by 13%.", "e:keyword": ["Product development", "Software development", "Tax software", "Workforce management", "Resource allocation grouping index", "Integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1112", "e:abstract": "The current financial crisis motivates the study of correlated defaults in financial systems. In this paper we focus on such a model, which is based on Markov random fields. This is a probabilistic model in which uncertainty in default probabilities incorporates experts' opinions on the default risk (based on various credit ratings). We consider a bilevel optimization model for finding an optimal recovery policy: which companies should be supported given a fixed budget. This is closely linked to the problem of finding a maximum likelihood estimator of the defaulting set of agents, and we show how to compute this solution efficiently using combinatorial methods. We also prove properties of such optimal solutions and give a practical procedure for estimation of model parameters. Computational examples are presented, and experiments indicate that our methods can find optimal recovery policies for up to about 100 companies. The overall approach is evaluated on a real-world problem concerning the major banks in Scandinavia and public loans. To our knowledge, this is a first attempt to apply combinatorial optimization techniques to this important and expanding area of default risk analysis.", "e:keyword": ["Financial models", "Discrete optimization", "Bilevel programming", "Markov random field"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1113", "e:abstract": "We consider the Arrow-Debreu market with linear utilities in which there is a set <i>G</i> of divisible goods and a set <i>B</i> of buyers. Each buyer starts with an initial endowment of goods. The buyer's utility function is a linearly separable function of the goods that the buyer purchases. We develop a simple and efficient algorithm for determining an approximate market equilibrium. Our algorithm finds an (epsilon)-approximate solution in <i>O</i>(<i>n</i>/(epsilon)(|<i>B</i>||<i>G</i>|)) time, where <i>n</i> = |<i>B</i>| + |<i>G</i>|. The running time can be further improved to <i>O</i>(<i>n</i>/(epsilon)(<i>m</i> + |<i>B</i>|log|<i>B</i>|)) where <i>m</i> is the number of pairs (<i>i, j</i>) such that buyer <i>i</i> has some utility for purchasing good <i>j</i>.", "e:keyword": ["Network optimization", "Market equilibrium", "Arrow-Debreu market"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1114", "e:abstract": "We consider a stochastic inventory control problem in which a buyer makes procurement decisions while facing periodic random demand and two supply sources, namely, a long-term contract supplier and a spot market. The contract between the buyer and the supplier partially shields the latter from the vicissitudes of the spot market, in that the price paid by the buyer to the supplier is only partially linked to the spot price at the moment. After fulfilling the minimum-order commitment with the supplier, the buyer has the full freedom to source from both the supplier and the market. Procurement from the spot market also incurs a fixed setup cost. We show that an optimal policy consists of three different policy forms, with the realization of each depending on the buyer's inventory level and the prevalent spot price. Certain conditions are identified under which monotone trends exist between policy parameters and the current spot price.", "e:keyword": ["Inventory control", "Contract supplier", "Spot price", "Markov process"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1115", "e:abstract": "The Black-Litterman (BL) model is a widely used asset allocation model in the financial industry. In this paper, we provide a new perspective. The key insight is to replace the statistical framework in the original approach with ideas from inverse optimization. This insight allows us to significantly expand the scope and applicability of the BL model. We provide a richer formulation that, unlike the original model, is flexible enough to incorporate investor information on volatility and market dynamics. Equally importantly, our approach allows us to move beyond the traditional mean-variance paradigm of the original model and construct “BL”-type estimators for more general notions of risk such as coherent risk measures. Computationally, we introduce and study two new “BL”-type estimators and their corresponding portfolios: a mean variance inverse optimization (MV-IO) portfolio and a robust mean variance inverse optimization (RMV-IO) portfolio. These two approaches are motivated by ideas from arbitrage pricing theory and volatility uncertainty. Using numerical simulation and historical backtesting, we show that both methods often demonstrate a better risk-reward trade-off than their BL counterparts and are more robust to incorrect investor views.", "e:keyword": ["Finance", "Portfolio optimization", "Programming", "Inverse optimization", "Statistics", "Estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1116", "e:abstract": "We study the problem of finding an envy-free allocation of a cake to <i>d</i> + 1 players using <i>d</i> cuts. Two models are considered, namely, the oracle-function model and the polynomial-time function model. In the oracle-function model, we are interested in the number of times an algorithm has to query the players about their preferences to find an allocation with the envy less than (epsilon). We derive a matching lower and upper bound of (theta)(1/(epsilon))<sup><i>d</i> - 1</sup> for players with Lipschitz utilities and any <i>d</i> > 1. In the polynomial-time function model, where the utility functions are given explicitly by polynomial-time algorithms, we show that the envy-free cake-cutting problem has the same complexity as finding a Brouwer's fixed point, or, more formally, it is PPAD-complete. On the flip side, for monotone utility functions, we propose a fully polynomial-time algorithm (FPTAS) to find an approximate envy-free allocation of a cake among three people using two cuts.", "e:keyword": ["Fair division", "Cake cutting", "Envy-free", "FPTAS", "Fixed point", "PPAD"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1117", "e:abstract": "We consider a congestible system serving multiple classes of customers who differ in their delay sensitivity and valuation of service (or product). Customers are endowed with convex-concave delay cost functions. A system manager offers a menu of lead times and corresponding prices to arriving customers, who then choose the lead-time--price pair that maximizes their net utility (value minus disutility of delay and price). We investigate how such menus should be chosen dynamically (depending on the system backlog) to maximize welfare. We formulate a novel fluid model of the problem and show that the cost-balancing policy (based on the convex hulls of the delay cost functions) is socially optimal if the system manager can tell customer types apart. If types are indistinguishable to the system manager, the cost-balancing policy is also incentive compatible under social optimization. Finally, we show through a simulation study that the cost-balancing policy does well in the context of the original (stochastic) problem by testing it against various natural benchmarks.", "e:keyword": ["Stochastic uncertainty", "Diffusion models", "Review/lead-times policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1118", "e:abstract": "Scientists in all disciplines attempt to identify and document causal relationships. Those not fortunate enough to be able to design and implement randomized control trials must resort to observational studies. To make causal inferences outside the experimental realm, researchers attempt to control for bias sources by postprocessing observational data. Finding the subset of data most conducive to unbiased or least biased treatment effect estimation is a challenging, complex problem. However, the rise in computational power and algorithmic sophistication leads to an operations research solution that circumvents many of the challenges presented by methods employed over the past 30 years.", "e:keyword": ["Causal inference", "Balance optimization", "Subset selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1119", "e:abstract": "We study lot-size policies in a serial, multistage manufacturing/inventory system with two key generalizations, namely (1) random yields at each production stage and (2) an autoregressive demand process. Previous research shows that the optimal policies in models with random yields (even in models with a single installation) lack the familiar order-up-to structure and are not myopic.  Thus, dynamic programming algorithms are needed to compute optimal policies, and one encounters the “curse of dimensionality”; this is exacerbated here by the need to expand the size and dimension of the state space to accommodate the autoregressive demand feature.  Nevertheless, although our model is more complex, we prove that there is an optimal policy with the order-up-to feature and, more importantly, that the optimal policy is myopic. This avoids the computational burden of dynamic programming.  Our results depend on two assumptions concerning the stochastic yield, namely that the expected yield at a work station is proportional to the lot size, and the distribution of the deviation of the yield from its mean does not depend on the lot size. We introduce the concept of echelon-like variables, a generalization of Clark and Scarf's classical concept of echelon variables, to derive the structure of optimal policies. Furthermore, we show that the same kind of policy is optimal for several criteria: infinite-horizon discounted cost, infinite-horizon long-run average cost, and finite-horizon discounted cost (with the appropriate choice of the salvage value function).", "e:keyword": ["Inventory/production", "Multi-item/echelon/stage", "Inventory/production", "Perishable/aging items", "Dynamic programming/optimal control", "Models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1120", "e:abstract": "We propose a new modeling and solution method for probabilistically constrained optimization problems. The methodology is based on the integration of the stochastic programming and combinatorial pattern recognition fields. It permits the fast solution of stochastic optimization problems in which the random variables are represented by an extremely large number of scenarios. The method involves the binarization of the probability distribution and the generation of a consistent partially defined Boolean function (pdBf) representing the combination <i>(F,p)</i> of the binarized probability distribution <i>F</i> and the enforced probability level <i>p</i>. We show that the pdBf representing <i>(F,p)</i> can be compactly extended as a disjunctive normal form (DNF). The DNF is a collection of combinatorial <i>p</i>-patterns, each defining sufficient conditions for a probabilistic constraint to hold. We propose two linear programming formulations for the generation of <i>p</i>-patterns that can be subsequently used to derive a linear programming inner approximation of the original stochastic problem. A formulation allowing for the concurrent generation of a <i>p</i>-pattern and the solution of the deterministic equivalent of the stochastic problem is also proposed. The number of binary variables included in the deterministic equivalent formulation is not an increasing function of the number of scenarios used to represent uncertainty. Results show that large-scale stochastic problems, in which up to 50,000 scenarios are used to describe the stochastic variables, can be consistently solved to optimality within a few seconds.", "e:keyword": ["Programming", "Stochastic", "Probability", "Combinatorial pattern", "Probabilistic constraint", "Boolean programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1121", "e:abstract": "Nonstationary infinite-horizon Markov decision processes (MDPs) generalize the most well-studied class of sequential decision models in operations research, namely, that of stationary MDPs, by relaxing the restrictive assumption that problem data do not change over time. Linear programming (LP) has been very successful in obtaining structural insights and devising solution methods for stationary MDPs. However, an LP approach for nonstationary MDPs is currently missing. This is because the LP formulation of a nonstationary infinite-horizon MDP includes countably infinite variables and constraints, and research on such infinite-dimensional LPs has traditionally faced several hurdles. For instance, duality results may not hold; an extreme point may not be a basic feasible solution; and in the context of a simplex algorithm, a pivot operation may require infinite data and computations, and a sequence of improving extreme points need not converge in value to optimal. In this paper, we tackle these challenges and establish (1) weak and strong duality, (2) complementary slackness, (3) a basic feasible solution characterization of extreme points, (4) a one-to-one correspondence between extreme points and deterministic Markovian policies, and (5) we devise a simplex algorithm for an infinite-dimensional LP formulation of nonstationary infinite-horizon MDPs. Pivots in this simplex algorithm use finite data, perform finite computations, and generate a sequence of improving extreme points that converges in value to optimal. Moreover, this sequence of extreme points gets arbitrarily close to the set of optimal extreme points. We also prove that decisions prescribed by these extreme points are eventually exactly optimal in all states of the nonstationary infinite-horizon MDP in early periods.", "e:keyword": ["Simplex algorithm", "Infinite linear programs", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1122", "e:abstract": "It is known that the incorporation of weight restrictions in models of data envelopment analysis may result in their infeasibility. In our paper we investigate this effect in detail. We show that the infeasibility is only one of several possible outcomes that point to a particular problem with weight restrictions. For example, the use of weight restrictions may also lead to zero or negative efficiency scores of some units. Removing problematic units from the data set does not necessarily remove the underlying problem caused by the weight restrictions and only makes it undetected. We prove that all such problems arise when weight restrictions induce free or unlimited production of outputs in the underlying technology. This is unacceptable from the production theory point of view and indicates that the weight restrictions need reassessing. We develop analytical criteria and computational methods that allow us to identify the above problematic situations.", "e:keyword": ["Data envelopment analysis", "Weight restrictions", "Production trade-offs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1123", "e:abstract": "We study project scheduling in a competitive setting taking the perspective of a project manager with an adversary, using a Stackelberg game format. The project manager seeks to limit the adversary's opportunity to react to the project and therefore wants to manage the project in a way that keeps the adversary “in the dark” as long as possible while completing the project on time. We formulate and illustrate a new form of project management problem for secret projects where the project manager uses a combination of deception, task scheduling, and crashing to minimize the time between when the adversary initiates a response to the project to when the project is completed. We propose a novel mixed-integer linear programming formulation for the problem and determine characteristics of optimal schedules in this context. Using a detailed example of nuclear weapons development, we illustrate the interconnectedness of the deception, task scheduling, and crashing, and how these influence adversary behavior.", "e:keyword": ["Competitive project scheduling", "Exposed time", "Secret projects", "Covert operations", "Interdiction", "Mixed-integer linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1124", "e:abstract": "We examine the problem of allocating an item repeatedly over time amongst a set of agents. The value that each agent derives from consumption of the item may vary over time. Furthermore, it is private information to the agent, and prior to consumption it may be unknown to that agent. We describe a mechanism based on a sampling-based learning algorithm that under suitable assumptions is asymptotically individually rational, asymptotically Bayesian incentive compatible, and asymptotically ex ante efficient. Our mechanism can be interpreted as a pay-per-action or pay-per-acquisition (PPA) charging scheme in online advertising. In this scheme, instead of paying per click, advertisers pay only when a user takes a specific action (e.g., purchases an item or fills out a form) on their websites.", "e:keyword": ["Dynamic mechanism design", "Prior-free mechanisms", "Online advertising", "Sponsored search", "Pay-per-action", "Cost-per-action", "Click fraud"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1126", "e:abstract": "A general form of minimizing the Rayleigh ratio on discrete variables is shown here, for the first time, to be polynomial time solvable. This is significant because major problems in clustering, partitioning, and imaging can be presented as the Rayleigh ratio minimization on discrete variables and an <i>orthogonality</i> constraint. These challenging problems are modeled as the <i>normalized cut</i> problem, the graph expander ratio problem, the Cheeger constant problem, or the <i>conductance</i> problem, all of which are NP-hard. These problems have traditionally been solved, heuristically, using the “spectral technique.” A unified framework is provided here whereby all these problems are formulated as a constrained minimization form of a quadratic ratio, referred to here as the <i>Rayleigh ratio</i>. The quadratic ratio is to be minimized on discrete variables and a single sum constraint that we call the <i>balance</i> or <i>orthogonality</i> constraint. When the discreteness constraints on the variables are disregarded, the resulting continuous relaxation is solved by the spectral method. It is shown here that the Rayleigh ratio minimization subject to the discreteness constraints requiring each variable to assume one of two values in {-<i>b</i>,1} is solvable in strongly polynomial time, equivalent to a single minimum <i>s</i>,<i>t</i> cut algorithm on a graph of same size as the input graph, for any nonnegative value of <i>b</i>. This discrete form for the Rayleigh ratio problem was often assumed to be NP-hard. Not only is it shown here that the discrete Rayleigh ratio problem is polynomial time solvable, but also the algorithm is more efficient than the spectral algorithm. Furthermore, an experimental study demonstrates that the new algorithm provides in practice an improvement, often dramatic, on the quality of the results of the spectral method, both in terms of approximating the true optimum of the Rayleigh ratio problem on both the discrete variables and the balance constraint, and in terms of the subjective partition quality. A further contribution here is the introduction of a problem, the <i>quantity-normalized cut</i>, generalizing all the Rayleigh ratio problems. The discrete version of that problem is also solved with the efficient algorithm presented. This problem is shown, in a companion paper, to enable the modeling of features essential to clustering that are valuable in practical applications.", "e:keyword": ["Cheeger constant", "Parametric cut algorithm", "Fiedler eigenvector", "Quantity-normalized cut"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1127", "e:abstract": "We develop a family of rollout policies based on fixed routes to obtain dynamic solutions to the <i>vehicle routing problem with stochastic demand and duration limits</i> (VRPSDL). In addition to a traditional one-step rollout policy, we leverage the notions of the pre- and post-decision state to distinguish two additional rollout variants. We tailor our rollout policies by developing a dynamic decomposition scheme that achieves high quality solutions to large problem instances with reasonable computational effort. Computational experiments demonstrate that our rollout policies improve upon the performance of a rolling horizon procedure and commonly employed fixed-route policies, with improvement over the latter being more substantial.", "e:keyword": ["Rollout policy", "Approximate dynamic programming", "Stochastic vehicle routing", "Fixed routes"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1128", "e:abstract": "Information systems play a very important role in managerial decision making within modern organizations. While making different types of decisions (at operational, tactical, and strategic levels), managers are increasingly relying on information gleaned from various databases, data warehouses, and data streams feeding them. The quality of organizational decisions, therefore, often depends on the quality of the information derived from these databases and data streams, and a manager is able to make better use of the information if she also understands the quality level of that information. Previous research has examined how the quality level of a database query output can be estimated based on the quality level of the input data. In this research, we generalize this stream of research and allow a query to have general selection conditions involving multiple attributes with any combination of conjunction or disjunction of subconditions that may include functions of multiple attributes. Results of this research can easily be implemented in real-world decision contexts.", "e:keyword": ["Data quality", "Selection operation", "Error propagation", "Type-I and type-II errors"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1129", "e:abstract": "In a call center, arriving customers must be routed to available servers, and servers that have just become available must be scheduled to help waiting customers. These dynamic routing and scheduling decisions are very difficult, because customers have different needs and servers have different skill levels. A further complication is that it is preferable that these decisions are made blindly; that is, they depend only on the system state and not on system parameter information such as call arrival rates and service speeds. This is because this information is generally not known with certainty. Ideally, a dynamic control policy for making routing and scheduling decisions balances customer and server needs by keeping customer delays low but still fairly dividing the workload amongst the various servers. In this paper, we propose a blind dynamic control policy for parallel-server systems with multiple customer classes and server pools that is based on the number of customers waiting and the number of agents idling. We show that in the Halfin-Whitt many-server heavy-traffic limiting regime, our proposed blind policy performs extremely well when the objective is to minimize customer holding costs subject to “server fairness,” as defined by how the system idleness is divided among servers. To do this, we formulate an approximating diffusion control problem (DCP) and compare the performance of the nonblind DCP solution to a feasible policy for the DCP that is blind. We establish that the increase in the DCP objective function value is small over a wide range of parameter values. We then use simulation to validate that a small increase in the DCP objective function value is indicative of our proposed blind policy performing very well.", "e:keyword": ["Probability", "Diffusion", "Stochastic model applications", "Queues", "Approximations", "Diffusion models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1130", "e:abstract": "Vendor-managed inventory (VMI) replenishment is a collaboration between a supplier and its customers, where the supplier is responsible for managing the customers' inventory levels. In the VMI setting we consider, the supplier exploits synergies between customers, e.g., their locations, usage rates, and storage capacities, to reduce distribution costs. Due to the intricate interactions between customers, calculating a fair cost-to-serve for each customer is a daunting task. However, cost-to-serve information is useful when marketing to new customers or when revisiting routing and delivery quantity decisions. We design mechanisms for this cost allocation problem and determine their characteristics both analytically and computationally.", "e:keyword": ["Cost allocation", "Cost-to-serve", "Vendor-managed inventory", "Inventory routing problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1131", "e:abstract": "In this paper we model the behavior of a risk-averse agent who seeks to maximize expected utility and who has an indivisible asset and a timing option over when to sell this asset. Our main contribution is to show that, contrary to intuition, optimal behavior for such a risk-averse agent can include risk-increasing gambles. For example, a manager with a choice over when to disinvest from a project, a private homeowner with a property to sell, or an employee with a grant of American-style stock options may be better off taking positions in other assets with zero Sharpe ratio that are uncorrelated with the underlying project, house, or stock price risk. The results have wider implications for the modeling and interpretation of portfolio optimization problems involving American-style timing decisions.", "e:keyword": ["Finance", "Portfolio", "Corporate finance", "Dynamic programming/optimal control", "Applications", "Decision analysis", "Sequential", "Utility/preferences", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1132", "e:abstract": "We propose a class of functions, called multiple objective satisficing (MOS) criteria, for evaluating the level of compliance of a set of objectives in meeting their targets collectively under uncertainty. The MOS criteria include the joint targets' achievement probability (joint success probability criterion) as a special case and also extend to situations when the probability distributions are not fully characterized. We focus on a class of MOS criteria that favors diversification, which has the potential to mitigate severe shortfalls in scenarios when any objective fails to achieve its target. Naturally, this class excludes joint success probability. We further propose the shortfall-aware MOS criterion (S-MOS), which is inspired by the probability measure and is diversification favoring. We also show how to build tractable approximations of the S-MOS criterion. Because the S-MOS criterion maximization is not a convex optimization problem, we propose improvement algorithms via solving sequences of convex optimization problems. We report encouraging computational results on a blending problem in meeting specification targets even in the absence of full probability distribution description.", "e:keyword": ["Satisficing", "Targets", "Multiple objectives", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1133", "e:abstract": "We consider a user who buys a number of identical technical systems (e.g., medical, manufacturing, or communication systems) for which she must have very high availability. In such a situation, there are typically several options that the user can choose to facilitate this availability: cold standby redundancy for critical components, buying spare parts with the systems so failed parts can be replaced quickly, and/or application of an emergency procedure to expedite repairs when there is a stock out. To these options we introduce another: the possibility of initiating an emergency shipment when stock is one. Thus, the user may choose different combinations of the redundancy decision and the timing of applications of the emergency procedure, as well as how much spare parts inventory to purchase. We formulate the problem as the minimization of the total costs---acquisition, spare parts, and repair---incurred for the systems over their lifetimes, under a constraint for the total uptime of all systems. We optimally solve the problem by decomposing the multicomponent problem into single-component problems and then conducting exact analysis on these single-component problems. Using these, we construct an efficient frontier that reflects the trade-off between the uptime and the total costs of the systems. In addition, we provide a method to rank the components by the relative value of investing in redundancy. We illustrate these results through numerical examples.", "e:keyword": ["Reliability optimization", "Total cost of ownership", "Spare parts"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1134", "e:abstract": "We show how to optimize the search for a hidden object, terrorist, or simply Hider, located at a point <i>H</i> according to a known or unknown distribution <i>(nu)</i> on a rooted network <i>Q</i>. We modify the traditional “pathwise search” approach to a more general notion of “expanding search.” When the Hider is restricted to the nodes of <i>Q</i>, an expanding search <i>S</i> consists of an ordering <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$(a_{1},a_{2},\\ldots)$\\end{document}</tex-math></inline-formula> of the arcs of a spanning subtree such that the root node is in <i>a</i><sub>1</sub> and every arc <i>a<sub>i</sub></i> is adjacent to a previous arc <i>a<sub>j</sub></i>, <i>j</i> < <i>i</i>. If <i>a<sub>k</sub></i> contains <i>H</i>, the search time <i>T</i> is <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\lambda (a_{1}) +\\cdots +\\lambda (a_{k})$\\end{document}</tex-math></inline-formula>, where <i>(lambda)</i> is length measure on <i>Q</i>. For more general distributions <i>(nu)</i>, an expanding search <i>S</i> is described by the nested family of connected sets <i>S</i>(<i>t</i>) that specify the area of <i>Q</i> that has been covered by time <i>t</i>. <i>S</i>(0) is the root, <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\lambda (S(t) ) =t$, and $T=\\min \\{ t: H \\in S(t)\\}$\\end{document}</tex-math></inline-formula>. For a known Hider distribution <i>(nu)</i> on a tree <i>Q</i>, the expected time minimizing strategy <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\bar{S}$\\end{document}</tex-math></inline-formula> begins with the rooted subtree <i>Q</i>' maximizing the “density” <i>(nu)</i>(<i>Q</i>')/<i>(lambda)</i>(<i>Q</i>'). (For arbitrary networks, we use this criterion on all spanning subtrees.) The search <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\bar{S}$\\end{document}</tex-math></inline-formula> can be interpreted as the optimal method of mining known coal seams, when the time to move miners or machines is negligible compared to digging time. When the Hider distribution is unknown, we consider the zero-sum search game where the Hider picks <i>H</i>, the Searcher <i>S</i>, and the payoff is <i>T</i>. For trees <i>Q</i>, the value is <i>V</i> = (<i>(lambda)</i>(<i>Q</i>) + <i>D</i>)/2, where <i>D</i> is a mean distance from root to leaf nodes. If <i>Q</i> is 2-arc connected, <i>V</i> = <i>(lambda)</i>(<i>Q</i>)/2. Applications and interpretations of the expanding search paradigm are given, particularly to multiple agent search.", "e:keyword": ["Teams", "Games/group decisions", "Search/surveillance", "Tree algorithms", "Networks/graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1135", "e:abstract": "We consider two variants of a probabilistic set covering (PSC) problem. The first variant assumes that there is uncertainty regarding whether a selected set can cover an item, and the objective is to determine a minimum-cost combination of sets so that each item is covered with a prespecified probability. The second variant seeks to maximize the minimum probability that a selected set can cover all items. To date, literature on this problem has focused on the special case in which uncertainties are independent. In this paper, we formulate deterministic mixed-integer programming models for distributionally robust PSC problems with correlated uncertainties. By exploiting the supermodularity of certain substructures and analyzing their polyhedral properties, we develop strong valid inequalities to strengthen the formulations. Computational results illustrate that our modeling approach can outperform formulations in which correlations are ignored and that our algorithms can significantly reduce overall computation time.", "e:keyword": ["Distributionally robust models", "Integer programming", "Set covering", "Stochastic programming", "Supermodularity"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1136", "e:abstract": "The robust capacitated vehicle routing problem (CVRP) under demand uncertainty is studied to address the minimum cost delivery of a product to geographically dispersed customers using capacity-constrained vehicles. Contrary to the deterministic CVRP, which postulates that the customer demands for the product are deterministic and known, the robust CVRP models the customer demands as random variables, and it determines a minimum cost delivery plan that is feasible for all anticipated demand realizations. Robust optimization counterparts of several deterministic CVRP formulations are derived and compared numerically. Robust rounded capacity inequalities are developed, and it is shown how they can be separated efficiently for two broad classes of demand supports. Finally, it is analyzed how the robust CVRP relates to the chance-constrained CVRP, which allows a controlled degree of supply shortfall to decrease delivery costs.", "e:keyword": ["Programming", "Robust optimization", "Transportation", "Vehicle routing", "Probability", "Chance constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1137", "e:abstract": "Game-theoretic tools are becoming a popular design choice for distributed resource allocation algorithms. A central component of this design choice is the assignment of utility functions to the individual agents. The goal is to assign each agent an admissible utility function such that the resulting game possesses a host of desirable properties, including scalability, tractability, and existence and efficiency of pure Nash equilibria. In this paper we formally study this question of utility design on a class of games termed distributed welfare games. We identify several utility design methodologies that guarantee desirable game properties irrespective of the specific application domain. Lastly, we illustrate the results in this paper on two commonly studied classes of resource allocation problems: “coverage” problems and “coloring” problems.", "e:keyword": ["Resource allocation", "Game theory", "Distributed control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1138", "e:abstract": "We propose a scalable, data-driven method for designing national policies for the allocation of deceased donor kidneys to patients on a waiting list in a fair and efficient way. We focus on policies that have the same form as the one currently used in the United States. In particular, we consider policies that are based on a point system that ranks patients according to some priority criteria, e.g., waiting time, medical urgency, etc., or a combination thereof. Rather than making specific assumptions about fairness principles or priority criteria, our method offers the designer the flexibility to select his desired criteria and fairness constraints from a broad class of allowable constraints. The method then designs a point system that is based on the selected priority criteria and approximately maximizes medical efficiency---i.e., life-year gains from transplant---while simultaneously enforcing selected fairness constraints. Among the several case studies we present employing our method, one case study designs a point system that has the same form, uses the same criteria, and satisfies the same fairness constraints as the point system that was recently proposed by U.S. policy makers. In addition, the point system we design delivers an 8% increase in extra life-year gains. We evaluate the performance of all policies under consideration using the same statistical and simulation tools and data as the U.S. policy makers use. Other case studies perform a sensitivity analysis (for instance, demonstrating that the increase in extra life-year gains by relaxing certain fairness constraints can be as high as 30%) and also pursue the design of policies targeted specifically at remedying criticisms leveled at the recent point system proposed by U.S. policy makers.", "e:keyword": ["Health care", "Dynamic programming/optimal control", "Applications", "Linear programming", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1139", "e:abstract": "We develop a novel framework, the <i>implicit hitting set approach</i>, for solving a class of combinatorial optimization problems. The explicit hitting set problem is as follows: given a set <i>U</i> and a family <i>S</i> of subsets of <i>U</i>, find a minimum-cardinality set that intersects (hits) every set in <i>S</i>. In the implicit hitting set problem (IHSP), the family of subsets <i>S</i> is not explicitly listed (its size is, generally, exponential in terms of the size of <i>U</i>); instead, it is given via a polynomial-time oracle that verifies if a given set <i>H</i> is a hitting set or returns a set in <i>S</i> that is not hit by <i>H</i>. Many NP-hard problems can be straightforwardly formulated as implicit hitting set problems. We show that the implicit hitting set approach is valuable in developing exact and heuristic algorithms for solving this class of combinatorial optimization problems. Specifically, we provide a generic algorithmic strategy, which combines efficient heuristics and exact methods, to solve any IHSP. Given an instance of an IHSP, the proposed algorithmic strategy gives a sequence of feasible solutions and lower bounds on the optimal solution value and ultimately yields an optimal solution. We specialize this algorithmic strategy to solve the multigenome alignment problem and present computational results that illustrate the effectiveness of the implicit hitting set approach.", "e:keyword": ["Combinatorial optimization", "Exact algorithms", "Heuristic algorithms", "Multigenome alignment problem", "Multisequence alignment problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1140", "e:abstract": "Computing the nondominated set of a multiple objective mathematical program has long been a topic in multiple criteria decision making. In this paper, motivated by the desire to extend Markowitz portfolio selection to an additional linear criterion (dividends, liquidity, sustainability, etc.), we demonstrate an exact method for computing the nondominated set of a tri-criterion program that is all linear except for the fact that one of its objectives is to minimize a convex quadratic function. With the nondominated set of the resulting quad-lin-lin program being a surface composed of curved platelets, a multiparametric algorithm is devised for computing the platelets so that they can be graphed precisely. In this way, graphs of the tri-criterion nondominated surface can be displayed so that, as in traditional portfolio selection, a most preferred portfolio can be selected while in full view of all other contenders for optimality. Finally, by giving an example for socially responsible investors, we demonstrate that our algorithm can outperform standard portfolio strategies for multicriterial decision makers.", "e:keyword": ["Multiple criteria decision making", "Multicriteria optimization", "Nondominated surfaces", "Portfolio selection", "Multiparametric quadratic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1141", "e:abstract": "We consider the multilevel lot-sizing problem with production capacities (MLSP-PC), in which production and transportation decisions are made for a serial supply chain with capacitated production and concave cost functions. Existing approaches to the multistage version of this problem are limited to nonspeculative cost functions---up to now, no algorithm for the multistage version of this model with general concave cost functions has been developed. In this paper, we develop the first polynomial algorithm for the MLSP-PC with general concave costs at all of the stages, and we introduce a novel approach to overcome the limitations of previous approaches. In contrast to traditional approaches to lot-sizing problems, in which the problem is decomposed by time periods and is analyzed unidirectionally in time, we solve the problem by introducing the concept of a basis path, which is characterized by time and stage. Our dynamic programming algorithm proceeds both forward and backward in time along this basis path, enabling us to solve the problem in polynomial time.", "e:keyword": ["Lot-sizing", "Production capacity", "Inventory and logistics", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1142", "e:abstract": "This paper considers abstract election games motivated by the United States Electoral College. There are two political parties, and the electoral votes in each state go to the party that spends the most money there, with an adjustment for a “head start” that one party or the other may have in that state. The states have unequal numbers of electoral votes, and elections are decided by majority rules. Each party has a known budget, and much depends on the information that informs how that budget is spent. Three situations are considered: (1) one party's spending plan is known to the other, (2) spending is gradually revealed as the parties spend continuously in time, and (3) neither side knows anything about the other's spending. The last situation resembles a Blotto game, hence the title.", "e:keyword": ["Political campaign spending", "Blotto", "Presidential", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1143", "e:abstract": "Stochastic kriging is a new metamodeling technique for effectively representing the mean response surface implied by a stochastic simulation; it takes into account both stochastic simulation noise and uncertainty about the underlying response surface of interest. We show theoretically, through some simplified models, that incorporating gradient estimators into stochastic kriging tends to significantly improve surface prediction. To address the issue of which type of gradient estimator to use, when there is a choice, we briefly review stochastic gradient estimation techniques; we then focus on the properties of infinitesimal perturbation analysis and likelihood ratio/score function gradient estimators and make recommendations. To conclude, we use simulation experiments with no simplifying assumptions to demonstrate that the use of stochastic kriging with gradient estimators provides more reliable prediction results than stochastic kriging alone.", "e:keyword": ["Stochastic simulation", "Metamodeling", "Gradient estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1144", "e:abstract": "Networks have become increasingly pervasive in our lives: social networks not  only have a defining impact on consumer choice but are also central in  social and political decisions, ranging from political discourse in the  blogosphere to the organization and coordination of protests in the Arab Spring. Economic and financial transactions also increasingly occur over large, complex, dynamically changing networks, not only contributing to the efficient allocation of resources, but also occasionally creating large, systemic, and poorly understood risks. Similarly, network effects are now seen as major elements in healthcare, smart power grids, urban transportation, and more.We invite high-quality submissions in social and economic networks, construed broadly to include any system in which human action and control  play a major role. We welcome contributions based on rigorous mathematical  models, as well as empirical and simulation studies, that contribute to the understanding of network formation, cascades, reliability, information and action propagation, decision making, inference, measurement, and efficient optimization algorithms over networks. We seek papers that apply tools from operations research, statistics, computer science, applied mathematics, and physics to offer novel insights, build new models, and possibly provide recommendations to policy makers.To submit your paper, go to the <i>Operations Research</i> ScholarOne Manuscripts site at <ext-link ext-link-type=\"uri\"  href=\"http://mc.manuscriptcentral.com/opre.manuscriptcentral.com/opre\">http://mc.manuscriptcentral.com/opre.manuscriptcentral.com/opre</ext-link> and follow the instructions provided.<b>July/August 2015:</b> Expected issue publication date.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1146", "e:abstract": "Flight delays have been a growing issue and they have reached an all-time high in recent years, with the airlines' on-time performance at its worst level in 2007 since 1995. A recent report by the Joint Economic Committee of the U.S. Congress chaired by Senator Charles E. Schumer has estimated that the total cost to the U.S. economy because of flight delays was as much as $41 billion in 2007. The goal of this paper is to build stochastic models of airline networks and utilize publicly available data to answer the following policy questions: Which are the bottleneck airports in the U.S. air-travel infrastructure (i.e., airports that cause most delay propagation)? How would increasing airport capacity at these airports alleviate delay propagation? What are the appropriate metrics for measuring the robustness of airline schedules? How could these schedules be made more robust? Which flight in an aircraft rotation is a bottleneck flight (and, hence, deserves managerial attention)? Flight delays are typically attributed to two factors: (i) the randomness in the <i>intrinsic</i> travel time for a scheduled flight (which is the travel time excluding propagated delays), and (ii) the propagation of this randomness through the air-travel network and infrastructure. We model both of these factors that cause travel delays. The contribution of this paper is twofold. First, we develop stochastic models, using empirical data, to analyze the propagation of delays through air-transportation networks. Our stochastic models allow us to develop three important robustness measures for airline networks. Second, our analysis enables us to make policy recommendations regarding managing bottleneck resources in the air-travel infrastructure, which, if addressed, could lead to a significant improvement in air-travel reliability.", "e:keyword": ["Air-travel infrastructure", "Schedule robustness", "Empirical analysis", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1147", "e:abstract": "This paper considers the problem of designing a multicommodity network with single facility type subject to the requirement that under failure of any single edge, the network should permit a feasible flow of all traffic. We study the polyhedral structure of the problem by considering the multigraph obtained by shrinking the nodes, but not the edges, in a <i>k</i>-partition of the original graph. A key theorem is proved according to which a facet of the <i>k</i>-node problem defined on the multigraph resulting from a <i>k</i>-partition is also facet defining for the larger problem under a mild condition. After reviewing the prior work on two-partition inequalities, we develop two classes of three-partition inequalities and a large number of inequality classes based on four-partitions. Proofs of facet-defining status for some of these are provided, while the rest are stated without proof. Computational results show that the addition of three- and four-partition inequalities results in substantial increase in the bound values compared to those possible with two-partition inequalities alone. Problems of 35 nodes and 80 edges with fully dense traffic matrices have been solved optimally within a few minutes of computer time.", "e:keyword": ["Multicommodity flow", "Network design", "Survivability", "Integer programming", "Polyhedral structure", "Facet inequalities", "k-partition"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1148", "e:abstract": "Knowledge discovery in databases (KDD) techniques have been extensively employed to extract knowledge from massive data stores to support decision making in a wide range of critical applications. Maintaining the currency of discovered knowledge over evolving data sources is a fundamental challenge faced by all KDD applications. This paper addresses the challenge from the perspective of deciding the right times to refresh knowledge. We define the knowledge-refreshing problem and model it as a Markov decision process. Based on the identified properties of the Markov decision process model, we establish that the optimal knowledge-refreshing policy is monotonically increasing in the system state within every appropriate partition of the state space. We further show that the problem of searching for the optimal knowledge-refreshing policy can be reduced to the problem of finding the optimal thresholds and propose a method for computing the optimal knowledge-refreshing policy. The effectiveness and the robustness of the computed optimal knowledge-refreshing policy are examined through extensive empirical studies addressing a real-world knowledge-refreshing problem. Our method can be applied to refresh knowledge for KDD applications that employ major data-mining models.", "e:keyword": ["Data mining", "Knowledge discovery in databases", "Knowledge refreshing", "Markov decision process"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1149", "e:abstract": "This paper presents a patrol problem, where a patroller traverses a graph through edges to detect potential attacks at nodes. To design a patrol policy, the patroller needs to take into account not only the graph structure, but also the different attack time distributions, as well as different costs incurred due to successful attacks, at different nodes. We consider both random attackers and strategic attackers. A random attacker chooses which node to attack according to a probability distribution known to the patroller. A strategic attacker plays a two-person zero-sum game with the patroller. For each case, we give an exact linear program to compute the optimal solution. Because the linear programs quickly become computationally intractable as the problem size grows, we develop index-based heuristics. In the random-attacker case, our heuristic is optimal when there are two nodes, and in a suitably chosen asymptotic regime. In the strategic-attacker case, our heuristic is optimal when there are two nodes if the attack times are deterministic taking integer values. In our numerical experiments, our heuristic typically achieves within 1% of optimality with computation time orders of magnitude less than what is required to compute the optimal policy.", "e:keyword": ["Military", "Search/surveillance", "Dynamic programming/optimal control", "Game/group decisions"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1150", "e:abstract": "We consider the feasibility problem OPP (orthogonal packing problem) in higher-dimensional orthogonal packing: given a set of <i>d</i>-dimensional (<i>d</i> (ge) 2) rectangular items, decide whether all of them can be orthogonally packed in the given rectangular container without rotation. The one-dimensional (1D) “bar” LP relaxation of OPP reduces the latter to a 1D cutting-stock problem where the packing of each stock bar represents a possible 1D stitch through an OPP layout. The dual multipliers of the LP provide us with another kind of powerful bounding information (conservative scales). We investigate how the set of possible 1D packings can be tightened using the overlapping information of item projections on the axes, with the goal to tighten the relaxation. We integrate the bar relaxation into an interval-graph algorithm for OPP, which operates on such overlapping relations. Numerical results on 2D and 3D instances demonstrate the efficiency of tightening leading to a speedup and stabilization of the algorithm.", "e:keyword": ["Packing", "Branch-and-price", "Dimension reduction", "Interval graph"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1151", "e:abstract": "In addition to traditional call centers, many companies have started building a new kind of customer contact center, in which agents communicate with customers via instant messaging (IM) over the Internet rather than phone calls. A distinctive feature of the service centers based on IM is that one agent can serve multiple customers in parallel. We choose to model such a center as a server pool consisting of many limited processor-sharing servers. We characterize the underlying stochastic processes by establishing a fluid approximation in the many-server heavy-traffic regime. The limiting behavior of the stochastic processes is shown to involve a stochastic averaging principle, and the fluid approximation provides insights into the optimal staffing and control for such service centers.", "e:keyword": ["Many-server queues", "Limited processor sharing", "Fluid models", "Staffing and control"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1152", "e:abstract": "The construction of a multiattribute utility function is an important step in decision analysis and can be a challenging task unless some decomposition of the utility function is performed. When every attribute is utility independent of its complement, the utility elicitation task is significantly simplified because the functional form of the utility function requires only one conditional utility function for each attribute, and some normalizing constants. When utility independence conditions do not hold, the conditional utility function of an attribute may vary across the domain of the complement attributes, and therefore a single conditional utility assessment for each attribute may not be sufficient to capture the decision maker's preferences. This paper proposes a method to construct utility functions that have the flexibility to match the variations in the conditional utility function, across the domain of the attributes, using univariate utility assessments at the boundary values. The approach incorporates the boundary assessments into a new function, which we call the double-sided utility copula. This formulation provides a wealth of new functional forms that the analyst may use to incorporate utility dependence in multiattribute decision problems. The utility copula function also allows for the flexibility to incorporate a wide range of trade-off assessments among the attributes, while keeping the utility assessments at the boundary values fixed. It is also useful in determining the order of approximation provided by using certain independence assumptions in a multiattribute decision problem when the attributes are utility dependent.", "e:keyword": ["Utility assessment", "Multiattribute utility", "Decision support"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1153", "e:abstract": "In the <i>two-echelon capacitated vehicle routing problem</i> (2E-CVRP), the delivery to customers from a <i>depot</i> uses intermediate depots, called <i>satellites</i>. The 2E-CVRP involves two levels of routing problems. The first level requires a design of the routes for a vehicle fleet located at the depot to transport the customer demands to a subset of the satellites. The second level concerns the routing of a vehicle fleet located at the satellites to serve all customers from the satellites supplied from the depot. The objective is to minimize the sum of routing and handling costs. This paper describes a new mathematical formulation of the 2E-CVRP used to derive valid lower bounds and an exact method that decomposes the 2E-CVRP into a limited set of <i>multidepot capacitated vehicle routing problems</i> with side constraints. Computational results on benchmark instances show that the new exact algorithm outperforms the state-of-the-art exact methods.", "e:keyword": ["Two-echelon vehicle routing", "Dual ascent", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1154", "e:abstract": "We study an extension of the capacitated arc routing problem (CARP) called the capacitated arc routing problem with deadheading demand (CARPDD). This problem extends the classical capacitated arc routing problem by introducing an additional capacity consumption incurred by a vehicle deadheading an edge. It can be used, e.g., to model time or distance constrained arc routing problems. We show that the strongest CARP lower bounds can be weak when directly applied to the CARPDD, and we introduce a new family of valid inequalities shown to significantly strengthen these bounds. We develop an exact algorithm for the CARPDD based on cut-and-column generation and branch and price, and we report extensive computational results on a large set of benchmark instances. The same exact algorithm is also tested on classical CARP benchmark sets and is shown to improve upon the best known exact algorithms for the CARP.", "e:keyword": ["Capacitated arc routing problem", "Double demand", "Cut-and-column generation", "Branch and price"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1155", "e:abstract": "This paper studies a repeated game between a manufacturer and two competing suppliers with imperfect monitoring. We present a principal-agent model for managing long-term supplier relationships using a unique form of measurement and incentive scheme. We measure a supplier's overall performance with a rating equivalent to its continuation utility (the expected total discounted utility of its future payoffs), and incentivize supplier effort with larger allocations of future business. We obtain the vector of the two suppliers' ratings as the state of a Markov decision process, and we solve an infinite horizon contracting problem in which the manufacturer allocates business volume between the two suppliers and updates their ratings dynamically based on their current ratings and the current performance outcome.Our contributions are both theoretical and managerial: we propose a repeated principal-agent model with a novel incentive scheme to tackle a common, but challenging, incentive problem in a multiperiod supply chain setting. Assuming binary effort choices and performance outcomes by the suppliers, we characterize the structure of the optimal contract through a novel fixed-point analysis. Our results provide a theoretical foundation for the emergence of “business-as-usual” (low effort) trapping states and tournament competition (high effort) recurrent states as the long-run incentive drivers for motivating critical suppliers.", "e:keyword": ["Asymmetric information", "Performance-based contract", "Volume incentive", "Repeated moral hazard", "Principal-agent model", "Supply chain contracting"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1156", "e:abstract": "We propose a level-set approach to characterize the value function of a pure linear integer program with inequality constraints. We study theoretical properties of our characterization and show how they can be exploited to optimize a class of stochastic integer programs through a value function reformulation. Specifically, we develop algorithmic approaches that solve two-stage multidimensional knapsack problems with random budgets, yielding encouraging computational results.", "e:keyword": ["Integer programming", "Value function", "Characterization", "Level set", "Stochastic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.1120.1157", "e:abstract": "We consider the question of how routing and allocation can be coordinated to meet the challenge of demand variability in a parallel queueing system serving two types of customers. A decision maker decides whether to keep customers at the station at which they arrived or to reroute them to the other station. At the same time, the decision maker has two servers and must decide where to allocate their effort. We analyze this joint decision-making scenario with both routing and station-dependent holding costs, but add an important twist. We allow the combined service rate (when the servers work at the same station) to be <i>superadditive</i> or <i>subadditive</i>. This captures positive or negative externalities that arise during collaboration. We seek an optimal control policy under the discounted or long-run average cost criteria. Our results show that in the superadditive case jobs should never be routed away from the lower-cost queue. When jobs are rerouted from the higher-cost queue to the low-cost queue the optimal control is monotone in the respective queue lengths. Moreover, we show that the optimal allocation is a nonidling priority rule based on the holding costs. In the subadditive case we find that the optimal policy need not exhibit such a simple structure. In fact, the optimal allocation need not prioritize one station (it may split the servers), and the optimal routing need not be monotone in the number of customers in each queue. We characterize the optimal policy for a few canonical cases and discuss why intuitive policies need not be optimal in the general case. An extensive numerical study examines the benefit of dynamically controlling both routing and resource allocation; we discuss when using one of the two levers---dynamic routing and dynamic allocation---is sufficient and when using both controls is warranted.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Queues", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1158", "e:abstract": "In this paper we investigate a stochastic appointment-scheduling problem in an outpatient clinic with a single doctor. The number of patients and their sequence of arrivals are fixed, and the scheduling problem is to determine an appointment time for each patient. The service durations of the patients are stochastic, and only the mean and covariance estimates are known. We do not assume any exact distributional form of the service durations, and we solve for distributionally robust schedules that minimize the expectation of the weighted sum of patients' waiting time and the doctor's overtime. We formulate this scheduling problem as a convex conic optimization problem with a tractable semidefinite relaxation. Our model can be extended to handle additional support constraints of the service durations. Using the primal--dual optimality conditions, we prove several interesting structural properties of the optimal schedules. We develop an efficient semidefinite relaxation of the conic program and show that we can still obtain near-optimal solutions on benchmark instances in the existing literature. We apply our approach to develop a practical appointment schedule at an eye clinic that can significantly improve the efficiency of the appointment system in the clinic, compared to an existing schedule.", "e:keyword": ["Appointment scheduling", "Copositive programming", "Semidefinite programming", "Network flow"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1159", "e:abstract": "We introduce a simple elicitation process where subject-matter experts provide only ordinal judgments of the attractiveness of potential targets, and the adversary utility of each target is assumed to involve multiple attributes. Probability distributions over the various attribute weights are then mathematically derived (using either probabilistic inversion or Bayesian density estimation). This elicitation process reduces the burden of time-consuming orientation and training in traditional methods of attribute weight elicitation, and explicitly captures the existing uncertainty and disagreement among experts, rather than attempts to achieve consensus by eliminating them. We identify the relationship between the two methods and conduct sensitivity analysis to elucidate how these methods handle expert consensus or disagreement. We also present a real-world application on elicitation of adversarial preferences over various attack scenarios to show the applicability of our proposed methods.", "e:keyword": ["Expert elicitation", "Adversarial preference", "Ordinal judgment", "Probabilistic inversion", "Bayesian density estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1160", "e:abstract": "The problem of coordinating a fleet of vehicles so that all demand points on a territory are serviced and the workload is most evenly distributed among the vehicles is a hard one. For this reason, it is often an effective strategy to first divide the service region and impose that each vehicle is only responsible for its own subregion. This heuristic also has the practical advantage that over time, drivers become more effective at serving their territory and customers. In this paper, we assume that client locations are unknown at the time of partitioning the territory and that each of them will be drawn identically and independently according to a distribution that is actually also <i>unknown</i>. In practice, it might be impossible to identify precisely the distribution if, for instance, information about the demand is limited to historical data. Our approach suggests partitioning the region with respect to the worst-case distribution that satisfies first- and second-order moments information. As a side product, our analysis constructs for each subregion a closed-form expression for the worst-case density function, thus providing useful insights about what affects the completion time most heavily. The successful implementation of our approach relies on two branch-and-bound algorithms: whereas the first finds a globally optimal partition of a convex polygon into two convex subregions, the second finds a local optimum for the harder <i>n</i>-partitioning problem. Finally, simulations of a parcel delivery problem will demonstrate that our data-driven approach makes better use of historical data as it becomes available.", "e:keyword": ["Programming", "Infinite dimensional", "Transportation", "Vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1162", "e:abstract": "We develop new algorithmic approaches to compute provably near-optimal policies for multiperiod stochastic lot-sizing inventory models with positive lead times, general demand distributions, and dynamic forecast updates. The policies that are developed have worst-case performance guarantees of 3 and typically perform very close to optimal in extensive computational experiments. The newly proposed algorithms employ a novel randomized decision rule. We believe that these new algorithmic and performance analysis techniques could be used in designing provably near-optimal randomized algorithms for other stochastic inventory control models and more generally in other multistage stochastic control problems.", "e:keyword": ["Inventory", "Stochastic lot-sizing", "Approximation algorithms", "Randomized cost balancing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1163", "e:abstract": "The sample average approximation approach to solving stochastic programs induces a sampling error, caused by replacing an expectation by a sample average, as well as an optimization error due to approximating the solution of the resulting sample average problem. We obtain estimators of an optimal solution and the optimal value of the original stochastic program after executing a finite number of iterations of an optimization algorithm applied to the sample average problem. We examine the convergence rate of the estimators as the computing budget tends to infinity, and we characterize the allocation policies that maximize the convergence rate in the case of sublinear, linear, and superlinear convergence regimes for the optimization algorithm.", "e:keyword": ["Sample average approximation", "Stochastic programming", "Optimal budget allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1164", "e:abstract": "This paper was motivated by the problem of developing an optimal policy for exploring an oil and gas field in the North Sea. Where should we drill first? Where do we drill next? In this and many other problems, we face a trade-off between earning (e.g., drilling immediately at the sites with maximal expected values) and learning (e.g., drilling at sites that provide valuable information) that may lead to greater earnings in the future. These “sequential exploration problems” resemble a multiarmed bandit problem, but probabilistic dependence plays a key role: outcomes at drilled sites reveal information about neighboring targets. Good exploration policies will take advantage of this information as it is revealed. We develop heuristic policies for sequential exploration problems and complement these heuristics with upper bounds on the performance of an optimal policy. We begin by grouping the targets into clusters of manageable size. The heuristics are derived from a model that treats these clusters as independent. The upper bounds are given by assuming each cluster has perfect information about the results from all other clusters. The analysis relies heavily on results for bandit superprocesses, a generalization of the multiarmed bandit problem. We evaluate the heuristics and bounds using Monte Carlo simulation and, in the North Sea example, we find that the heuristic policies are nearly optimal.", "e:keyword": ["Dynamic programming", "Multiarmed bandits", "Bandit superprocesses", "Information relaxations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1165", "e:abstract": "Modern electronic markets have been characterized by a relentless drive toward faster decision making. Significant technological investments have led to dramatic improvements in latency, the delay between a trading decision and the resulting trade execution. We describe a theoretical model for the quantitative valuation of latency. Our model measures the trading frictions created by the presence of latency, by considering the optimal execution problem of a representative investor. Via a dynamic programming analysis, our model provides a closed-form expression for the cost of latency in terms of well-known parameters of the underlying asset. We implement our model by estimating the latency cost incurred by trading on a human time scale. Examining NYSE common stocks from 1995 to 2005 shows that median latency cost across our sample roughly tripled during this time period. Furthermore, using the same data set, we compute a measure of implied latency and conclude that the median implied latency decreased by approximately two orders of magnitude. Empirically calibrated, our model suggests that the reduction in cost achieved by going from trading on a human time scale to a low latency time scale is comparable with other execution costs faced by the most cost efficient institutional investors, and it is consistent with the rents that are extracted by ultra-low latency agents, such as providers of automated execution services or high frequency traders.", "e:keyword": ["Market microstructure", "Electronic markets", "High frequency trading"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1166", "e:abstract": "We consider the “classical” single-product dynamic pricing problem allowing the “scale” of demand intensity to be modulated by an exogenous “market size” stochastic process. This is a natural model of dynamically changing market conditions. We show that for a broad family of Gaussian market-size processes, simple dynamic pricing rules that are essentially agnostic to the specification of this market-size process perform provably well. The pricing policies we develop are shown to compensate for forecast imperfections (or a lack of forecast information altogether) by frequent reoptimization and reestimation of the “instantaneous” market size.", "e:keyword": ["Optimal control", "Dynamic programming", "Suboptimal algorithms", "Analysis of algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1167", "e:abstract": "This paper proposes a new approach to solve finite-horizon optimal stopping problems for a class of Markov processes that includes one-dimensional diffusions, birth--death processes, and jump diffusions and continuous-time Markov chains obtained by time-changing diffusions and birth-and-death processes with Lévy subordinators. When the expectation operator has a purely discrete spectrum in the Hilbert space of square-integrable payoffs, the value function of a discrete optimal stopping problem has an expansion in the eigenfunctions of the expectation operator. The Bellman's dynamic programming for the value function then reduces to an explicit recursion for the expansion coefficients. The value function of the continuous optimal stopping problem is then obtained by extrapolating the value function of the discrete problem to the limit via Richardson extrapolation. To illustrate the method, the paper develops two applications: American-style commodity futures options and Bermudan-style abandonment and capacity expansion options in commodity extraction projects under the subordinate Ornstein-Uhlenbeck model with mean-reverting jumps with the value function given by an expansion in Hermite polynomials.", "e:keyword": ["Finance", "Asset pricing", "Option pricing", "Finance", "Securities", "Bermudan options", "American options", "Probability", "Markov processes", "Diffusions", "Birth-and-death processes", "Time change", "Optimal stopping", "Spectral theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1168", "e:abstract": "This paper studies the optimal control policy for capacitated periodic-review inventory systems with remanufacturing. The serviceable products can be either manufactured from raw materials or remanufactured from returned products; but the system has finite capacities in manufacturing, remanufacturing, and/or total manufacturing/remanufacturing operations in each period. Using <i>L</i>-natural convexity and lattice analysis, we show that, for systems with a remanufacturing capacity and a manufacturing/total capacity, the optimal remanufacturing policy is a modified remanufacture-down-to policy and the optimal manufacturing policy is a modified total-up-to policy. Our study reveals that the optimal policies always give production priority to remanufacturing for systems with a remanufacturing capacity and/or a total capacity; but this priority fails to hold for systems with a manufacturing capacity.", "e:keyword": ["Inventory model", "Product returns", "Remanufacturing", "Finite capacity", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1169", "e:abstract": "Generating valid synthetic instances for branch problems---those that contain a core problem like knapsack or graph coloring, but add several complications---is hard. It is even harder to generate instances that are applicable to the specific goals of an experiment and help to support the claims made. This paper presents a methodology for tuning instance generators of branch problems so that synthetic instances are similar to real ones and are capable of eliciting different behaviors from solvers. A statistic is proposed to summarize the applicability of instances for drawing a valid conclusion. The methodology is demonstrated on the Udine timetabling problem. Examples and the necessary cyberinfrastructure are available as a project from Computational Infrastructure for Operations Research (COIN-OR).", "e:keyword": ["Instance generation", "Data mining", "Optimization", "Timetabling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1170", "e:abstract": "One long-standing challenge in both the optimization and investment communities is to devise an efficient algorithm to select a small number of assets from an asset pool such that a portfolio objective is optimized. This cardinality constrained investment situation naturally arises due to the presence of various forms of market friction, such as transaction costs and management fees, or even due to the consideration of mental cost. Unfortunately, the combinatorial nature of such a portfolio selection problem formulation makes the exact solution process NP-hard in general. We focus in this paper on the cardinality constrained mean-variance portfolio selection problem. Instead of tailoring such a difficult problem into the general solution framework of mixed-integer programming formulation, we explore the special structures and rich geometric properties behind the mathematical formulation. Applying the Lagrangian relaxation to the primal problem results in a pure cardinality constrained portfolio selection problem, which possesses a symmetric property, and to which geometric approaches can be developed. Different from the existing literature that has primarily focused on some direct relaxations of the cardinality constraint, we consider modifying the objective function to some separable relaxations, which are immune to the hard cardinality constraint. More specifically, we develop efficient lower bounding schemes by using the circumscribed box, the circumscribed ball, and the circumscribed axis-aligned ellipsoid to approximate the objective contour of the problem. In particular, all these cardinality constrained relaxation problems can be solved analytically. Furthermore, we derive efficient polynomial-time algorithms for the corresponding dual search problems. Most promisingly, the lower bounding scheme using the circumscribed axis-aligned ellipsoid leads to a semidefinite programming (SDP) formulation and offers a sharp bound and high-quality feasible solution. By integrating these lower bounding schemes into a branch-and-bound algorithm (BnB), our solution scheme outperforms CPLEX significantly in identifying the exact optimal portfolio.", "e:keyword": ["Cardinality constrained quadratic programming", "Cardinality constrained portfolio selection", "Mean-variance formulation", "Semidefinite programming", "Relaxation", "Branch-and-bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1171", "e:abstract": "Stochastic control problems that arise in reliability and maintenance optimization typically assume that information used for decision-making is obtained according to a predetermined sampling schedule. In many real applications, however, there is a high sampling cost associated with collecting such data. It is therefore of equal importance to determine when information should be collected and to decide how this information should be utilized for maintenance decision-making. This type of joint optimization has been a long-standing problem in the operations research and maintenance optimization literature, and very few results regarding the structure of the optimal sampling and maintenance policy have been published. In this paper, we formulate and analyze the joint optimization of sampling and maintenance decision-making in the partially observable Markov decision process framework. We prove the optimality of a policy that is characterized by three critical thresholds, which have practical interpretation and give new insight into the value of condition-based maintenance programs in life-cycle asset management. Illustrative numerical comparisons are provided that show substantial cost savings over existing suboptimal policies.", "e:keyword": ["Reliability", "Maintenance/repairs", "Inspection", "Failure models", "Dynamic programming/optimal control", "Markov", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1172", "e:abstract": "This paper considers a particular class of dynamic robust optimization problems, where a large number of decisions must be made in the first stage, which consequently fix the constraints and cost structure underlying a one-dimensional, linear dynamical system. We seek to bridge two classical paradigms for solving such problems, namely, (1) dynamic programming (DP), and (2) policies parameterized in model uncertainties (also known as decision rules), obtained by solving tractable convex optimization problems.We show that if the uncertainty sets are integer sublattices of the unit hypercube, the DP value functions are convex and supermodular in the uncertain parameters, and a certain technical condition is satisfied, then decision rules that are <i>affine</i> in the uncertain parameters are optimal. We also derive conditions under which such rules can be obtained by optimizing simple (i.e., linear) objective functions over the uncertainty sets. Our results suggest new modeling paradigms for dynamic robust optimization, and our proofs, which bring together ideas from three areas of optimization typically studied separately—robust optimization, combinatorial optimization (the theory of lattice programming and supermodularity), and global optimization (the theory of concave envelopes)—may be of independent interest.We exemplify our findings in a class of applications concerning the design of flexible production processes, where a retailer seeks to optimally compute a set of strategic decisions (before the start of a selling season), as well as in-season replenishment policies. We show that, when the costs incurred are jointly convex, replenishment policies that depend linearly on the realized demands are optimal. When the costs are also piecewise affine, all the optimal decisions can be found by solving a single linear program of small size (when all decisions are continuous) or a mixed-integer, linear program of the same size (when some strategic decisions are discrete).", "e:keyword": ["Dynamic robust optimization", "Supermodularity", "Concave envelopes", "Lattices", "Lovász extension", "Production planning", "Inventory management"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1173", "e:abstract": "Data envelopment analysis (DEA), as originally proposed, is a methodology for evaluating the relative efficiencies of a set of <i>homogeneous</i> decision-making units (DMUs) in the sense that each uses the same input and output measures (in varying amounts from one DMU to another). In some situations, however, the assumption of homogeneity among DMUs may not apply. As an example, consider the case where the DMUs are plants in the same industry that may not all produce the same products. Evaluating efficiencies in the absence of homogeneity gives rise to the issue of how to fairly compare a DMU to other units, some of which may not be exactly in the same “business.” A related problem, and one that has been examined extensively in the literature, is the <i>missing data</i> problem; a DMU produces a certain output, but its value is not known. One approach taken to address this problem is to “create” a value for the missing output (e.g., substituting zero, or by taking the average of known values), and use it to fill in the gaps. In the present setting, however, the issue isn't that the data for the output is missing for certain DMUs, but rather that the output isn't produced. We argue herein that if a DMU has chosen not to produce a certain output, or for any reason cannot produce that output, and therefore does not put the resources in place to do so, then it would be inappropriate to artificially assign that DMU a zero value or some “average” value for the nonexistent factor. Specifically, the desire is to fairly evaluate a DMU for what it does, rather than penalize or credit it for what it doesn't do. In the current paper we present DEA-based models for evaluating the relative efficiencies of a set of DMUs where the requirement of homogeneity is relaxed. We then use these models to examine the efficiencies of a set of manufacturing plants.", "e:keyword": ["Nonhomogeneous DMUs", "Missing outputs", "Subgroups", "Assurance regions"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1174", "e:abstract": "In this paper we present a unit commitment model for studying the impact of large-scale wind integration in power systems with transmission constraints and system component failures. The model is formulated as a two-stage stochastic program with uncertain wind production in various locations of the network as well as generator and transmission line failures. We present a scenario selection algorithm for selecting and weighing wind power production scenarios and composite element failures, and we provide a parallel dual decomposition algorithm for solving the resulting mixed-integer program. We validate the proposed scenario selection algorithm by demonstrating that it outperforms alternative reserve commitment approaches in a 225 bus model of California with 130 generators and 375 transmission lines. We use our model to quantify day-ahead generator capacity commitment, operating cost impacts, and renewable energy utilization levels for various degrees of wind power integration. We then demonstrate that failing to account for transmission constraints and contingencies can result in significant errors in assessing the economic impacts of renewable energy integration.", "e:keyword": ["Unit commitment", "Stochastic programming", "Wind power", "Transmission constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1175", "e:abstract": "We consider a class of multistage stochastic linear programs in which at each stage a coherent risk measure of future costs is to be minimized. A general computational approach based on dynamic programming is derived that can be shown to converge to an optimal policy. By computing an inner approximation to future cost functions, we can evaluate an upper bound on the cost of an optimal policy, and an outer approximation delivers a lower bound. The approach we describe is particularly useful in sampling-based algorithms, and a numerical example is provided to show the efficacy of the methodology when used in conjunction with stochastic dual dynamic programming.", "e:keyword": ["Dynamic programming", "Stochastic programming", "Energy", "Coherent risk measure"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1176", "e:abstract": "In recent years, growth in the demand for emergency medical services, along with decline in the number of hospitals with emergency departments (EDs), has raised concerns about the ability of the EDs to provide adequate service. Many EDs frequently report periods of overcrowding during which they are forced to divert incoming ambulances to neighboring hospitals, a phenomenon known as “ambulance diversion.” The objective of this paper is to study the impact of key operational characteristics of the hospitals such as the number of ED beds, the number of inpatient beds, and the utilization of inpatient beds on the extent to which hospitals go on ambulance diversion. We propose a simple queueing network model to describe the patient flow between the ED and the inpatient department. We analyze this network using two different approximations---diffusion and fluid---to derive two separate sets of measures for inpatient occupancy and ED size. We use these sets of measures to form hypotheses and test them by estimating a sample selection model using data on a cross section of hospitals from California. We find that the measures derived from the diffusion approximation provide better explanation of the data than those derived from the fluid approximation. For this model, we find that the fraction of time that the ED spends on diversion is decreasing in the spare capacity of the inpatient department and in the size of the ED, where both are appropriately normalized for the size of the inpatient department. In addition, controlling for these hospital-specific factors, we find that the fraction of time on diversion at a hospital increases with the number of hospitals in its neighborhood. We also find that certain hospitals, owing to their location, ownership, and trauma center status, are more likely to choose ambulance diversion to mitigate overcrowding than others.", "e:keyword": ["Emergency department", "Ambulance diversion", "Sample selection model", "Queueing approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1178", "e:abstract": "Networks have become increasingly pervasive in our lives: social networks not only have a defining impact on consumer choice but are also central in social and political decisions, ranging from political discourse in the blogosphere to the organization and coordination of protests in the Arab Spring. Economic and financial transactions also increasingly occur over large, complex, dynamically changing networks, not only contributing to the efficient allocation of resources, but also occasionally creating large, systemic, and poorly understood risks. Similarly, network effects are now seen as major elements in healthcare, smart power grids, urban transportation, and more.We invite high-quality submissions in social and economic networks, construed broadly to include any system in which human action and control play a major role. We welcome contributions based on rigorous mathematical models, as well as empirical and simulation studies, that contribute to the understanding of network formation, cascades, reliability, information and action propagation, decision making, inference, measurement, and efficient optimization algorithms over networks. We seek papers that apply tools from operations research, statistics, computer science, applied mathematics, and physics to offer novel insights, build new models, and possibly provide recommendations to policy makers.To submit your paper, go to the <i>Operations Research</i> ScholarOne Manuscripts site at <ext-link ext-link-type=\"uri\"  href=\"http://mc.manuscriptcentral.com/opre\">http://mc.manuscriptcentral.com/opre</ext-link> and follow the instructions provided.<b>July/August 2015:</b> Expected issue publication date.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1179", "e:abstract": "Subjective expected utility is the most widely used model to represent preferences under uncertainty (when objective probabilities of events may not be known). This paper presents a new behavioral characterization (preference axiomatization) of subjective expected utility. The latter is derived from a behavioral assumption of cardinal independence, also known as standard sequence invariance. This axiom requires that a standard sequence of outcomes (equally spaced in terms of utility) is independent of the conditional event. This axiom is a weaker version of the trade-off consistency condition of Wakker [Wakker PP (1984) Cardinal coordinate independence for expected utility. <i>J. Math. Psych.</i> 28:110–117]. The main representation theorem is derived both in the connected topology approach and the algebraic approach (when step-continuity is replaced with solvability and Archimedean axioms).", "e:keyword": ["Subjective uncertainty", "Ambiguity", "Subjective expected utility", "Trade-off consistency", "Connected topology approach", "Algebraic approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1180", "e:abstract": "Portfolio selection is vulnerable to the error-amplifying effects of combining optimization with statistical estimation and model error. For dynamic portfolio control, sources of model error include the evolution of market factors and the influence of these factors on asset returns. We develop portfolio control rules that are robust to this type of uncertainty, applying a stochastic notion of robustness to uncertainty in model dynamics. In this stochastic formulation, robustness reflects uncertainty about the probability law generating market data, and not just uncertainty about model parameters. We analyze both finite- and infinite-horizon problems in a model in which returns are driven by factors that evolve stochastically. The model incorporates transaction costs and leads to simple and tractable optimal robust controls for multiple assets. We illustrate the performance of the controls on historical data. As one would expect, in-sample tests show no evidence of improved performance through robustness—evaluating performance on the same data used to estimate a model leaves no room to capture model uncertainty. However, robustness does improve performance in out-of-sample tests in which the model is estimated on a rolling window of data and then applied over a subsequent time period. By acknowledging uncertainty in the estimated model, the robust rules lead to less aggressive trading and are less sensitive to sharp moves in underlying prices.", "e:keyword": ["Portfolio optimization", "Robustness", "Transaction costs", "Relative entropy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1181", "e:abstract": "We assume that the cumulative consumer demand for an item follows a Brownian motion, with both the drift and the variance parameters modulated by a continuous-time Markov chain that represents the regime of the economy. The management of the company would like to maintain the inventory level as close as possible to a target inventory level and would also like to produce at a rate that is as close as possible to a target production rate. The company is penalized for deviations from the target levels, and the objective is to minimize the total discounted penalty costs. We consider two models. In the first model the management of the company knows the state of the economy, whereas in the second model the management does not know it. We solve both problems and obtain the optimal production policy and the minimal total expected discounted cost. Furthermore, we compare the total expected discounted costs of the two models and determine the value of knowing the regime of the economy. We also solve the above problems in the case when the consumer demand rate follows a geometric Brownian motion modulated by a continuous-time Markov chain that represents the regime of the economy.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1182", "e:abstract": "We propose a new approach to optimize operations of hydro storage systems with multiple connected reservoirs whose operators participate in wholesale electricity markets. Our formulation integrates short-term intraday with long-term interday decisions. The intraday problem considers bidding decisions as well as storage operation during the day and is formulated as a stochastic program. The interday problem is modeled as a Markov decision process of managing storage operation over time, for which we propose integrating stochastic dual dynamic programming with approximate dynamic programming. We show that the approximate solution converges toward an upper bound of the optimal solution. To demonstrate the efficiency of the solution approach, we fit an econometric model to actual price and inflow data and apply the approach to a case study of an existing hydro storage system. Our results indicate that the approach is tractable for a real-world application and that the gap between theoretical upper and a simulated lower bound decreases sufficiently fast.", "e:keyword": ["OR in energy", "Stochastic programming", "Markov decision processes", "Approximate dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1183", "e:abstract": "Connectivity requirements are a common component of forest planning models, with important examples arising in wildlife habitat protection. In harvest scheduling models, one way of addressing preservation concerns consists of requiring that large contiguous patches of mature forest are maintained. In the context of nature reserve design, it is common practice to select a connected region of forest, as a reserve, in such a way as to maximize the number of species and habitats protected. Although a number of integer programming formulations have been proposed for these forest planning problems, most are impractical in that they fail to solve reasonably sized scheduling instances. We present a new integer programming methodology and test an implementation of it on five medium-sized forest instances publicly available in the Forest Management Optimization Site repository. Our approach allows us to obtain near-optimal solutions for multiple time-period instances in fewer than four hours.", "e:keyword": ["Integer programming", "Cutting plane", "Natural resources"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1184", "e:abstract": "We consider a service system model primarily motivated by the problem of efficient assignment of virtual machines to physical host machines in a network cloud, so that the number of occupied hosts is minimized.There are multiple input flows of different type customers, with a customer mean service time depending on its type. There is an infinite number of servers. A server-packing <i>configuration</i> is the vector <i>k</i> = {<i>k<sub>i</sub></i>}, where <i>k<sub>i</sub></i> is the number of type <i>i</i> customers the server “contains.” Packing constraints must be observed; namely, there is a fixed finite set of configurations <i>k</i> that are allowed. Service times of different customers are independent; after a service completion, each customer leaves its server and the system. Each new arriving customer is placed for service immediately; it can be placed into a server already serving other customers (as long as packing constraints are not violated), or into an idle server.We consider a simple parsimonious real-time algorithm, called <i>Greedy</i>, that attempts to minimize the increment of the objective function <inline-formula><tex-math>\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\sum_k X_k^{1+\\alpha}$\\end{document}</tex-math></inline-formula>, <i>(alpha)</i> > 0, caused by each new assignment; here <i>X<sub>k</sub></i> is the number of servers in configuration <i>k</i>. (When <i>(alpha)</i> is small, <inline-formula><tex-math>\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\sum_k X_k^{1+\\alpha}$\\end{document}</tex-math></inline-formula> approximates the total number <inline-formula><tex-math>\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\sum_k X_k$\\end{document}</tex-math></inline-formula> of occupied servers.) Our main results show that certain versions of the Greedy algorithm are <i>asymptotically optimal</i>, in the sense of minimizing <inline-formula><tex-math>\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\sum_k X_k^{1+\\alpha}$\\end{document}</tex-math></inline-formula> in stationary regime as the input flow rates grow to infinity. We also show that in the special case when the set of allowed configurations is determined by <i>vector-packing</i> constraints, the Greedy algorithm can work with <i>aggregate configurations</i> as opposed to exact configurations <i>k</i>, thus reducing computational complexity while preserving the asymptotic optimality.", "e:keyword": ["Queueing networks", "Stochastic bin packing", "Vector packing", "Infinite server system", "Fluid limit", "Cloud computing", "Virtual machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1185", "e:abstract": "We develop a new data envelopment analysis (DEA)-based methodology for measuring the efficiency of decision-making units (DMUs) characterized by multiple inputs and multiple outputs. The distinguishing feature of our method is that it explicitly includes information about output-specific inputs and joint inputs in the efficiency evaluation. This method contributes to opening the “black box” of efficiency measurement in two different ways. First, including information on the input allocation substantially increases the discriminatory power of the efficiency measurement. Second, it allows us to decompose the efficiency value of a DMU into output-specific efficiency values, which facilitates the identification of the outputs the manager should focus on to remedy the observed inefficiency. We demonstrate the usefulness and managerial implications of our methodology by means of a unique data set collected from the activity-based costing (ABC) system of a large service company with 290 DMUs.", "e:keyword": ["Efficiency measurement", "DEA", "Input allocation", "Efficiency decomposition", "ABC systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1186", "e:abstract": "For many decision-making problems under uncertainty, it is crucial to develop risk-averse models and specify the decision makers' risk preferences based on multiple stochastic performance measures (or criteria). Incorporating such multivariate preference rules into optimization models is a fairly recent research area. Existing studies focus on extending univariate stochastic dominance rules to the multivariate case. However, enforcing multivariate stochastic dominance constraints can often be overly conservative in practice. As an alternative, we focus on the widely applied risk measure <i>conditional value-at-risk</i> (CVaR), introduce a multivariate CVaR relation, and develop a novel optimization model with multivariate CVaR constraints based on polyhedral scalarization. To solve such problems for finite probability spaces, we develop a cut generation algorithm, where each cut is obtained by solving a mixed-integer problem. We show that a multivariate CVaR constraint reduces to finitely many univariate CVaR constraints, which proves the finite convergence of our algorithm. We also show that our results can be naturally extended to a wider class of coherent risk measures. The proposed approach provides a flexible and computationally tractable way of modeling preferences in stochastic multicriteria decision making. We conduct a computational study for a budget allocation problem to illustrate the effect of enforcing multivariate CVaR constraints and demonstrate the computational performance of the proposed solution methods.", "e:keyword": ["Multivariate risk aversion", "Conditional value-at-risk", "Multiple criteria", "Cut generation", "Coherent risk measures", "Stochastic dominance", "Kusuoka representation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1187", "e:abstract": "We study optimal bidding strategies for advertisers in sponsored search auctions. In general, these auctions are run as variants of second-price auctions but have been shown to be incentive incompatible. Thus, advertisers have to be strategic about bidding. Uncertainty in the decision-making environment, budget constraints, and the presence of a large portfolio of keywords makes the bid optimization problem nontrivial. We present an analytical model to compute the optimal bids for keywords in an advertiser's portfolio. To validate our approach, we estimate the parameters of the model using data from an advertiser's sponsored search campaign and use the bids proposed by the model in a field experiment. The results of the field implementation show that the proposed bidding technique is very effective in practice. We extend our model to account for interactions between keywords, in the form of positive spillovers from generic keywords into branded keywords. The spillovers are estimated using a dynamic linear model framework and are used to jointly optimize the bids of the keywords using an approximate dynamic programming approach. Accounting for the interaction between keywords leads to an additional improvement in the campaign performance.", "e:keyword": ["Sponsored search", "Search engine marketing", "Bid optimization", "Stochastic optimization", "Stochastic modeling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1188", "e:abstract": "In this paper, we study an <i>online make-to-order</i> variant of the classical joint replenishment problem (JRP) that has been studied extensively over the years and plays a fundamental role in broader planning issues, such as the management of supply chains. In contrast to the traditional approaches of the stochastic inventory theory, we study the problem using competitive analysis against a worst-case adversary.Our main result is a 3-competitive deterministic algorithm for the online version of the JRP. We also prove a lower bound of approximately 2.64 on the competitiveness of any deterministic online algorithm for the problem. Our algorithm is based on a novel primal-dual approach using a new linear programming relaxation of the offline JRP model. The primal-dual approach that we propose departs from previous primal-dual and online algorithms in rather significant ways. We believe that this approach can extend the range of problems to which online and primal-dual algorithms can be applied and analyzed.", "e:keyword": ["Joint replenishment problem", "Online algorithm", "Competitive analysis", "Primal dual"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1189", "e:abstract": "Emissions trading is a market-based mechanism for curbing emissions, and it has been implemented in Europe, North America, and several other parts of the world. To study its impact on production planning, we develop a dynamic production model, where a manufacturer produces a single product to satisfy random market demands. The manufacturer has access to both a green and a regular production technology, of which the former is more costly but yields fewer emissions. To comply with the emissions regulations, the manufacturer can buy or sell the allowances in each period via forward contracts in an outside market with stochastic trading prices while needing to keep a nonnegative allowance account balance at the end of the planning horizon. We first derive several important structural properties of the model, and based upon them, we characterize the optimal emissions trading and production policies that minimize the manufacturer's expected total discounted cost. In particular, the optimal emissions trading policy is a target interval policy with two thresholds that decrease with the starting inventory level. The optimal production policy is established by first determining the optimal technology choice and then showing the optimality of a base-stock type of production policy. We show that the optimal base-stock level is independent of the starting inventory level and the allowance level when the manufacturer trades the allowance or uses both technologies simultaneously. A numerical study using representative data from the cement industry is conducted to illustrate the analytical results and to examine the value of green technology for the manufacturer.", "e:keyword": ["Emissions trading", "Production planning", "Technology choice", "Optimal policy", "Base-stock", "Supermodular functions"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1190", "e:abstract": "This paper presents a methodology for the valuation, optimization, market, margin and credit risk management of gas-fired power plants and associated tolling contracts. Term structure models for the power and gas forward curves are employed to facilitate hedging and risk adjustment and for improved forecasting of short-term prices. The model for the power forward curve is capable of reproducing the important phenomena often observed in power markets, including spot price spikes and spike clustering, negative prices, and the empirically observed volatility term structures of power and gas forward prices as well as the correlation term structure between these forward curves. The method solves the stochastic dynamic optimization problem that arises from the inclusion of the various operational constraints of gas-fired power plants including minimum uptime and downtime requirements, ramp rate restrictions and costs, variable output and efficiency rates, and minimum generation levels. The model involves the solution of a system of partial differential equations (PDEs), which are solved using the radial basis function (RBF) method. At each time step and operational configuration the model produces an analytic function (RBF expansion) for the value of the power plant as a function of the independent risk factors. These functions can be used for determining optimal operating strategies and can be differentiated analytically to obtain the relevant hedging statistics for the dynamic management of market risk. In addition, these value functions facilitate the calculation of the credit value adjustment (CVA) and potential future exposure (PFE) measurement of tolling contracts. The analytic differentiability of these value functions also facilitates the pricing and risk management of commodity contingent revolvers (CCRs), credit vehicles used to manage margin requirements that result from hedging market risk on an exchange.", "e:keyword": ["Natural resources", "Energy", "Dynamic programming", "Application"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1191", "e:abstract": "This paper develops a method for the exact simulation of a skeleton, a hitting time, and other functionals of a one-dimensional jump diffusion with state-dependent drift, volatility, jump intensity, and jump size. The method requires the drift function to be <i>C</i><sup>1</sup>, the volatility function to be <i>C</i><sup>2</sup>, and the jump intensity function to be locally bounded. No further structure is imposed on these functions. The method leads to unbiased simulation estimators of security prices, transition densities, hitting probabilities, and other quantities. Numerical results illustrate its features.", "e:keyword": ["Jump-diffusion process", "Stochastic differential equation", "Exact simulation", "Exact sampling", "Unbiased simulation estimator"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1192", "e:abstract": "We study a class of stochastic dynamic games that exhibit <i>strategic complementarities</i> between players; formally, in the games we consider, the payoff of a player has increasing differences between her own state and the empirical distribution of the states of other players. Such games can be used to model a diverse set of applications, including network security models, recommender systems, and dynamic search in markets. Stochastic games are generally difficult to analyze, and these difficulties are only exacerbated when the number of players is large (as might be the case in the preceding examples).We consider an approximation methodology called <i>mean field equilibrium</i> to study these games. In such an equilibrium, each player reacts to only the long-run average state of other players. We find necessary conditions for the existence of a mean field equilibrium in such games. Furthermore, as a simple consequence of this existence theorem, we obtain several natural monotonicity properties. We show that there exist a “largest” and a “smallest” equilibrium among all those where the equilibrium strategy used by a player is nondecreasing, and we also show that players converge to each of these equilibria via natural <i>myopic learning dynamics</i>; as we argue, these dynamics are more reasonable than the standard best-response dynamics. We also provide sensitivity results, where we quantify how the equilibria of such games move in response to changes in parameters of the game (for example, the introduction of incentives to players).", "e:keyword": ["Stochastic games", "Mean field equilibrium", "Strategic complementarities"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1193", "e:abstract": "The theory supporting Little's Law (L=(lambda)W) is now well developed, applying to both limits of averages and expected values of stationary distributions, but applications of Little's Law with actual system data involve measurements over a finite-time interval, which are neither of these. We advocate taking a statistical approach with such measurements. We investigate how estimates of <i>L</i> and <i>(lambda)</i> can be used to estimate <i>W</i> when the waiting times are not observed. We advocate estimating confidence intervals. Given a single sample-path segment, we suggest estimating confidence intervals using the method of batch means, as is often done in stochastic simulation output analysis. We show how to estimate and remove bias due to interval edge effects when the system does not begin and end empty. We illustrate the methods with data from a call center and simulation experiments.", "e:keyword": ["Little's Law", "L=W", "Measurements", "Parameter estimation", "Confidence intervals", "Bias", "Finite-time versions of Little's Law", "Confidence intervals with L=W", "Edge effects in L=W", "Performance analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1194", "e:abstract": "We consider the problem of designing optimal mechanisms for settings where agents have dynamic private information. We present the virtual-pivot mechanism, which is optimal in a large class of environments that satisfy a separability condition. The mechanism satisfies a rather strong equilibrium notion (it is periodic ex post incentive compatible and individually rational). We provide both necessary and sufficient conditions for immediate incentive compatibility for mechanisms that satisfy periodic ex post incentive compatibility in future periods. The result also yields a strikingly simple mechanism for selling a sequence of items to a single buyer. We also show that the allocation rule of the virtual-pivot mechanism has a very simple structure (a virtual index) in multiarmed bandit settings. Finally, we show through examples that the relaxation technique we use does not produce optimal dynamic mechanisms in general nonseparable environments.", "e:keyword": ["Optimal mechanism design", "Dynamic mechanisms", "Dynamic private information", "Online advertising", "Sponsored search"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1196", "e:abstract": "Comment on “Blotto Politics” by Alan Washburn.", "e:keyword": ["Elections", "Games"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1197", "e:abstract": "This paper presents a summary of the discrete mathematical part of my work, the Analytic Hierarchy Process (AHP) and its generalization to dependence and feedback, the Analytic Network Process (ANP), for measuring tangible and intangible factors, particularly as applied to decision making. The factors of the decision are arranged in hierarchical or network structures and judgments are then made by the decision maker, or by an expert, about the dominant element for each pair with respect to a common property. From simple judgments on two elements at a time with respect to a common property, priority vectors are obtained that are combined throughout the structure to give the best outcome for a decision. The judgments may be inconsistent, and there is a mathematical way to measure inconsistency so that the outlying judgments may be revised by the decision maker in an acceptable way or a decision may be delayed until more consistent information is obtained. In practical applications using either hierarchical or network structures, decisions are often analyzed in separate parts for their benefits, opportunities, costs, and risks, and the results are then combined in an appropriate way into an overall synthesis of those priorities. The mathematics has been generalized in the literature to the Neural Network Process (NNP), the continuous case for modeling how the brain synthesizes signals. There has been a diversity of applications over the past 30 to 40 years, and some of these are reported here. A brief mention is made of other methods of decision making and how AHP/ANP may compare with them.", "e:keyword": ["Decision making", "Analytic Hierarchy Process", "Hierarchy", "Comparisons", "Priorities", "Application of AHP"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1198", "e:abstract": "The generalized failure rate of a continuous random variable has demonstrable importance in operations management. If the valuation distribution of a product has an increasing generalized failure rate (that is, the distribution is IGFR), then the associated revenue function is unimodal, and when the generalized failure rate is strictly increasing, the global maximum is uniquely specified. The assumption that the distribution is IGFR is thus useful and frequently held in recent pricing, revenue, and supply chain management literature. This note contributes to the IGFR literature in several ways. First, it investigates the prevalence of the IGFR property for the left and right truncations of valuation distributions. Second, we extend the IGFR notion to discrete distributions and contrast it with the continuous distribution case. The note also addresses two errors in the previous IGFR literature. Finally, for future reference, we analyze all common (continuous and discrete) distributions for the prevalence of the IGFR property, and derive and tabulate their generalized failure rates.", "e:keyword": ["Probability distributions", "Hazard rate functions", "Revenue management", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1199", "e:abstract": "In the <i>bipartite rationing</i> problem, a set of agents share a single resource available in different “types,” each agent has a claim over only a subset of the resource types, and these claims overlap in arbitrary fashion. The goal is to divide fairly the various types of resources between the claimants when resources are in short supply. With a single type of resource, this is the standard rationing problem [O'Neill B (1982) A problem of rights arbitration from the Talmud. <i>Math. Soc. Sci.</i> 2(4):345--371], of which the three benchmark solutions are the proportional, uniform gains, and uniform losses methods. We extend these methods to the bipartite context, imposing the familiar <i>consistency</i> requirement: the division is unchanged if we remove an agent (respectively, a resource), and take away at the same time his share of the various resources (respectively, reduce the claims of the relevant agents). The uniform gains and uniform losses methods have infinitely many consistent extensions, but the proportional method has only one. In contrast, we find that most parametric rationing methods [Young HP (1987a) On dividing an amount according to individual claims or liabilities. <i>Math. Oper. Res.</i> 12(3):397--414], [Thomson W (2003) Axiomatic and game-theoretic analysis of bankruptcy and taxation problems. <i>Math. Soc. Sci.</i> 45(3):249--297] cannot be consistently extended.", "e:keyword": ["Games/group decisions", "Networks/graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1200", "e:abstract": "We study network flow problems in an uncertain environment from the viewpoint of robust optimization. In contrast to previous work, we consider the case that the network parameters (e.g., capacities) are known and deterministic, but the network structure (e.g., nodes and arcs) is subject to uncertainty. In this paper, we study the robust and adaptive versions of the maximum flow problem and minimum cut problems in networks with node and arc failures, and establish structural and computational results. The adaptive two-stage model adjusts the solution after the realization of the failures in the network. This leads to a more flexible model and yields less conservative solutions compared to the robust model.We show that the robust maximum flow problem can be solved in polynomial time, but the robust minimum cut problem is NP-hard. We also prove that the adaptive versions are NP-hard. We further characterize the adaptive model as a two-person zero-sum game and prove the existence of an equilibrium in such games.Moreover, we consider a path-based formulation of flows in contrast to the more commonly used arc-based version of flows. This leads to a different model of robustness for maximum flows. We analyze this problem as well and develop a simple linear optimization model to obtain approximate solutions. Furthermore, we introduce the concept of adaptive maximum flows over time in networks with transit times on the arcs. Unlike the deterministic case, we show that this problem is NP-hard on series-parallel graphs even for the case that only one arc is allowed to fail. Finally, we propose heuristics based on linear optimization models that exhibit strong computational performance for large-scale instances.", "e:keyword": ["Network flows", "Robust optimization", "Game theory", "Flows over time"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1202", "e:abstract": "In this paper a novel variational inequality (VI) formulation of the dynamic user optimal (DUO) route choice problem is proposed using the concept of approach proportion. An approach proportion represents the proportion of travelers that select a turning or through movement when leaving a node. Approach proportions contain travelers' route information so that the realistic effects of physical queues can be captured in a formulation when a physical-queue traffic flow model is adopted, and so that route enumeration and path-set generation can be avoided in the solution procedure. In addition, the simple structure of the approach proportion representation allows us to decompose the constraint set for solving large-scale DUO route choice problems. This paper also discusses the existence and uniqueness of the solutions to the VI problem and develops a solution algorithm based on the extragradient method to solve the proposed VI problem. This solution algorithm makes use of the decomposition property of the constraint set and is convergent if the travel time functions are pseudomonotone and Lipschitz continuous. It is not necessary to know the Lipschitz constant of the travel time functions in advance. Finally, numerical examples are given to demonstrate the properties of the proposed model and the performance of the solution algorithm.", "e:keyword": ["Dynamic traffic assignment", "Dynamic user optimal", "Approach proportion", "Variational inequality", "Extragradient method"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1203", "e:abstract": "This paper establishes a new preservation property of supermodularity in a class of two-dimensional parametric optimization problems, where the constraint sets may not be lattices. This property and its extensions unify several results in the literature and provide powerful tools to analyze a variety of operations models including a two-product coordinated pricing and inventory control problem with cross-price effects that we use as an illustrative example.", "e:keyword": ["Supermodularity", "L-concavity", "Parametric optimization problems", "Inventory", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1204", "e:abstract": "We present an application of decision analysis to global production capacity expansion under uncertainty for Bayer Group's proposed new biotechnology drug to treat hemophilia A. This decision analysis developed an improved approach to Bayer's decision for product-generation transition and global production capacity expansion that more realistically addresses potential regional supply shortages and overages due to demand and supply uncertainties that can result in supply-demand imbalances. With the added confidence provided by this more realistic approach, Bayer's executive leadership team acted on the recommendation from the decision analysis in contrast to an earlier analysis that had not resulted in management action. The paper makes two major contributions: First, it describes the details of what is involved in conducting applied decision analysis in a real-world setting in more detail than what is typically in textbooks on decision analysis. Second, the paper illustrates the important role of economic modeling in a large-scale applied decision analysis. The approach is applicable to other product-generation transition decision making for expanding global production capacity under uncertainty in a supply-constrained environment, especially for new product introductions and new product development decisions when supply is limited.", "e:keyword": ["Decision analysis", "Product-generation transition decision making", "Global capacity expansion", "Regional supply-demand imbalances", "Supply limitations", "Production", "Manufacturing", "Economic modeling", "Hemophilia A"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1207", "e:abstract": "We consider the problem of efficiently allocating simulation effort to determine which of several simulated systems have mean performance exceeding a threshold of known value. Within a Bayesian formulation of this problem, the optimal fully sequential policy for allocating simulation effort is the solution to a dynamic program. When sampling is limited by probabilistic termination or sampling costs, we show that this dynamic program can be solved efficiently, providing a tractable way to compute the Bayes-optimal policy. The solution uses techniques from optimal stopping and multiarmed bandits. We then present further theoretical results characterizing this Bayes-optimal policy, compare it numerically to several approximate policies, and apply it to applications in emergency services and manufacturing.", "e:keyword": ["Multiple comparisons with a standard", "Sequential experimental design", "Dynamic programming", "Bayesian statistics", "Value of information"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1208", "e:abstract": "We consider the classical scheduling problems of processing jobs on identical parallel machines to minimize (i) the makespan (the maximum completion time) or (ii) the total flow time (the sum of the completion times). The focus of this study is on the impact that additional machines may have, if added to the system. We measure such a machine impact by the ratio of the value of the objective function computed with the original number of machines to the one computed with extra machines. We give tight bounds on the machine impact for the problem of minimizing the makespan, for both the preemptive and non-preemptive versions, as well as for the problem of minimizing the total flow time. We also present polynomial-time exact and approximation algorithms to make a cost-effective choice of the number of machines, provided that each machine incurs a cost and the objective function captures the trade-off between the cost of the used machines and a scheduling objective.", "e:keyword": ["Production/scheduling: approximations/heuristic", "Production/scheduling: sequencing: deterministic: multiple machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1209", "e:abstract": "Preference learning has been a topic of research in many fields, including operations research, marketing, machine learning, and behavioral economics. In this work, we strive to combine the ideas from these different fields into a single methodology to learn preferences and make decisions. We use robust and integer optimization in an adaptive and dynamic way to determine preferences from data that are consistent with human behavior. We use integer optimization to address human inconsistency, robust optimization and conditional value at risk (CVaR) to address loss aversion, and adaptive conjoint analysis and linear optimization to frame the questions to learn preferences. The paper makes the following methodological contributions: to the robust optimization literature by proposing a method to derive uncertainty sets from adaptive questionnaires, to the marketing literature by using the analytic center of discrete sets (as opposed to polyhedra) to capture errors and inconsistencies, and to the risk modeling literature by using efficient methods from computer science for sampling to optimize CVaR. We have implemented an online software that uses the proposed approach and report empirical evidence of its strength.", "e:keyword": ["Programming", "Integer", "Linear", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1210", "e:abstract": "Outsourcing of equipment repair and restoration is commonly practiced by firms in many industries. The operational performance of equipment is determined by joint decisions of the firm (client) and the service provider (vendor). Although some decisions are verifiable and thus directly contractible, many decisions are not. The result is a double-sided moral hazard environment in which each party has incentives to free ride on the other's effort. A performance-based contract allows the client to align the incentives of the vendor, but it also exposes the vendor to stochastic earnings and thereby creates disincentives to make first-best decisions. To capture these issues, we develop a novel principal-agent model by integrating elements of the machine repairman model and a stochastic financial distress model within the double-sided moral hazard framework. We apply our model to solve the client's problem of designing the optimal performance-based contract. We find that the client can attain the first-best profit by restricting the search space to only two classes of performance-based contract structures: linear and tiered. We show that the linear contract structure has limited ability in attaining the first-best outcome, contingent on the vendor's exogenous characteristics. In contrast, the tiered contract structure enables the client to attain the first-best outcome regardless of vendor characteristics. Our results provide normative insights on the role of contract structures in eliminating any loss due to double-sided moral hazard or to the vendor's financial concerns. These results also provide theoretical support for the extensive use of tiered contracts observed in practice.", "e:keyword": ["Double-sided moral hazard", "Performance-based contracts", "Financial distress", "Machine repairman model"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1211", "e:abstract": "Comment on “The Cost of Latency in High-Frequency Trading” by Ciamac C. Moallemi and Mehmet Sağlam.", "e:keyword": ["Finance", "Trading", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1212", "e:abstract": "In this paper, we propose a new probabilistic model for minimizing the anticipated regret in combinatorial optimization problems with distributional uncertainty in the objective coefficients. The interval uncertainty representation of data is supplemented with information on the marginal distributions. As a decision criterion, we minimize the worst-case conditional value at risk of regret. The proposed model includes the interval data minmax regret model as a special case. For the class of combinatorial optimization problems with a compact convex hull representation, a polynomial sized mixed-integer linear program is formulated when (a) the range and mean are known, and (b) the range, mean, and mean absolute deviation are known; and a mixed-integer second order cone program is formulated when (c) the range, mean, and standard deviation are known. For the subset selection problem of choosing <i>K</i> elements of maximum total weight out of a set of <i>N</i> elements, the probabilistic regret model is shown to be solvable in polynomial time in the instances (a) and (b) above. This extends the current known polynomial complexity result for minmax regret subset selection with range information only.", "e:keyword": ["Minmax regret", "Probability", "Mixed-integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1213", "e:abstract": "When several individuals are asked to forecast an uncertain quantity, they often face implicit or explicit incentives to be the most accurate. Despite the desire to elicit honest forecasts, such competition induces forecasters to report strategically and nontruthfully. The question we address is whether the competitive crowd's forecast (the average of strategic forecasts) is more accurate than the truthful crowd's forecast (the average of truthful forecasts from the same forecasters). We analyze a forecasting competition in which a prize is awarded to the forecaster whose point forecast is closest to the actual outcome. Before reporting a forecast, we assume each forecaster receives two signals: one common and one private. These signals represent the forecasters' past shared and personal experiences relevant for forecasting the uncertain quantity of interest. In a set of equilibrium results, we characterize the nature of the strategic forecasts in this game. As the correlation among the forecasters' private signals increases, the forecasters switch from using a pure to a mixed strategy. In both cases, forecasters exaggerate their private information and thereby make the competitive crowd's forecast more accurate than the truthful crowd's forecast.", "e:keyword": ["Forecasting competitions", "Strategic forecasting", "Expert opinion", "Averaging opinions", "Combining forecasts"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1214", "e:abstract": "This paper studies a model of <i>community-based</i> healthcare delivery for a <i>chronic disease</i>. In this setting, patients periodically visit the healthcare delivery system, which influences their disease progression and consequently their health outcomes. We investigate how the provider can maximize community-level health outcomes through better operational decisions pertaining to capacity allocation across different patients. To do so, we develop an integrated capacity allocation model that incorporates clinical (disease progression) and operational (capacity constraint) aspects. Specifically, we model the provider's problem as a finite horizon stochastic dynamic program, where the provider decides which patients to schedule at the beginning of each period. Therapy is provided to scheduled patients, which may improve their health states. Patients that are not seen follow their natural disease progression. We derive a quantitative measure for comparison of patients' health states and use it to design an easy-to-implement myopic heuristic that is provably optimal in special cases of the problem. We employ the myopic heuristic in a more general setting and test its performance using operational and clinical data obtained from Mobile C.A.R.E. Foundation, a community-based provider of pediatric asthma care in Chicago. Our extensive computational experiments suggest that the myopic heuristic can improve the health gains at the community level by up to 15% over the current policy. The benefit is driven by the ability of our myopic heuristic to alter the duration between visits for patients with different health states depending on the tightness of the capacity and the health states of the entire patient population.", "e:keyword": ["Capacity allocation", "Chronic disease", "Mobile care", "Disease progression", "Appointment scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1215", "e:abstract": "We develop practical operations research models to support decision making in the design and management of public bicycle-sharing systems. We develop a network flow model with proportionality constraints to estimate the flow of bicycles within the network and the number of trips supported, given an initial allocation of bicycles at each station. We also examine the effectiveness of periodic redistribution of bicycles in the network to support greater flow, and the impact on the number of docks needed.We conduct our numerical analysis using transit data from train operators in Singapore. Given that a substantial proportion of passengers in the train system commute a short distance---more than 16% of passengers alight within two stops from the origin---this forms a latent segment of demand for a bicycle-sharing program. We argue that for a bicycle-sharing system to be most effective for this customer segment, the system must deploy the right number of bicycles at the right places, because this affects the utilization rate of the bicycles and how bicycles circulate within the system. We also identify the appropriate operational environments in which periodic redistribution of bicycles will be most effective for improving system performance.", "e:keyword": ["Bicycle sharing", "Proportional network flow", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1216", "e:abstract": "We study the performance of two popular and widely used heuristics for revenue management known as the booking limit and bid price controls. In contrast to a recent result in the literature where frequent re-solvings of a certain heuristic are shown to significantly reduce revenue loss, we show that the asymptotic revenue loss of either booking limit or bid price control <i>cannot</i> be reduced regardless of the choice of re-solving times and the frequency of re-solving. Moreover, we also show that further variations within the policy classes, such as <i>nested</i> instead of <i>partition</i> booking limit, or <i>certainty equivalent</i> instead of <i>additive</i> bid price, are simply indistinguishable in terms of their order of revenue loss under frequent re-solvings. This negative result highlights the limitation of re-solving deterministic linear programs when the solution is interpreted as either a booking limit or a bid price. Finally, we briefly discuss how to modify the traditional booking limit control to make it more responsive to frequent re-solvings and test its performance using numerical experiments.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Revenue management", "Reoptimization", "Asymptotic optimality"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1217", "e:abstract": "This paper presents decision analysis methodology for decisions based on data from geographic information systems. The consequences of a decision alternative are modeled as distributions of outcomes across a geographic region. We discuss conditions that may conform with the decision maker's preferences over a specified set of alternatives; then we present specific forms for value or utility functions that are implied by these conditions. Decisions in which there is certainty about the consequences resulting from each alternative are considered first; then probabilistic uncertainty about the consequences is included as an extension. The methodology is applied to two hypothetical urban planning decisions involving water use and temperature reduction in regional urban development, and fire coverage across a city. These examples illustrate the applicability of the approach and the insights that can be gained from using it.", "e:keyword": ["Decision analysis", "Geographic information systems", "Multiattribute utility", "Multiattribute value", "Additive independence", "Preferential independence", "Homogeneity"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1218", "e:abstract": "We consider a service provider in a market with two segments. Members of the first request a reservation ahead of service and will not patronize the firm without one. Members of the second walk in and demand service immediately. These customers have a fixed cost of reaching the firm and may behave strategically. In equilibrium, they randomize between walking in and staying home. The service provider must decide how much of a limited capacity to make available to advance customers. When the advance demand segment offers a higher per customer margin, the firm may opt to decline some reservation requests in order to bolster walk-in demand. When walk-in customers are more valuable, classical revenue management models would dictate that at least some capacity be set aside for high-value later arrivals. Here it is possible that the optimal policy saves <i>no</i> capacity for walk-ins. Thus, it may be better to ignore rather than pamper walk-in customers. This outcome is robust to changes in the model.", "e:keyword": ["Revenue management", "Service management", "Reservations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1219", "e:abstract": "The hub-and-spoke distribution paradigm has been a fundamental principle in geographic network design for more than 40 years. One of the primary advantages that such networks possess is their ability to exploit economies of scale in transportation by aggregating network flows through common sources. In this paper, we consider the problem of designing an optimal hub-and-spoke network in continuous Euclidean space: the “spokes” of the network are distributed uniformly over a service region, and our objective is to determine the optimal number of hub nodes and their locations. We consider seven different backbone network topologies for connecting the hub nodes, namely, the Steiner and minimum spanning trees, a travelling salesman tour, a star network, a capacitated vehicle routing tour, a complete bipartite graph, and a complete graph. We also perform an additional analysis on a multilevel network in which network flows move through multiple levels of transshipment before reaching the service region. We describe the asymptotically optimal (or near-optimal) configurations that minimize the total network costs as the demand in the region becomes large and give an approximation algorithm that solves our problem on a convex planar region for any values of the relevant input parameters.", "e:keyword": ["Continuous facility location", "Backbone network design"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1220", "e:abstract": "We study the optimal configuration of hospital inpatient rooms with private and semiprivate rooms when some of the patients have infectious diseases and need to be isolated. We assume that the demand is random and seasonal. We propose a computationally efficient solution procedure that is based on a stochastic program that uses asymptotic approximations for the system performance under different admission policies and show its accuracy for large systems. Using our model, we study the appropriateness of the recent trends in hospital design calling for 100% private rooms. We show that even with isolation patients such an extreme approach could result in a significant degradation in the access of patients to hospital beds.", "e:keyword": ["Hospital bed configuration", "Queueing theory", "Admission control", "Hospital operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1221", "e:abstract": "Sequencing problems are among the most prominent problems studied in operations research, with primary application in, e.g., scheduling and routing. We propose a novel approach to solving generic sequencing problems using multivalued decision diagrams (MDDs). Because an MDD representation may grow exponentially large, we apply MDDs of limited size as a discrete relaxation to the problem. We show that MDDs can be used to represent a wide range of sequencing problems with various side constraints and objective functions, and we demonstrate how MDDs can be added to existing constraint-based scheduling systems. Our computational results indicate that the additional inference obtained by our MDDs can speed up a state-of-the art solver by several orders of magnitude, for a range of different problem classes.", "e:keyword": ["Sequencing", "Single machine scheduling", "Networks/graphs", "Constraint programming", "Decision diagrams"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1222", "e:abstract": "In this paper, we consider an optimal portfolio deleveraging problem, where the objective is to meet specified debt/equity requirements at the minimal execution cost. Permanent and temporary price impact is taken into account. With no restrictions on the relative magnitudes of permanent and temporary price impact, the optimal deleveraging problem reduces to a nonconvex quadratic program with quadratic and box constraints. Analytical results on the optimal deleveraging strategy are obtained. They provide guidance on how we liquidate a portfolio according to endogenous and exogenous factors. A Lagrangian method is proposed to solve the nonconvex quadratic program numerically. By studying the breakpoints of the Lagrangian problem, we obtain conditions under which the Lagrangian method returns an optimal solution of the deleveraging problem. When the Lagrangian algorithm returns a suboptimal approximation, we present upper bounds on the loss in equity caused by using such an approximation.", "e:keyword": ["Optimal deleveraging", "Permanent and temporary price impact", "Nonconvex quadratic program", "Lagrangian method", "Breakpoint"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1223", "e:abstract": "We propose a preference condition called shifted difference independence to axiomatize a general habit formation and satiation model (GHS). This model allows for a general habit formation and satiation function that contain many functional forms in the literature as special cases. Since the GHS model can be reduced to either a general satiation model (GSa) or a general habit formation model (GHa), our theory also provides approaches to axiomatize both the GSa model and the GHa model. Furthermore, by adding extra preference conditions into our axiomatization framework, we obtain a GHS model with a linear habit formation function and a recursively defined linear satiation function.", "e:keyword": ["Time preference", "Habit formation", "Satiation", "Axiomatization", "Discounted utility", "Measurable preference"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1224", "e:abstract": "We design a dynamic rate scheduling policy of Markov type by using the solution (a social optimal Nash equilibrium point) to a utility-maximization problem over a randomly evolving capacity set for a stochastic system of generalized processor-sharing queues in a random environment whose job arrivals to each queue follow a <i>doubly stochastic renewal process</i> (DSRP). Both the random environment and the random arrival rate of each DSRP are driven by a finite state continuous time Markov chain. The scheduling policy optimizes in a <i>greedy</i> fashion with respect to each queue and environmental state. Since the closed-form solution for the performance of such a queuing system under the policy is difficult to obtain, we establish a reflecting diffusion with regime-switching model for its measures of performance. Furthermore, we justify its asymptotic optimality by deriving the stochastic fluid and diffusion limits for the corresponding system under heavy traffic. In addition, we identify a cost function related to the utility function, which is minimized by minimizing the workload process in the diffusion limit. More importantly, our queuing model includes typical systems in the future wireless networks, such as the <i>J</i>-user multi-input multioutput multiple access channel and the broadcast channel under Markov fading with cooperation and admission control as special cases.", "e:keyword": ["Processor-sharing queues", "Random environment", "MIMO wireless channel", "Markov fading", "Doubly stochastic renewal process", "Utility-maximization scheduling", "Concave game", "Heavy traffic", "Asymptotic optimality", "Reflecting diffusion with regime-switching"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1225", "e:abstract": "We consider cooperative traveling salesman games with nonnegative asymmetric costs satisfying the triangle inequality. We construct a stable cost allocation with budget balance guarantee equal to the Held-Karp integrality gap for the asymmetric traveling salesman problem, using the parsimonious property and a previously unknown connection to linear production games. We also show that our techniques extend to larger classes of network design games. We then provide a simple example showing that our cost allocation does not necessarily achieve the best possible budget balance guarantee, even among cost allocations stable for the game defined by the Held-Karp relaxation, and discuss its implications on future work on traveling salesman games.", "e:keyword": ["Games/group decisions", "Cooperative game", "Networks/graphs", "Traveling salesman problem", "Integer programming", "Integrality gap"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1226", "e:abstract": "This paper proposes a simulation-based optimization (SO) method that enables the efficient use of complex stochastic urban traffic simulators to address various transportation problems. It presents a metamodel that integrates information from a simulator with an analytical queueing network model. The proposed metamodel combines a general-purpose component (a quadratic polynomial), which provides a detailed local approximation, with a physical component (the analytical queueing network model), which provides tractable analytical and global information. This combination leads to an SO framework that is computationally efficient and suitable for complex problems with very tight computational budgets.We integrate this metamodel within a derivative-free trust region algorithm. We evaluate the performance of this method considering a traffic signal control problem for the Swiss city of Lausanne, different demand scenarios, and tight computational budgets. The method leads to well-performing signal plans. It leads to reduced, as well as more reliable, average travel times.", "e:keyword": ["Simulation-based optimization", "Traffic control", "Metamodel", "Queueing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1227", "e:abstract": "This study presents new exact algorithms for the clustered vehicle routing problem (CluVRP). The CluVRP is a generalization of the capacitated vehicle routing problem (CVRP), in which the customers are grouped into <i>clusters</i>. As in the CVRP, all the customers must be visited exactly once, but a vehicle visiting one customer in a cluster must visit all the remaining customers therein before leaving it. Based on an exponential time preprocessing scheme, an integer programming formulation for the CluVRP is presented. The formulation is enhanced by a polynomial time graph reduction scheme. Two exact algorithms for the CluVRP, a branch and cut as well as a branch and cut and price, are presented. The computational performances of the algorithms are tested on benchmark instances adapted from the vehicle routing problem literature as well as real-world instances from a solid waste collection application.", "e:keyword": ["Clusters", "Vehicle routing", "Branch and cut"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1229", "e:abstract": "In this paper we discuss multistage programming with the data process subject to uncertainty. We consider a situation where the data process can be naturally separated into two components: one can be modeled as a random process, with a specified probability distribution, and the other one can be treated from a robust (worst-case) point of view. We formulate this in a time consistent way and derive the corresponding dynamic programming equations. To solve the obtained multistage problem, we develop a variant of the stochastic dual dynamic programming method. We give a general description of the algorithm and present computational studies related to planning of the Brazilian interconnected power system.", "e:keyword": ["Multistage stochastic programming", "Robust optimization", "Time consistency", "Dynamic equations", "Stochastic dual dynamic programming", "Sample average approximation", "Risk-neutral and risk-averse approaches", "Case studies"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1230", "e:abstract": "This paper provides necessary and sufficient preference conditions for average utility maximization over sequences of variable length. We obtain full generality by using a new algebraic technique that exploits the richness structure naturally provided by the variable length of the sequences. Thus we generalize many preceding results in the literature. For example, continuity in outcomes, a condition needed in other approaches, now is an option rather than a requirement. Applications to expected utility, decisions under ambiguity, welfare evaluations for variable population size, discounted utility, and quasilinear means in functional analysis are presented.", "e:keyword": ["Utility", "Multiattribute", "Mathematics functions", "Decision analysis", "Multiple criteria"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1231", "e:abstract": "High sensitivity to initial conditions is generally viewed as a drawback of tree search methods because it leads to erratic behavior to be mitigated somehow. In this paper we investigate the opposite viewpoint and consider this behavior as an opportunity to exploit. Our working hypothesis is that erraticism is in fact just a consequence of the exponential nature of tree search that acts as a chaotic amplifier, so it is largely unavoidable. We propose a bet-and-run approach to actually turn erraticism to one's advantage. The idea is to make a number of short sample runs with randomized initial conditions, to bet on the “most promising” run selected according to certain simple criteria, and to bring it to completion. Computational results on a large testbed of mixed integer linear programs from the literature are presented, showing the potential of this approach even when embedded in a proof-of-concept implementation.", "e:keyword": ["Enumerative algorithms", "Mixed integer programming", "Restart"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1232", "e:abstract": "In the operations research literature, the queue joining probability is monotonic decreasing in the queue length; the longer the queue, the fewer consumers join. Recent academic and empirical evidence indicates that queue-joining probabilities may not always be decreasing in the queue length. We provide a simple explanation for these nonmonotonic queue-joining strategies by relaxing the informational assumptions in Naor's model. Instead of imposing that the expected service time and service value are common knowledge, we assume that they are unknown to consumers, but positively correlated. Under such informational assumptions, the posterior expected waiting cost <i>and</i> service value increase in the observed queue length. As a consequence, we show that queue-joining equilibria may emerge for which the joining probability increases locally in the queue length. We refer to these as “sputtering equilibria.” We discuss when and why such sputtering equilibria exist for discrete as well as continuously distributed priors on the expected service time (with positively correlated service value).", "e:keyword": ["Queueing games", "Threshold policies", "Nonmonotone queue joining", "Randomization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1233", "e:abstract": "In a novel multiplayer extension of the famous secretary problem, multiple players seek to employ secretaries from a common labour pool. Secretaries do not accept being put on hold, always accept job offers immediately, and leave the labour pool once rejected by a single player. All players have an identical preference for secretaries, and all players seek to optimize the probability of obtaining the best of all <i>n</i> secretaries. We find that in the Nash equilibrium, as the number, <i>N</i>, of players searching the labour pool grows, the optimal strategy converges to a simple function of <i>N</i>. For the two-player case we also compute how much players can gain through cooperation and how the optimal strategy changes under a payoff structure that promotes spite.", "e:keyword": ["Secretary problem", "Game theory", "Sequential decision analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1234", "e:abstract": "When retailers' orders exceed the supplier's available capacity, the supplier allocates his capacity according to some allocation rule. When retailers are local monopolists, <i>uniform allocation</i> eliminates the “gaming effect” so that each retailer orders her ideal allocation. However, when two retailers engage in Cournot competition under complete information, a recent study has shown that uniform allocation fails to eliminate the gaming effect so that some retailer may inflate her order strategically. By examining a more general situation in which two or more retailers engage in Cournot competition under complete information, we establish exact conditions under which uniform allocation fails to eliminate the gaming effect. These exact conditions enable us to construct a new rule called <i>competitive allocation</i> that can eliminate the gaming effect. Without inflated orders from the retailers, the supplier's profit could be lower under competitive allocation than under uniform allocation when certain restrictive conditions hold. In contrast, competitive allocation generates higher average profits for the retailers and for the supply chain; hence, it reduces the inefficiency of the decentralized supply chain.", "e:keyword": ["Games/group decisions", "Inventory/production: capacity allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1235", "e:abstract": "Crowdsourcing systems, in which numerous tasks are electronically distributed to numerous “information pieceworkers,” have emerged as an effective paradigm for human-powered solving of large-scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all such systems must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in an appropriate manner, e.g., majority voting.In this paper, we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm, inspired by belief propagation and low-rank matrix approximation, significantly outperforms majority voting and, in fact, is optimal through comparison to an oracle that knows the reliability of every worker. Further, we compare our approach with a more general class of algorithms that can dynamically assign tasks. By adaptively deciding which questions to ask to the next set of arriving workers, one might hope to reduce uncertainty more efficiently. We show that, perhaps surprisingly, the minimum price necessary to achieve a target reliability scales in the same manner under both adaptive and nonadaptive scenarios. Hence, our nonadaptive approach is order optimal under both scenarios. This strongly relies on the fact that workers are fleeting and cannot be exploited. Therefore, architecturally, our results suggest that building a reliable worker-reputation system is essential to fully harnessing the potential of adaptive designs.", "e:keyword": ["Networks/graphs", "Stochastic", "Information systems", "Expert systems", "Statistics", "Estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1236", "e:abstract": "The most common measure of waiting time is the overall expected waiting time for service. However, in service networks the perception of waiting may also depend on how it is distributed among different stations. Therefore, reducing the probability of a long wait at any station may be important in improving customers' perception of service quality. In a single-station queue it is known that the policy that minimizes the waiting time and the probability of long waits is nonidling. However, this is not necessarily the case for queueing networks with several stations. We present a family of threshold-based policies (TBPs) that strategically idle some stations. We demonstrate the advantage of strategically idling by applying TBP in a network with two single-server queues in tandem. We provide closed form results for the special case where the first station has infinite capacity and develop efficient algorithms when this is not the case. We compare TBPs with the nonidling and Kanban policies, and we discuss when a TBP is advantageous. Using simulation, we demonstrate that the analytical insights for the two-station case hold for a three-station serial queue as well.", "e:keyword": ["Strategic idleness", "Threshold-based policy", "Customer service experience", "Service network"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1237", "e:abstract": "We consider a general infinite horizon inventory control model that combines demand and supply risks and the firm's ability to mitigate the supply risks by diversifying its procurement orders among a set of <i>N</i> potential suppliers. Supply risks arise because only a random percentage of any given replenishment order is delivered as useable units. The suppliers are characterized by the price they charge and the distribution of their yield factor. Assuming unsatisfied demand is backlogged, the firm incurs, as in standard inventory models, three types of costs: (i) procurement costs, (ii) inventory carrying costs for units carried over from one period to the next, and (iii) backlogging costs. We establish the existence of an optimal stationary policy, under both the long-run discounted and average cost criteria, and characterize its structure. Assuming each period's inventory level distribution can be approximated as a normal, we develop an efficient solution method identifying additional structural properties. Finally, we identify a simple class of heuristic policies that come close to being optimal.", "e:keyword": ["Infinite horizon inventory control", "Supply diversification", "Random yield"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1238", "e:abstract": "In this paper we investigate joint pricing and inventory control problems in a finite-horizon, single-product, periodic-review setting with certain/uncertain supply capacities. The demands in different periods are random variables whose distributions depend on the posted price exhibiting the additive form. The order quantity in each period is required to be of integral multiples of a given specific batch size (denoted by <i>Q</i>). Inventory replenishment incurs a linear ordering cost. Referred to as the cost-rate function, the sum of holding and backorder costs can either be convex or quasi-convex. The objective is to determine a joint ordering and pricing decision that can maximize the total expected profit over the planning horizon.We first consider the case in which the cost-rate function is convex and show that the <i>modified</i> (<i>r</i>, <i>Q</i>) list-price policy is optimal for the system with certain and limited capacities, a special case of which is the (<i>r</i>, <i>Q</i>) list-price policy when capacities become unlimited. As supply capacities become random, the optimal policy follows a new structure wherein the optimal order-up-to level and posted price must be coordinated to make the optimal safety stock level follow the (<i>r</i>, <i>Q</i>) policy. We further consider the case of a quasi-convex cost-rate function, which may arise when a service level constraint is used as a surrogate for the shortage cost. We demonstrate that the (<i>r</i>, <i>Q</i>) list-price policy is optimal for the system without supply capacity constraints. In addition, extensions to several other models are discussed. The enabling technique is based on the notion of <i>Q</i>-jump convexity and its variants.", "e:keyword": ["Inventory-pricing systems", "Supply capacity", "Q-jump convexity", "(r", "Q) list-price policy", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1239", "e:abstract": "We study fixed-dimensional stochastic dynamic programs in a discrete setting over a finite horizon. Under the primary assumption that the cost-to-go functions are discrete <i>L</i><sup>(natural)</sup>-convex, we propose a pseudo-polynomial time approximation scheme that solves this problem to within an arbitrary prespecified additive error of <i>(epsilon)</i> > 0. The proposed approximation algorithm is a generalization of the explicit-enumeration algorithm and offers us full control in the trade-off between accuracy and running time.The main technique we develop for obtaining our scheme is approximation of a fixed-dimensional <i>L</i><sup>(natural)</sup>-convex function on a bounded rectangular set, using only a selected number of points in its domain. Furthermore, we prove that the approximation function preserves <i>L</i><sup>(natural)</sup>-convexity. Finally, to apply the approximate functions in a dynamic program, we bound the error propagation of the approximation. Our approximation scheme is illustrated on a well-known problem in inventory theory, the single-product problem with lost sales and lead times. We demonstrate the practical value of our scheme by implementing our approximation scheme and the explicit-enumeration algorithm on instances of this inventory problem.", "e:keyword": ["Discrete convexity", "Multidimensional stochastic dynamic programs", "Approximation algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1240", "e:abstract": "A firm facing price dependent stochastic demand aims to maximize its total expected profit over a planning horizon. In addition to the regular unit selling price, the firm can utilize quantity discounts to increase sales. We refer to this dual-pricing strategy as quantity-based price differentiation. At the beginning of each period, the firm needs to make three decisions: replenish the inventory, set the unit selling price if the unit sales mode is deployed, and set the quantity-discount price if the quantity-sales mode is deployed (or the combination of the two modes of sales). We identify conditions under which the optimal inventory control policy and selling/pricing strategy is well structured. Remarkably, under a utility-based demand framework, these conditions can be unified by a simple regularity assumption that has long been used in the auction and mechanism design literature. Moreover, sharper structural results are yielded for the optimal selling strategy. We also examine the comparative advantage of quantity-based price differentiation with respect to model parameters. Our numerical study shows that substantial profit improvement can be gained as a result of shifting from uniform pricing to quantity-based pricing, especially when the product has a low unit ordering cost and high utility.", "e:keyword": ["Stochastic inventory systems", "Dynamic pricing", "Multiple sales modes", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1241", "e:abstract": "This model examines the role of intelligence gathering and screening in providing airport security. We analyze this problem using a game between the government and a terrorist. By investing in intelligence gathering, the government can improve the precision of its information. In contrast, screening can be used to search a passenger and thereby deter terrorist attacks. We determine the optimal allocation of resources between these two strategies wherein we model the role of intelligence using the concept of supermodular precision. One striking result is that under certain circumstances, an increase in the investment in intelligence can induce a more devious terrorist to attack with a higher probability. We also find that when there is a cost-reducing innovation in the screening technology, then the optimal investment in intelligence gathering can go either way. However, such an innovation unambiguously improves social welfare. Another interesting implication is that a developed economy would value intelligence inputs more than a developing economy. We also examine the efficacy of a program such as PreCheck that allows some select passengers expedited screening in exchange for voluntarily revealing information about themselves. Our analysis shows that such a program can be used to cushion the adverse effect of budgetary shortages. Finally, we also examine the role of enhanced punishment on the optimal level of intelligence. We find that the result can go both ways. If the initial level of punishment is high, then any further enhancement reduces the optimal level of intelligence gathering. However, this result is reversed if the initial level of punishment is low.", "e:keyword": ["Agencies", "Cost effectiveness", "Tactics/strategy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1242", "e:abstract": "In this paper we refer to the requirement for industrialized countries to reach a domestic target for greenhouse emissions, as ratified in the Kyoto Protocol, and propose a market-consistent model of futures price dynamics for cap-and-trade schemes designed in the spirit of the European Union Emissions Trading Scheme (EU ETS). Historical price dynamics for the EU ETS suggest that both European emission allowance (EUA) and certified emission reduction (CER) certificates, generated by a nondomestic offset mechanism, are significantly related. We use an equilibrium framework to demonstrate that compliance regulation singles out special price dynamics. Based on this result, we propose an arbitrage-free model and apply it to the pricing of spread options between EUAs and CERs.", "e:keyword": ["Environment", "Asset pricing", "Stochastic model applications", "Markov processes", "Economics"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1244", "e:abstract": "This paper studies a sequential resource allocation problem motivated by distribution operations of a nonprofit organization. The alternate objectives that arise in nonprofit (as opposed to commercial) operations lead to new variations on traditional problems in operations research and inventory management. Specifically, we consider the problem of distributing a scarce resource to meet customers' demands that are observed sequentially. An allocation policy that seeks to maximize profit may lead to inequitable distributions among customers. Our work in a nonprofit setting solves the sequential resource allocation problem with an objective function aimed at <i>equitable</i> and <i>effective</i> service. We define service in terms of fill rate (the ratio of the allocated amount to observed demand) and develop an objective function to maximize the expected minimum fill rate among customers, which balances equity in fill rates with effectiveness in the use of resources (low waste). Through a dynamic programming framework, we characterize the structure of the optimal allocation policy for a given sequence of customers when demand follows continuous probability distributions. We use that optimal structure to develop a heuristic allocation policy for instances with discrete demand distribution. In addition, we identify customer demand properties to consider when sequencing customer visits to optimize the fill rate objective. For both inventory allocation and customer sequencing decisions, the proposed heuristic methods yield near-optimal solutions.", "e:keyword": ["Nonprofit operations", "Sequential resource allocation", "Equity and efficiency"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1245", "e:abstract": "We consider a retailer selling a single product with limited on-hand inventory over a finite selling season. Customer demand arrives according to a Poisson process, the rate of which is influenced by a single action taken by the retailer (such as price adjustment, sales commission, advertisement intensity, etc.). The relationship between the action and the demand rate is not known in advance. However, the retailer is able to learn the optimal action <i>on the fly</i> as she maximizes her total expected revenue based on the observed demand reactions.Using the pricing problem as an example, we propose a dynamic <i>learning-while-doing</i> algorithm that only involves function value estimation to achieve a <i>near-optimal performance</i>. Our algorithm employs a series of shrinking price intervals and iteratively tests prices within that interval using a set of carefully chosen parameters. We prove that the performance of our algorithm is among the best of all possible algorithms in terms of the <i>asymptotic regret</i> (the relative loss compared to the full information optimal solution). Our result closes the performance gaps between parametric and nonparametric learning and between the post-price mechanism and the customer-bidding mechanism. Important managerial insight from this research is that the values of information on both the parametric form of the demand function as well as each customer's exact reservation price are less important than prior literature suggests. Our results also suggest that firms would be better off to perform dynamic learning and action concurrently rather than sequentially.", "e:keyword": ["Revenue management", "Pricing", "Nonparametric", "Learning", "Asymptotic optimality", "Dynamic decision making"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1246", "e:abstract": "Heavy and uncertain traffic conditions exacerbate the commuting experience of millions of people across the globe. When planning important trips, commuters typically add an extra buffer to the expected trip duration to ensure on-time arrival. Motivated by this, we propose a new traffic assignment model that takes into account the stochastic nature of travel times. Our model extends the traditional model of Wardrop competition when uncertainty is present in the network. The focus is on strategic <i>risk-averse</i> users who capture the trade-off between travel times and their variability in a <i>mean-standard deviation</i> objective, defined as the mean travel time plus a risk-aversion factor times the standard deviation of travel time along a path. We consider both infinitesimal users, leading to a <i>nonatomic</i> game, and <i>atomic</i> users, leading to a discrete finite game. We establish conditions that characterize an equilibrium traffic assignment and find when it exists. The main challenge is posed by the users' risk aversion, since the mean-standard deviation objective is nonconvex and nonseparable, meaning that a path cannot be split as a sum of edge costs. As a result, even an individual user's subproblem---a stochastic shortest path problem---is a nonconvex optimization problem for which no polynomial time algorithms are known. In turn, the mathematical structure of the traffic assignment model with stochastic travel times is fundamentally different from the deterministic counterpart. In particular, an equilibrium characterization requires exponentially many variables, one for each path in the network, since an edge flow has multiple possible path-flow decompositions that are not equivalent. Because of this, characterizing the equilibrium and the socially optimal assignment, which minimizes the total user cost, is more challenging than in the traditional deterministic setting. Nevertheless, we prove that both can be encoded by a representation with just polynomially many paths. Finally, under the assumption that the standard deviations of travel times are independent from edge loads, we show that the worst-case ratio between the social cost of an equilibrium and that of an optimal solution is not higher than the analogous ratio in the deterministic setting. In other words, uncertainty does not further degrade the system performance in addition to strategic user behavior alone.", "e:keyword": ["Congestion game", "Stochastic networks", "Mean-stdev Wardrop equilibrium", "Nash equilibrium", "Risk aversion", "Nonadditive traffic assignment problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1247", "e:abstract": "Since the 1970s, several authors have studied the structure of the set partitioning polytope and proposed adaptations of the simplex algorithm that find an optimal solution via a sequence of basic integer solutions. Balas and Padberg in 1972 proved the existence of such a sequence with nonincreasing costs, but degeneracy makes it difficult to find the terms of the sequence. This paper uses ideas from the improved primal simplex to deal efficiently with degeneracy and find subsequent terms in the sequence. When there is no entering variable that leads to a better integer solution, the algorithm referred to as the integral simplex using decomposition algorithm uses a subproblem to find a group of variables to enter into the basis in order to obtain such a solution. We improve the Balas and Padberg results by introducing a constructive method that finds this sequence by only using normal pivots on positive coefficients. We present results for large-scale problems (with up to 500,000 variables) for which optimal integer solutions are often obtained without any branching.", "e:keyword": ["Integral simplex", "Set partitioning problem", "Decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1248", "e:abstract": "We study the strip packing problem, in which a set of two-dimensional rectangular items has to be packed in a rectangular strip of fixed width and infinite height, with the aim of minimizing the height used. The problem is important because it models a large number of real-world applications, including cutting operations where stocks of materials such as paper or wood come in large rolls and have to be cut with minimum waste, scheduling problems in which tasks require a contiguous subset of identical resources, and container loading problems arising in the transportation of items that cannot be stacked one over the other.The strip packing problem has been attacked in the literature with several heuristic and exact algorithms, nevertheless, benchmark instances of small size remain unsolved to proven optimality. In this paper we propose a new exact method that solves a large number of the open benchmark instances within a limited computational effort. Our method is based on a Benders' decomposition, in which in the master we cut items into unit-width slices and pack them contiguously in the strip, and in the slave we attempt to reconstruct the rectangular items by fixing the vertical positions of their unit-width slices. If the slave proves that the reconstruction of the items is not possible, then a cut is added to the master, and the algorithm is reiterated.We show that both the master and the slave are strongly (N-script)(P-script)-hard problems and solve them with tailored preprocessing, lower and upper bounding techniques, and exact algorithms. We also propose several new techniques to improve the standard Benders' cuts, using the so-called combinatorial Benders' cuts, and an additional lifting procedure. Extensive computational tests show that the proposed algorithm provides a substantial breakthrough with respect to previously published algorithms.", "e:keyword": ["Strip packing problem", "Exact algorithm", "Benders' decomposition", "Combinatorial Benders' cut"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1249", "e:abstract": "We study firms that sell multiple substitutable products and customers whose purchase behavior follows a nested logit model, of which the multinomial logit model is a special case. Customers make purchasing decisions sequentially under the nested logit model: they first select a nest of products and subsequently purchase one within the selected nest. We consider the multiproduct pricing problem under the general nested logit model with product-differentiated price sensitivities and arbitrary nest coefficients. We show that the <i>adjusted markup</i>, defined as price minus cost minus the reciprocal of price sensitivity, is constant for all the products within a nest at optimality. This reduces the problem's dimension to a single variable per nest. We also show that the <i>adjusted nest-level markup</i> is nest invariant for all the nests, which further reduces the problem to maximizing a single-variable unimodal function under mild conditions. We also use this result to simplify the oligopolistic multiproduct price competition and characterize the Nash equilibrium. We also consider more general attraction functions that include the linear utility and the multiplicative competitive interaction models as special cases, and we show that similar techniques can be used to significantly simplify the corresponding pricing problems.", "e:keyword": ["Substitutable products", "Multiproduct pricing", "Attraction model", "Nested logit model", "Multinomial logit model"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1250", "e:abstract": "We provide a review of the types of equilibria typically found in operations management inventory papers and a discussion on when the commonly used stationary infinite-horizon (open-loop) equilibrium may be sufficient for study. We focus particularly on order-up-to and basestock equilibria in the context of inventory duopolies. We give conditions under which the stationary infinite-horizon equilibrium is also a Markov perfect (closed-loop) equilibrium. These conditions are then applied to three specific duopolies. The first application is one with stockout-based substitution, where the firms face independent direct demand but some fraction of a firm's lost sales will switch to the other firm. The second application is one where shelf-space display stimulates primary demand and reduces demand for the other firm's product. The final application is one where the state variables represent goodwill rather than inventory. These specific problems have been previously studied in both the single period and/or stationary infinite-horizon (open-loop) settings but not in Markov perfect (closed-loop) settings. Under the Markov perfect setting, a variety of interesting dynamics may occur, including that there may be a so-called commitment value to inventory.", "e:keyword": ["Stochastic games", "Inventory policies", "Noncooperative games", "Markov equilibrium", "Inventory games"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1251", "e:abstract": "The total cost problem for discrete-time controlled transient Markov models is considered. The objective functional is a Markov dynamic risk measure of the total cost. Two solution methods, value and policy iteration, are proposed, and their convergence is analyzed. In the policy iteration method, we propose two algorithms for policy evaluation: the nonsmooth Newton method and convex programming, and we prove their convergence. The results are illustrated on a credit limit control problem.", "e:keyword": ["Dynamic programming", "Risk measures", "Transient Markov models", "Value iteration", "Policy iteration"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1252", "e:abstract": "It is often the case that the computed optimal solution of an optimization problem cannot be implemented directly, irrespective of data accuracy, because of either (i) technological limitations (such as physical tolerances of machines or processes), (ii) the deliberate simplification of a model to keep it tractable (by ignoring certain types of constraints that pose computational difficulties), and/or (iii) human factors (getting people to “do” the optimal solution). Motivated by this observation, we present a modeling paradigm called “fabrication-adaptive optimization” for treating issues of implementation/fabrication. We develop computationally focused theory and algorithms, and we present computational results for incorporating considerations of implementation/fabrication into constrained optimization problems that arise in photonic crystal design. The fabrication-adaptive optimization framework stems from the robust regularization of a function. When the feasible region is not a normed space (as typically encountered in application settings), the fabrication-adaptive optimization framework typically yields a nonconvex optimization problem. (In the special case where the feasible region is a finite-dimensional normed space, we show that fabrication-adaptive optimization can be recast as an instance of modern robust optimization.) We study a variety of problems with special structures on functions, feasible regions, and norms for which computation is tractable and develop an algorithmic scheme for solving these problems in spite of the challenges of nonconvexity. We apply our methodology to compute fabrication-adaptive designs of two-dimensional photonic crystals with a variety of prescribed features.", "e:keyword": ["Fabrication-adaptivity", "Robust regularization", "Robust optimization", "Bandgap optimization", "Photonic crystal design", "Topology optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1253", "e:abstract": "In 1969 H. Emmons provided three theorems (Emmons 1--3) for determining precedence relations between pairs of jobs for the single-machine tardiness problem. We show here a fourth straightforward theorem that uses the information when the jobs in the pair are both known to precede a third job in an optimum sequence. The new theorem augments the three Emmons theorems and is shown to be a generalization of a theorem by Elmaghraby.", "e:keyword": ["Sequencing", "Production/scheduling", "Single machine", "Deterministic"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1254", "e:abstract": "This paper addresses the scheduled service network design problem for freight rail transportation. The proposed model integrates service selection and scheduling, car classification and blocking, train makeup, and routing of time-dependent customer shipments based on a cyclic three-layer space--time network representation of the associated operations and decisions and their relations and time dimensions. This paper also proposes a matheuristic solution methodology integrating slope scaling, a dynamic block-generation mechanism, long-term-memory-based perturbation strategies, and ellipsoidal search, a new intensification mechanism to thoroughly explore very large neighborhoods of elite solutions restricted using information from the history of the search. Experimental results show that the proposed solution method is efficient and robust, yielding high-quality solutions for realistically sized problem instances.", "e:keyword": ["Scheduled service network design", "Rail freight transportation", "Capacitated multicommodity network design", "Slope scaling", "Ellipsoidal search"]}, {"@id": "http://dx.doi.org/10.1287/opre.2013.1255", "e:abstract": "Production postponement, the strategy to hold reserved production capacity that can be deployed based on actual demand signals, is often used to mitigate supply-demand mismatch risk. The effectiveness of this strategy depends crucially on the ease, or flexibility, in deploying the reserved capacity to meet product demands. Existing literature assumes that the reserved capacity is fully flexible, i.e., capable of being deployed to meet the demand of any item in a multiproduct system. Little is known if reserved capacity is held at many different locations, with each location having only a limited range of flexibility on production options. This paper examines how effective the production postponement strategy is in this environment.When the amount of reserved capacity is small (i.e., postponement level near 0%), no amount of flexibility can reap significant benefits. When the reserved capacity is high (i.e., postponement level near 100%), it is well known that a sparse structure such as a 2-chain can perform nearly as well as a fully flexible structure. Hence, process flexibility beyond 2-chain has little impact on the effectiveness of production postponement strategy in these two extreme environments. Interestingly, in a symmetric system, we prove that the performance of 2-chain, vis-à-vis the full flexibility structure, has a wider gap when postponement level (i.e., amount of reserved capacity) is moderate, and thus process flexibility beyond 2-chain matters and affects appreciably the performance of the production postponement strategy. Fortunately, adding a little more flexibility, say turning a 2-chain into a 3-chain, the system can perform almost as well as a full flexibility structure for all postponement levels. This is important as first stage production capacity can be allocated as if the reserve capacity is fully flexible. Our analysis hinges on an exact analytical expression for the performance of <i>d</i>-chain, obtained from solving a related class of random walk problems. To the best of our knowledge, this is the first paper with analytical results on the performance of <i>d</i>-chain for <i>d</i> > 2.", "e:keyword": ["Process flexibility", "Production postponement", "Chaining strategy", "Multi-item newsvendor", "Stochastic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1256", "e:abstract": "We study a class of assortment optimization problems where customers choose among the offered products according to the nested logit model. There is a fixed revenue associated with each product. The objective is to find an assortment of products to offer so as to maximize the expected revenue per customer. We show that the problem is polynomially solvable when the nest dissimilarity parameters of the choice model are less than one and the customers always make a purchase within the selected nest. Relaxing either of these assumptions renders the problem NP-hard. To deal with the NP-hard cases, we develop parsimonious collections of candidate assortments with worst-case performance guarantees. We also formulate a convex program whose optimal objective value is an upper bound on the optimal expected revenue. Thus, we can compare the expected revenue provided by an assortment with the upper bound on the optimal expected revenue to get a feel for the optimality gap of the assortment. By using this approach, our computational experiments test the performance of the parsimonious collections of candidate assortments that we develop.", "e:keyword": ["Nested logit", "Discrete choice models", "Revenue management", "Assortment planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1257", "e:abstract": "The scenario approach is a recently introduced method to obtain feasible solutions to chance-constrained optimization problems based on random sampling. It has been noted that the sample complexity of the scenario approach rapidly increases with the number of optimization variables and this may pose a hurdle to its applicability to medium- and large-scale problems. We here introduce the Fast Algorithm for the Scenario Technique, a variant of the scenario optimization algorithm with reduced sample complexity.", "e:keyword": ["Stochastic programming", "Chance-constrained optimization", "Randomized algorithms", "Sample-based methods", "Scenario approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1258", "e:abstract": "In a number of service systems, there can be substantial latitude to vary service rates. However, although speeding up service rate during periods of congestion may address a present congestion issue, it may actually exacerbate the problem by increasing the need for rework. We introduce a state-dependent queuing network where service times and return probabilities depend on the “overloaded” and “underloaded” state of the system. We use a fluid model to examine how different definitions of “overload” affect the long-term behavior of the system and provide insight into the impact of using speedup. We identify scenarios where speedup can be helpful to temporarily alleviate congestion and increase access to service. For such scenarios, we provide approximations for the likelihood of speedup to service. We also identify scenarios where speedup should never be used; moreover, in such a situation, an interesting bi-stability arises, such that the system shifts randomly between two equilibria states. Hence, our analysis sheds light on the potential benefits and pitfalls of using speedup when the subsequent returns may be unavoidable.", "e:keyword": ["Speedup", "State-dependent queues", "Erlang-R", "Fluid models", "Return to service", "Bi-stability"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1259", "e:abstract": "Robust optimisation is a well-established concept to deal with uncertainty. In particular, <i>recovery-robust</i> models are suitable for real-world contexts, where a certain amount of recovery---although limited---is often available. In this paper we describe a general framework to optimise <i>event-based</i> problems against <i>delay propagation</i>. We also present a real-world application to train platforming in the Italian railways in order to show the practical effectiveness of our framework.", "e:keyword": ["Robust optimization", "Recovery", "Delay propagation", "Railway planning", "Event scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1260", "e:abstract": "Scrip systems provide a nonmonetary trade economy for exchange of resources. We model a scrip system as a stochastic game and study system design issues on selection rules to match potential trade partners over time. We show the optimality of one particular rule in terms of maximizing social welfare for a given scrip system that guarantees players' incentives to participate. We also investigate the optimal number of scrips to issue under this rule. In particular, if the time discount factor is close enough to one, or trade benefits one partner much more than it costs the other, the maximum social welfare is always achieved no matter how many scrips are in the system. When the benefit of trade and time discount are not sufficiently large, on the other hand, injecting more scrips in the system hurts most participants; as a result, there is an upper bound on the number of scrips allowed in the system, above which some players may default. We show that this upper bound increases with the discount factor as well as the ratio between the benefit and cost of service. Finally, we demonstrate similar properties for a different service provider selection rule that has been analyzed in previous literature.", "e:keyword": ["Repeated games", "Stochastic games", "Dynamic program", "Game theory", "P2P networks", "Scrip systems", "Artificial currency", "Nonmonetary trade economies"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1261", "e:abstract": "We analyze a joint pricing and inventory control problem for a perishable product with a fixed lifetime over a finite horizon. In each period, demand depends on the price of the current period plus an additive random term. Inventories can be intentionally disposed of, and those that reach their lifetime have to be disposed of. The objective is to find a joint pricing, ordering, and disposal policy to maximize the total expected discounted profit over the planning horizon taking into account linear ordering cost, inventory holding and backlogging or lost-sales penalty cost, and disposal cost. Employing the concept of <i>L</i><sup>(natural)</sup>-concavity, we show some monotonicity properties of the optimal policies. Our results shed new light on perishable inventory management, and our approach provides a significantly simpler proof of a classical structural result in the literature. Moreover, we identify bounds on the optimal order-up-to levels and develop an effective heuristic policy. Numerical results show that our heuristic policy performs well in both stationary and nonstationary settings. Finally, we show that our approach also applies to models with random lifetimes and inventory rationing models with multiple demand classes.", "e:keyword": ["Perishable inventory management", "Pricing strategies", "L-convexity"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1262", "e:abstract": "Scarf's max-min order formula for the <i>risk-neutral</i> and <i>ambiguity-averse</i> newsvendor problem is a classical result in the field of inventory management. In this article, we extend Scarf's formula by deriving an analogous closed-form order formula for the <i>risk</i>- and <i>ambiguity</i>-<i>averse</i> newsvendor problem. Specifically, we provide and analyze the newsvendor order quantity that maximizes the worst-case expected profit versus risk trade-off (risk-averse) when only the mean and standard deviation of the product's demand distribution are known (ambiguity-averse), and the risk is measured by the standard deviation of the newsvendor's profit. We provide both analytical and numerical results to illustrate the combined effect of considering risk aversion and ambiguity aversion in computing the newsvendor order.", "e:keyword": ["Distributionally robust", "Newsvendor", "Risk averse", "Ambiguity averse", "Max-min rule"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1263", "e:abstract": "This paper highlights how a general class of hub location problems can be modeled as the minimization of a real-valued supermodular set function. Well-known problems such as uncapacitated hub location, <i>p</i>-hub median, and hub arc location, among others, are shown to be particular cases of this class. Two integer programming formulations are introduced and compared. One uses path-based variables, frequently employed in hub location, whereas the other exploits properties of supermodular functions. In addition, several worst case bounds for a greedy and a local improvement heuristic are obtained for the general class and for some particular cases in which sharper bounds can be devised. Computational experiments are performed to compare both formulations when used with a general purpose solver. Computational results obtained on benchmark instances confirm the superiority of the supermodular formulation.", "e:keyword": ["Hub location", "Supermodular functions", "Cutting plane methods", "Integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1264", "e:abstract": "This article proposes the estimation of the marginal cost of individual firms using semiparametric and nonparametric methods. These methods have a number of appealing features when applied to cost functions. The empirical analysis uses data from a unique sample of the California electricity industry for which we observe the actual marginal cost and estimate the marginal cost from these data. We compare the actual values of marginal cost with the estimates from semiparametric and nonparametric methods, as well as with the estimates obtained through conventional parametric methods. We show that the semiparametric and nonparametric methods produce marginal cost estimates that very closely approximate the actual. In contrast, the results from conventional parametric methods are significantly biased and provide invalid inference.", "e:keyword": ["Estimation of marginal cost", "Parametric models", "Semiparametric models", "Nonparametric models", "California electricity industry", "Actual and simulated data"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1265", "e:abstract": "We propose a new way to derive tractable robust counterparts of a linear program based on the duality between the robust (“pessimistic”) primal problem and its “optimistic” dual. First we obtain a new <i>convex</i> reformulation of the dual problem of a robust linear program, and then show how to construct the primal robust solution from the dual optimal solution. Our result allows many new uncertainty regions to be considered. We give examples of tractable uncertainty regions that were previously intractable. The results are illustrated by solving a multi-item newsvendor problem. We also apply the new method to the globalized robust counterpart scheme and show its tractability.", "e:keyword": ["Robust optimization", "General convex uncertainty regions", "Uncertain linear optimization", "Globalized robust counterpart"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1267", "e:abstract": "We generalize the standard method of solving inverse optimization problems to allow for the solution of inverse problems that would otherwise be ill posed or infeasible. In multiobjective linear optimization, given a solution that is not a weakly efficient solution to the forward problem, our method generates objective function weights that make the given solution a near-weakly efficient solution. Our generalized inverse optimization model specializes to the standard model when the given solution is weakly efficient and retains the complexity of the underlying forward problem. We provide a novel interpretation of our inverse formulation as the dual of the well-known Benson's method and by doing so develop a new connection between inverse optimization and Pareto surface approximation techniques. We apply our method to prostate cancer data obtained from Princess Margaret Cancer Centre in Toronto, Canada. We demonstrate that clinically acceptable treatments can be generated using a small number of objective functions and inversely optimized weights---current treatments are designed using a complex formulation with a large parameter space in a trial-and-error reoptimization process. We also show that our method can identify objective functions that are most influential in treatment plan optimization.", "e:keyword": ["Inverse optimization", "Multiobjective linear optimization", "Radiation therapy treatment planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1268", "e:abstract": "Many of the existing methods for evaluating an airline's on-time performance are based on flight-centric measures of delay. However, recent research has demonstrated that passenger delays depend on many factors in addition to flight delays. For instance, significant passenger delays result from flight cancellations and missed connections, which themselves depend on a significant number of factors. Unfortunately, lack of publicly available passenger travel data has made it difficult for researchers to explore the nature of these relationships. In this paper, we develop methodologies to model historical travel and delays for U.S. domestic passengers. We develop a multinomial logit model for estimating historical passenger travel and extend a previously developed greedy reaccommodation heuristic for estimating the resulting passenger delays. We report and analyze the estimated passenger delays for calendar year 2007, developing insights into factors that affect the performance of the National Air Transportation System in the United States.", "e:keyword": ["Industries transportation/shipping", "Transportation costs", "Utility/preference estimation", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1269", "e:abstract": "This article argues that the traditional model of the theory of social choice is not a good model and does not lead to acceptable methods of ranking and electing. It presents a more meaningful and realistic model that leads naturally to a method of ranking and electing---<i>majority judgment</i>---that better meets the traditional criteria of what constitutes a good method. It gives descriptions of its successful use in several different practical situations and compares it with other methods including Condorcet's, Borda's, first-past-the-post, and approval voting.", "e:keyword": ["Methods of electing and ranking", "Condorcet and Arrow paradoxes", "Strategic manipulation", "Faithful representation", "Meaningful measurement", "Figure skating", "Presidential elections", "Jury decision"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1270", "e:abstract": "This paper examines the effect that a central bank's interventions have on longer term interest rate securities by examining a stochastic short rate process that can be controlled by the central bank. Rather than investigate the motivations for the intervention, we assume that the bank is able to quantify its preferences and tolerances for various rates. We allow for a very general class of stochastic processes for the short rate, and most of the popular models in literature fall within this class. Interventions are best modeled as impulse controls, which are very difficult to handle, even computationally, except in very special cases. Allowing interventions to be modeled by impulse controls, we develop a computational method and provide relevant convergence results. We also derive error bounds for intermediate iterations. Using this method, we solve for the central bank's optimal control policy and also study the effect of this on longer term interest rate securities using a change of measure. The method developed here can easily be applied to a very wide range of impulse control problems beyond the realm of interest rate models.", "e:keyword": ["Impulse control", "Interest rates", "Free boundary problems", "Central bank intervention"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1271", "e:abstract": "We consider an assemble-to-order generalized <i>M</i>-system with multiple components and multiple products, batch ordering of components, random lead times, and lost sales. We model the system as an infinite-horizon Markov decision process and seek an optimal policy that specifies when a batch of components should be produced (i.e., inventory replenishment) and whether an arriving demand for each product should be satisfied (i.e., inventory allocation). We characterize optimal inventory replenishment and allocation policies under a mild condition on component batch sizes via a <i>new</i> type of policy: <i>lattice-dependent base stock</i> and <i>lattice-dependent rationing</i>.", "e:keyword": ["Assemble-to-order systems", "Markov decision processes", "Optimal control", "Lattice-dependent policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1272", "e:abstract": "In this paper we introduce the <i>knapsack problem for perishable inventories</i> concerning the optimal dynamic allocation of a collection of products to a limited knapsack. The motivation for designing such a problem comes from retail revenue management, where different products often have an associated lifetime during which they can only be sold, and the managers can regularly select some products to be allocated to a limited promotion space that is expected to attract more customers than the standard shelves. Another motivation comes from scheduling of requests in modern multiserver data centers so that quality-of-service requirements given by completion deadlines are satisfied. Using the Lagrangian approach we derive an optimal index policy for the <i>Whittle relaxation</i> of the problem in which the knapsack capacity is used only on average. Assuming a certain structure of the optimal policy for the single-inventory control, we prove indexability and derive an efficient, linear-time algorithm for computing the index values. To the best of our knowledge, our paper is the first to provide indexability analysis of a restless bandit with bi-dimensional state (lifetime and inventory level). We illustrate that these index values are numerically close to the true index values when such a structure is not present. We test two index-based heuristics for the original, nonrelaxed problem: (1) a conventional <i>index rule</i>, which prescribes to order the products according to their current index values and promotes as many products as fit in the knapsack, and (2) a recently proposed <i>index-knapsack heuristic</i>, which employs the index values as a proxy for the price of promotion and proposes to solve a deterministic knapsack problem to select the products. By a systematic computational study we show that the performance of both heuristics is nearly optimal, and that the index-knapsack heuristic outperforms the conventional index rule.", "e:keyword": ["Markov decision processes", "Resource allocation", "Knapsack problem", "Index policies", "Whittle index", "Lagrangian relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1273", "e:abstract": "In this paper, we develop efficient Monte Carlo methods for estimating American option sensitivities. The problem can be reformulated as how to perform sensitivity analysis for a stochastic optimization problem with model uncertainty. We introduce a generalized infinitesimal perturbation analysis (IPA) approach to resolve the difficulty caused by discontinuity of the optimal decision with respect to the underlying parameter. The IPA estimators are unbiased if the optimal decisions are explicitly known. To quantify the estimation bias caused by intractable exercising policies in the case of pricing American options, we also provide an approximation guarantee that relates the sensitivity under the optimal exercise policy to that computed under a suboptimal policy. The price-sensitivity estimators yielded from this approach demonstrate significant advantages numerically in both high-dimensional environments and various process settings. We can easily embed them into many of the most popular pricing algorithms without extra simulation effort to obtain sensitivities as a by-product of the option price. Our generalized approach also casts new insights on how to perform sensitivity analysis using IPA: we do not need path-wise continuity to apply it.", "e:keyword": ["Finance", "Asset pricing", "American option", "Price sensitivities", "Simulation", "Applications", "Dynamic programming", "Applications", "Optimal stopping"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1274", "e:abstract": "We consider the problem of helping a decision maker (DM) choose from a set of multiattributed objects when her preferences are “concavifiable,” i.e. representable by a concave value function. We establish conditions under which preferences or preference intensities are concavifiable. We also derive a characterization for the family of concave value functions compatible with a set of such preference statements expressed by the DM. This can be used to validate dominance relations over discrete sets of alternatives and forms the basis of an interactive procedure. We report on the practical use of this procedure with several DMs for a flat-choice problem and its computational performance on a set of project-portfolio selection problem instances. The use of preference intensities is found to provide significant improvements to the performance of the procedure.", "e:keyword": ["Decision analysis", "Multiple criteria", "Theory", "Programming", "Multiple criteria", "Utility/preference", "Multiattribute", "Value theory", "Concave value function", "Preference intensity"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1276", "e:abstract": "Labor markets can often be viewed as many-to-one matching markets. It is well known that if complementarities are present in such markets, a stable matching may not exist. We study large random matching markets with couples. We introduce a new matching algorithm and show that if the number of couples grows slower than the size of the market, a stable matching will be found with high probability. If however, the number of couples grows at a linear rate, with constant probability (not depending on the market size), no stable matching exists. Our results explain data from the market for psychology interns.", "e:keyword": ["Stability", "Matching", "Couples", "Deferred acceptance", "Market design"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1277", "e:abstract": "Opaque selling has been widely adopted by service providers in the travel industry to sell off leftover capacity under stochastic demand. We consider a two-stage model to study the impact of different selling mechanisms, posted price (PP) versus name-your-own-price (NYOP), of an opaque reseller on competing service providers who face forward-looking customers. We find that in this environment, providers prefer that the opaque reseller uses a posted price instead of a bidding model. This is because the ability to set retail prices is critical for extracting surplus from customers who wait to purchase from the reseller. Such surplus extraction enables providers to set high prices for advance sales and obtain high profits. The dominance of PP over NYOP disappears, however, when competition between sellers is minimal or absent. We extend our model to multiple opaque resellers who compete in selling off last-minute capacity for service providers and find that our main insights continue to hold with differentiated resellers. Despite providers' preference in favor of PP, there are circumstances under which the opaque reseller earns higher profits under NYOP. Leisure customers might also prefer the bidding mechanism, which allows them to retain some surplus. This can help explain the rapid growth of the NYOP model over the last decade. Our findings are consistent with the evolution of opaque selling in the travel industry, and in particular, the recent trend towards more published price sales for opaque products.", "e:keyword": ["Competition", "Name-your-own-price", "Posted price", "Opaque selling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1278", "e:abstract": "We study an integrated charge batching and casting width selection problem arising in the continuous casting operation of the steelmaking process at Shanghai, China based Baosteel. This decision-making problem is not unique to Baosteel; it exists in every large iron and steel company in the world. We collaborated with Baosteel on this problem from 2006 to 2008 by developing and implementing a decision support system (DSS) that replaced their manual planning method. The DSS is still in active use at Baosteel. This paper describes the solution algorithms we developed and imbedded in the DSS. For the general problem that is strongly NP-hard, a column generation-based branch-and-price (B&P) solution approach is developed to obtain optimal solutions. By exploiting the problem structure, efficient dynamic programming algorithms are designed to solve the subproblems involved in the column generation procedure. Branching strategies are designed in a way that ensures that after every stage of branching the structure of the subproblems is preserved such that they can still be solved efficiently. We also consider a frequently occurring case of the problem where each steel grade is incompatible with any other grade. For this special case, a two-level polynomial-time algorithm is developed to obtain optimal solutions. Computational tests on a set of real production data as well as on a more diverse set of randomly generated problem instances show that our algorithms outperform the manual planning method that Baosteel used to use by a significant margin both in terms of tundish utilization for almost every case, and in terms of total cost for most cases. Consequently, by replacing their manual method with our DSS, the estimated benefits to Baosteel include an annual cost saving of about US $1.6 million and an annual revenue increase of about US $3.25 million.", "e:keyword": ["Decision support system", "Continuous casting", "Tundish", "Charge batching", "Branch and price", "Column generation", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1279", "e:abstract": "Airline operations are subject to frequent disruptions typically due to unexpected aircraft maintenance requirements and undesirable weather conditions. Recovery from a disruption often involves propagating delays in downstream flights and increasing cruise stage speed when possible in an effort to contain the delays. However, there is a critical trade-off between fuel consumption (and its adverse impact on air quality and greenhouse gas emissions) and cruise speed. Here we consider delays caused by such disruptions and propose a flight rescheduling model that includes adjusting cruise stage speed on a set of affected and unaffected flights as well as swapping aircraft optimally.To the best of our knowledge, this is the first study in which the cruise speed is explicitly included as a decision variable into an airline recovery optimization model along with the environmental constraints and costs. The proposed model allows one to investigate the trade-off between flight delays and the cost of recovery. We show that the optimization approach leads to significant cost savings compared to the popular recovery method delay propagation.Flight time controllability, nonlinear delay, fuel burn and CO<sub>2</sub> emission cost functions, and binary aircraft swapping decisions complicate the aircraft recovery problem significantly. In order to mitigate the computational difficulty we utilize the recent advances in conic mixed integer programming and propose a strengthened formulation so that the nonlinear mixed integer recovery optimization model can be solved efficiently. Our computational tests on realistic cases indicate that the proposed model may be used by operations controllers to manage disruptions in real time in an optimal manner instead of relying on ad-hoc heuristic approaches.", "e:keyword": ["Airline disruption management", "Fuel burn", "Cruise speed control", "Conic integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1280", "e:abstract": "We consider a transportation station, where customers arrive according to a Poisson process. A transportation facility visits the station according to a renewal process and serves at each visit a random number of customers according to its capacity. We assume that the arriving customers decide whether to join the station or balk, based on a natural reward-cost structure. We study the strategic behavior of the customers and determine their symmetric Nash equilibrium strategies under two levels of information.", "e:keyword": ["Queueing", "Strategic customers", "Transportation station", "Clearing system", "Balking", "Nash equilibrium strategy", "Observable model", "Unobservable model", "Partial information"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1281", "e:abstract": "We identify a rich class of finite-horizon Markov decision problems (MDPs) for which the variance of the optimal total reward can be bounded by a simple linear function of its expected value. The class is characterized by three natural properties: <i>reward nonnegativity and boundedness</i>, <i>existence of a do-nothing action</i>, and <i>optimal action monotonicity</i>. These properties are commonly present and typically easy to check. Implications of the class properties and of the variance bound are illustrated by examples of MDPs from operations research, operations management, financial engineering, and combinatorial optimization.", "e:keyword": ["Markov decision problems", "Variance bounds", "Optimal total reward"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1282", "e:abstract": "We consider the indifference-zone (IZ) formulation of the ranking and selection problem with independent normal samples. In this problem, we must use stochastic simulation to select the best among several noisy simulated systems, with a statistical guarantee on solution quality. Existing IZ procedures sample excessively in problems with many alternatives, in part because loose bounds on probability of correct selection lead them to deliver solution quality much higher than requested. Consequently, existing IZ procedures are seldom considered practical for problems with more than a few hundred alternatives. To overcome this, we present a new sequential elimination IZ procedure, called BIZ (Bayes-inspired indifference zone), whose lower bound on worst-case probability of correct selection in the preference zone is tight in continuous time, and nearly tight in discrete time. To the author's knowledge, this is the first sequential elimination procedure with tight bounds on worst-case preference-zone probability of correct selection for more than two alternatives. Theoretical results for the discrete-time case assume that variances are known and have an integer multiple structure, but the BIZ procedure itself can be used when these assumptions are not met. In numerical experiments, the sampling effort used by BIZ is significantly smaller than that of another leading IZ procedure, the KN procedure, especially on the largest problems tested (2<sup>14</sup> = 16,384 alternatives).", "e:keyword": ["Ranking and selection", "Indifference zone", "Sequential analysis", "Bayesian statistics"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1283", "e:abstract": "A cooperative game with transferable utility is said to be <i>homogeneous of degree one</i> if for any integer <i>m</i>, the value of cloning <i>m</i> times all players at any given coalition, leads to <i>m</i> times the value of the original coalition. We show that this property coupled with subadditivity, guarantees the nonemptyness of the core of the game and of all its subgames, namely, the game is totally balanced. Examples for games stemming from the areas of retailing and of facility location are given.", "e:keyword": ["Games/group decisions", "Cooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1284", "e:abstract": "We consider customer service chat (CSC) systems where customers can receive real time service from agents using an instant messaging (IM) application over the Internet. A unique feature of these systems is that agents can serve multiple customers simultaneously. The number of customers that an agent is serving determines the rate at which each customer assigned to that agent receives service. We consider the staffing problem in CSC systems with impatient customers where the objective is to minimize the number of agents while providing a certain service level. The service level is measured in terms of the proportion of customers who abandon the system in the long run. First we propose effective routing policies based on a static planning LP, both for the cases when the arrival rate is observable and for when the rate is unobservable. We show that these routing policies minimize the proportion of abandoning customers in the long run asymptotically for large systems. We also prove that the staffing solution obtained from a staffing LP, when used with the proposed routing policies, is asymptotically optimal. We illustrate the effectiveness of our solution procedure in systems with small to large sizes via numerical and simulation experiments.", "e:keyword": ["Call center management", "Queuing theory and stochastic methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1285", "e:abstract": "We consider a dynamic pricing problem for a monopolistic company selling a perishable product when customer demand is both uncertain and occurs in batches that must be fulfilled as a whole. The seller can price-discriminate between different sized batches by setting different unit prices. The problem is modeled as a stochastic optimal control problem to find an inventory-contingent dynamic pricing policy that maximizes the expected total revenues. We find the optimal pricing policy and prove several monotonicity results. First, we establish stochastic order conditions on the unit willingness-to-pay distributions that determine when quantity discounts or premiums take place for a batch purchase compared to a rapid sequence of purchases with the same total size. Second, we give sufficient conditions for prices to be monotonically decreasing or increasing in inventory. Third, we characterize the conditions for the perceived quantity discounts and premiums that result from comparing unit prices for different batch sizes under a particular inventory level.", "e:keyword": ["Dynamic pricing", "Batched demand", "Stochastic orders", "Dynamic programming applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1286", "e:abstract": "Motivated by the rising popularity of electronic appointment booking systems, we develop appointment scheduling models that take into account the patient preferences regarding when they would like to be seen. The service provider dynamically decides which appointment days to make available for the patients. Patients arriving with appointment requests may choose one of the days offered to them or leave without an appointment. Patients with scheduled appointments may cancel or not show up for the service. The service provider collects a “revenue” from each patient who shows up and incurs a “service cost” that depends on the number of scheduled appointments. The objective is to maximize the expected net “profit” per day. We begin by developing a static model that does not consider the current state of the scheduled appointments. We give a characterization of the optimal policy under the static model and bound its optimality gap. Building on the static model, we develop a dynamic model that considers the current state of the scheduled appointments, and we propose a heuristic solution procedure. In our computational experiments, we test the performance of our models under the patient preferences estimated through a discrete choice experiment that we conduct in a large community health center. Our computational experiments reveal that the policies we propose perform well under a variety of conditions.", "e:keyword": ["Appointment scheduling", "Healthcare management", "Markov decision process", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1287", "e:abstract": "This paper studies the problem of simultaneously locating trauma centers and helicopters. The standard approach to locating helicopters involves the use of helicopter busy fractions to model the random availability of helicopters. However, busy fractions cannot be estimated a priori in our problem because the demand for each helicopter cannot be determined until the trauma center locations are selected. To overcome this challenge, we endogenize the computation of busy fractions within an optimization problem. The resulting formulation has nonconvex bilinear terms in the objective, for which we develop an integrated method that iteratively solves a sequence of problem relaxations and restrictions. Specifically, we devise a specialized algorithm, called the shifting quadratic envelopes algorithm, that (1) generates tighter outer approximations than linear McCormick envelopes and (2) outperforms a Benders-like cut generation scheme. We apply our integrated method to the design of a nationwide trauma care system in Korea. By running a trace-based simulation on a full year of patient data, we find that the solutions generated by our model outperform several benchmark heuristics by up to 20%, as measured by an industry-standard metric: the proportion of patients successfully transported to a care facility within one hour. Our results have helped the Korean government to plan its nationwide trauma care system. More generally, our method can be applied to a class of optimization problems that aim to find the locations of both fixed and mobile servers when service needs to be carried out within a certain time threshold.", "e:keyword": ["Healthcare", "Hospitals and ambulance service", "Facilities/equipment planning", "Discrete location", "Integer programming", "Benders/decomposition algorithms", "Simulation", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1288", "e:abstract": "The fixed-charge transportation problem (FCTP) is a generalization of the transportation problem where an additional fixed cost is paid for sending a flow from an origin to a destination. We propose an iterated local search heuristic based on the utilization of reduced costs for guiding the restart phase. The reduced costs are obtained by applying a lower bounding procedure that computes a sequence of nondecreasing lower bounds by solving a three-index mathematical formulation of the problem strengthened with valid inequalities. The proposed method was tested on two sets of benchmark instances from the literature. The first set was used to evaluate the state-of-the-art heuristics for the problem; the proposed heuristic was able to provide new best-knownupper bounds on all 124 open instances. On the second set of instances, which was recently introduced for testing the currently best exact method for the problem, the new heuristic was able to provide provably good upper bounds within short computing times.", "e:keyword": ["Iterated local search", "Reduced costs", "Fixed charge transportation problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1289", "e:abstract": "A natural optimization model that formulates many online resource allocation problems is the online linear programming (LP) problem in which the constraint matrix is revealed column by column along with the corresponding objective coefficient. In such a model, a decision variable has to be set each time a column is revealed without observing the future inputs, and the goal is to maximize the overall objective function. In this paper, we propose a near-optimal algorithm for this general class of online problems under the assumptions of random order of arrival and some mild conditions on the size of the LP right-hand-side input. Specifically, our learning-based algorithm works by dynamically updating a threshold price vector at geometric time intervals, where the dual prices learned from the revealed columns in the previous period are used to determine the sequential decisions in the current period. Through dynamic learning, the competitiveness of our algorithm improves over the past study of the same problem. We also present a worst case example showing that the performance of our algorithm is near optimal.", "e:keyword": ["Online algorithms", "Linear programming", "Primal-dual", "Dynamic price update"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1290", "e:abstract": "We consider a problem involving estimation of a high-dimensional covariance matrix that is the sum of a diagonal matrix and a low-rank matrix, and making a decision based on the resulting estimate. Such problems arise, for example, in portfolio management, where a common approach employs principal component analysis (PCA) to estimate factors used in constructing the low-rank term of the covariance matrix. The decision problem is typically treated separately, with the estimated covariance matrix taken to be an input to an optimization problem. We propose <i>directed PCA</i>, an efficient algorithm that takes the decision objective into account when estimating the covariance matrix. Directed PCA effectively adjusts factors that would be produced by PCA so that they better guide the specific decision at hand. We demonstrate through computational studies that directed PCA yields significant benefit, and we prove theoretical results establishing that the degree of improvement over conventional PCA can be arbitrarily large.", "e:keyword": ["Principal component analysis", "Covariance matrix estimation", "High-dimensional data", "Portfolio management", "Convex optimization", "Decision analysis", "Stochastic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1291", "e:abstract": "We consider the classic continuous-review <i>N</i>stage serial inventory system with a homogeneous Poisson demand arrival process at the most downstream stage (Stage 1). Any <i>shipment</i> to each stage, regardless of its size, incurs a <i>positive</i> fixed setup cost and takes a <i>positive</i> constant lead time. The optimal policy for this system under the long-run average cost criterion is unknown. Finding a good worst-case performance guarantee remains an open problem. We tackle this problem by introducing a class of <i>modified echelon (r, Q) policies</i> that do not require <i>Q</i><sub><i>i</i> + 1</sub>/<i>Q</i><sub><i>i</i></sub> to be a positive integer: Stage <i>i</i> + 1 ships to Stage <i>i</i> based on its observation of the echelon inventory position at Stage <i>i</i>; if it is at or below <i>r</i><sub><i>i</i></sub> and Stage <i>i</i> + 1 has positive on-hand inventory, then a shipment is sent to Stage <i>i</i> to raise its echelon inventory position to <i>r</i><sub><i>i</i></sub> + <i>Q</i><sub><i>i</i></sub> <i>as close as possible</i>. We construct a heuristic policy within this class of policies, which has the following features: First, it has provably primitive-dependent performance bounds. In a two-stage system, the performance of the heuristic policy is guaranteed to be within (1 + <i>K</i><sub>1</sub>/<i>K</i><sub>2</sub>) times the optimal cost, where <i>K</i><sub>1</sub> is the downstream fixed cost and <i>K</i><sub>2</sub> is the upstream fixed cost. We also provide an alternative performance bound, which depends on efficiently computable optimal (<i>r, Q</i>) solutions to <i>N</i> single-stage systems but tends to be tighter. Second, the heuristic is simple, it is efficiently computable and it performs well numerically; it is even likely to outperform the optimal integer-ratio echelon (<i>r,Q</i>) policies when <i>K</i><sub>1</sub> is dominated by <i>K</i><sub>2</sub>. Third, the heuristic is asymptotically optimal when we take some dominant relationships between the setup or holding cost primitives at an upstream stage and its immediate downstream stage to the extreme, for example, when <i>h</i><sub>2</sub>/<i>h</i><sub>1</sub> (rightarrow) 0, where <i>h</i><sub>1</sub> is the downstream holding cost parameter and <i>h</i><sub>2</sub> is the upstream holding cost parameter.", "e:keyword": ["Multi-echelon", "Serial system", "Stochastic demand", "Performance evaluation", "(r", "Q) policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1292", "e:abstract": "Recommender systems have been widely used by online stores to suggest items of interest to users. These systems often identify a subset of items from a much larger set that best matches the user's interest. A key concern with existing approaches is <i>overspecialization</i>, which results in returning items that are too similar to each other. Unlike existing solutions that rely on diversity metrics to reduce similarity among recommended items, we propose using choice probability to measure the overall quality of a recommendation list, which unifies the desire to achieve both relevancy and diversity in recommendation. We first define the recommendation problem from the discrete choice perspective. We then model the problem under the multilevel nested logit model, which is capable of handling similarities between alternatives along multiple dimensions. We formulate the problem as a nonlinear binary integer programming problem and develop an efficient dynamic programming algorithm that solves the problem to optimum in <i>O</i>(<i>nKSR</i><sup>2</sup>) time, where <i>n</i> is the number of levels and <i>K</i> is the maximum number of children nests a nest can have in the multilevel nested logit model, <i>S</i> is the total number of items in the item pool, and <i>R</i> is the number of items wanted in recommendation.", "e:keyword": ["Recommender systems", "Discrete choice", "Relevancy", "Diversity", "Nested logit"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1293", "e:abstract": "Any institution that disseminates data in aggregated form has the duty to ensure that individual confidential information is not disclosed, either by not releasing data or by perturbing the released data while maintaining data utility. Controlled tabular adjustment (CTA) is a promising technique of the second type where a protected table that is close to the original one in some chosen distance is constructed. The choice of the specific distance shows a trade-off: although the Euclidean distance has been shown (and is confirmed here) to produce tables with greater “utility,” it gives rise to mixed integer quadratic problems (MIQPs) with pairs of linked semi-continuous variables that are more difficult to solve than the mixed integer linear problems corresponding to linear norms. We provide a novel analysis of perspective reformulations (PRs) for this special structure; in particular, we devise a <i>projected</i> PR (P<sup>2</sup>R), which is piecewise-conic but simplifies to a (nonseparable) MIQP when the instance is symmetric. We then compare different formulations of the CTA problem, showing that the ones based on P<sup>2</sup>R most often obtain better computational results.", "e:keyword": ["Mixed integer quadratic programming", "Perspective reformulation", "Data privacy", "Statistical disclosure control", "Tabular data protection", "Controlled tabular adjustment"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1294", "e:abstract": "We consider a monopolist who sells a set of products over a time horizon of <i>T</i> periods. The seller initially does not know the parameters of the products' linear demand curve, but can estimate them based on demand observations. We first assume that the seller knows nothing about the parameters of the demand curve, and then consider the case where the seller knows the expected demand under an incumbent price. It is shown that the smallest achievable revenue loss in <i>T</i> periods, relative to a clairvoyant who knows the underlying demand model, is of order <inline-formula id=\"IEq0001\"><mml:math id=\"IEq0001-mml\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msqrt><mml:mi>T</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> in the former case and of order log <i>T</i> in the latter case. To derive pricing policies that are practically implementable, we take as our point of departure the widely used policy called greedy iterated least squares (ILS), which combines sequential estimation and myopic price optimization. It is known that the greedy ILS policy itself suffers from incomplete learning, but we show that certain variants of greedy ILS achieve the minimum asymptotic loss rate. To highlight the essential features of well-performing pricing policies, we derive sufficient conditions for asymptotic optimality.", "e:keyword": ["Revenue management", "Pricing", "Sequential estimation", "Exploration-exploitation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1297", "e:abstract": "We consider a standard dynamic pricing problem with finite inventories, finite selling horizon, and stochastic demands, where the objective of the seller is to maximize total expected revenue. We introduce a simple improvement of the popular static price control known in the literature. The proposed heuristic only requires a single optimization at the beginning of the selling horizon and does <i>not</i> require any reoptimization at all. This provides an advantage over the potentially heavy computational burden of reoptimization, especially for <i>very large</i> applications with frequent price adjustments. In addition, our heuristic can be implemented in combination with a few reoptimizations to achieve a high-level revenue performance. This hybrid of real-time adjustment and reoptimization allows the seller to enjoy the benefit of reoptimization without overdoing it.", "e:keyword": ["Dynamic pricing", "Asymptotic analysis", "Network revenue management"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1298", "e:abstract": "We study an inventory system under periodic review when excess demand is lost. It is known (Huh et al. 2009) that the <i>best</i> base-stock policy is asymptotically optimal as the lost-sales penalty cost parameter grows. We now show that this result is robust in the following sense: Consider the base-stock level which is optimal in a <i>backordering system</i> (with a per-unit-per-period backordering cost) in which the backorder cost parameter is a function of the lost-sales parameter in the original system. Then there is a large family of functions (mapping the lost-sales cost parameter to the backorder cost parameter) such that the resulting base-stock policy is asymptotically optimal. We also demonstrate the robustness phenomenon through a second result. We consider the base-stock level which is optimal in a backordering system in which a unit of backorder is charged a penalty cost only once (such a system has been studied by Rosling). We show that this base-stock policy is also asymptotically optimal. Furthermore, we show that a modification suggested by Archibald of this base-stock level also results in an asymptotically optimal policy. Finally, we numerically test the performance of this heuristic policy for a wide spectrum of values for the lost-sales penalty cost parameter and illustrate the superior performance of Archibald's method.", "e:keyword": ["Inventory control", "Lost sales", "Base-stock policy", "Robustness", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1299", "e:abstract": "This paper considers a continuous-review, single-product, production-inventory system with a constant replenishment rate, compound Poisson demands, and lost sales. Two objective functions that represent metrics of operational costs are considered: (1) the sum of the expected discounted inventory holding costs and lost-sales penalties, both over an infinite time horizon, given an initial inventory level; and (2) the long-run time average of the same costs. The goal is to minimize these cost metrics with respect to the replenishment rate. It is, however, not possible to obtain closed-form expressions for the aforementioned cost functions directly in terms of positive replenishment rate (<i>PRR</i>). To overcome this difficulty, we construct a bijection from the PRR space to the space of positive roots of <i>Lundberg's fundamental equation</i>, to be referred to as the <i>Lundberg positive root</i> (<i>LPR</i>) space. This transformation allows us to derive closed-form expressions for the aforementioned cost metrics with respect to the LPR variable, in lieu of the PRR variable. We then proceed to solve the optimization problem in the LPR space and, finally, recover the optimal replenishment rate from the optimal LPR variable via the inverse bijection. For the special cases of constant or loss-proportional penalty and exponentially distributed demand sizes, we obtain simpler explicit formulas for the optimal replenishment rate.", "e:keyword": ["Compound Poisson arrivals", "Integro-differential equation", "Laplace transform", "Lundberg's fundamental equation", "Lost sales", "Production-inventory system", "Constant replenishment rate"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1300", "e:abstract": "We study a double-sided queue with batch arrivals and abandonment. There are two types of customers, patient ones who queue but may later abandon, and impatient ones who depart immediately if their order is not filled. The system matches units from opposite sides of the queue based on a first-come first-served policy. The model is particularly applicable to a class of alternative trading systems called crossing networks that are increasingly important in the operation of modern financial markets. We characterize, in closed form, the steady-state queue length distribution and the system-level average system time and fill rate. These appear to be the first closed-form results for a double-sided queuing model with batch arrivals and abandonment. For a customer who arrives to the system in steady state, we derive formulae for the expected fill rate and system time as a function of her order size and deadline. We compare these system- and customer-level results for our model that captures abandonment in aggregate, to simulation results for a system in which customers abandon after some random deadline. We find close correspondence between the predicted performance based on our analytical results and the performance observed in the simulation. Our model is particularly accurate in approximating the performance in systems with low fill rates, which are representative of crossing networks.", "e:keyword": ["Abandonment", "Double-sided queues", "Balking and reneging", "Batch arrivals", "Level crossing", "Crossing networks", "Queueing systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1301", "e:abstract": "We propose a dynamic traveling salesman problem (TSP) with stochastic arc costs motivated by applications, such as dynamic vehicle routing, in which the cost of a decision is known only probabilistically beforehand but is revealed dynamically before the decision is executed. We formulate this as a dynamic program (DP) and compare it to static counterparts to demonstrate the advantage of the dynamic paradigm over an a priori approach. We then apply approximate linear programming (ALP) to overcome the DP's curse of dimensionality, obtain a semi-infinite linear programming lower bound, and discuss its tractability. We also analyze a rollout version of the price-directed policy implied by our ALP and derive worst-case guarantees for its performance. Our computational study demonstrates the quality of a heuristically modified rollout policy using a computationally effective a posteriori bound.", "e:keyword": ["Traveling salesman problem", "Stochastic vehicle routing", "Approximate dynamic program", "Semi-infinite linear program"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1302", "e:abstract": "Ambulance redeployment is the practice of repositioning ambulance fleets in real time in an attempt to reduce response times to future calls. When redeployment decisions are based on real-time information on the status and location of ambulances, the process is called system-status management. An important performance measure is the long-run fraction of calls with response times over some time threshold. We construct a lower bound on this performance measure that holds for nearly any ambulance redeployment policy through comparison methods for queues. The computation of the bound involves solving a number of integer programs and then simulating a multiserver queue. This work originated when one of the authors was asked to analyze a response to a request-for-proposals (RFP) for ambulance services in a county in North America.", "e:keyword": ["System-status management", "Ambulance location", "Ambulance relocation", "Ambulance deployment", "Move-up", "Coupling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1303", "e:abstract": "We study an inventory system wherein a customer may leave the seller's market after experiencing an inventory stockout. Traditionally, researchers and practitioners assume a single penalty cost to model this customer behavior of stockout aversion. Recently, a stream of researchers explicitly model this customer behavior and support the traditional penalty cost approach. We enrich this literature by studying the statistical learning of service-dependent demand.We build and solve four models: a baseline model, where the seller can observe the demand distribution; a second model, where the seller cannot observe the demand distribution but statistically learns the demand distribution; a third model, where the seller can learn or pay to obtain the exact information of the demand distribution; and a fourth model, where demand in excess of available inventory is lost and unobserved. Interestingly, we find that all four models support the traditional penalty cost approach. This result confirms the use of a state-independent stockout penalty cost in the presence of demand learning. More strikingly, the first three models imply the same stockout penalty cost, which is larger than the stockout penalty cost implied by the last model.", "e:keyword": ["Service-dependent demand", "POMDP model", "Partial information", "Newsvendor model", "Dynamic programming/optimal control", "Markov models", "Inventory/production"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1305", "e:abstract": "Introduction of a new, innovative product or service is a fundamental problem that managers face regularly. The temporal sales pattern of such a product is often dynamically influenced by word of mouth as well as by marketing and distribution support. Appropriate marketing support strategies must be specified to induce the best sales pattern; however, the success of these strategies is heavily tied to the accessibility of the retail facilities, whether physical stores or virtual ones such as the Internet or phone. Managing the relation between accessibility and marketing support becomes more challenging when the firm faces a limited time, often due to short product life cycle. In this work, we present a general model for the joint design of the network of retail facilities and marketing strategies in the presence of word-of-mouth effects and limited time horizon. We develop exact and heuristic solution methods and provide insights on the structure of the optimal solution. Our solution methods identify the number and location of retail facilities to carry the product, as well as the proper mix of marketing channels and expenditures in them over time. Our results demonstrate that significant profit improvement can be achievable by jointly optimizing the design of the network of retail facilities with the choice of marketing strategies. Results of numerical experiments and an illustrative case study on opening Nespresso boutiques are also reported.", "e:keyword": ["Facility location", "Marketing mix", "Distribution of a new product", "New product diffusion", "Word of mouth dynamics", "Joint decision on location and marketing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1306", "e:abstract": "We analyze a finite horizon periodic review joint pricing and inventory management model for a firm that replenishes and sells a product under the <i>scarcity effect</i> of inventory. The demand distribution in each period depends negatively on the sales price and customer-accessible inventory level at the beginning of the period. The firm can withhold or dispose of its on-hand inventory to deal with the scarcity effect. We show that a customer-accessible-inventory-dependent order-up-to/dispose-down-to/display-up-to list-price policy is optimal. Moreover, the optimal order-up-to/display-up-to and list-price levels are decreasing in the customer-accessible inventory level. When the scarcity effect of inventory is sufficiently strong, the firm should display no positive inventory and deliberately make every customer wait. The analysis of two important special cases wherein the firm cannot withhold (or dispose of) inventory delivers sharper insights showing that the inventory-dependent demand drives both optimal prices and order-up-to levels down. In addition, we demonstrate that an increase in the operational flexibility (e.g., a higher salvage value or the inventory withholding opportunity) mitigates the demand loss caused by high excess inventory and increases the optimal order-up-to levels and sales prices. We also generalize our model by incorporating responsive inventory reallocation after demand realizes. Finally, we perform extensive numerical studies to demonstrate that both the profit loss of ignoring the scarcity effect and the value of dynamic pricing under the scarcity effect are significant.", "e:keyword": ["Joint pricing and inventory management", "Inventory-dependent demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1307", "e:abstract": "This paper describes an exact algorithm for solving a two-dimensional orthogonal packing problem with unloading constraints, which occurs as a subproblem of mixed vehicle routing and loading problems. The packing considered in this work is basically a feasibility problem involving a single bin. The problem is addressed through a decomposition approach wherein a branch-and-cut algorithm is designed for solving a one-dimensional relaxation of the original problem. When an integer solution is found in the branching tree, a subsidiary problem is solved to identify a two-dimensional packing that does not lead to any overlap and satisfies the unloading constraints. Cuts are added when the subsidiary problem proves to be infeasible. Several preprocessing techniques aimed at reducing the size of the solution space and uncovering infeasibility are also described. A numerical comparison with the best known exact method is reported at the end based on benchmark instances.", "e:keyword": ["Two-dimensional packing", "Unloading constraints", "Vehicle routing", "Branch-and-cut"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1308", "e:abstract": "Banking regulations set minimum levels of capital for banks. These requirements are generally formulated through a ratio of capital to <i>risk-weighted</i> assets. A risk-weighting scheme assigns a weight to each asset or category of assets and effectively functions as a linear constraint on a bank’s portfolio choice; it also changes the incentives for banks to hold various kinds of assets. In this paper, we investigate the design of risk weights to align regulatory and private objectives in a simple mean-variance framework for portfolio selection. By setting risk weights proportional to profitability rather than risk, the regulator can induce a bank to reduce its overall level of risk without distorting its asset mix. Because the regulator is unlikely to know the true profitability of assets, we introduce an adaptive formulation in which the regulator sets weights by observing a bank’s portfolio. The adaptive scheme converges to the same combination of weights and portfolio choice that would hold if the regulator knew the asset profitability. We also investigate other objectives, including steering banks to a target mix of assets, adding robustness, mitigating procyclicality, and reducing system-wide risk in a setting with multiple heterogeneous banks.", "e:keyword": ["Portfolio analysis", "Financial institutions", "Banks", "Quadratic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1309", "e:abstract": "This paper develops a model of short-range ballistic missile defense and uses it to study the performance of Israel's Iron Dome system. The deterministic base model allows for inaccurate missiles, unsuccessful interceptions, and civil defense. Model enhancements consider the trade-offs in attacking the interception system, the difficulties faced by militants in assembling large salvos, and the effects of imperfect missile classification by the defender. A stochastic model is also developed. Analysis shows that system performance can be highly sensitive to the missile salvo size, and that systems with higher interception rates are more “fragile” when overloaded. The model is calibrated using publically available data about Iron Dome's use during Operation Pillar of Defense in November 2012. If the systems performed as claimed, they saved Israel an estimated 1,778 casualties and $80 million in property damage, and thereby made preemptive strikes on Gaza about eight times less valuable to Israel. Gaza militants could have inflicted far more damage by grouping their rockets into large salvos, but this may have been difficult given Israel's suppression efforts. Counter-battery fire by the militants is unlikely to be worthwhile unless they can obtain much more accurate missiles.", "e:keyword": ["Warfare models", "Tactical ballistic missiles", "Counterinsurgency", "Hamas-Israel conflict", "Defense systems", "Ballistic missile defense", "Civil defense"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1310", "e:abstract": "We deal with the problem faced by a portfolio manager in charge of multiple accounts. We argue that because of market impact costs, this setting differs in several subtle ways from the classical (single account) case, with the key distinction being that the performance of each individual account typically depends on the trading strategies of other accounts, as well. We propose a novel, tractable approach for jointly optimizing the trading activities of all accounts and also splitting the associated market impact costs between the accounts. Our approach allows the manager to balance the conflicting objectives of maximizing the aggregate gains from joint optimization and distributing them across the accounts in an equitable way. We perform numerical studies that suggest that our approach outperforms existing methods employed in the industry or discussed in the literature.", "e:keyword": ["Portfolio optimization", "Multiportfolio", "Fairness", "Cost sharing", "Transaction costs", "Convex optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1311", "e:abstract": "This paper studies the stability of communication protocols that deal with transmission errors. We consider a coordination game between an informed sender and an uninformed receiver, who communicate over a noisy channel. The sender’s strategy, called a code, maps states of nature to signals. The receiver’s best response is to decode the received channel output as the state with highest expected receiver payoff. Given this decoding, an equilibrium or “Nash code” results if the sender encodes every state as prescribed. We show two theorems that give sufficient conditions for Nash codes. First, a receiver-optimal code defines a Nash code. A second, more surprising observation holds for communication over a binary channel, which is used independently a number of times, a basic model of information transmission: under a minimal “monotonicity” requirement for breaking ties when decoding, which holds generically, <i>every</i> code is a Nash code.", "e:keyword": ["Sender-receiver game", "Communication", "Noisy channel"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1312", "e:abstract": "Pedestrian dynamics plays an important role in public facility design and evacuation management. During an escape process from a large public space, crowd behavior is a collection of pedestrian exit/route choice behavior, and movement behavior. Modelling such an escape process is an extremely complex challenge. In this paper, an integrated macro-micro approach is developed to simulate the escape process. An analysis of the simulation reveals the mechanisms of the formation of crowd congestion and flow distribution. At the macroscopic level, a mathematical model, based on the concept of the dynamic user optimal (DUO) criterion, is formulated to describe the pedestrian exit/route choice behavior. A method based on the fundamental diagram and point-queuing theory is developed to estimate the pedestrian escape time. At the microscopic level, a modified social force model is adopted to formulate pedestrians' dynamic movements during the escape process. A solution algorithm is proposed to solve the macro-micro integrated model and a series of experiments are carried out to validate the proposed model. The simulation results agree with the extracted experimental data. Finally, the integrated model and algorithm are used to simulate the escape process in a large public place. The proposed approach is able to generate the bandwagon effect, bottleneck effect, and route choice patterns.", "e:keyword": ["Large public places", "Escape process", "Dynamic route choice", "Simulation", "Travel time estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1313", "e:abstract": "We consider the problem of <i>graph discovery</i> in settings where the graph topology is known and the edge weights are hidden. The setting consists of a weighted graph <i>G</i> with <i>n</i> vertices and <i>m</i> edges and with designated source <i>s</i> and destination <i>t</i>. We study two different <i>discovery</i> problems, namely, (i) edge weight discovery, where the goal is to discover all edge weights, and (ii) shortest path discovery, where the goal is to discover a shortest (<i>s</i>, <i>t</i>)-path. This discovery is done by means of <i>agents</i> that traverse different (<i>s</i>, <i>t</i>)-paths in multiple <i>rounds</i> and report back the total cost they incurred. Three <i>cost models</i> are considered, differing from each other in their approach to congestion effects. In particular, we consider congestion-free models as well as models with positive and negative congestion effects. We seek bounds on the number of rounds and the number of agents required to complete the edge weights or the shortest path discovery. Several results concerning such bounds for both directed and undirected graphs are established. Among these results, we show that (1) for undirected graphs, all edge weights can be discovered within a single round consisting of <i>m</i> agents, (2) discovering a shortest path in either undirected or directed acyclic graphs requires at least <i>m</i> − <i>n</i> + 1 agents, and (3) the edge weights in a directed acyclic graph can be discovered in <i>m</i> rounds with <i>m</i> + <i>n</i> − 2 agents under congestion-aware cost models. Our study introduces a new setting of graph discovery under uncertainty and provides fundamental understanding of the problem.", "e:keyword": ["Graph discovery", "Shortest path", "Congestion", "Cost sharing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1314", "e:abstract": "Distributionally robust optimization is a paradigm for decision making under uncertainty where the uncertain problem data are governed by a probability distribution that is itself subject to uncertainty. The distribution is then assumed to belong to an ambiguity set comprising all distributions that are compatible with the decision maker’s prior information. In this paper, we propose a unifying framework for modeling and solving distributionally robust optimization problems. We introduce standardized ambiguity sets that contain all distributions with prescribed conic representable confidence sets and with mean values residing on an affine manifold. These ambiguity sets are highly expressive and encompass many ambiguity sets from the recent literature as special cases. They also allow us to characterize distributional families in terms of several classical and/or robust statistical indicators that have not yet been studied in the context of robust optimization. We determine conditions under which distributionally robust optimization problems based on our standardized ambiguity sets are computationally tractable. We also provide tractable conservative approximations for problems that violate these conditions.", "e:keyword": ["Robust optimization", "Ambiguous probability distributions", "Conic optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1315", "e:abstract": "Random search algorithms are often used to solve discrete optimization-via-simulation (DOvS) problems. The most critical component of a random search algorithm is the sampling distribution that is used to guide the allocation of the search effort. A good sampling distribution can balance the trade-off between the effort used in searching around the current best solution (which is called exploitation) and the effort used in searching largely unknown regions (which is called exploration). However, most of the random search algorithms for DOvS problems have difficulties in balancing this trade-off in a seamless way. In this paper we propose a new scheme that derives a sampling distribution from a fast fitted Gaussian process based on previously evaluated solutions. We show that the sampling distribution has the desired properties and can automatically balance the exploitation and exploration trade-off. Furthermore, we integrate this sampling distribution into a random research algorithm, called a Gaussian process-based search (GPS) and show that the GPS algorithm has the desired global convergence as the simulation effort goes to infinity. We illustrate the properties of the algorithm through a number of numerical experiments.", "e:keyword": ["Optimization-via-simulation", "Exploitation and exploration", "Gaussian process-based search"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1316", "e:abstract": "When we use simulation to estimate the performance of a stochastic system, the simulation often contains input models that were estimated from real-world data; therefore, there is both simulation and input uncertainty in the performance estimates. In this paper, we provide a method to measure the overall uncertainty while simultaneously reducing the influence of simulation estimation error due to output variability. To reach this goal, a Bayesian framework is introduced. We use a Bayesian posterior for the input-model parameters, conditional on the real-world data, to quantify the input-parameter uncertainty; we propagate this uncertainty to the output mean using a Gaussian process posterior distribution for the simulation response as a function of the input-model parameters, conditional on a set of simulation experiments. We summarize overall uncertainty via a credible interval for the mean. Our framework is fully Bayesian, makes more effective use of the simulation budget than other Bayesian approaches in the stochastic simulation literature, and is supported with both theoretical analysis and an empirical study. We also make clear how to interpret our credible interval and why it is distinctly different from the confidence intervals for input uncertainty obtained in other papers.", "e:keyword": ["Input uncertainty", "Bayesian inference", "Credible interval", "Simulation output analysis", "Metamodel", "Gaussian process"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1317", "e:abstract": "Hospitals typically lack effective enterprise level strategic planning of bed and care resources, contributing to bed census levels that are statistically “out of control.” This system dysfunction manifests itself in bed block, surgical cancelation, ambulance diversions, and operational chaos. This is the classic <i>hospital admission scheduling and control</i> (HASC) problem, which has been addressed in its entirety only through inexact simulation-based search heuristics. This paper develops new analytical models of controlled hospital census that can, for the first time, be incorporated into a mixed-integer programming model to optimally solve the <i>strategic planning/scheduling</i> portion of the HASC. Our new solution method coordinates elective admissions with other hospital subsystems to reduce system congestion. We formulate a new Poisson-arrival-location model (PALM) based on an innovative stochastic location process that we developed and call the patient temporal resource needs model. We further extend the PALM approach to the class of deterministic controlled-arrival-location models (d-CALM) and develop linearizing approximations to stochastic blocking metrics. This work provides the theoretical foundations for an efficient scheduled admissions planning system as well as a practical decision support methodology to stabilize hospital census.", "e:keyword": ["Hospital admissions and bed management", "Stochastic patient flow modeling", "Mixed-integer programming", "Census smoothing", "Stochastic arrival-location models"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1318", "e:abstract": "Although stochastic programming is probably the most effective framework for handling decision problems that involve uncertain variables, it is always a costly task to formulate the stochastic model that accurately embodies our knowledge of these variables. In practice, this might require one to collect a large amount of observations, to consult with experts of the specialized field of practice, or to make simplifying assumptions about the underlying system. When none of these options seem feasible, a common heuristic has been to simply seek the solution of a version of the problem where each uncertain variable takes on its expected value (otherwise known as the solution of the mean value problem). In this paper, we show that when (1) the stochastic program takes the form of a two-stage mixed-integer stochastic linear programs, and (2) the uncertainty is limited to the objective function, the solution of the mean value problem is in fact robust with respect to the selection of a stochastic model. We also propose tractable methods that will bound the actual value of stochastic modeling: i.e., how much improvement can be achieved by investing more efforts in the resolution of the stochastic model. Our framework is applied to an airline fleet composition problem. In the three cases that are considered, our results indicate that resolving the stochastic model can not lead to more than a 7% improvement of expected profits, thus providing arguments against the need to develop these more sophisticated models.", "e:keyword": ["Robust optimization", "Value of stochastic solution", "Mean value problem", "Regret minimization", "Fleet mix optimization", "Programming", "Stochastic", "Inventory", "Uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1319", "e:abstract": "In school choice, children submit a preference ranking over schools to a centralized assignment algorithm, which takes into account schools’ priorities over children and uses randomization to break ties. One criticism of existing school choice mechanisms is that they tend to disperse communities, so children do not go to school with others from their neighborhood. We suggest improving community cohesion by implementing a <i>correlated lottery</i> in a given school choice mechanism: we find a convex combination of deterministic assignments that maintains the original assignment probabilities, thus maintaining choice but improving community cohesion. To analyze the gain in cohesion for a wide class of mechanisms, we first prove the following characterization, which may be of independent interest: any mechanism that, in the large market limit, is nonatomic, Bayesian incentive compatible, symmetric, and efficient within each priority class is a “lottery-plus-cutoff” mechanism. This means that the large market limit can be described as follows: given the distribution of preferences, every student receives an identically distributed lottery number, every school sets a lottery cutoff for each priority class, and a student is assigned to her most preferred school for which she meets the cutoff. This generalizes liu-pycia-2012 to allow arbitrary priorities. Using this, we derive analytic expressions for maximum cohesion under a large market approximation. We show that the benefit of lottery-correlation is greater when students’ preferences are more correlated. In practice, although the correlated-lottery implementation problem is NP-hard, we present a heuristic that does well. We apply this to real data from Boston elementary school choice 2012 and find that we can increase cohesion by 79% for kindergarten 1 (K1) and 37% for kindergarten 2 (K2) new families. Greater cohesion gain is possible (tripling cohesion for K1 and doubling for K2) if we reduce the choice menus on top of applying lottery correlation.", "e:keyword": ["Market design", "Matchings", "School choice"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1321", "e:abstract": "Bandwidth-sharing networks provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers in computer and communication systems, and can be used to develop traffic pricing/charging mechanisms. At the same time, such models are exciting from an operations research perspective because their analysis requires techniques from stochastic modeling and optimization.In this paper, we develop a framework to approximate bandwidth-sharing networks under the assumption that the number of users as well as the capacities of the system are large, and the assumption that the traffic that each user is allowed to submit is bounded above by some rate, which is standard in practice. We also assume that customers on each route in the network abandon according to exponential patience times. Under Markovian assumptions, we develop fluid and diffusion approximations, which are quite tractable: for most parameter combinations, the invariant distribution is multivariate normal, with mean and diffusion coefficients that can be computed in polynomial time as a function of the size of the network.", "e:keyword": ["Networks", "Limit theorems", "Markovian"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1322", "e:abstract": "We consider the information relaxation approach for calculating performance bounds for stochastic dynamic programs (DPs). This approach generates performance bounds by solving problems with relaxed nonanticipativity constraints and a penalty that punishes violations of these nonanticipativity constraints. In this paper, we study DPs that have a convex structure and consider gradient penalties that are based on first-order linear approximations of approximate value functions. When used with perfect information relaxations, these penalties lead to subproblems that are deterministic convex optimization problems. We show that these gradient penalties can, in theory, provide tight bounds for convex DPs and can be used to improve on bounds provided by other relaxations, such as Lagrangian relaxation bounds. Finally, we apply these results in two example applications: first, a network revenue management problem that describes an airline trying to manage seat capacity on its flights; and second, an inventory management problem with lead times and lost sales. These are challenging problems of significant practical interest. In both examples, we compute performance bounds using information relaxations with gradient penalties and find that some relatively easy-to-compute heuristic policies are nearly optimal.", "e:keyword": ["Dynamic programming", "Information relaxations", "Network revenue management", "Lost-sales inventory models"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1323", "e:abstract": "This paper introduces a framework for robustifying convex, law invariant risk measures. The robustified risk measures are defined as the worst case portfolio risk over neighborhoods of a reference probability measure, which represent the investors' beliefs about the distribution of future asset losses. It is shown that under mild conditions, the infinite dimensional optimization problem of finding the worst-case risk can be solved analytically and closed-form expressions for the robust risk measures are obtained. Using these results, robust versions of several risk measures including the standard deviation, the Conditional Value-at-Risk, and the general class of distortion functionals are derived. The resulting robust risk measures are convex and can be easily incorporated into portfolio optimization problems, and a numerical study shows that in most cases they perform significantly better out-of-sample than their nonrobust variants in terms of risk, expected losses, and turnover.", "e:keyword": ["Robust optimization", "Kantorovich distance", "Norm-constrained portfolio optimization", "Soft robust constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1324", "e:abstract": "This paper studies a cooperative game of inventory transshipment among multiple firms. In this game, firms first make their inventory decisions independently and then decide collectively how to transship excess inventories to satisfy unmet demands. In modeling transshipment, we use networks of firms as the primitive, which offer a richer representation of relationships among firms by taking the coalitions used in all previous studies as special cases. For any given cooperative network, we construct a dual price allocation under which the network is stable for any residual demands and supplies in the sense that no firms find it more profitable to form subnetworks. Under the allocation based on the marginal contribution of each firm to its network (called the MJW value), we show that various network structures such as complete, hub-spoke, and chain networks are stable only under certain conditions on residual amounts. Moreover, these conditions differ across network structures, implying that a network structure plays an important role in establishing the stability of a decentralized transshipment system. Finally, we consider the case when firms establish networks endogenously, and show that pairwise Nash stable networks underperform the corresponding networks in centralized systems.", "e:keyword": ["Games/group decisions", "Cooperative", "Networks", "Inventory"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1325", "e:abstract": "This paper analyzes a periodic-review, joint inventory and pricing control problem for a firm that faces stochastic, price-sensitive demand under a nonstationary environment with fixed ordering costs. Any unsatisfied demand is backlogged. The objective is to maximize expected profit over a finite selling horizon by coordinating the inventory and pricing decisions in each period. We show that for an additive demand model, an (<i>s</i>, <i>S</i>, <i>p</i>) policy is optimal when the expected revenue is quasi-concave in price, the inventory cost (of holding and/or backlogging) is quasi-convex, and the nonnegative random demand has a Pólya or uniform density function. For the special case with no fixed ordering cost, the optimality of a base stock list price policy is demonstrated for more general demand distributions and convex inventory cost. These sets of sufficient conditions generalize the existing conditions in the literature that require, for example, the demand and/or revenue functions to be concave or the model parameters to be stationary in time. Our generalization makes the structural results applicable to models broadly supported by economic theory and empirical data. In addition, our proof uses a distinct sequential optimization technique for iteratively establishing the quasi-<i>K</i>-concavity of dynamic optimal value functions.", "e:keyword": ["Inventory/production", "Uncertainy", "Stochastic", "General additive demand", "Marketing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1326", "e:abstract": "Retailers facing uncertain demand can use observed sales to update demand estimates. However, such learning is limited by the amount of inventory carried; when demand exceeds inventory (i.e., when a stock-out event occurs), a retailer in general cannot observe actual demand. We propose using observations on the <i>timing</i> of sales occurrences in a Bayesian fashion to learn about demand, and we analyze this learning method for a multiperiod newsvendor setting. We find that, as previously shown with the use of only stock-out <i>event</i> observations, the optimal order quantity with timing observations is greater than the optimal order quantity with full demand observations. We prove this result using a novel methodology from the statistics literature on comparison of experiments. Although the optimal over-ordering with timing observations tends to be less than that with only stock-out event observations in most cases, we do observe cases where the opposite is true. Such cases correspond to high demand uncertainty and low margins, where marginal learning from timing observations is significantly higher than using only a stock-out event. In an extensive numerical study we find that, on average and with respect to uncensored demand observations, the use of timing observations eliminates 76.1% of the loss in expected profit from using only stock-out event observations. We show that, for Poisson and normal demand with unknown mean, the proposed learning method is tractable as well as intuitively appealing: the information contained in the timing of sales occurrences is fully captured by a single number—the timing of stock-out. We also investigate checkpoint models in which the newsvendor can make observations only at predetermined times in a period, and illustrate its convergence to the models with timing and stock-out event observations.", "e:keyword": ["Bayesian inventory", "Lost sales", "Censored observations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1327", "e:abstract": "Trains’ movements on a railway network are regulated by official timetables. Deviations and delays occur quite often in practice, demanding fast rescheduling and rerouting decisions in order to avoid conflicts and minimize overall delay. This is the real-time train dispatching problem. In contrast with the classic “holistic” approach, we show how to decompose the problem into smaller subproblems associated with the line and the stations. This decomposition is the basis for a master-slave solution algorithm, in which the master problem is associated with the line and the slave problem is associated with the stations. The two subproblems are modeled as mixed integer linear programs, with specific sets of variables and constraints. Similarly to the classical Benders’ decomposition approach, slave and master communicate through suitable feasibility cuts in the variables of the master. Extensive tests on real-life instances from single and double-track lines in Italy showed significant improvements over current dispatching performances. A decision support system based on this exact approach has been in operation in Norway since February 2014 and represents one of the first operative applications of mathematical optimization to train dispatching.", "e:keyword": ["Train dispatching", "Railway optimization", "Discrete mathematics"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1328", "e:abstract": "This paper addresses two concerns with the state of the art in network revenue management with dependent demands. The first concern is that the basic attraction model (BAM), of which the multinomial logit (MNL) model is a special case, tends to overestimate demand recapture in practice. The second concern is that the choice-based deterministic linear program, currently in use to derive heuristics for the stochastic network revenue management problem, has an exponential number of variables. We introduce a generalized attraction model (GAM) that allows for partial demand dependencies ranging from the BAM to the independent demand model (IDM). We also provide an axiomatic justification for the GAM and a method to estimate its parameters. As a choice model, the GAM is of practical interest because of its flexibility to adjust product-specific recapture. Our second contribution is a new formulation called the sales-based linear program (SBLP) that works for the GAM. This formulation avoids the exponential number of variables in the earlier choice-based network RM (revenue management) approaches and is essentially the same size as the well-known LP formulation for the IDM. The SBLP should be of interest to revenue managers because it makes choice-based network RM problems tractable to solve. In addition, the SBLP formulation yields new insights into the assortment problem that arises when capacities are infinite. Together these contributions move forward the state of the art for network revenue management under customer choice and competition.", "e:keyword": ["Pricing", "Choice models", "Network revenue management", "Dependent demands", "Origin and destination", "Upsell", "Recapture"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1329", "e:abstract": "This paper reports a new test of intransitive choice using individual measurements of regret- and similarity-based intransitive models of choice under uncertainty. Our test is tailor-made and uses subject-specific stimuli. Despite these features, we observed only a few intransitivities. A possible explanation for the poor predictive performance of intransitive choice models is that they only allow for interactions between acts. They exclude within-act interactions by retaining the assumption that preferences are separable over states of nature. Prospect theory, which relaxes separability but retains transitivity, predicted choices better. Our data suggest that descriptively realistic models must allow for within-act interactions but may retain transitivity.", "e:keyword": ["Utility/preference", "Estimation", "Decision analysis", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1330", "e:abstract": "We propose a static service differentiation policy for a single-server queueing system serving homogeneous customers. We show that by randomly assigning customers different service grades with different service rates, the average waiting time can be reduced without affecting the mean service time. Such differentiation introduces more service time variability, but it also creates information that enables the implementation of service rate-based scheduling, which mitigates the increased variance and may even reduce the total waiting time. We provide conditions under which our static service differentiation reduces waiting, and further derive closed-form expressions for the optimal differentiation policy, which shows that both optimal service rates and allocation probabilities form geometric sequences. We illustrate our policy in the context of quality-based service domains, in which customers value service time but dislike waiting. Numerically, we find that providing differentiated service can improve system performance by 5% without any additional capacity.", "e:keyword": ["Service rate differentiation", "Service time variability", "Quality-based service", "State independent"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1331", "e:abstract": "A Searcher seeks to find a stationary Hider located at some point <i>H</i> (not necessarily a node) on a given network <i>Q</i>. The Searcher can move along the network from a given starting point at unit speed, but to actually <i>find</i> the Hider she must pass it <i>while moving at a fixed slower speed</i> (which may depend on the arc). In this “bimodal search game,” the payoff is the first time the Searcher passes the Hider while moving at her slow speed. This game models the search for a small or well hidden object (e.g., a contact lens, improvised explosive device, predator search for camouflaged prey). We define a bimodal Chinese postman tour as a tour of minimum time <i>δ</i> which traverses every point of every arc at least once in the slow mode. For trees and <i>weakly Eulerian</i> networks (networks containing a number of disjoint Eulerian cycles connected in a tree-like fashion) the value of the bimodal search game is <i>δ</i>/2. For trees, the optimal Hider strategy has full support on the network. This differs from traditional search games, where it is optimal for him to hide only at leaf nodes. We then consider the notion of a <i>lucky Searcher</i> who can also detect the Hider with a positive probability <i>q</i> even when passing him at her fast speed. This paper has particular importance for demining problems.", "e:keyword": ["Teams: games/group decisions", "Search/surveillance", "Tree algorithms: networks/graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1332", "e:abstract": "This paper provides a relaxation of the sufficient conditions and an extension of the structural results for partially observed Markov decision processes (POMDPs) obtained by Lovejoy in 1987. Sufficient conditions are provided so that the optimal policy can be upper and lower bounded by judiciously chosen myopic policies. These myopic policy bounds are constructed to maximize the volume of belief states where they coincide with the optimal policy. Numerical examples illustrate these myopic bounds for both continuous and discrete observation sets.", "e:keyword": ["POMDP", "Myopic policy upper and lower bounds", "Structural result", "Likelihood ratio dominance"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1333", "e:abstract": "We consider the problem of efficiently allocating the capacity of a number of service facilities (prone to congestion) to a set of users with private information regarding their willingness to pay for different combinations of throughput versus latency. Auction mechanisms can be used to schedule the service capacity of available facilities. However, the interdependency of users’ valuations implies that simple uniform price adjustment processes (e.g., tatonnement) either fail to effectively clear or are subject to strategic manipulation. In this paper, we propose an iterative auction design and show that (i) it is efficient (i.e., the auction closes with the allocation of service that maximizes the social welfare) and (ii) it is strategy-proof, that is, it is a dominant strategy for users to truthfully reveal their demand for service capacity throughout the auction.", "e:keyword": ["Auction design", "Congestible service", "Capacity allocation", "Incentive compatibility", "Efficiency"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1334", "e:abstract": "Theoretical studies of process flexibility designs have mostly focused on expected sales. In this paper, we take a different approach by studying process flexibility designs from the worst-case point of view. To study the worst-case performances, we introduce the plant cover indices (PCIs), defined by bottlenecks in flexibility designs containing a fixed number of products. We prove that given a flexibility design, a general class of worst-case performance measures can be expressed as functions of the design’s PCIs and the given uncertainty set. This result has several major implications. First, it suggests a method to compare the worst-case performances of different flexibility designs without the need to know the specifics of the uncertainty sets. Second, we prove that under symmetric uncertainty sets and a large class of worst-case performance measures, the long chain, a celebrated sparse design, is superior to a large class of sparse flexibility designs, including any design that has a degree of two on each of its product nodes. Third, we show that under stochastic demand, the classical Jordan and Graves (JG) index can be expressed as a function of the PCIs. Furthermore, the PCIs motivate a modified JG index that is shown to be more effective in our numerical study. Finally, the PCIs lead to a heuristic for finding sparse flexibility designs that perform well under expected sales and have lower risk measures in our computational study.", "e:keyword": ["Process flexibility", "Flexible production", "Capacity planning", "Robust optimization", "Worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1335", "e:abstract": "In today’s business environment, unpredictable economic and noneconomic forces can affect firms’ operational costs and discount factors, as well as demand. In this paper, we incorporate these uncertainties into a single-product, periodic-review, finite-horizon stochastic inventory system by modeling operational costs, discount factors, and demands as stochastic processes that evolve over time. We study three stockout protocols and establish conditions under which (<i>s, S</i>) inventory policies are optimal when discount factors, operational costs, and demands are stochastic and correlated both to one another and over time. Examples are provided to demonstrate nontrivial optimal policies in the absence of these sufficient conditions.", "e:keyword": ["Inventory policy", "Stochastic discount factor", "Multiple uncertainties"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1336", "e:abstract": "Comment on “Design of Risk Weights” by Paul Glasserman and Wanmo Kang.", "e:keyword": ["Finance", "Banks", "Quadratic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1338", "e:abstract": "Queueing networks models typically assume that the arrival process is exogenous and unaffected by admission control, scheduling policies, etc. In many situations, however, users choose the time of their arrival strategically, taking delay and other metrics into account. In this paper, we develop a framework to study such strategic arrivals into queueing networks. We study the population game wherein users strategically choose when to arrive at a parallel queueing network and upon arrival, which of the queues to join. The queues start service at given times, which can potentially be different. We characterize the (strategic) arrival process at each of the queues and the price of anarchy of the ensuing strategic arrival game. We then extend the analysis to multiple populations of users, each with a different cost metric. The equilibrium arrival profile and price of anarchy are derived. Finally, we extend this to general feedforward network architectures by modeling the arrival timing game as a two-stage extensive form game. We prove the existence and essential uniqueness of equilibria. We also study more specific network topologies, like tandem and trellis networks, and we derive the equilibrium arrival and routing profiles. We show that there exists an equivalent parallel queueing network that has the same equilibrium arrival profile. Thus, the price of anarchy of the arrival game is then implied by that of the parallel queueing network.", "e:keyword": ["Strategic arrivals", "Population games", "Game theory", "Queueing networks", "Games/group decisions", "Bidding/auctions", "Natural resources", "Energy", "Communications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1339", "e:abstract": "Extant models assume that awareness decline commences instantly. In contrast, we incorporate the possibility that awareness declines with a delay due to the memory for advertisements. To this end, we use delay differential equations to understand the evolution of awareness in the presence of ad memorability. This extended model generates optimal advertising policies that include the even spending policy, blitz policy, and various cyclic pulsing policies, depending on whether ad memorability exceeds a critical threshold. The extended model not only unifies the various patterns of advertising spending over time, but also augments the prior research by furnishing the optimality of pulsing advertising. Thus ad memorability could drive pulsing. We discuss the implications for practicing managers and identify avenues for future researchers.", "e:keyword": ["Marketing: advertising and media", "Optimal control", "Memory effects", "Awareness formation", "Pulsing advertising", "Hamiltonian", "Lambert W", "Delay differential equations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1340", "e:abstract": "Almost stochastic dominance allows small violations of stochastic dominance rules to avoid situations where most decision makers prefer one alternative to another but stochastic dominance cannot rank them. While the idea behind almost stochastic dominance is quite promising, it has not caught on in practice. Implementation issues and inconsistencies between integral conditions and their associated utility classes contribute to this situation. We develop generalized almost second-degree stochastic dominance and almost second-degree risk in terms of the appropriate utility classes and their corresponding integral conditions, and extend these concepts to higher degrees. We address implementation issues and show that generalized almost stochastic dominance inherits the appealing properties of stochastic dominance. Finally, we define convex generalized almost stochastic dominance to deal with risk-prone preferences. Generalized almost stochastic dominance could be useful in decision analysis, empirical research (e.g., in finance), and theoretical analyses of applied situations.", "e:keyword": ["Decision analysis", "Stochastic dominance", "Utility", "Risk", "Probability", "Distribution comparisons"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1341", "e:abstract": "In revenue management research and practice, demand models are used that describe how demand for a seller’s products depends on the decisions, such as prices, of that seller. Even in settings where the demand for a seller’s products also depends on decisions of other sellers, the models often do not explicitly account for such decisions. It has been conjectured in the revenue management literature that such monopoly models may incorporate the effects of competition, because the parameter estimates of the monopoly models are based on data collected in the presence of competition. In this paper we take a closer look at such a setting to investigate the behavior of parameter estimates and decisions if monopoly models are used in the presence of competition. We consider repeated pricing games in which two competing sellers use mathematical models to choose the prices of their products. Over the sequence of games, each seller attempts to estimate the values of the parameters of a demand model that expresses demand as a function only of its own price using data comprised only of its own past prices and demand realizations. We analyze the behavior of the sellers’ parameter estimates and prices under various assumptions regarding the sellers’ knowledge and estimation procedures, and we identify situations in which (a) the sellers’ prices converge to the Nash equilibrium associated with knowledge of the correct demand model, (b) the sellers’ prices converge to the cooperative solution, and (c) the sellers’ prices have many potential limit points that are neither the Nash equilibrium nor the cooperative solution and that depend on the initial conditions. We compare the sellers’ revenues at potential limit prices with their revenues at the Nash equilibrium and the cooperative solution, and we show that it is possible for sellers to be better off when using a monopoly model than at the Nash equilibrium.", "e:keyword": ["Revenue management", "Pricing", "Competition", "Misspecification", "Parameter estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1342", "e:abstract": "Debris management is one of the most time consuming and complicated activities among post-disaster operations. Debris clearance is aimed at pushing the debris to the sides of the roads so that relief distribution and search-and-rescue operations can be maintained in a timely manner. Given the limited resources, uncertainty, and urgency during disaster response, efficient and effective planning of debris clearance to achieve connectivity between relief demand and supply is important. In this paper, we define the stochastic debris clearance problem (SDCP), which captures post-disaster situations where the limited information on the debris amounts along the roads is updated as clearance activities proceed. The main decision in SDCP is to determine a sequence of roads to clear in each period such that benefit accrued by satisfying relief demand is maximized. To solve SDCP to optimality, we develop a partially observable Markov decision process model. We then propose a heuristic based on a continuous-time approximation, and we further reduce the computational burden by applying a limited look ahead on the search tree and heuristic pruning. The performance of these approaches is tested on randomly generated instances that reflect various geographical and information settings, and instances based on a real-world earthquake scenario. The results of these experiments underline the importance of applying a stochastic approach and indicate significant improvements over heuristics that mimic the current practice for debris clearance.", "e:keyword": ["Debris clearance", "Online and stochastic networks", "Continuous-time approximations", "Partially observable Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.2014.1343", "e:abstract": "Given uncertain popularity of new products by location, fast fashion retailer Zara faces a trade-off. Large initial shipments to stores reduce lost sales in the critical first days of the product life cycle, but maintaining stock at the warehouse allows restocking flexibility once initial sales are observed. In collaboration with Zara, we develop and test a decision support system featuring a data-driven model of forecast updating and a dynamic optimization formulation for allocating limited stock by location over time. A controlled field experiment run worldwide with 34 articles during the 2012 season showed an increase in total average season sales by approximately 2% and a reduction in the number of unsold units at the end of the regular selling season by approximately 4%.", "e:keyword": ["Field experiment", "Retailing", "Apparel industry", "Inventory control", "Demand learning"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1344", "e:abstract": "We consider a firm that can use one of several costly learning modes to dynamically reduce uncertainty about the unknown value of a project. Each learning mode incurs cost at a particular rate and provides information of a particular quality. In addition to dynamic decisions about its learning mode, the firm must decide when to stop learning and either invest or abandon the project. Using a continuous-time Bayesian framework, and assuming a binary prior distribution for the project’s unknown value, we solve both the discounted and undiscounted versions of this problem. In the undiscounted case, the optimal learning policy is to choose the mode that has the smallest cost per signal quality. When the discount rate is strictly positive, we prove that an optimal learning and investment policy can be summarized by a small number of critical values, and the firm only uses learning modes that lie on a certain convex envelope in cost-rate-versus-signal-quality space. We extend our analysis to consider a firm that can choose multiple learning modes simultaneously, which requires the analysis of both investment timing and dynamic subset selection decisions. We solve both the discounted and undiscounted versions of this problem and explicitly identify sets of learning modes that are used under the optimal policy.", "e:keyword": ["Optimal control", "Optimal stopping", "Costly learning", "Bayesian sequential hypothesis testing", "Subset selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1345", "e:abstract": "We examine the interplay between two important decisions that impact environmental performance in a production setting: inspections performed by a regulator and noncompliance disclosure by a production firm. To preempt the penalty that will be levied once a compliance violation is discovered in an inspection, the firm dynamically decides whether it should disclose a random occurrence of noncompliance. Anticipating this, the regulator determines inspection frequency and penalty amounts to minimize environmental and social costs, performing either random inspections or periodic inspections. We study this problem by developing a novel analytical framework that combines features from reliability theory and law enforcement economics. We find that, contrary to common belief, surprising the firm with random inspections is not always preferred to inspecting the firm periodically according to a set schedule. We also find that the firm’s opportunistic disclosure timing behavior may lead to a partial disclosure equilibrium in which the substitutable relationship between inspection intensity and penalty is reversed; a threat of increased penalty is accompanied by more frequent inspections.", "e:keyword": ["Environment", "Reliability: inspection", "Games/group decisions: noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1346", "e:abstract": "Many service systems exhibit service slowdowns when the system is congested. Our goal in this paper is to investigate this phenomenon and its effect on system performance. We modify the Erlang-A model to account for service slowdowns and carry out the performance analysis in the quality- and efficiencydriven (QED) regime. We find that when the load sensitivity is low, the system can achieve QED performance, but the square-root staffing parameter requires an adjustment to achieve the same performance as an ordinary Erlang-A queue. When the load sensitivity is high, the system alternates randomly between a QED and an efficiency-driven (ED) regime performance levels, a phenomenon that we refer to as <i>bistability</i>. We analyze how the system scale and model parameters affect the bistability phenomenon and propose an admission control policy to avoid ED performance.", "e:keyword": ["Service systems", "Halfin-Whitt regime", "QED", "Erlang-A model", "State-dependent queues", "Load-dependent queues", "Bistability"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1347", "e:abstract": "We consider a periodic-review inventory system in which <i>N</i> non-identical retailers replenish from a warehouse, which further replenishes from an outside vendor with ample supply. Each facility faces Poisson demand and replenishes according to a base-stock policy in a fixed time interval. Fixed costs are incurred for placing an order. The warehouse fills the retailers’ orders in the same sequence as the occurrence of the demand at the retailers. The objective is to minimize the average system cost per period. This paper develops an evaluation scheme and provides a method to obtain the optimal base-stock levels and reorder intervals. Specifically, with fixed reorder intervals, we show that the optimal base-stock levels can be obtained by generalizing the result in the literature. To find the optimal reorder intervals, we first allocate the total system cost to each facility and then construct a lower bound to the allocated facility cost. These lower bound functions, which are separable functions of reorder intervals, can be used to derive bounds for the optimal reorder intervals. The key to tightening the bounds is to obtain a near-optimal total cost. Thus, we propose a simple heuristic that modifies the algorithm that solves the deterministic counterpart. The results of numerical studies suggest that the optimal reorder intervals tend to satisfy integer-ratio relationships and that the suggested heuristic can generate effective integer-ratio policies for large systems.", "e:keyword": ["Multi-echelon", "Inventory/production", "Stochastic", "Approximations/heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1348", "e:abstract": "Flows over time provide a natural and convenient description for the dynamics of a continuous stream of particles traveling from a source to a sink in a network, allowing to track the progress of each infinitesimal particle along time. A basic model for the propagation of flow is the so-called <i>fluid queue model</i> in which the time to traverse an edge is composed of a flow-dependent waiting time in a queue at the entrance of the edge plus a constant travel time after leaving the queue. In a dynamic network routing game each infinitesimal particle is interpreted as a player that seeks to complete its journey in the least possible time. Players are forward looking and anticipate the congestion and queuing delays induced by others upon arrival to any edge in the network. Equilibrium occurs when each particle travels along a shortest path.This paper is concerned with the study of equilibria in the fluid queue model and provides a constructive proof of existence and uniqueness of equilibria in single origin-destination networks with piecewise constant inflow rate. This is done through a detailed analysis of the underlying static flows obtained as derivatives of a dynamic equilibrium. Furthermore, for multicommodity networks, we give a general nonconstructive proof of existence of equilibria when the inflow rates belong to <i>L<sup>p</sup></i>.", "e:keyword": ["Dynamic equilibrium", "Flows over time", "Fluid queues", "Congestion", "Networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1349", "e:abstract": "We consider the search for a target whose precise location is uncertain. The search region is divided into grid cells, and the searcher decides which cell to visit next and whether to search it quickly or slowly. A quick search of a cell containing the target may damage it, resulting in a failed search, or it may locate the target safely. If the target is not in the cell, the search continues over the remaining cells. If a slow search is performed on a cell, then the search ends in failure with a fixed probability regardless of whether or not the target is in that cell (e.g., because of enemy fire while performing the slow search). If the slow search survives this failure possibility, then the search ends in success if the target is in that cell; otherwise, the search continues over the remaining cells. We seek to minimize the probability of the search ending in failure and consider two types of rules for visiting cells: the unconstrained search, in which the searcher may visit cells in any order, and the constrained search, in which the searcher may only visit adjacent cells (e.g., up, down, left, or right of cells already visited). We prove that the optimal policy for the unconstrained search is to search quickly some initial set of cells with the lowest probabilities of containing the target before slowly searching the remaining cells in decreasing order of probabilities. For the special case in which a quick search on a cell containing the target damages it with certainty, the optimal policy is to search all cells slowly, in decreasing order of probabilities. We use the optimal solution of the unconstrained search in a branch-and-bound optimal solution algorithm for the constrained search. For larger instances, we evaluate heuristics and approximate dynamic programming approaches for finding good solutions.", "e:keyword": ["Optimal search", "Markov decision process", "Approximate dynamic programming", "Branch-and-bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1350", "e:abstract": "The construction of a multiattribute utility function is an important step in decision analysis. One of the most widely used conditions for constructing the utility function is the assumption of mutual preferential independence where trade-offs among any subset of the attributes do not depend on the instantiations of the remaining attributes. Mutual preferential independence asserts that ordinal preferences can be represented by an additive function of the attributes. This paper derives the most general form of a multiattribute utility function that (i) exhibits mutual preferential independence and (ii) is strictly increasing with each argument at the maximum value of the complement attributes. We show that a multiattribute utility function satisfies these two conditions if and only if it is an Archimedean combination of univariate utility assessments. This result enables the construction of multiattribute utility functions that satisfy additive ordinal preferences using univariate utility assessments and a single generating function. We also provide a nonparametric approach for estimating the generating function of the Archimedean form by iteration.", "e:keyword": ["Multiattribute utility", "Archimedean utility copula", "Preferential independence"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1351", "e:abstract": "The 2007–2009 financial turmoil highlighted the need for more active management of credit portfolios. After measuring portfolio credit risk, an important step toward active risk management is to measure risk contributions of individual obligors to the overall risk of the portfolio. In practice, value-at-risk is often used as a risk measure for credit portfolios, and it can be decomposed into a sum of the risk contributions of individual obligors. Estimation of these risk contributions is computationally challenging, mainly because they are expectations conditioned on a rare event. In this paper, we tackle this computational problem by developing a restricted importance sampling (RIS) method for a class of conditional-independence credit risk models, where defaults of obligors are conditionally independent given an appropriately chosen random vector. We propose fast estimators for risk contributions and their confidence intervals. Furthermore, we study the incorporation of traditional importance sampling methods into the RIS method to further improve its efficiency for the widely used Gaussian copula model. Numerical examples show that the proposed method works well.", "e:keyword": ["Credit risk", "Risk contributions", "Simulation", "Importance sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1352", "e:abstract": "We clarify that Theorem 4 in Kanet (2014) [Kanet JJ (2014) <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/opre.2013.1253\">One-machine sequencing to minimize total tardiness: A fourth theorem for Emmons.</ext-link> <i>Oper. Res.</i> 62(2):345–347] is not incremental to Theorem 3 of Emmons (1969) [Emmons H (1969) <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/opre.17.4.701\">One-machine sequencing to minimize certain functions of job tardiness</ext-link>. <i>Oper. Res.</i> 17(4):701–715.] for the single-machine total tardiness problem.", "e:keyword": ["Production/scheduling", "Deterministic", "Single machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1353", "e:abstract": "We analyzed the planning problem for HIV screening, testing, and care. This problem consists of determining the optimal fraction of patients to be screened in every period as well as the optimum staffing level at each part of the healthcare system to maximize the total health benefits to the patients measured by quality-adjusted life-years (QALYs) gained. We modeled this problem as a nonlinear mixed integer programming program comprising disease progression (the transition of the patients across health states), system dynamics (the flow of patients in different health states across various parts of the healthcare delivery system), and budgetary and capacity constraints. We applied the model to the Greater Los Angeles (GLA) station in the Veterans Health Administration system. We found that a Centers for Disease Control and Prevention recommended routine screening policy in which all patients visiting the system are screened for HIV irrespective of risk factors may not be feasible because of budgetary constraints. Consequently, we used the model to develop and evaluate managerially relevant policies within existent capacity and budgetary constraints to improve upon the current risk based screening policy of screening only high risk patients. Our computational analysis showed that the GLA station can achieve substantial increase (20% to 300%) in the QALYs gained by using these policies over risk based screening. The GLA station has already adapted two of these policies that could yield better patient health outcomes over the next few years. In addition, our model insights have influenced the decision making process at this station.", "e:keyword": ["Planning", "Community", "Healthcare", "Diagnosis", "Treatment", "Programming", "Nonlinear", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1354", "e:abstract": "We consider facility location problems where the demand is continuously and uniformly distributed over a convex polygon with <i>m</i> vertices in the rectilinear plane, <i>n</i> facilities are already present, and the goal is to find an optimal location for an additional facility. Based on an analysis of structural properties of incremental Voronoi diagrams, we develop polynomial exact algorithms for five conditional location problems. The developed methodology is applicable to a variety of other facility location problems with continuous demand. Moreover, we briefly discuss the Euclidean case.", "e:keyword": ["Continuous facilities location", "Voronoi diagrams", "Median problem", "Market share problem", "Covering problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1355", "e:abstract": "We consider assortment and price optimization problems under the <i>d</i>-level nested logit model. In the assortment optimization problem, the goal is to find the revenue-maximizing assortment of products to offer, when the prices of the products are fixed. Using a novel formulation of the <i>d</i>-level nested logit model as a tree of depth <i>d</i>, we provide an efficient algorithm to find the optimal assortment. For a <i>d</i>-level nested logit model with <i>n</i> products, the algorithm runs in <i>O</i>(<i>d n</i> log <i>n</i>) time. In the price optimization problem, the goal is to find the revenue-maximizing prices for the products, when the assortment of offered products is fixed. Although the expected revenue is not concave in the product prices, we develop an iterative algorithm that generates a sequence of prices converging to a stationary point. Numerical experiments show that our method converges faster than gradient-based methods, by many orders of magnitude. In addition to providing solutions for the assortment and price optimization problems, we give support for the <i>d</i>-level nested logit model by demonstrating that it is consistent with the random utility maximization principle and equivalent to the elimination by aspects model.", "e:keyword": ["Customer choice model", "Multi-level nested logit", "Assortment planning", "Price optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1356", "e:abstract": "Hong (2009) [Hong LJ (2009) Estimating quantile sensitivities. <i>Oper. Res.</i> 57(1):118-130.] introduced a general framework based on probability sensitivities and a conditional expectation relationship for estimating quantile sensitivities by infinitesimal perturbation analysis (IPA). We present an alternative more direct derivation of the IPA estimators that leads to simplified proofs for strong consistency and convergence rate of the unbatched estimator, and strong consistency and a central limit theorem for the batched estimator.", "e:keyword": ["Quantile", "Gradient estimation", "Monte Carlo simulation", "Sensitivity analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1357", "e:abstract": "The <i>pq</i>-relaxation for the pooling problem can be constructed by applying McCormick envelopes for each of the bilinear terms appearing in the so-called <i>pq</i>-formulation of the pooling problem. This relaxation can be strengthened by using piecewise-linear functions that over- and under-estimate each bilinear term. Although there is a significant amount of empirical evidence to show that such piecewise-linear relaxations, which can be written as mixed-integer linear programs (MILPs), yield good bounds for the pooling problem, to the best of our knowledge, no formal result regarding the quality of these relaxations is known. In this paper, we prove that the ratio of the upper bound obtained by solving piecewise-linear relaxations (objective function is maximization) to the optimal objective function value of the pooling problem is at most <i>n</i>, where <i>n</i> is the number of output nodes. Furthermore for any <i>ϵ</i> > 0 and for any piecewise-linear relaxation, there exists an instance where the ratio of the relaxation value to the optimal value is at least <i>n</i> − <i>ϵ</i>. This analysis naturally yields a polynomial-time <i>n</i>-approximation algorithm for the pooling problem. We also show that if there exists a polynomial-time approximation algorithm for the pooling problem with guarantee better than <i>n</i><sup>1−<i>ϵ</i></sup> for any <i>ϵ</i> > 0, then NP-complete problems have randomized polynomial-time algorithms. Finally, motivated by the approximation algorithm, we design a heuristic that involves solving an MILP-based restriction of the pooling problem. This heuristic is guaranteed to provide solutions within a factor of <i>n</i>. On large-scale test instances and in significantly lesser time, this heuristic provides solutions that are often orders of magnitude better than those given by commercial local and global optimization solvers.", "e:keyword": ["Integer", "Heuristic", "Nonlinear", "Networks/graphs", "Multicommodity"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1358", "e:abstract": "Mismatch between supply and demand when the uncertainty of the demand is high and the supply lead time is relatively long, such as seasonal good markets, can result in high overstocking and understocking costs. In this paper we propose transshipment as a powerful mechanism to mitigate the mismatch between the supply and demand. We consider a finite horizon multi-period inventory system where in each period two retailers have the option to replenish their inventory from a supplier (if there is any supply) or via transshipment from the other retailer. Each retailer observes nonnegative stochastic demand with general distribution in each period and incurs overstocking/understocking costs as well as costs for replenishment and transshipment that may be time dependent. We study a stochastic control problem where the objective is to determine the optimal joint replenishment and transshipment policies so as to minimize the total expected cost over the season. We characterize the structure of the optimal policy and show that unlike the known order-up-to level inventory policy, the optimal ordering policy in each period is determined based on two switching curves. Similarly, the optimal transshipment policy is also identified by two switching curves. These four curves together partition the optimal joint ordering and transshipment polices to eight regions where in each region the optimal policy is an order-up-to-curve policy. We demonstrate that the structure of the optimal policy holds for any known sequence and combination of ordering and transshipment over time.", "e:keyword": ["Inventory management", "Transshipment", "Replenishment", "Lost sales", "Stochastic optimization", "Finite horizon", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1359", "e:abstract": "Alex Green was a pioneering operations analyst/researcher for the U.S. Army Air Force during World War II. His February 1945 operations analysis “Report on the Combat Performance of the Remote Control Turrets of B-29 Aircraft” was classified and buried for 70 years. Stationed in the China-Burma-India theatre and addressing a problem of combat losses posed by General Curtis LeMay, Green used written reports and interviews to draw conclusions regarding direction of enemy attack on the B-29s, opposite those of a large stateside simulation study. Resulting in LeMay’s changes in B-29 flight formations and frontal armaments, his report also addressed B-29 gun dispersion adjustments and modifications to the analog computer in the plane’s nose. This paper examines how Green drew his conclusions under wartime conditions before digital computers. Apart from the extraordinary advances in computer technology, much of his methodology is still relevant today and a part of operations research (OR). This paper offers a window into the origins of OR and remarkable efforts of its pioneers.", "e:keyword": ["Defense systems", "Force effectiveness", "Tactics/strategy", "History of OR"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1360", "e:abstract": "We study a situation where several independent service providers collaborate by complete pooling of their resources and customer streams into a joint service system. These service providers may represent such diverse organizations as hospitals that pool beds or maintenance firms that pool repairmen. We model the service systems as Erlang delay systems (<i>M/M/s</i> queues) that face a fixed cost rate per server and homogeneous delay costs for waiting customers. We examine rules to fairly allocate the collective costs of the pooled system amongst the participants by applying concepts from cooperative game theory. We consider both the case where players’ numbers of servers are exogenously given and the scenario where any coalition picks an optimal number of servers. By exploiting new analytical properties of the continuous extension of the classic Erlang delay function, we provide sufficient conditions for the games under consideration to possess a core allocation (i.e., an allocation that gives no group of players an incentive to split off and form a separate pool) and to admit a population monotonic allocation scheme (whereby adding extra players does not make anyone worse off). This is not guaranteed in general, as illustrated via examples.", "e:keyword": ["Game theory", "Queueing theory", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1361", "e:abstract": "Random assignment, typically seen as the standard in controlled trials, aims to make experimental groups statistically equivalent before treatment. However, with a small sample, which is a practical reality in many disciplines, randomized groups are often too dissimilar to be useful. We propose an approach based on discrete linear optimization to create groups whose discrepancy in their means and variances is several orders of magnitude smaller than with randomization. We provide theoretical and computational evidence that groups created by optimization have exponentially lower discrepancy than those created by randomization and that this allows for more powerful statistical inference.", "e:keyword": ["Experimental design", "Clinical trials", "Partitioning problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1363", "e:abstract": "We investigate the influence of different algorithmic choices on the approximation ratio in selfish scheduling. Our goal is to design local policies that minimize the inefficiency of resulting equilibria. In particular, we design optimal coordination mechanisms for unrelated machine scheduling, and improve the known approximation ratio from Θ(<i>m</i>) to Θ(log <i>m</i>), where <i>m</i> is the number of machines.A <i>local</i> policy for each machine orders the set of jobs assigned to it only based on parameters of those jobs. A <i>strongly local</i> policy only uses the processing time of", "e:keyword": ["Analysis of algorithms", "Approximations/heuristic", "Production/scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1364", "e:abstract": "We study a sequential model of Bayesian social learning in networks in which agents have heterogeneous preferences, and neighbors tend to have similar preferences—a phenomenon known as homophily. We find that the density of network connections determines the impact of preference diversity and homophily on learning. When connections are sparse, diverse preferences are harmful to learning, and homophily may lead to substantial improvements. In contrast, in a dense network, preference diversity is beneficial. Intuitively, diverse ties introduce more independence between observations while providing less information individually. Homophilous connections individually carry more useful information, but multiple observations become redundant.", "e:keyword": ["Network/graphs", "Probability", "Games/group decisions"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1365", "e:abstract": "In recent years, decision rules have been established as the preferred solution method for addressing computationally demanding, multistage adaptive optimization problems. Despite their success, existing decision rules (a) are typically constrained by their a priori design and (b) do not incorporate in their modeling adaptive binary decisions. To address these problems, we first derive the structure for optimal decision rules involving continuous and binary variables as piecewise linear and piecewise constant functions, respectively. We then propose a methodology for the optimal design of such decision rules that have a finite number of pieces and solve the problem robustly using mixed-integer optimization. We demonstrate the effectiveness of the proposed methods in the context of two multistage inventory control problems. We provide global lower bounds and show that our approach is (i) practically tractable and (ii) provides high quality solutions that outperform alternative methods.", "e:keyword": ["Adaptive optimization", "Decision rules", "Mixed-integer optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1366", "e:abstract": "The price-setting newsvendor problem, which models the economic trade-offs associated with uncertain demand of a perishable product, is fundamental to supply chain analysis. However, in settings such as agriculture, there is significant economic risk associated with supply uncertainty. We analyze how risk aversion and the source of uncertainty—demand and/or supply—affect tractability and optimal decisions. We find that concavity of the objective function is preserved under the introduction of risk aversion if the source of uncertainty is demand, but it is not necessarily preserved if the source of uncertainty is supply. We identify a structural difference that explains this result, and show that this difference can lead to opposing directional effects of risk aversion on optimal decisions.", "e:keyword": ["Inventory", "Production", "Marketing", "Pricing", "Supply uncertainty", "Risk aversion"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1367", "e:abstract": "We propose an alternative approach for studying queues based on robust optimization. We model the uncertainty in the arrivals and services via polyhedral uncertainty sets, which are inspired from the limit laws of probability. Using the generalized central limit theorem, this framework allows us to model heavy-tailed behavior characterized by bursts of rapidly occurring arrivals and long service times. We take a worst-case approach and obtain closed-form upper bounds on the system time in a multi-server queue. These expressions provide qualitative insights that mirror the conclusions obtained in the probabilistic setting for light-tailed arrivals and services and generalize them to the case of heavy-tailed behavior. We also develop a calculus for analyzing a network of queues based on the following key principles: (a) the departure from a queue, (b) the superposition, and (c) the thinning of arrival processes have the same uncertainty set representation as the original arrival processes. The proposed approach (a) yields results with error percentages in single digits relative to simulation, and (b) is to a large extent insensitive to the number of servers per queue, network size, degree of feedback, and traffic intensity; it is somewhat sensitive to the degree of diversity of external arrival distributions in the network.", "e:keyword": ["Queueing theory", "Robust optimization", "Heavy tails", "Queueing networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1368", "e:abstract": "We analyze a noncooperative bargaining game with a general coalition structure. In each period an opportunity for a feasible coalition to form arises according to a stochastic process, and a randomly selected agent in the coalition makes a take-it-or-leave-it offer to the other agents in the coalition. We develop a new technique based on convex programming to characterize the unique stationary equilibrium payoff of the game. We apply the framework to various settings including trading networks with middlemen and cooperation networks with overlapping communities. In these applications, feasible coalitions are determined by an underlying network structure. We study the effect of the underlying network on the pattern of trade and show how an agent’s payoff is related to her position in the network.", "e:keyword": ["Network games", "Noncooperative bargaining", "Coalition formation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1369", "e:abstract": "In this paper, we show how any model with a general <i>shelf-age-dependent</i> holding cost and <i>delay-dependent</i> backlogging cost structure may be transformed into an equivalent model in which all expected inventory costs are <i>level dependent</i>. We develop our equivalency results, first, for periodic review models with full backlogging of stockouts. These equivalency results permit us to characterize the optimal procurement strategy in various settings and to adopt known algorithms to compute such strategies. For models in which all or part of stockouts are lost, we show that the addition of any shelf-age and delay-dependent cost structure does not complicate the structure of the model beyond what is required under the simplest, i.e., linear, holding and backlogging costs. We proceed to show that our results carry over to continuous review models, with demands generated by compound renewal processes; the continuous review models with shelf-age and delay-dependent carrying and backlogging costs are shown to be equivalent to periodic review models with convex level-dependent inventory cost functions.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Polices", "Operating characteristics", "Probability", "Renewal processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1370", "e:abstract": "Process flexibility has been widely applied in many industries as a competitive strategy to improve responsiveness to demand uncertainty. An important flexibility concept is the long chain proposed by Jordan and Graves (1995) [Jordan WC, Graves SC (1995) Principles on the benefits of manufacturing process flexibility. <i>Management Sci.</i> 41(4):577–594.]. The effectiveness of the long chain has been investigated via numerical as well as theoretical analysis for specific probability distributions of the random demand. In this paper, we obtain in closed form a distribution-free bound on the ratio of the expected sales of the long chain relative to that of full flexibility. Our bound depends only on the mean and standard deviation of the random demand, but compares very well with the ratio that uses complete information of the demand distribution. This suggests the robustness of the performance of the long chain under different distributions. We also prove a similar result for <i>k</i>-chain, a more general flexibility structure. We further tighten the bounds by incorporating more distributional information of the random demand.", "e:keyword": ["Process flexibility", "Problem of moments", "Asymptotic analysis", "Worst-case bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1371", "e:abstract": "The importance of the global financial system cannot be exaggerated. When a large financial institution becomes problematic and is bailed out, that bank is often claimed as “too big to fail.” On the other hand, to prevent bank’s failure, regulatory authorities adopt the Prompt Corrective Action (PCA) against a bank that violates certain criteria, often measured by its leverage ratio. In this article, we provide a framework where one can analyze the cost and effect of PCAs. We model a large bank that has deteriorating assets and regulatory actions attempting to prevent the bank’s failure. The model uses the excursion theory of Lévy processes and finds an optimal leverage ratio that triggers a PCA. A nice feature includes that it incorporates the fact that social cost associated with PCAs are greatly affected by the size of banks subject to PCAs. In other words, one can see the cost of rescuing a bank that is too big to fail.", "e:keyword": ["Prompt corrective actions", "Excursion theory", "Spectrally negative Lévy processes", "Scale functions"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1372", "e:abstract": "Optimizing multiproduct assemble-to-order (ATO) inventory systems is a long-standing difficult problem. We consider ATO systems with identical component lead times and a general “bill of materials.” We use a related two-stage stochastic program (SP) to set a lower bound on the average inventory cost and develop inventory control policies for the dynamic ATO system using this SP. We apply the first-stage SP optimal solution to specify a base-stock replenishment policy, and the second-stage SP recourse linear program to make allocation decisions. We prove that our policies are asymptotically optimal on the diffusion scale, so the percentage gap between the average cost from its lower bound diminishes to zero as the lead time grows.", "e:keyword": ["Assemble-to-order", "Inventory management", "Stochastic linear program", "Stochastic control", "Asymptotic optimality", "Diffusion scale"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1373", "e:abstract": "Elimination by aspects (EBA) is a random utility model that is considered to represent the choice process used by consumers more faithfully than logit and probit models. One limitation of the model is that it does not have a known error theory. We show that EBA can be derived by assuming that aspects have random utilities with independent, extreme value distributions. Multinomial logit and rank-ordered logit models are special cases of EBA.", "e:keyword": ["Elimination by aspects", "Luce axiom", "Multinomial logit model", "Rank-ordered logit model", "Independence from irrelevant alternatives", "Lexicographic choice", "Choice models"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1374", "e:abstract": "Robust optimization is a common optimization framework under uncertainty when problem parameters are unknown, but it is known that they belong to some given uncertainty set. In the robust optimization framework, a min-max problem is solved wherein a solution is evaluated according to its performance on the worst possible realization of the parameters. In many cases, a straightforward solution to a robust optimization problem of a certain type requires solving an optimization problem of a more complicated type, which might be NP-hard in some cases. For example, solving a robust conic quadratic program, such as those arising in a robust support vector machine (SVM) with an ellipsoidal uncertainty set, leads in general to a semidefinite program. In this paper, we develop a method for approximately solving a robust optimization problem using tools from online convex optimization, where at every stage a standard (nonrobust) optimization program is solved. Our algorithms find an approximate robust solution using a number of calls to an oracle that solves the original (nonrobust) problem that is inversely proportional to the square of the target accuracy.", "e:keyword": ["Robust optimization", "Online learning", "Online convex optimization", "Oracle-based algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1375", "e:abstract": "This paper provides the first exact analysis of a preemptive <i>M/M/c</i> queue with two priority classes having different service rates. To perform our analysis, we introduce a new technique to reduce the two-dimensionally infinite Markov chain (MC), representing the two class state space, into a one-dimensionally infinite MC, from which the generating function (GF) of the number of low-priority jobs can be derived in <i>closed form</i>. (The high-priority jobs form a simple <i>M/M/c</i> system and are thus easy to solve.) We demonstrate our methodology for the <i>c</i> = 1, 2 cases; when <i>c</i> > 2, the closed-form expression of the GF becomes cumbersome. We thus develop an <i>exact</i> algorithm to calculate the moments of the number of low-priority jobs for any <i>c</i> ≥ 2. Numerical examples demonstrate the accuracy of our algorithm and generate insights on (i) the relative effect of improving the service rate of either priority class on the mean sojourn time of low-priority jobs; (ii) the performance of a system having many slow servers compared with one having fewer fast servers; and (iii) the validity of the square root staffing rule in maintaining a fixed service level for the low-priority class. Finally, we demonstrate the potential of our methodology to solve other problems using the <i>M/M/c</i> queue with two priority classes, where the high-priority class is completely impatient.", "e:keyword": ["Multiserver queue", "Multiclass", "Preemptive priority", "Different service rates"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1376", "e:abstract": "We study a periodic review inventory model with a nonperishable product over an infinite planning horizon. The demand for the nonperishable product arrives according to a Poisson process. Lost sales are unobservable but the stockout times are observable. We formulate the problem as a dynamic programming model with learning on arrival rate according to stockout times and further simplify it by using unnormalized probabilities. We then compare the system performance with those under other two information scenarios where lost sales are observable or both lost sales and stockout times are unobservable. We show that the optimal inventory order-up-to level with observable stockout times is larger than the one with observable lost sales. We also show that more information improves the system performance.", "e:keyword": ["Information updating", "Inventory management", "Bayesian statistics", "Nonperishable products"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1377", "e:abstract": "Discrete-choice demand models are important and fundamental tools for understanding consumers’ choice behavior and for analyzing firms’ operations and pricing strategies. In these models, products are often described as a vector of observed characteristics. A consumer chooses the product that maximizes her utility, assumed to be a function of the observed product characteristics and the consumer’s preference over these product characteristics. One central task in the demand estimation literature is to infer, based on observed data, consumers’ preferences on product characteristics. We consider such an estimation problem for pure characteristics models, a class of random coefficients demand models without the idiosyncratic logit error term in a consumer’s utility function. The absence of the logit error term and the use of numerical integration to approximate the integral in aggregate market shares lead to a nonsmooth formulation of approximated market share equations. As a result, solving the approximated market share equations and estimating the model by using existing methods proposed in the econometrics literature remain computationally intractable. To overcome this difficulty, we first characterize consumers’ purchase decisions by a system of complementarity constraints. This new characterization leads to smooth approximated market share equations and allows us to cast the corresponding generalized method of moments (GMM) estimation problem essentially as a quadratic program with linear complementarity constraints, parameterized by an exponential, thus nonlinear, function of the structural parameter on price. We also extend this estimation framework to incorporate an endogenous pricing mechanism that captures the competitive profit maximization behavior of the producing firms. We provide existence results of a solution for the GMM estimator and present numerical results to demonstrate the computational effectiveness of our approach.", "e:keyword": ["Pure characteristics demand models", "Generalized method of moments", "Mathematical programs with complementarity constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1378", "e:abstract": "We study the role of local information channels in enabling coordination among strategic agents. Building on the standard finite-player global games framework, we show that the set of equilibria of a coordination game is highly sensitive to how information is locally shared among different agents. In particular, we show that the coordination game has multiple equilibria if there exists a collection of agents such that (i) they do not share a common signal with any agent outside of that collection and (ii) their information sets form an increasing sequence of nested sets. Our results thus extend the results on the uniqueness and multiplicity of equilibria beyond the well-known cases in which agents have access to purely private or public signals. We then provide a characterization of the set of equilibria as a function of the penetration of local information channels. We show that the set of equilibria shrinks as information becomes more decentralized.", "e:keyword": ["Coordination", "Local information", "Social networks", "Global games"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1380", "e:abstract": "We address a generic price competition model in an industry with an arbitrary number of competitors, each offering all or a subset of a given line of <i>N</i> products. The products are <i>substitutes</i> in the sense that the demand volume of each product weakly increases whenever the price of another product increases. The cost structure is linear, with arbitrary cost rates. Our demand model is the unique <i>regular</i> extension of a set of demand functions that are affine in a limited polyhedral subset of the price space. A set of demand functions is regular if it satisfies the following conditions: Under any given price vector, when some product is priced out of the market, i.e., has zero demand, any <i>increase</i> of its price has no impact on the demand volumes. Depending on the set of prices selected by the competing firms, a different product assortment is offered in the market. We characterize the equilibrium prices, product assortment, and sales volumes in the price competition model, under this demand model. Under minimal conditions, we show that a pure Nash equilibrium always exists; while multiple price equilibria may arise, they are equivalent in the sense of generating an identical product assortment and sales volumes.", "e:keyword": ["Games/group decisions", "Noncooperative", "Marketing", "Competitive strategy", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1381", "e:abstract": "We study a dynamic game in which short-run players repeatedly play a symmetric, strictly supermodular game whose payoffs depend on a fixed unknown state of nature. Each short-run player inherits the beliefs of his immediate predecessor in addition to observing the actions of the players in his social neighborhood in the previous stage. Because of the strategic complementary between their actions, players have the incentive to coordinate with others and learn from them. We show that in any Markov Bayesian equilibrium of the game, players eventually reach consensus in their actions. They also asymptotically receive similar payoffs despite initial differences in their access to information. We further show that, if the players’ payoffs can be represented by a quadratic function, then the private observations are optimally aggregated in the limit for generic specifications of the game. Therefore, players asymptotically coordinate on choosing the best action given the aggregate information available throughout the network. We provide extensions of our results to the case of changing networks and endogenous private signals.", "e:keyword": ["Consensus", "Information aggregation", "Supermodular games", "Networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1382", "e:abstract": "We consider a project selection problem where each project has an uncertain return with partially characterized probability distribution. The decision maker selects a feasible subset of projects so that the risk of the portfolio return not meeting a specified target is minimized. To model and evaluate this risk, we propose and justify a general performance measure, the underperformance riskiness index (URI). We define a special case of the URI, the entropic underperformance riskiness index (EURI), for the project selection problem. We minimize the EURI of the project portfolio, which is the reciprocal of the absolute risk aversion (ARA) of an ambiguity-averse individual with constant ARA who is indifferent between the target return with certainty and the uncertain portfolio return. The EURI extends the riskiness index of Aumann and Serrano (2008) by incorporating the target and distributional ambiguity, and controls the underperformance probability (UP) for any target level. Our model includes correlation and interaction effects such as synergies. Since the model is a discrete nonlinear optimization problem, we derive the optimal solution using Benders decomposition techniques. We show that computationally efficient solution of the model is possible. Furthermore, the project portfolios generated by minimizing the underperformance risk are more than competitive in achieving the target with those found by benchmark approaches, including maximization of expected return, minimization of UP, mean-variance analysis, and maximization of Roy’s safety-first ratio (1952). When there is only a single constraint for the budget, we describe a heuristic which routinely provides project portfolios with near-optimal underperformance risk.", "e:keyword": ["Project management", "Decision analysis", "Risk", "Programming", "Stochastic", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1383", "e:abstract": "We consider assortment optimization problems when customers choose according to the nested logit model and there is a capacity constraint limiting the total capacity consumption of all products offered in all nests. When each product consumes one unit of capacity, our capacity constraint limits the cardinality of the offered assortment. For the cardinality constrained case, we develop an efficient algorithm to compute the optimal assortment. When the capacity consumption of each product is arbitrary, we give an algorithm to obtain a 4-approximate solution. We show that we can compute an upper bound on the optimal expected revenue for an individual problem instance by solving a linear program. In our numerical experiments, we consider problem instances involving products with arbitrary capacity consumptions. Comparing the expected revenues from the assortments obtained by our 4-approximation algorithm with the upper bounds on the optimal expected revenues, our numerical results indicate that the 4-approximation algorithm performs quite well, yielding less than 2% optimality gap on average.", "e:keyword": ["Analysis of algorithms", "Suboptimal algorithms", "Marketing", "Choice models"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1384", "e:abstract": "Comment on “A Glimpse at An Operation Analyst’s World War II Work: ‘Report on the Combat Performance of the Remote Control Turrets of B-29 Aircraft”' by Alex E. S. Green, Deborah S. Green, and Richard L. Francis", "e:keyword": ["Defense systems", "Force effectiveness", "Tactics/strategy", "History of OR"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1385", "e:abstract": "A general framework is proposed for pricing both continuously and discretely monitored Asian options under one-dimensional Markov processes. For each type (continuously monitored or discretely monitored), we derive the double transform of the Asian option price in terms of the unique bounded solution to a related functional equation. In the special case of continuous-time Markov chain (CTMC), the functional equation reduces to a linear system that can be solved analytically via matrix inversion. Thus the Asian option prices under a one-dimensional Markov process can be obtained by first constructing a CTMC to approximate the targeted Markov process model, and then computing the Asian option prices under the approximate CTMC by numerically inverting the double transforms. Numerical experiments indicate that our pricing method is accurate and fast under popular Markov process models, including the CIR model, the CEV model, Merton’s jump diffusion model, the double-exponential jump diffusion model, the variance gamma model, and the CGMY model.", "e:keyword": ["Finance", "Asset pricing", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1386", "e:abstract": "We develop the first approximation algorithms with worst-case performance guarantees for periodic-review perishable inventory systems with general product lifetime, for both backlogging and lost-sales models. The demand process can be nonstationary and correlated over time, capturing such features as demand seasonality and forecast updates. The optimal control policy for such systems is notoriously complicated, thus finding effective heuristic policies is of practical importance. In this paper, we construct a computationally efficient inventory control policy, called the <i>proportional-balancing policy</i>, for systems with an arbitrarily correlated demand process and show that it has a worst-case performance guarantee less than 3. In addition, when the demands are independent and stochastically nondecreasing over time, we propose another policy, called the <i>dual-balancing policy</i>, which admits a worst-case performance guarantee of 2. We demonstrate through an extensive numerical study that both policies perform consistently close to optimal.", "e:keyword": ["Inventory/production", "Approximation/heuristics", "Perishing/aging items", "Uncertainty", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1387", "e:abstract": "Consider a firm that produces multiple products on dedicated production lines (stage 1), which are further customized/calibrated on a shared resource (stage 2), common to all products. The dedicated production lines and the shared resource for calibration face capacity uncertainties. The firm holds inventory of products that are not yet calibrated and carries out calibration when an order is received. We analyze a multiperiod inventory model for two products and derive the optimal production policy at stage 1 as well as the optimal allocation of the shared resource to demands at stage 2. For the shared resource, because of its capacity uncertainty, not only the total planned production quantities matter, but also the sequence in which the products are processed. We characterize the optimal allocation of the shared resource and show that the optimal policy keeps the ending inventories of products as close to a so-called “<i>target path</i>” as possible. For the dedicated production lines, because of their capacity uncertainty, the optimal production policy depends on the initial inventories. We identify and characterize the structural properties of the optimal production policy. Through a numerical study, we explore how the presence of finite shared capacity influences the inventory targets. We find that the behavior may be counterintuitive: when multiple products share a finite capacity in stage 2, the inventory target for the product having a larger dedicated production capacity or less capacity variability in stage 1 can be higher than the other product. We finally provide sensitivity analysis for the optimal policy and test the performance of simple heuristic policies.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Multi-item/echelon/stage", "Production/scheduling", "Planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1388", "e:abstract": "We study a special class of multi-item valuations (tree valuations) that exhibit both value complementarity and substitutability. We provide a linear programming formulation of the efficient allocation problem that is of polynomial size in the number of agents and items. This reveals a new class of valuations for which a Walrasian equilibrium exists in the presence of value complementarities. An iterative algorithm for this linear program, in conjunction with an appropriate payment rule, yields an iterative auction that implements the efficient outcome (at an ex post perfect equilibrium). This auction relies on a simple pricing rule, compact demand reports, and uses a novel (interleaved) price update structure to assign final payments to bidders that guarantee truthful bidding.", "e:keyword": ["Games/group decisions", "Bidding/auctions", "Noncooperative", "Programming", "Linear"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1389", "e:abstract": "We consider the control of patient flow through physicians in emergency departments (EDs). The physicians must choose between catering to patients right after triage, who are yet to be checked, and those who are in process (IP) and are occasionally returning to be checked. Physician capacity is thus modeled as a queueing system with multiclass customers, where some of the classes face deadline constraints on their time-till-first-service, whereas the other classes feedback through service while incurring congestion costs. We consider two types of such costs: first, costs that are incurred at queue-dependent rates and second, costs that are functions of IP sojourn time. The former is our base model, which paves the way for the latter (perhaps more ED realistic). In both cases, we propose and analyze scheduling policies that, asymptotically in conventional heavy traffic, minimize congestion costs while adhering to all deadline constraints. Our policies have two parts: the first chooses between triage and IP patients; assuming triage patients are chosen, the physicians serve the one who is closest to violating the deadline; alternatively, IP patients are served according to a G<i>cμ</i> rule, in which <i>μ</i> is simply modified to account for feedbacks. For our proposed policies, we establish asymptotic optimality, and develop some congestion laws (snapshot principles) that support forecasting of waiting and sojourn times. Simulation then shows that these policies outperform some commonly used ones. It also validates our laws and demonstrates that some ED features, the complexity of which reaches beyond our model (e.g., time-varying arrival rates, leave without being seen (LWBS) or leave against medical advice (LAMA)), do not lead to significant performance degradation.", "e:keyword": ["Health care", "Hospitals", "Emergency departments", "Queues", "Approximations", "Heavy traffic", "c rule", "Feedback"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1390", "e:abstract": "We consider a standard network revenue management (RM) problem and study the performance of a linear program (LP)-based control, the <i>Probabilistic Allocation Control</i> (PAC), in the presence of unknown demand parameters. We show that frequent re-optimizations of PAC without re-estimation suffice to <i>shrink</i> the asymptotic impact of estimation error on revenue loss. If, in addition to re-optimizations, we also frequently re-estimate the parameters, we prove that the performance of PAC in the unknown parameters setting is almost as good as the performance of PAC in the known parameters setting. Our numerical experiments show that PAC yields a revenue improvement of order 0.5%–1.5% relative to LP-based Booking Limit and Bid Price in most cases. Given the small margin in RM industries, such as the airline industry (about 2%), this level of improvement can easily translate into a significant increase in profit.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Uncertainty", "Stochastic", "Probability", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1391", "e:abstract": "Specialty clinics provide specialized care for patients referred by primary care physicians, emergency departments, or other specialists. Urgent patients must often be seen on the referral day, whereas nonurgent referrals are typically booked an appointment for the future. To deliver a balanced performance, the clinics must know how much “appointment capacity” is needed for achieving a reasonably quick access for nonurgent patients. To help identify the capacity that leads to the desired performance, we model the dynamics of appointment backlog as novel discrete-time bulk service queues and develop numerical methods for efficient computation of corresponding performance metrics. Realistic features such as arbitrary referral and clinic appointment cancellation distributions, delay-dependent no-show behaviour, and rescheduling of no-shows are explicitly captured in our models. The accuracy of the models in predicting performance as well as their usefulness in appointment capacity planning is demonstrated using real data. We also show the application of our models in capacity planning in clinics where patient panel size, rather than appointment capacity, is the major decision variable.", "e:keyword": ["Probability", "Stochastic model", "Health care", "Queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1392", "e:abstract": "Over the last two decades, robust optimization has emerged as a computationally attractive approach to formulate and solve single-stage decision problems affected by uncertainty. More recently, robust optimization has been successfully applied to multistage problems with continuous recourse. This paper takes a step toward extending the robust optimization methodology to problems with integer recourse, which have largely resisted solution so far. To this end, we approximate two-stage robust binary programs by their corresponding <i>K</i>-adaptability problems, in which the decision maker precommits to <i>K</i> second-stage policies, here -and-now, and implements the best of these policies once the uncertain parameters are observed. We study the approximation quality and the computational complexity of the <i>K</i>-adaptability problem, and we propose two mixed-integer linear programming reformulations that can be solved with off-the-shelf software. We demonstrate the effectiveness of our reformulations for stylized instances of supply chain design, route planning, and capital budgeting problems.", "e:keyword": ["Programming", "Integer", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1395", "e:abstract": "We create the first computationally tractable Bayesian statistical model for learning unknown correlation structures in fully sequential simulation selection. Correlations represent similarities or differences between various design alternatives and can be exploited to extract much more information from each individual simulation. However, in most applications, the correlation structure is unknown, thus creating the additional challenge of simultaneously learning unknown mean performance values and unknown correlations. Based on our new statistical model, we derive a Bayesian procedure that seeks to optimize the expected opportunity cost of the final selection based on the value of information, thus anticipating future changes to our beliefs about the correlations. Our approach outperforms existing methods for known correlation structures in numerical experiments, including one motivated by the problem of optimal wind farm placement, where real data are used to calibrate the simulation model.", "e:keyword": ["Simulation", "Statistical analysis", "Design of experiments", "Decision analysis", "Sequential"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1396", "e:abstract": "We study a multistate partially observable process control model with a general state transition structure. The process is initially in control and subject to Markovian deterioration that can bring it to out-of-control states. The process may continue making transitions among the out-of-control states, or even back to the in-control state until it reaches an absorbing state. We assume that at least one out-of-control state is absorbing. The objective is to minimize the expected total cost over a finite horizon. By transforming the standard Cartesian belief space into the spherical coordinate system, we show that the optimal policy has a simple control-limit structure. We also examine two specialized models. The first is the phase-type transition time model, in which we develop an algorithm whose complexity is not affected by the number of phases. The second is a model with multiple absorbing out-of-control states, by which we show that certain out-of-control states may incur less total cost than the in-control state, a phenomenon never occurs in the two-state models. We conclude that there are fundamental differences between multistate models and two-state models, and that the spherical coordinate transformation offers significant analytical and computational benefits.", "e:keyword": ["Dynamic programming", "Markov", "Finite state", "Bayesian control chart"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1397", "e:abstract": "We study a dynamic pricing problem with finite inventory and parametric uncertainty on the demand distribution. Products are sold during selling seasons of finite length, and inventory that is unsold at the end of a selling season perishes. The goal of the seller is to determine a pricing strategy that maximizes the expected revenue. Inference on the unknown parameters is made by maximum-likelihood estimation.We show that this problem satisfies an endogenous learning property, which means that the unknown parameters are learned on the fly if the chosen selling prices are sufficiently close to the optimal ones. We show that a small modification to the certainty equivalent pricing strategy—which always chooses the optimal price w.r.t. current parameter estimates—satisfies Regret(<i>T</i>) = <i>O</i>(log<sup>2</sup>(<i>T</i>)), where Regret(<i>T</i>) measures the expected cumulative revenue loss w.r.t. a clairvoyant who knows the demand distribution. We complement this upper bound by showing an instance for which the regret of any pricing policy satisfies Ω(log <i>T</i>).", "e:keyword": ["Marketing", "Pricing", "Dynamic programming", "Markov", "Finite state", "Inventory/production", "Uncertainty", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1398", "e:abstract": "Designing optimal—that is, revenue-maximizing—combinatorial auctions (CAs) is an important elusive problem. It is unsolved even for two bidders and two items for sale. Rather than pursuing the manual approach of attempting to characterize the optimal CA, we introduce a family of CAs and then seek a high-revenue auction within that family. The family is based on bidder weighting and allocation boosting; we coin such CAs <i>virtual valuations combinatorial auctions (<i>VVCAs</i>)</i>. VVCAs are the Vickrey-Clarke-Groves (VCG) mechanism executed on virtual valuations that are affine transformations of the bidders’ valuations. The auction family is parameterized by the coefficients in the transformations. The problem of designing a CA is thereby reduced to search in the parameter space of VVCA—or the more general space of <i>affine maximizer auctions</i>.We first construct VVCAs with logarithmic approximation guarantees in canonical special settings: (1) limited supply with additive valuations and (2) unlimited supply.In the main part of the paper, we develop algorithms that design high-revenue CAs for general valuations using samples from the prior distribution over bidders’ valuations. (Priors turn out to be necessary for achieving high revenue.) We prove properties of the problem that guide our design of algorithms. We then introduce a series of algorithms that use economic insights to guide the search and thus reduce the computational complexity. Experiments show that our algorithms create mechanisms that yield significantly higher revenue than the VCG and scale dramatically better than prior automated mechanism design algorithms. The algorithms yielded deterministic mechanisms with the highest known revenues for the settings tested, including the canonical setting with two bidders, two items, and uniform additive valuations.<xref ref-type=\"fn\" rid=\"foot1\"><sup>1</sup></xref>", "e:keyword": ["Combinatorial auction", "Optimal auction", "Revenue maximization", "Automated mechanism design", "Parametric mechanism design"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1399", "e:abstract": "We formalize the reference point adaptation process by relating it to a way people perceive prior gains and losses. We then develop a dynamic trading model with reference point adaptation and loss aversion, and derive its semi-analytical solution. The derived optimal stock holding has an asymmetric V-shaped form with respect to prior outcomes, and the related sensitivities are directly determined by the sensitivities of reference point shifts with respect to the outcomes. We also find that the effects of reference point adaptation can be used to shed light on some well documented trading patterns, e.g., house money, break even, and disposition effects.", "e:keyword": ["Finance", "Portfolio", "Decision analysis", "Utility/preference", "Value theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1400", "e:abstract": "We develop a solution approach to the centralized pricing problem of a nested attraction model with a multistage tree structure. We identify conditions under which the optimal solution can be uniquely determined, and we characterize the optimal solution as a fixed point of a single variable. In the special case of a multistage nested logit model, we show the impact of asymmetry in price sensitivity and adjustment index (also known as the dissimilarity index) and we derive a closed-form solution when the tree structure is symmetric. Many existing results in the literature regarding the single or two-stage nested attraction model are shown to be special cases of the results we have derived. We show that the equal markup property, which holds for the single-stage logit model with symmetric price sensitivity, in general does not hold for products that do not share the same immediate parent node in the nested choice structure even when price sensitivities are the same for all products.", "e:keyword": ["Marketing", "Choice models", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1401", "e:abstract": "The production routing problem (PRP) is a generalization of the inventory routing problem and concerns the production and distribution of a single product from a production plant to multiple customers using capacitated vehicles in a discrete- and finite-time horizon. In this study, we consider the stochastic PRP with demand uncertainty in two-stage and multistage decision processes. The decisions in the first stage include production setups and customer visit schedules, while the production and delivery quantities are determined in the subsequent stages. We introduce formulations for the two problems, which can be solved by a branch-and-cut algorithm. To handle a large number of scenarios, we propose a Benders decomposition approach, which is implemented in a single branch-and-bound tree and enhanced through lower-bound lifting inequalities, scenario group cuts, and Pareto-optimal cuts. For the multistage problem, we also use a warm start procedure that relies on the solution of the simpler two-stage problem. Finally, we exploit the reoptimization capabilities of Benders decomposition in a sample average approximation method for the two-stage problem and in a rollout algorithm for the multistage problem. Computational experiments show that instances of realistic size can be solved to optimality for the two-stage and multistage problems, and that Benders decomposition provides significant speedups compared to a classical branch-and-cut algorithm.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Transportation", "Vehicle routing", "Programming", "Integer", "Algorithms", "Benders decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1402", "e:abstract": "The objective of this study is to develop a majorization-based tool to compare financial networks with a focus on the implications of liability concentration. Specifically, we quantify liability concentration by applying the majorization order to the liability matrix that captures the interconnectedness of banks in a financial network. We develop notions of balancing and unbalancing networks to bring out the qualitatively different implications of liability concentration on the system’s loss profile. We illustrate how to identify networks that are balancing or unbalancing, and we make connections to interbank structures identified by empirical research, such as perfect and imperfect tiering schemes. An empirical analysis of the network formed by the banking sectors of eight representative European countries suggests that the system is either unbalancing or close to it, persistently over time. This empirical finding, along with the majorization results, supports regulatory policies aiming at limiting the size of gross exposures to individual counterparties.", "e:keyword": ["Majorization", "Systemic risk", "Liability concentration", "Financial networks", "Financial contagion"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1403", "e:abstract": "The Centers for Medicare and Medicaid Services (CMS) has introduced a “bundled payments for care improvement” (BPCI) initiative. Each bundle pertains to a specific medical condition, a set of linked services, and a length of time referred to as an episode of care. Proposers choose bundles, design service chains, and propose target values of quality metrics and payments per episode. Expert panels evaluate proposals based on CMS-announced relative weights, but there is no limit on the number of proposers that may be selected. Moreover, there is no minimum score that will guarantee selection, which makes selection uncertain for proposers. We develop normative models for the parameter selection problems faced by potential proposers within the CMS’ proposal selection process. Proposers have private information about their costs of achieving different quality targets, which determine their equilibrium responses. We show that an optimal strategy for CMS, under its current approach, may be to either announce a fixed threshold or keep the selection process uncertain, depending on market characteristics. We also formulate and solve the proposer selection problem as a constrained mechanism design problem, which reveals that CMS’ current approach is not optimal. We present policy guidelines for government agencies pursuing bundled payment innovations.", "e:keyword": ["Games/group decisions", "Bidding/auction", "Government", "Agencies", "Healthcare"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1404", "e:abstract": "In many settings in which Monte Carlo methods are applied, there may be no known algorithm for exactly generating the random object for which an expectation is to be computed. Frequently, however, one can generate arbitrarily close approximations to the random object. We introduce a simple randomization idea for creating unbiased estimators in such a setting based on a sequence of approximations. Applying this idea to computing expectations of path functionals associated with stochastic differential equations (SDEs), we construct finite variance unbiased estimators with a “square root convergence rate” for a general class of multidimensional SDEs. We then identify the optimal randomization distribution. Numerical experiments with various path functionals of continuous-time processes that often arise in finance illustrate the effectiveness of our new approach.", "e:keyword": ["Unbiased estimation", "Exact estimation", "Square root convergence rate", "Stochastic differential equations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1405", "e:abstract": "In managing chronic diseases such as glaucoma, the timing of periodic examinations is crucial, as it may significantly impact patients’ outcomes. We address the question of when to monitor a glaucoma patient by integrating a dynamic, stochastic state space system model of disease evolution with novel optimization approaches to predict the likelihood of progression at any future time. Information about each patient’s disease state is learned sequentially through a series of noisy medical tests. This information is used to determine the best time to next test based on each patient’s individual disease trajectory as well as population information. We develop closed-form solutions and study structural properties of our algorithm. While some have proposed that fixed-interval monitoring can be improved upon, our methodology validates a sophisticated model-based approach to doing so. Based on data from two large-scale, 10+ years clinical trials, we show that our methods significantly outperform fixed-interval schedules and age-based threshold policies by achieving greater accuracy of identifying progression with fewer examinations. Although this work is motivated by our collaboration with glaucoma specialists, the methodology developed is applicable to a variety of chronic diseases.", "e:keyword": ["Linear Gaussian systems modeling", "Controlled observations", "Stochastic control", "Disease monitoring", "Medical decision making", "Glaucoma", "Visual field"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1406", "e:abstract": "We study the necessity of predictive information in a class of queueing admission control problems, where a system manager is allowed to divert incoming jobs up to a fixed rate, in order to minimize the queueing delay experienced by the admitted jobs.Spencer et al. (2014) [Spencer J, Sudan M, Xu K (2014) Queuing with future information. <i>Ann. Appl. Probab.</i> 24(5):2091–2142.] show that the system’s delay performance can be significantly improved by having access to future information in the form of a lookahead window, during which the times of future arrivals and services are revealed. They prove that, while delay under an optimal online policy diverges to infinity in the heavy-traffic regime, it can stay <i>bounded</i> by making use of future information. However, the diversion polices of Spencer et al. (2014) require the length of the lookahead window to grow to infinity at a nontrivial rate in the heavy-traffic regime, and it remained open whether substantial performance improvement could still be achieved with <i>less</i> future information.We resolve this question to a large extent by establishing an asymptotically tight lower bound on how much future information is necessary to achieve superior performance, which matches the upper bound of Spencer et al. (2014) up to a constant multiplicative factor. Our result hence demonstrates that the system’s heavy-traffic delay performance is highly sensitive to the amount of future information available. Our proof is based on analyzing certain excursion probabilities of the input sample paths, and exploiting a connection between a policy’s diversion decisions and subsequent server idling, which may be of independent interest for related dynamic resource allocation problems.", "e:keyword": ["Admission control", "Queueing", "Algorithm", "Future information", "Predictive model", "Heavy-traffic asymptotics"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1407", "e:abstract": "Commodity merchants use various heuristics to value leasing contracts on storage facilities as real options and make inventory trading decisions. Two prominent heuristics sequentially reoptimize simple models, leading to the so-called rolling intrinsic (RI) policy and rolling basket of spread options (RSO) policy. The extant literature numerically demonstrates that these two policies are nearly optimal in many realistic settings and can be used with Monte Carlo simulation to obtain fairly accurate estimates of the value of storage contracts. This paper provides a theoretical basis for the observed benefit of reoptimization with these heuristics and additional numerical evidence for the near optimal performance of the RI and RSO policies in several practical cases, but shows that the RI policy significantly outperforms the RSO policy in some of these cases. This research also proves that the RSO policy has a double basestock target structure, a known property of an optimal policy that is trivially true for the RI policy. Moreover, this work develops efficient and effective dual bounds to assess the performance of merchant commodity storage heuristics. In particular, these bounds are immediately relevant to the developers and users of the two considered heuristics.", "e:keyword": ["Commodity and energy storage", "Dual bounds", "Heuristics", "Linear programming", "Monte Carlo simulation", "Natural gas", "Real options", "Reoptimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1408", "e:abstract": "We consider a non-stationary variant of a sequential stochastic optimization problem, in which the underlying cost functions may change along the horizon. We propose a measure, termed <i>variation budget</i>, that controls the extent of said change, and study how restrictions on this budget impact achievable performance. We identify sharp conditions under which it is possible to achieve long-run average optimality and more refined performance measures such as rate optimality that fully characterize the complexity of such problems. In doing so, we also establish a strong connection between two rather disparate strands of literature: (1) adversarial online convex optimization and (2) the more traditional stochastic approximation paradigm (couched in a non-stationary setting). This connection is the key to deriving well-performing policies in the latter, by leveraging structure of optimal policies in the former. Finally, tight bounds on the minimax regret allow us to quantify the “price of non-stationarity,” which mathematically captures the added complexity embedded in a temporally changing environment versus a stationary one.", "e:keyword": ["Stochastic approximation", "Non-stationary", "Minimax regret", "Online convex optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1409", "e:abstract": "This paper develops a centralized supply chain model that integrates material flows with cash flows. The supply chain is owned by a single firm with two divisions. The downstream division (headquarters), facing random customer demand, replenishes materials from the upstream division. The firm installs a financial services platform that pools the divisions’ cash into a master account managed by the headquarters. In each period, cash is received from customers and paid to the outside vendor after materials are delivered. The headquarters determines how much cash to retain for inventory replenishment. The objective is to determine an optimal joint inventory replenishment and cash retention policy for the entire supply chain. We prove that the optimal policy has a surprisingly simple structure—both divisions implement a base-stock policy for inventory replenishment; the headquarters monitors the corporate working capital and implements a two-threshold policy for cash retention. This result is obtained by extending the well-known Clark-Scarf decomposition with newly derived cash-related penalty functions. The optimal policy enables us to investigate the interaction between cash and inventory decisions. We show that in the presence of transaction costs, a firm may stock more even if the inventory holding cost increases. To quantify the value of financial integration, we compare the cash pooling model with systems under different levels of financial integration. Our study suggests that the value of cash pooling can be significant when demand is increasing (respectively, stationary) and the internal transfer price is low (respectively, high). Nevertheless, a significant amount of cash pooling benefit may be recovered if the headquarters can optimize the internal transfer price.", "e:keyword": ["Multi-echelon", "Cash pooling", "Supply chain integration", "Financial flows"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1411", "e:abstract": "The quality of alternatives is crucial for making good decisions. This research, based on five empirical studies of important personally relevant decisions, examines the ability of decision makers to create alternatives for their important decisions and the effectiveness of different stimuli for improving this ability. For decisions for which the full set of potentially desirable alternatives is not readily apparent, our first study indicates that decision makers identify less than half of their alternatives and that the average quality of the overlooked alternatives is the same as those identified. Four other studies provide insight about how to use objectives to stimulate the alternative-creation process of decision makers and confirm with high significance that such use enhances both the number and quality of created alternatives. Using results of the studies, practical guidelines to create alternatives for important decisions are presented.", "e:keyword": ["Decision analysis: creating alternatives"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1412", "e:abstract": "Motivated by empirical facts, we develop a theoretical model for optimal currency government debt portfolio and debt payments, which allows both government debt aversion and jumps in the exchange rates. We obtain first a realistic stochastic differential equation for public debt and then solve explicitly the optimal currency debt problem. We show that higher debt aversion and jumps in the exchange rates lead to a lower proportion of optimal debt in foreign currencies. Furthermore, we show that for a government with extreme debt aversion it is optimal not to issue debt in foreign currencies. To the best of our knowledge, this is the first theoretical model that provides a rigorous explanation of why developing countries have reduced consistently their proportion of foreign debt in their debt portfolios.", "e:keyword": ["Debt control", "Optimal currency debt portfolio", "Optimal debt payments", "Stochastic control"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1413", "e:abstract": "Fully sequential ranking-and-selection (R&S) procedures to find the best from a finite set of simulated alternatives are often designed to be implemented on a single processor. However, parallel computing environments, such as multi-core personal computers and many-core servers, are becoming ubiquitous and easily accessible for ordinary users. In this paper, we propose two types of fully sequential procedures that can be used in parallel computing environments. We call them vector-filling procedures and asymptotic parallel selection procedures, respectively. Extensive numerical experiments show that the proposed procedures can take advantage of multiple parallel processors and solve large-scale R&S problems.", "e:keyword": ["Fully sequential procedures", "Parallel computing", "Statistical issues", "Asymptotic validity"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1414", "e:abstract": "We show that partial versus full multilateral netting of interbank liabilities increases bank shortfall and reduces clearing asset price and aggregate bank surplus. We also show that partial multilateral netting can be worse than no netting at all.", "e:keyword": ["Over the counter markets", "Financial network", "Multilateral netting", "Central counterparty"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1415", "e:abstract": "We consider a model in which a trader aims to maximize expected risk-adjusted profit while trading a single security. In our model, each price change is a linear combination of observed factors, impact resulting from the trader’s current and prior activity, and unpredictable random effects. The trader must learn coefficients of a price impact model while trading. We propose a new method for simultaneous execution and learning—the confidence-triggered regularized adaptive certainty equivalent (CTRACE) policy—and establish a poly-logarithmic finite-time expected regret bound. In addition, we demonstrate via Monte Carlo simulation that CTRACE outperforms the certainty equivalent policy and a recently proposed reinforcement learning algorithm that is designed to explore efficiently in linear-quadratic control problems.", "e:keyword": ["Adaptive execution", "Price impact", "Reinforcement learning", "Regret bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1416", "e:abstract": "We study the problem of how to design a sparse flexible process structure in a balanced and symmetrical production system to match supply with random demand more effectively. Our goal is to provide a <i>sparsest design</i> to achieve (1 − <i>ϵ</i>)-optimality relative to the fully flexible system. In a balanced system with <i>n</i> plants and <i>n</i> products, Chou et al. (2011) proved that there exists a graph expander with <i>O</i>(<i>n</i>/<i>ϵ</i>) arcs to achieve (1 − <i>ϵ</i>)-optimality for every demand realization. Wang and Zhang (2015) showed that the simple <i>k</i>-chain design with <i>O</i>(<i>n</i>/<i>ϵ</i>) arcs can achieve (1 − <i>ϵ</i>)-optimality in expectation.In this paper, we introduce a new concept called <i>probabilistic graph expanders</i>. We prove that a probabilistic expander with <i>O</i>(<i>n</i> ln(<i>1</i>/<i>ϵ</i>)) arcs guarantees (1 − <i>ϵ</i>)-optimality with high probability (w.h.p.), which is a stronger notion of optimality as compared to the expected performance. Easy-to-implement randomized and deterministic constructions of probabilistic expanders are provided. We show our bound is best possible in the sense that any structure should need at least Ω(<i>n</i> ln(1/<i>ϵ</i>)) arcs to achieve (1 − <i>ϵ</i>)-optimality in expectation (and hence w.h.p.). We also show that in order to achieve (1 − <i>ϵ</i>)-optimality in the worst case, any design would need at least Ω(<i>n</i>/<i>ϵ</i>) arcs; and in order to achieve (1 − <i>ϵ</i>)-optimality in expectation, <i>k</i>-chain needs at least Ω(<i>n</i>/<i>ϵ</i>) arcs.", "e:keyword": ["Flexible manufacturing", "Graph expanders", "Probabilistic expanders"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1417", "e:abstract": "We consider a discrete optimization via simulation (DOvS) problem with stochastic constraints on secondary performance measures in which both objective and secondary performance measures need to be estimated by stochastic simulation. To solve the problem, we develop a new method called the Penalty Function with Memory (PFM). It is similar to an existing penalty-type method—which consists of a penalty parameter and a measure of violation of constraints—in a sense that it converts a DOvS problem with constraints into a series of unconstrained problems. However, PFM uses a different penalty parameter, called a penalty sequence, determined by the past history of feasibility checks on a solution. Specifically, assuming a minimization problem, a penalty sequence diverges to infinity for any infeasible solution but converges to zero for any feasible solution under certain conditions. As a result, a DOvS algorithm combined with PFM performs well even when an optimal feasible solution is a boundary solution with one or more active constraints. We prove convergence properties and discuss parameter selection for the implementation of PFM. Experimental results on a number of numerical examples show that a DOvS algorithm combined with PFM works well.", "e:keyword": ["Discrete optimization via simulation", "Stochastic constraints", "Penalty function with memory"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1418", "e:abstract": "We investigate a dynamic decision model that facilitates a target-oriented decision maker in regulating her risky consumption based on her desired target consumption level in every period in a finite planning horizon. We focus on dynamic operational decision problems of a firm where risky cash flows are being resolved over time. The firm can finance consumption by borrowing or saving to attain prescribed consumption targets over time. To evaluate the ability of the consumption in meeting respective targets, we propose the consumption shortfall risk (CSR) criterion, which has salient properties of attainment content, starvation aversion, subadditivity, and positive homogeneity. We show that if borrowing and saving are unrestricted and their interest rates are common, the optimal policy that minimizes the CSR criterion is to finance consumption at the target level for all periods except the last. For general convex dynamic decision problems, the optimal policies correspond to those that maximize an additive expected utility, in which the underlying utility functions are concave and increasing. Despite the interesting properties, this approach violates the principle of normative utility theory and we discuss the limitations of our target-oriented decision model.", "e:keyword": ["Dynamic programming", "Targets", "Riskiness index"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1419", "e:abstract": "We introduce a regression-based nested Monte Carlo simulation method for the estimation of financial risk. An outer simulation level is used to generate financial risk factors and an inner simulation level is used to price securities and compute portfolio losses given risk factor outcomes. The mean squared error (MSE) of standard nested simulation converges at the rate <i>k</i><sup>−2/3</sup>, where <i>k</i> measures computational effort. The proposed regression method combines information from different risk factor realizations to provide a better estimate of the portfolio loss function. The MSE of the regression method converges at the rate <i>k</i><sup>−1</sup> until reaching an asymptotic bias level which depends on the magnitude of the regression error. Numerical results consistent with our theoretical analysis are provided and numerical comparisons with other methods are also given.", "e:keyword": ["Statistics: estimation", "Decision analysis: risk", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1420", "e:abstract": "Regulatory changes are transforming the multitrillion dollar swaps market from a network of bilateral contracts to one in which swaps are cleared through central counterparties (CCPs). The stability of the new framework depends on the CCPs’ resilience. Margin requirements are a CCP’s first line of defense against the default of a counterparty. To capture liquidity costs at default, margin requirements need to increase superlinearly in position size. However, convex margin requirements create an incentive for a swaps dealer to split its positions across multiple CCPs, effectively “hiding” potential liquidation costs from each CCP. To compensate, each CCP needs to set higher margin requirements than it would in isolation. In a model with two CCPs, we define an equilibrium as a pair of margin schedules through which both CCPs collect sufficient margin under a dealer’s optimal allocation of trades. In the case of linear price impact, we show that a necessary and sufficient condition for the existence of an equilibrium is that the two CCPs agree on liquidity costs, and we characterize all equilibria when this holds. A difference in views can lead to a race to the bottom. We provide extensions of this result and discuss its implications for CCP oversight and risk management.", "e:keyword": ["Financial institutions", "Finance securities", "Government regulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1421", "e:abstract": "We study the problem of integrated staffing and scheduling under demand uncertainty. This problem is formulated as a two-stage stochastic integer program with mixed-integer recourse. The here-and-now decision is to find initial staffing levels and schedules. The wait-and-see decision is to adjust these schedules at a time closer to the actual date of demand realization. We show that the mixed-integer rounding inequalities for the second-stage problem convexify the recourse function. As a result, we present a tight formulation that describes the convex hull of feasible solutions in the second stage. We develop a modified multicut approach in an integer <i>L</i>-shaped algorithm with a prioritized branching strategy. We generate 20 instances (each with more than 1.3 million integer and 4 billion continuous variables) of the staffing and scheduling problem using 3.5 years of patient volume data from Northwestern Memorial Hospital. Computational results show that the efficiency gained from the convexification of the recourse function is further enhanced by our modifications to the <i>L</i>-shaped method. The results also show that compared with a deterministic model, the two-stage stochastic model leads to a significant cost savings. The cost savings increase with mean absolute percentage errors in the patient volume forecast.", "e:keyword": ["Stochastic programming", "Hospitals", "Personnel scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1422", "e:abstract": "Consider the newsvendor model, but under the assumption that the underlying demand distribution is not known as part of the input. Instead, the only information available is a random, independent sample drawn from the demand distribution. This paper analyzes the sample average approximation (SAA) approach for the data-driven newsvendor problem. We obtain a new analytical bound on the probability that the relative regret of the SAA solution exceeds a threshold. This bound is significantly tighter than existing bounds, and it matches the empirical accuracy of the SAA solution observed in extensive computational experiments. This bound reveals that the demand distribution’s <i>weighted mean spread</i> affects the accuracy of the SAA heuristic.", "e:keyword": ["Data-driven stochastic program", "Newsvendor problem", "Sample average approximation", "Weighted mean spread"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1423", "e:abstract": "The admission of emergency patients in a hospital is unscheduled, urgent, and takes priority over elective patients, who are usually scheduled several days in advance. Hospital beds are a critical resource, and the management of elective admissions by enforcing quotas could reduce incidents of shortfall. We propose a distributionally robust optimization approach for managing elective admissions to determine these quotas. Based on an ambiguous set of probability distributions, we propose an optimized <i>budget of variation</i> approach that maximizes the level of uncertainty the admission system can withstand without violating the expected bed shortfall constraint. We solve the robust optimization model by deriving a second order conic problem (SOCP) equivalent of the model. The proposed model is tested in simulations based on real hospital admission data, and we report favorable results for adopting the robust optimization models.", "e:keyword": ["Robust optimization", "Elective admission", "Healthcare", "Hospital operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1424", "e:abstract": "In this paper, we develop a distributionally robust portfolio optimization model where the robustness is across different dependency structures among the random losses. For a Fréchet class of discrete distributions with overlapping marginals, we show that the distributionally robust portfolio optimization problem is efficiently solvable with linear programming. To guarantee the existence of a joint multivariate distribution consistent with the overlapping marginal information, we make use of a graph theoretic property known as the running intersection property. Building on this property, we develop a tight linear programming formulation to find the optimal portfolio that minimizes the worst-case conditional value-at-risk measure. Lastly, we use a data-driven approach with financial return data to identify the Fréchet class of distributions satisfying the running intersection property and then optimize the portfolio over this class of distributions. Numerical results in two different data sets show that the distributionally robust portfolio optimization model improves on the sample-based approach.", "e:keyword": ["Distributionally robust optimization", "Portfolio optimization", "Overlapping marginals"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1425", "e:abstract": "Many sequential decision problems can be formulated as Markov decision processes (MDPs) where the optimal value function (or <i>cost-to-go</i> function) can be shown to satisfy a monotone structure in some or all of its dimensions. When the state space becomes large, traditional techniques, such as the backward dynamic programming algorithm (i.e., backward induction or value iteration), may no longer be effective in finding a solution within a reasonable time frame, and thus we are forced to consider other approaches, such as approximate dynamic programming (ADP). We propose a provably convergent ADP algorithm called <i>Monotone-ADP</i> that exploits the monotonicity of the value functions to increase the rate of convergence. In this paper, we describe a general finite-horizon problem setting where the optimal value function is monotone, present a convergence proof for Monotone-ADP under various technical assumptions, and show numerical results for three application domains: <i>optimal stopping</i>, <i>energy storage</i>/<i>allocation</i>, and <i>glycemic control for diabetes patients</i>. The empirical results indicate that by taking advantage of monotonicity, we can attain high quality solutions within a relatively small number of iterations, using up to two orders of magnitude less computation than is needed to compute the optimal solution exactly.", "e:keyword": ["Approximate dynamic programming", "Monotonicity", "Optimal stopping", "Energy storage", "Glycemic control"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1426", "e:abstract": "We study the problem of finding ordinal utility functions to rank the outcomes of a decision when preference independence between the attributes is not present. We propose that the next level of complexity is to assume that preferences over one attribute can switch at most once as another attribute varies from low to high. We refer to this property as ordinal one-switch independence. We present both necessary and sufficient conditions for this ordinal property to hold and provide families of functions that satisfy this new property.", "e:keyword": ["Utility theory", "Dependence"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1427", "e:abstract": "The basic optimization problem of road design is quite challenging due to an objective function that is the sum of nonsmooth functions and the presence of set constraints. In this paper, we model and solve this problem by employing the Douglas-Rachford splitting algorithm. This requires a careful study of new proximity operators related to minimizing area and to the stadium norm. We compare our algorithm to a state-of-the-art projection algorithm. Our numerical results illustrate the potential of this algorithm to significantly reduce cost in road design.", "e:keyword": ["Convex function", "Convex set", "Douglas-Rachford algorithm", "Fenchel conjugate", "Intrepid projector", "Method of cyclic intrepid projections", "Norm", "Projection", "Projector", "Proximal mapping", "Proximity operator", "Road design", "Stadium norm"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1428", "e:abstract": "Most flight delays are created by imbalances between demand and capacity at the busiest airports. Absent large increases in capacity, airport congestion can only be mitigated through scheduling interventions or improved capacity utilization. This paper presents an integrated approach that jointly optimizes the airport’s flight schedule at the strategic level and the utilization of airport capacity at the tactical level, subject to scheduling, capacity, and delay-reduction constraints. The capacity-utilization part involves controlling the runway configuration and the balance of arrival and departure service rates to minimize congestion costs. The schedule optimization reschedules a selected set of flights to reduce the demand-capacity mismatches while minimizing interference with airline competitive scheduling. We develop an original iterative solution algorithm that integrates a stochastic queuing model of airport congestion, a dynamic programming model of capacity utilization, and an integer programming model of scheduling interventions. The algorithm is shown to converge in reasonable computational times. Extensive computational results for JFK Airport suggest that substantial delay reductions can be achieved through limited changes in airline schedules. It is also shown that the proposed integrated approach to airport congestion mitigation performs significantly better than the typical sequential approach, where scheduling and operational decisions are made separately.", "e:keyword": ["Airport congestion mitigation", "Airport demand management", "Integer programming", "Dynamic programming", "Queuing model", "Benefits of integration"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1429", "e:abstract": "Peak producers of nonstorable products, such as electricity, provide crucial flexible operating capacity to respond to infrequent and transient high demand periods. Faced with these uncertain revenue-earning opportunities, despite often having significant price-setting power, they need to profit from a limited number of pricing decisions in order to meet financial targets for viability. We study the repeated interaction between peak producers with a model that captures both the uncertainty in their short-term revenues and their market power. We investigate the conditions under which peak producers can implicitly coordinate to achieve high prices, under varying demand conditions. We analyze how financial objectives in the form of annual performance targets dynamically impact peakers’ pricing decisions, and the conditions under which setting such targets may benefit or hurt the owners of the firm. We further show how portfolio integration with lower marginal cost technologies can be an important factor in peak price setting, beyond the usual considerations of direct price externalities, capacity manipulation, or risk. These insights are useful not only in understanding how purely energy-based revenues can sustain the financial viability of peakers, and the dynamic emergence of price spikes, but also in providing the underlying process for pricing derivative contracts that policy makers may encourage or offer for resource adequacy.", "e:keyword": ["Noncooperative games", "Energy", "Electric industries"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1430", "e:abstract": "Recent advances in information technology have allowed firms to gather vast amounts of data regarding consumers’ preferences and the structure and intensity of their social interactions. This paper examines a game-theoretic model of competition between firms that can target their marketing budgets to individuals embedded in a social network. We provide a sharp characterization of the optimal targeted advertising strategies and highlight their dependence on the underlying social network structure. Furthermore, we provide conditions under which it is optimal for the firms to asymmetrically target a subset of the individuals and establish a lower bound on the ratio of their payoffs in these asymmetric equilibria. Finally, we find that at equilibrium firms invest inefficiently high in targeted advertising and the extent of the inefficiency is increasing in the centralities of the agents they target. Taken together, these findings shed light on the effect of the network structure on the outcome of marketing competition between the firms.", "e:keyword": ["Social networks", "Competition", "Targeted advertising", "Targeting"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1431", "e:abstract": "As various firms initially make information and access to their products/services scarce within a social network, identifying influential players emerges as a pivotal step for their success. In this paper, we tackle this problem using a stylized model that features payoff externalities and local network effects. The network designer is allowed to release information to only a subset of players (leaders); these targeted players make their contributions first and the rest (followers) move subsequently after observing the leaders’ decisions. In the presence of incomplete information, the signaling incentive drives the optimal selection of leaders and can lead to a first-order materialistic effect on equilibrium contributions.We propose a novel index for the key leader selection with incomplete information that can be substantially different from the key player index in Ballester et al. (2006) [Ballester C, Calvó-Armengol A, Zenou Y (2006) Who’s who in networks. wanted: The key player. <i>Econometrica</i> 74(5):1403–1417] and the key leader index with complete information proposed in Zhou and Chen (2015) [Zhou J, Chen Y-J (2015) Key leaders in social networks. <i>J. Econom. Theory</i> 157:212–235]. We also show that in undirected graphs, the optimal leader group identified in Zhou and Chen (2015) is exactly the optimal follower group when signaling is present. In particular, if the graphs are complete, the network designer ranks the players by the <i>ascending</i> order of their intrinsic valuations, and the leaders are those with lower intrinsic valuations. In the out-tree hierarchical structure, the key leader turns out to be the one that stays in the middle, and it is not necessarily exactly the central player in the network.", "e:keyword": ["Social network", "Signaling", "Information management", "Targeted advertising", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1432", "e:abstract": "We study a model of collective real-time decision making (or learning) in a social network operating in an uncertain environment, for which no a priori probabilistic model is available. Instead, the environment’s impact on the agents in the network is seen through a sequence of cost functions, revealed to the agents in a causal manner only after all the relevant actions are taken. There are two kinds of costs: individual costs incurred by each agent and local-interaction costs incurred by each agent and its neighbors in the social network. Moreover, agents have inertia: each agent has a default mixed strategy that stays fixed regardless of the state of the environment, and must expend effort to deviate from this strategy in order to respond to cost signals coming from the environment. We construct a decentralized strategy, wherein each agent selects its action based only on the costs directly affecting it and on the decisions made by its neighbors in the network. In this setting, we quantify social learning in terms of regret, which is given by the difference between the realized network performance over a given time horizon and the best performance that could have been achieved in hindsight by a fictitious centralized entity with full knowledge of the environment’s evolution. We show that our strategy achieves the regret that scales polylogarithmically with the time horizon and polynomially with the number of agents and the maximum number of neighbors of any agent in the social network.", "e:keyword": ["Decentralized optimization", "Knightian uncertainty", "Regret minimization", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1433", "e:abstract": "We introduce a new type of preference condition for intertemporal choice, which requires present values to be independent of various other variables. The new conditions are more concise and more transparent than traditional ones. They are directly related to applications because present values are widely used tools in intertemporal choice. Our conditions give more general behavioral axiomatizations, which facilitate normative debates and empirical tests of time inconsistencies and related phenomena. Like other preference conditions, our conditions can be tested qualitatively. Unlike other preference conditions, our conditions can also be directly tested quantitatively, and we can verify the required independence of present values from predictors in regressions. We show how similar types of preference conditions, imposing independence conditions between directly observable quantities, can be developed for decision contexts other than intertemporal choice and can simplify behavioral axiomatizations there. Our preference conditions are especially efficient if several types of aggregation are relevant because we can handle them in one stroke. We thus give an efficient axiomatization of a market pricing system that is (i) arbitrage-free for hedging uncertainties and (ii) time consistent.", "e:keyword": ["Intertemporal optimization", "Present value", "Discounted utility", "Time inconsistency", "Arbitrage-free", "Preference axiomatization", "Decision analysis", "Sequential", "Utility", "Multi-attribute", "Mathematics", "Functions"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1434", "e:abstract": "Single commodity networks are considered, where demands at the nodes are random. The problem is to find minimum cost optimal built in capacities at the nodes and arcs subject to the constraint that all demands should be met on a prescribed probability level (reliability constraint) and some deterministic constraints should be satisfied. The reliability constraint is formulated in terms of the Gale–Hoffman feasibility inequalities, but their number is reduced by elimination technique. The concept of a <i>p</i>-efficient point is used in a smart way to convert and then relax the problem into an LP. The <i>p</i>-efficient points are simultaneously generated with the solution of the LP. The joint distribution of the demands is used to obtain the <i>p</i>-efficient points for all stochastic inequalities that were not eliminated and the solution of a multiple choice knapsack problem is used to generate <i>p</i>-efficient points. The model can be applied to planning in interconnected power systems, flood control networks, design of shelter and road capacities in evacuation, parking lot capacities, financial networks, cloud computing system design, etc. Numerical examples are presented.", "e:keyword": ["Network design", "Stochastic networks", "Stochastic programming", "Probabilistic constraint", "p-efficient points"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1435", "e:abstract": "Postmarketing drug surveillance is the process of monitoring the adverse events of pharmaceutical or medical devices after they are approved by the appropriate regulatory authorities. Historically, such surveillance was based on voluntary reports by medical practitioners, but with the widespread adoption of electronic medical records and comprehensive patient databases, surveillance systems that utilize such data are of considerable interest. Unfortunately, existing methods for analyzing the data in such systems ignore the open-ended exploratory nature of such systems that requires the assessment of multiple possible adverse events. In this article, we propose a method, SEQMEDS, that assesses the effect of a single drug on multiple adverse events by analyzing data that accumulate sequentially and explicitly captures interdependencies among the multiple events. The method continuously monitors a vector-valued test-statistic derived from the cumulative number of adverse events. It flags a potential adverse event once the test-statistic crosses a stopping boundary. We employ asymptotic analysis that assumes a large number of observations in a given window of time to show how to compute the stopping boundary by solving a convex optimization problem that achieves a desired Type I error and minimizes the expected time to detection under a pre-specified alternative hypothesis. We apply our method to a model in which the interdependency among the multiple adverse events is captured by a Cox proportional hazards model with time-dependent covariates and demonstrate that it provides an approximation of a fully sequential test for the maximum hazard ratio of the drug over multiple adverse events. A numerical study verifies that our method delivers Type I /II errors that are below pre-specified levels and is robust to distributional assumptions and parameter values.", "e:keyword": ["Health care", "Stochastic models", "Drug surveillance", "Adverse drug events"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1436", "e:abstract": "Linear regression models are traditionally built through trial and error to balance many competing goals such as predictive power, interpretability, significance, robustness to error in data, and sparsity, among others. This problem lends itself naturally to a mixed integer quadratic optimization (MIQO) approach but has not been modeled this way because of the belief in the statistics community that MIQO is intractable for large scale problems. However, in the last 25 years (1991–2015), algorithmic advances in integer optimization combined with hardware improvements have resulted in an astonishing 450 billion factor speedup in solving mixed integer optimization problems. We present an MIQO-based approach for designing high quality linear regression models that explicitly addresses various competing objectives and demonstrate the effectiveness of our approach on both real and synthetic data sets.", "e:keyword": ["Integer programming", "Statistics"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1438", "e:abstract": "We consider an infinite-horizon single-product pricing problem in which a fraction of customers is patient and the remaining fraction is impatient. A patient customer will wait up to some fixed number of time periods for the price of the product to fall below his or her valuation at which point the customer will make a purchase. If the price does not fall below a patient customer’s valuation at any time during those periods, then that customer will leave without buying. In contrast, impatient customers will not wait, and either buy immediately or leave without buying. We prove that there is an optimal dynamic pricing policy comprised of repeating cycles of decreasing prices. We obtain bounds on the length of these cycles, and we exploit these results to produce an efficient dynamic programming approach for computing such an optimal policy. We also consider problems in which customers have variable levels of patience. For such problems, cycles of decreasing prices may no longer be optimal, but numerical experiments nevertheless suggest that such a decreasing cyclic policy (suitably chosen) often performs quite well.", "e:keyword": ["Pricing", "Consumer behavior", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1439", "e:abstract": "In this paper we formulate and analyze a novel model on a firm’s dynamic inventory and markdown decisions for perishable goods. We consider a dynamic stochastic setting, where every period consists of two phases, clearance phase and regular-sales phase. In the clearance phase, the firm decides how much to order for regular sales, as well as whether to markdown some (or all) of the leftover inventory from the previous period that will be disposed otherwise. Since strategic consumers may buy the product during clearance sales for future consumption, markdown may cannibalize future sales at regular price. Hence, the firm needs to make a trade-off between product spoilage and intertemporal demand substitution. We show that the firm should either put all of the leftover inventory on discount or dispose all of it, and the choice depends on the amount of leftover inventory from the previous period. In particular, the firm should introduce markdown when the amount of leftover inventory is higher than a certain threshold, and dispose all otherwise. We also conduct numerical studies to further characterize the optimal policy, and to evaluate the loss of efficiency under static policies when compared to the optimal dynamic policy.", "e:keyword": ["Revenue management", "Inventory", "Markdown", "Intertemporal substitution"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1440", "e:abstract": "We consider a retailer that sells the same or different versions of the product season after season. At the beginning of each season (stage 1), the retailer places an order and sells the product at the full price. As the sales unfold, the retailer has an opportunity to mark down the price (stage 2), which creates an incentive for strategic consumers to delay their purchases for price discount. However, consumers do not know the markdown price exactly when they time their purchases; instead, they learn from the retailer’s past prices and form their estimate of the markdown price, called the reference price, to decide if they purchase at the full price or wait for the markdown in the current selling season. We characterize the properties of the optimal ordering and markdown decisions and show that markdowns, if adopted appropriately, do not necessarily destroy the stability of a business even in the presence of strategic consumers. Furthermore, the consumers' reference exhibits a mean reverting pattern under certain conditions; that is, the reference fluctuates around a mean value, reflecting the practice of most stable businesses. We conduct numerical studies to investigate the impact of consumer learning about the reference price and various system parameters on the retailer’s optimal strategies and profitability in the presence of strategic consumers.", "e:keyword": ["Strategic consumers", "Reference effects", "Inventory and markdown decisions"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1441", "e:abstract": "We consider an online multi-item retailer with multiple fulfillment facilities and finite inventory. The challenge faced by the retailer is to construct a fulfillment policy to decide from which facility each of the items in the arriving order should be fulfilled, in a way that minimizes the expected total shipping costs of fulfilling customer orders over a finite horizon. Shipping costs are linear in the size of the package shipped as well as the distance from the facility to the customer. We approximate the stochastic control formulation, which is computationally intractable, with a deterministic linear program (DLP) whose size is polynomial in the size of the input. We then study the performance of two fulfillment heuristics derived from the solution of the DLP. The first heuristic implements the solution of the DLP as fulfillment probability for each item. Since fulfillment decision for each item is made independently of fulfillment decision of other items in the same order, this heuristic does not have a satisfactory performance. The second heuristic improves the first heuristic by allowing fulfillment consolidation across different items in the same order. We do this by modifying the DLP solution through a carefully constructed correlated rounding (or coupling) among the decision variables. We provide a theoretical upper bound on the asymptotic competitive ratio of both heuristics with respect to the optimal policy. Our numerical experiments show that the second heuristic performs very close to optimal for a wide range of problem parameters.", "e:keyword": ["Inventory management", "Ecommerce", "Asymptotic analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1442", "e:abstract": "The linear programming approach to approximate dynamic programming has received considerable attention in the recent network revenue management literature. A major challenge of the approach lies in solving the resulting approximate linear programs (ALPs), which often have a huge number of constraints and/or variables. We show that the ALPs can be dramatically reduced in size for both affine and separable piecewise linear approximations to network revenue management problems, under both independent and discrete choice models of demand. Our key result is the equivalence between each ALP and a corresponding reduced program, which is more compact in size and admits an intuitive probabilistic interpretation. For the affine approximation to network revenue management under an independent demand model, we recover an equivalence result known in the literature, but provide an alternative proof. Our other equivalence results are new. We test the numerical performance of solving the reduced programs directly using off-the-shelf commercial solvers on a set of test instances taken from the literature.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Transportation", "Yield management"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1443", "e:abstract": "We analyze a general model in which, at each echelon of the supply process, an arbitrary number of firms compete, offering one or multiple products to some or all of the firms at the next echelon, with firms at the most downstream echelon selling to the end consumer. At each echelon, the offered products are differentiated and the firms belonging to this echelon engage in <i>price</i> competition. The model assumes a general set of piecewise linear consumer demand functions for all products (potentially) brought to the consumer market, where each product’s demand volume may depend on the retail prices charged for all products; consumers’ preferences over the various product/retailer combinations are <i>general</i> and <i>asymmetric</i>. Similarly, the cost rates incurred by the firms at the most upstream echelon are general as well. We fully characterize the equilibrium behavior under linear price-only contracts, and we show how all equilibrium performance measures can be computed via a simple recursive scheme. Moreover, we establish how changes in the model parameters, in particular, exogenous cost rates or intercept values in the demand functions, impact the system-wide equilibrium. These comparative statics results allow for the quantification of cost pass-through effects and the measurement and characterization of the firms’ brand value. Lastly, we illustrate what qualitative impacts various changes in the structure of the supply chain network may bring forth.", "e:keyword": ["Stackelberg game", "Price competition", "Differentiated products", "Product assortment", "Sequential oligopoly", "Multiechelon", "Supply chain network"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1444", "e:abstract": "Regions with abundant wind resources usually have no ready access to the existing electric grid. However, building transmission lines that instantaneously deliver all geographically distributed wind energy can be costly. Energy storage (ES) systems can help reduce the cost of bridging wind farms and grids and mitigate the intermittency of wind outputs. In this paper, we propose models of transmission network planning with colocation of ES systems. Our models determine the sizes and sites of ES systems as well as the associated topology and capacity of the transmission network under the feed-in-tariff policy instrument. We first formulate a location model as a mixed-integer second-order-conic program to solve for the ES-transmission network design with uncapacitated storage. Then we propose a method to choose ES sizes by deriving a closed-form upper bound. The major insight is that, in most cases, using even small-sized ES systems can significantly reduce the total expected cost, but their marginal values diminish faster than those of the transmission lines as their capacities expand. Despite uncertainties in climate, technologies, and construction costs, the cost-efficient infrastructure layout is remarkably robust. We also identify the major bottleneck cost factors for different forms of ES technologies.", "e:keyword": ["Facility location", "Wind energy", "Energy storage", "Infrastructure planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1445", "e:abstract": "We study a dynamic pricing problem of a firm facing reference price effects at an aggregate demand level, where demand is more sensitive to gains than losses. We find that even the myopic pricing strategy belongs to one type of discontinuous maps, which can exhibit complex dynamics over time. Our numerical examples show that, in general, the optimal pricing strategies may not admit any simple characterizations and the resulting reference price/price dynamics can be very complicated. We then show for a special case that a cyclic skimming pricing strategy is optimal, and we provide conditions to guarantee the optimality of high-low pricing strategies.", "e:keyword": ["Dynamic pricing", "Gain seeking", "Reference price effects", "Discontinuous maps"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1446", "e:abstract": "This paper studies a capacity management problem with upgrading. A firm needs to procure multiple classes of capacities and then allocate the capacities to satisfy multiple classes of customers that arrive over time. A general upgrading rule is considered, i.e., unmet demand can be satisfied using multistep upgrade. No replenishment is allowed and the firm has to make the allocation decisions without observing future demand. We first characterize the structure of the optimal allocation policy, which consists of parallel allocation and then sequential rationing. Specifically, the firm first uses capacity to satisfy the same-class demand as much as possible, then considers possible upgrading decisions in a sequential manner. We also propose a heuristic based on certainty equivalence control to solve the problem. Numerical analysis shows that the heuristic is fast and delivers close-to-optimal profit for the firm. Finally, we conduct extensive numerical studies to derive insights into the problem. It is found that under the proposed heuristic, the value of using sophisticated multistep upgrading can be quite significant; however, using simple approximations for the initial capacity leads to negligible profit loss, which suggests that the firm’s profit is not sensitive to the initial capacity decision if the optimal upgrading policy is used.", "e:keyword": ["Capacity management", "Inventory", "Upgrading", "Dynamic programming", "Revenue management"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1447", "e:abstract": "Tenure decisions, key decisions in academic institutions, are primarily based on subjective assessments of candidates. Using a large-scale bibliometric database containing 198,310 papers published 1975–2012 in the field of operations research (OR), we propose prediction models of whether a scholar would perform well on a number of future success metrics using statistical models trained with data from the scholar’s first five years of publication, a subset of the information available to tenure committees. These models, which use network centrality of the citation network, coauthorship network, and a dual network combining the two, significantly outperform simple predictive models based on citation counts alone. Using a data set of the 54 scholars who obtained a Ph.D. after 1995 and held an assistant professorship at a top-10 OR program in 2003 or earlier, these statistical models, using data up to five years after the scholar became an assistant professor and constrained to tenure the same number of candidates as tenure committees did, made a different decision than the tenure committees for 16 (30%) of the candidates. This resulted in a set of scholars with significantly better future A-journal paper counts, citation counts, and <i>h</i>-indexes than the scholars actually selected by tenure committees. These results show that analytics can complement the tenure decision-making process in academia and improve the prediction of academic impact.", "e:keyword": ["Citation analysis", "Academic impact", "Analytics", "Networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1448", "e:abstract": "We study the evolution of opinions (or beliefs) over a social network modeled as a signed graph. The sign attached to an edge in this graph characterizes whether the corresponding individuals or end nodes are friends (positive links) or enemies (negative links). Pairs of nodes are randomly selected to interact over time, and when two nodes interact, each of them updates its opinion based on the opinion of the other node and the sign of the corresponding link. This model generalizes the DeGroot model to account for negative links: when two adversaries interact, their opinions go in opposite directions. We provide conditions for convergence and divergence in expectation, in mean-square, and in almost sure sense and exhibit phase transition phenomena for these notions of convergence depending on the parameters of the opinion update model and on the structure of the underlying graph. We establish a <i>no-survivor</i> theorem, stating that the difference in opinions of any two nodes diverges whenever opinions in the network diverge as a whole. We also prove a <i>live-or-die</i> lemma, indicating that almost surely, the opinions either converge to an agreement or diverge. Finally, we extend our analysis to cases where opinions have hard lower and upper limits. In these cases, we study when and how opinions may become asymptotically clustered to the belief boundaries and highlight the crucial influence of (strong or weak) structural balance of the underlying network on this clustering phenomenon.", "e:keyword": ["Opinion dynamics", "Signed graph", "Social networks", "Opinion clustering"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1449", "e:abstract": "This paper develops a spectral theory of Markovian asset pricing models where the underlying economic uncertainty follows a continuous-time Markov process <i>X</i> with a general state space (Borel right process, or BRP) and the stochastic discount factor (SDF) is a positive semimartingale multiplicative functional of <i>X</i>. A key result is the uniqueness theorem for a positive eigenfunction of the pricing operator such that <i>X</i> is recurrent under a new probability measure associated with this eigenfunction (recurrent eigenfunction). As economic applications, we prove uniqueness of the Hansen and Scheinkman factorization of the Markovian SDF corresponding to the recurrent eigenfunction; extend the Recovery Theorem from discrete time, finite state irreducible Markov chains to recurrent BRPs; and obtain the long-maturity asymptotics of the pricing operator. When an asset pricing model is specified by given risk-neutral probabilities together with a short rate function of the Markovian state, we give sufficient conditions for the existence of a recurrent eigenfunction and provide explicit examples in a number of important financial models, including affine and quadratic diffusion models and an affine model with jumps. These examples show that the recurrence assumption, in addition to fixing uniqueness, rules out unstable economic dynamics, such as the short rate asymptotically going to infinity or to a zero lower bound trap without possibility of escaping.", "e:keyword": ["Asset pricing", "Markov processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1450", "e:abstract": "Assortment planning of substitutable products is a major operational issue that arises in many industries such as retailing, airlines, and consumer electronics. We consider a single-period joint assortment and inventory planning problem under dynamic substitution with stochastic demands, and provide complexity and algorithmic results as well as insightful structural characterizations of near-optimal solutions for important variants of the problem. First, we show that the assortment planning problem is NP-hard even for a very simple consumer choice model, where each consumer is willing to buy only two products. In fact, we show that the problem is hard to approximate within a factor better than 1 − 1/<i>e</i>. Second, we show that for several interesting and practical customer choice models, one can devise a <i>polynomial-time approximation scheme</i> (PTAS), i.e., the problem can be solved efficiently to within any level of accuracy. To the best of our knowledge, this is the first efficient algorithm with provably near-optimal performance guarantees for assortment planning problems under dynamic substitution. Quite surprisingly, the algorithm we propose stocks only a <i>constant</i> number of different product types; this constant depends only on the desired accuracy level. This provides an important managerial insight that assortments with a relatively small number of product types can obtain almost all of the potential revenue. Furthermore, we show that our algorithm can be easily adapted for more general choice models, and we present numerical experiments to show that it performs significantly better than other known approaches.", "e:keyword": ["Assortment planning", "Dynamic substitution", "Polynomial time approximation schemes"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1451", "e:abstract": "We consider a market in which suppliers with asymmetric capacities and asymmetric marginal and fixed costs compete to satisfy a deterministic and inelastic demand of a commodity in a single period. The suppliers bid their costs to an auctioneer who determines the optimal allocation and the resulting payments, a typical situation in deregulated electricity markets. Under classical marginal-cost pricing, the nonconvexity of the total cost may result in losses for some suppliers because they may fail to recover their fixed cost through commodity payments only. To address this problem, various pricing schemes that lift the price above marginal cost and/or provide side-payments (uplifts) have been proposed in the literature. We review several of these schemes, also proposing a new variant, in a two-supplier setting. We derive closed-form expressions for the price, uplifts, and profits that each scheme generates that enable us to analytically compare these schemes along these three dimensions. Our analysis complements known numerical comparisons available in the literature. We extend some of our analytical comparisons to the case of more than two suppliers and discuss extant numerical comparisons for this case. Further, we present known results concerning the potential for supplier strategic bidding behavior in the context of the considered pricing schemes, emphasizing when possibilities for market manipulation exist.", "e:keyword": ["Markets", "Non-convexities", "Pricing", "Energy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1452", "e:abstract": "Companies in diverse industries must decide the pricing policy of their inventories over time. This decision becomes particularly complex when customers are forward looking and may defer a purchase in the hope of future discounts and promotions. With such uncertainty, many customers may end up not buying or buying at a significantly lower price, reducing the firm’s profitability. Recent studies show that a way to mitigate this negative effect caused by strategic consumers is to use a posted or preannounced pricing policy. With that policy, firms commit to a price path that consumers use to evaluate their purchase timing decision. In this paper, we propose a class of preannounced pricing policies in which the price path corresponds to a price menu contingent on the available inventory. We present a two-period model, with a monopolist selling a fixed inventory of a good. Given a menu of prices specified by the firm and beliefs regarding the number of units to be sold, customers decide whether to buy upon arrival, buy at the clearance price, or not to buy. The firm determines the set of prices that maximizes revenues. The solution to this problem requires the concept of equilibrium between the seller and the buyers that we analyze using a novel approach based on ordinary differential equations. We show existence of equilibrium and uniqueness when only one unit is on sale. However, if multiple units are offered, we show that multiple equilibria may arise. We develop a gradient-based method to solve the firm’s optimization problem and conduct a computational study of different pricing schemes. The results show that under certain conditions the proposed contingent preannounced policy outperforms previously proposed pricing schemes. The source of the improvement comes from the use of the proposed pricing policy as a barrier to discourage strategic waiting and as a discrimination tool for those customers waiting.", "e:keyword": ["Pricing", "Announced discounts", "Strategic consumers"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1453", "e:abstract": "In recent years, several approximation methods have been proposed for the choice network revenue management problem. These approximation methods are proposed because the dynamic programming formulation of the choice network revenue management problem is intractable even for moderately sized instances. In this paper, we consider three approximation methods that obtain upper bounds on the value function, namely, the choice deterministic linear program (CDLP), the affine approximation (AF), and the piecewise-linear approximation (PL). It is known that the piecewise-linear approximation bound is tighter than the affine bound, which in turn is tighter than CDLP. In this paper, we prove bounds on how much the affine and piecewise-linear approximations can tighten CDLP. We show (i) the gap between the AF and CDLP bounds is at most a factor of <inline-formula id=\"IEq0001\"><mml:math id=\"IEq0001-mml\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>{</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>}</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id=\"IEq0002\"><mml:math id=\"IEq0002-mml\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> are the resource capacities, and (ii) the gap between the piecewise-linear and CDLP bounds is within a factor of 2. Moreover, we show that these gaps are essentially tight. Our results hold for <i>any</i> discrete-choice model and do not involve any asymptotic scaling. Our results are surprising because calculating the AF bound is NP-hard and CDLP is tractable for a single-segment multinomial logit model; our result implies that if a firm has all resource capacities of 100, the gap between the two bounds, however, is at most 1.01.", "e:keyword": ["Network revenue management", "Discrete choice models", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1454", "e:abstract": "Operations managers do not typically have full information about the demand distribution. Recognizing this, data-driven approaches have been proposed in which the manager has no information beyond the evolving history of demand observations. In practice, managers often have some partial information about the demand distribution in addition to demand observations. We consider a repeated newsvendor setting, and propose a maximum-entropy based technique, termed Second Order Belief Maximum Entropy (SOBME), which allows the manager to effectively combine demand observations with distributional information in the form of bounds on the moments or tails. In the proposed approach, the decision maker forms a belief about possible demand distributions, and dynamically updates it over time using the available data and the partial distributional information. We derive a closed-form solution for the updating mechanism, and highlight that it generalizes the traditional Bayesian mechanism with an exponential modifier that accommodates partial distributional information. We prove the proposed approach is (weakly) consistent under some technical regularity conditions and we analytically characterize its rate of convergence. We provide an analytical upper bound for the newsvendor’s cost of ambiguity, i.e., the extra per-period cost incurred because of ambiguity, under SOBME, and show that it approaches zero quite quickly. Numerical experiments demonstrate that SOBME performs very well. We find that it can be very beneficial to incorporate partial distributional information when deciding stocking quantities, and that information in the form of tighter moment bounds is typically more valuable than information in the form of tighter ambiguity sets. Moreover, unlike pure data-driven approaches, SOBME is fairly robust to the newsvendor quantile. Our results also show that SOBME quickly detects and responds to hidden changes in the unknown true distribution. We also extend our analysis to consider ambiguity aversion, and develop theoretical and numerical results for the ambiguity-averse, repeated newsvendor setting.", "e:keyword": ["Data-driven newsvendor", "Maximum entropy", "Distribution ambiguity", "Moment information", "Ambiguity aversion"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1455", "e:abstract": "We consider the problem of detecting the source of a rumor which has spread in a network using only observations about which set of nodes are infected with the rumor and with no information as to <i>when</i> these nodes became infected. In a recent work (<xref ref-type=\"bibr\" rid=\"B28\">Shah and Zaman 2010</xref>), this rumor source detection problem was introduced and studied. The authors proposed the graph score function <i>rumor centrality</i> as an estimator for detecting the source. They establish it to be the maximum likelihood estimator with respect to the popular Susceptible Infected (SI) model with exponential spreading times for regular trees. They showed that as the size of the infected graph increases, for a path graph (2-regular tree), the probability of source detection goes to 0 and for <i>d</i>-regular trees with <i>d</i> ≥ 3 the probability of detection, say <i>α<sub>d</sub></i>, remains bounded away from 0 and is less than 1/2. However, their results stop short of providing insights for the performance of the rumor centrality estimator in more general settings such as irregular trees or the SI model with nonexponential spreading times.This paper overcomes this limitation and establishes the effectiveness of rumor centrality for source detection for generic random trees and the SI model with a generic spreading time distribution. The key result is an interesting connection between a continuous time branching process and the effectiveness of rumor centrality. Through this, it is possible to quantify the detection probability precisely. As a consequence, we recover all previous results as a special case and obtain a variety of novel results including the <i>universality</i> of rumor centrality in the context of tree-like graphs and the SI model with a generic spreading time distribution.", "e:keyword": ["Rumors", "Networks", "Source detection", "Information diffusion"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1456", "e:abstract": "The growing uncertainty associated with the increasing penetration of wind and solar power generation has presented new challenges to the operation of large-scale electric power systems. Motivated by these challenges, we present a multistage adaptive robust optimization model for the most critical daily operational problem of power systems, namely, the unit commitment (UC) problem, in the situation where nodal net electricity loads are uncertain. The proposed multistage robust UC model takes into account the time causality of the hourly unfolding of uncertainty in the power system operation process, which we show to be relevant when ramping capacities are limited and net loads present significant variability. To deal with large-scale systems, we explore the idea of simplified affine policies and develop a solution method based on constraint generation. Extensive computational experiments on the IEEE 118-bus test case and a real-world power system with 2,736 buses demonstrate that the proposed algorithm is effective in handling large-scale power systems and that the proposed multistage robust UC model can significantly outperform the deterministic UC and existing two-stage robust UC models in both operational cost and system reliability.", "e:keyword": ["Electric energy systems", "Multistage robust optimization", "Affine policies", "Constraint generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1457", "e:abstract": "We present a unifying linear programming approach to the calculation of various directional derivatives for a very large class of production frontiers of data envelopment analysis (DEA). Special cases of this include different marginal rates, the scale elasticity, and a spectrum of partial and mixed elasticity measures. Our development applies to any polyhedral production technology including, to name a few, the conventional variable and constant returns-to-scale DEA technologies, their extensions with weight restrictions, technologies with weakly disposable undesirable outputs, and network DEA models. Furthermore, our development provides a general method for characterization of returns to scale (RTS) in any polyhedral technology. The new approach effectively removes the need to develop bespoke models for the RTS characterization and calculation of marginal rates and elasticity measures for each particular technology.", "e:keyword": ["Data envelopment analysis", "Elasticity measures", "Returns to scale", "Marginal rates", "Weight restrictions", "Undesirable outputs"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1458", "e:abstract": "This paper initiates the study of the testable implications of choice data in settings where agents have privacy preferences. We adapt the standard conceptualization of consumer choice theory to a situation where the consumer is aware of, and has preferences over, the information revealed by her choices. The main message of the paper is that little can be inferred about consumers’ preferences once we introduce the possibility that the consumer has concerns about privacy. This holds even when consumers’ privacy preferences are assumed to be monotonic and separable. This motivates the consideration of stronger assumptions and, to that end, we introduce an additive model for privacy preferences that has testable implications.", "e:keyword": ["Privacy", "Revealed preference"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1459", "e:abstract": "We investigate the use of a canonical version of a discrete choice model due to Daganzo (1979) [Daganzo C (1979) <i>Multinomial Probit: The Theory and Its Application to Demand Forecasting</i> (Academic Press, New York).] in optimal pricing and assortment planning. In contrast to multinomial and nested logit (the prevailing choice models used for optimizing prices and assortments), this model assumes a negatively skewed distribution of consumer utilities, an assumption we motivate by conceptual arguments as well as published work. The choice probabilities in this model can be derived in closed form as an <i>exponomial</i> (a linear function of exponential terms). The pricing and assortment planning insights we obtain from the exponomial choice (EC) model differ from the literature in two important ways. First, the EC model allows variable markups in optimal prices that increase with expected utilities. Second, when prices are exogenous, the optimal assortment may exhibit leapfrogging in prices, i.e., a product can be skipped in favor of a lower-priced one depending on the utility positions of neighboring products. These two plausible pricing and assortment patterns are ruled out by multinomial logit (and by nested logit within each nest). We provide structural results on optimal pricing for monopoly and oligopoly cases, and on the optimal assortments for both exogenous and endogenous prices. We also demonstrate how the EC model can be easily estimated—by establishing that the log-likelihood function is concave in model parameters and detailing an estimation example using real data.", "e:keyword": ["Discrete choice theory", "Willingness to pay", "Assortment planning", "Pricing", "Revenue management", "Multinomial logit", "Nested logit"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1460", "e:abstract": "Feed-in-tariff (FIT) policies aim at driving down the cost of renewable energy by fostering learning and accelerating the diffusion of green technologies. Under FIT mechanisms, governments purchase green energy at tariffs that are set above market price. The success or failure of FIT policies, in turn, critically depend on how these tariffs are determined and adjusted over time. This paper provides insights into designing cost-efficient and socially optimal FIT programs. Our modeling framework captures key market dynamics as well as investors’ strategic behavior. In this framework, we establish that the current practice of maintaining constant profitability is theoretically rarely optimal. By contrast, we characterize a no-delay region in the problem’s parameters, such that profitability should strictly decrease over time if the diffusion and learning rates belong to this region. In this case, investors never strategically postpone their investment to a later period. When the diffusion and learning rates fall outside the region, profitability should increase at least temporarily over some time periods and strategic delays occur. The presence of strategic delays, however, makes the practical problem of computing optimal FIT schedules very difficult. To address this issue, the regulator may focus on policies that disincentivize investors to postpone their investment. With this additional constraint, a constant profitability policy is optimal if and only if the diffusion and learning rates fall outside the no-delay region. This provides partial justifications for current FIT implementations.", "e:keyword": ["Technology diffusion", "Government incentive policies", "Renewable energy technology", "Feed-in-tariff", "Learning by doing", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1461", "e:abstract": "We study the impact of limited inventory on optimal sales-force compensation contracts. We adopt a principal-agent framework, characterized by limited liability and rent sharing with the agent. A commonly invoked assumption in the inventory management literature is that the demand distribution satisfies the increasing failure rate (IFR) property. Under this assumption, however, past research has established that a quota-bonus contract—a widely adopted sales-force compensation contract in the practice—cannot sustain in equilibrium. We show that because of demand censoring in the presence of limited inventory (i.e., demand realizations higher than the inventory level are unobservable), a quota-bonus contract is the optimal equilibrium contract, and it exists, even for a demand distribution with the IFR property. Since most well-known distributions satisfy the IFR property, and inventory constraints are operative in many real-world situations, our results significantly extend the scope of the optimality of quota-bonus contracts and underscore the importance of considering the inventory aspect while making sales-force compensation decisions.", "e:keyword": ["Quota-bonus contract", "Inventory", "Demand censoring", "IFR distribution"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1462", "e:abstract": "We consider a class of routing optimization problems under uncertainty in which all decisions are made before the uncertainty is realized. The objective is to obtain optimal routing solutions that would, as much as possible, adhere to a set of specified requirements after the uncertainty is realized. These problems include finding an optimal routing solution to meet the soft time window requirements at a subset of nodes when the travel time is uncertain, and sending multiple capacitated vehicles to different nodes to meet the customers’ uncertain demands. We introduce a precise mathematical framework for defining and solving such routing problems. In particular, we propose a new decision criterion, called the <i>Requirements Violation (RV) Index</i>, which quantifies the risk associated with the violation of requirements taking into account both the frequency of violations and their magnitudes whenever they occur. The criterion can handle instances when probability distributions are known, and ambiguity when distributions are partially characterized through descriptive statistics such as moments. We develop practically efficient algorithms involving Benders decomposition to find the exact optimal routing solution in which the RV Index criterion is minimized, and we give numerical results from several computational studies that show the attractive performance of the solutions.", "e:keyword": ["Vehicle routing", "Uncertain travel time", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1463", "e:abstract": "We analyze the equilibrium of an incomplete information game consisting of two capacity-constrained suppliers and a single retailer. The capacity of each supplier is her private information. Conditioned on their capacities, the suppliers simultaneously and noncooperatively offer quantity-price schedules to the retailer. Then, the retailer decides on the quantities to purchase from each supplier to maximize his own utility. We prove the existence of a (pure strategy) Nash equilibrium for this game. We show that at the equilibrium each (infinitesimal) unit of the supply is assigned a marginal price that is independent of the capacities and depends only on the valuation function of the retailer and the distribution of the capacities. In addition, the supplier with the larger capacity sells all her supply.", "e:keyword": ["Nonlinear pricing", "Horizontal competition", "Asymmetry of information", "Common agency", "Informed principal"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1464", "e:abstract": "By exploiting basic common practice accounting and risk-management rules, we propose a simple analytical dynamical model to investigate the effects of microprudential changes on macroprudential outcomes. Specifically, we study the consequence of the introduction of a financial innovation that allows reducing the cost of portfolio diversification in a financial system populated by financial institutions having capital requirements in the form of Value at Risk (VaR) constraint and following standard mark-to-market and risk-management rules. We provide a full analytical quantification of the multivariate feedback effects between investment prices and bank behavior induced by portfolio rebalancing in presence of asset illiquidity and show how changes in the constraints of the bank portfolio optimization endogenously drive the dynamics of the balance sheet aggregate of financial institutions and, thereby, the availability of bank liquidity to the economic system and systemic risk. The model shows that when financial innovation reduces the cost of diversification below a given threshold, the strength (because of higher leverage) and coordination (because of similarity of bank portfolios) of feedback effects increase, triggering a transition from a stationary dynamics of price returns to a nonstationary one characterized by steep growths (bubbles) and plunges (bursts) of market prices.", "e:keyword": ["Systemic risk", "Diversification", "Leverage", "Endogenous risk", "Financial innovation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1465", "e:abstract": "We study the problem of allocating supply under advance demand information. We consider a company that must allocate limited inventory to different markets that open sequentially. To reduce uncertainty, the company receives advance demand information and updates forecasts about its markets each time it makes an allocation decision. We study the value and optimal use of this information. This research is motivated by an agrifood manufacturer that operates in several European countries. We develop the optimal policy under relaxed conditions and an efficient heuristic policy that performs close to optimally under general conditions. We derive structural properties of the model to gain managerial insights, and we derive the optimal policy in closed form for the case of markets with identical prices. We use numerical experiments to demonstrate that the value of advance demand information can be significant. The managerial insights of this study include the observations that in environments such as the one that motivated our research, early markets receive systematically less supply than late markets and that the value of advance demand information is greatest if the initial supply is close to the initial forecasts.", "e:keyword": ["Agricultural supply chain", "Advance demand information", "Forecast updating", "Supply allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1466", "e:abstract": "In this paper we consider the notion of rectangularity of a set of probability measures from a somewhat different point of view. We define rectangularity as a property of dynamic decomposition of a distributionally robust stochastic optimization problem and show how it relates to the modern theory of coherent risk measures. Consequently, we discuss robust formulations of multistage stochastic optimization problems in frameworks of stochastic programming, stochastic optimal control, and Markov decision processes.", "e:keyword": ["Multistage stochastic optimization", "Rectangularity", "Risk-averse optimization", "Robust optimal control and Markov decision processes", "Dynamic programming", "Coherent risk measures", "Time consistency"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1469", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1470", "e:abstract": "Discontinuities are common in the pricing and hedging of complex financial derivatives. Quasi-Monte Carlo (QMC) methods for high-dimensional finance problems with discontinuities can be inefficient because of the lack of good smoothness and high dimensionality. Interestingly, path simulation method (PSM) may affect both factors, implying its significance in QMC methods. What defines a “good” PSM for problems with discontinuities? The ability to align the discontinuities with the coordinate axes is a desirable property for a PSM. We show that for an arbitrary PSM, there exists a class of options with discontinuous payoff functions such that the transformed functions have only axis-parallel discontinuities, for which good QMC performance can be expected. In this sense, any PSM can be “good” in QMC methods for a specific class of problems. We analyze the structure of discontinuities for digital options using the new approach and show the superiority and the uniqueness (up to a permutation) of the standard construction. We develop a two-step procedure for pricing and hedging derivatives with discontinuous payoff functions. The first step is to design a good PSM that has the ability to align the discontinuities with the coordinate axes and the second step is to further exploit this nice property to remove the discontinuities completely. We prove that the new estimate is unbiased and has smaller variance. Numerical experiments demonstrate that the two-step procedure is very effective in QMC methods for pricing options and estimating Greeks, leading to a dramatic variance reduction. Both the path simulation step and the smoothing step are crucial and beneficial for QMC methods, with the contribution from each step varying depending on the severity of discontinuity.", "e:keyword": ["Finance", "Financial engineering", "Option pricing", "Greeks", "Simulation", "Quasi-Monte Carlo methods", "Path simulation method", "Discontinuity", "Effective dimension"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1471", "e:abstract": "It is well known that optimizing network topology by switching on and off transmission lines improves the efficiency of power delivery in electrical networks. In fact, the USA Energy Policy Act of 2005 (Section 1223) states that the United States should “encourage, as appropriate, the deployment of advanced transmission technologies” including “optimized transmission line configurations.” As such, many authors have studied the problem of determining an optimal set of transmission lines to switch off to minimize the cost of meeting a given power demand under the direct current (DC) model of power flow. This problem is known in the literature as the <i>Direct-Current Optimal Transmission Switching</i> Problem (DC-OTS). Most research on DC-OTS has focused on heuristic algorithms for generating quality solutions or on the application of DC-OTS to crucial operational and strategic problems such as contingency correction, real-time dispatch, and transmission expansion. The mathematical theory of the DC-OTS problem is less well developed. In this work, we formally establish that DC-OTS is NP-Hard, even if the power network is a series-parallel graph with at most one load/demand pair. Inspired by Kirchoff’s Voltage Law, we provide a cycle-based formulation for DC-OTS, and we use the new formulation to build a cycle-induced relaxation. We characterize the convex hull of the cycle-induced relaxation; this characterization provides strong valid inequalities that can be used in a cutting-plane approach to solve the DC-OTS. We give details of a practical implementation, and we show promising computational results on standard benchmark instances.", "e:keyword": ["Integer programming", "Cutting plane/facets", "Electric industries"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1472", "e:abstract": "We consider new online variants of supply chain management models, where in addition to production decisions, one also has to actively decide on which customers to serve. Specifically, customers arrive sequentially during a selection phase, and one has to decide whether to accept or reject each customer upon arrival. If a customer is rejected, then a lost-sales cost is incurred. Once the selection decisions are all made, one has to satisfy all the accepted customers with minimum possible production cost. The goal is to minimize the total cost of lost sales and production. A key feature of the model is that customers arrive in an online manner, and the decision maker does not require any information about future arrivals.We provide two novel algorithms for online customer selection problems, which are based on repeatedly solving offline subproblems that ignore previously made decisions. For many important settings, our algorithms achieve small constant competitive ratio guarantees. That is, for any sequence of arriving customers, the cost incurred by the online algorithm is within a fixed constant factor of the cost incurred by the respective optimal solution that has full knowledge upfront on the sequence of arriving customers. Finally, we provide a computational study on the performance of these algorithms when applied to the economic lot sizing and joint replenishment problems with online customer selection.", "e:keyword": ["Supply chain management", "Online optimization", "Customer selection", "Economic lot sizing", "Facility location"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1473", "e:abstract": "We consider the dynamic pricing problem a monopolistic seller faces when customers arrive in heterogeneous time periods and their purchase decisions are affected by reference prices formed from their past observations of prices. We illustrate that a new form of price discrimination opportunity exists in such situations, where the seller’s optimal pricing strategy is a cyclic one, even when the customers are loss-neutral and their demand functions are identical. This result differs from those of prior studies where the optimal price paths are shown to be asymptotically constant and thus is unique due to the interaction between the heterogeneous arrivals and the reference price effects. We also provide the cycle length of the optimal pricing policy when the demand function is linear and when customers are loss-neutral. In this era when customer information is easier accessible, our results suggest the sellers consider this new dimension of price discrimination in conjunction with the old ones, to take advantage of the full power of customer data.", "e:keyword": ["Reference price effects", "Dynamic pricing", "Revenue management"]}, {"@id": "http://dx.doi.org/10.1287/opre.2015.1474", "e:abstract": "We propose a nonparametric data-driven algorithm called DDM for the management of stochastic periodic-review multiproduct inventory systems with a warehouse-capacity constraint. The demand distribution is not known a priori and the firm only has access to past sales data (often referred to as censored demand data). We measure performance of DDM through regret, the difference between the total expected cost of DDM and that of an oracle with access to the true demand distribution acting optimally. We characterize the rate of convergence guarantee of DDM. More specifically, we show that the average expected <i>T</i>-period cost incurred under DDM converges to the optimal cost at the rate of <i>O</i>(<i>T</i><sup>−1/2</sup>). Our asymptotic analysis significantly generalizes approaches used in <xref ref-type=\"bibr\" rid=\"B16\">Huh and Rusmevichientong (2009)</xref> for the uncapacitated single-product inventory systems. We also discuss several extensions and conduct numerical experiments to demonstrate the effectiveness of our proposed algorithm.", "e:keyword": ["Inventory", "Multiproduct", "Censored demand", "Nonparametric algorithms", "Asymptotic analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1476", "e:abstract": "This paper studies the design of voluntary disclosure regulations for a firm that faces a stochastic environmental hazard. The occurrence of such a hazard is known only to the firm. The regulator, if finding a hazard, collects a fine and mandates the firm to perform costly remediation that reduces the environmental damage. The regulator may inspect the firm at any time to uncover the hazard. However, because inspections are costly, the regulator also offers a reward to the firm for voluntarily disclosing the hazard. The reward corresponds to either a subsidy or a reduced fine, depending on whether it is positive or negative. Thus, the regulator needs to dynamically determine the reward and inspection policy that minimizes expected societal cost in the long run. We model this problem as a dynamic adverse selection problem with costly state verification in continuous time. Despite the complexity and generality of this setup, we show that the optimal regulation policy follows a very simple cyclic structure, which we fully characterize in closed form. Specifically, the regulator runs scheduled inspections periodically. After each inspection, the reward level decreases over time until a subsequent inspection takes place. If a hazard is not revealed, the reward level is reset to a high level, restarting the cycle. In contrast to the reward level, the mandated remediation level is constant over time. Nonetheless, when subsidies are not allowed in the industry, we show that the regulator should dynamically adjust this remediation level, which then acts as a substitute for a subsidy. Our analysis further reveals that optimal inspection frequency increases not only when the inspection accuracy decreases, but also when the penalty for not disclosing the hazard increases.", "e:keyword": ["Dynamic mechanism design", "Optimal control", "Asymmetric information", "Environmental regulation", "Voluntary disclosure"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1477", "e:abstract": "We consider tempered stable Lévy subordinators and develop a bridge sampling method. An approximate conditional probability density function (PDF) given the terminal values is derived with stable index less than one, using the double saddlepoint approximation. We then propose an acceptance-rejection algorithm based on the existing gamma bridge and the inverse Gaussian bridge as proposal densities. Its performance is comparable to existing sequential sampling methods such as <xref ref-type=\"bibr\" rid=\"B20\">Devroye (2009)</xref> [Devroye L (2009) Random variate generation for exponentially and ploynomially tilted stable distributions. <i>ACM Trans. Modeling Comput. Simulation</i> 19(4):18:1–20.] and <xref ref-type=\"bibr\" rid=\"B30\">Hofert (2011)</xref> [Hofert M (2011) Sampling exponentially tilted stable distributions. <i>ACM Trans. Modeling Comput. Simulation</i> 22(1):3:1–11.] when generating a fixed number of observations. As applications, we consider option pricing problems in Lévy models. First, we demonstrate the effectiveness of bridge sampling when combined with adaptive sampling under finite-variance CGMY processes. Second, further efficiency gain is achieved in terms of variance reduction via stratified sampling.", "e:keyword": ["Bridge sampling", "Lévy process", "Saddlepoint approximation", "Tempered stable subordinator"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1478", "e:abstract": "To increase resilience in supply chains, we investigate the optimal design of flexibility in a backup system. We model the dynamics of disruptions as Markov chains, and consider a multiproduct, multisupplier supply chain under dynamic disruption risks. Using our model, we first show that a little flexibility in the backup system can go a long way in mitigating dynamic disruption risks. This raises an important and fundamental question in designing flexibility in the backup system: to achieve the benefits of full backup flexibility, which unreliable suppliers should be backed up? To answer this question, we connect the supply chain to various queueing and dam models by analyzing the dynamics of the inventory shortfall process. Using this connection, we show that backing up suppliers merely based on first moment considerations such as their average reliability or average product demand can be misleading. All else equal, it is better to back up suppliers with (1) longer but less frequent disruptions, and (2) lower demand uncertainty. In addition to such second moment effects, by employing the Renyi’s Theorem, we demonstrate that when disruptions are relatively long (if they occur), backing up the suppliers for which the expected wasted backup capacity is minimum provides the best backup flexibility design. We also develop easy-to-compute and yet effective indices that (a) guide the supply chain designer in deciding which suppliers to backup, and (b) provide insights into the role of various factors such as inventory holding and shortage costs, purchasing costs, suppliers reliabilities, and product demand distributions in designing backup flexibility.", "e:keyword": ["Backup capacity", "Flexibility", "Dynamic disruption risk", "Inventory shortfall process"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1479", "e:abstract": "Comment on “An Algorithmic Approach to Linear Regression” by Dimitris Bertsimas and Angela King.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1480", "e:abstract": "This paper addresses discrete optimization via simulation. We show that allowing for both a correlated prior distribution on the means (e.g., with discrete Kriging models) and sampling correlation (e.g., with common random numbers, or CRN) can significantly improve the ability to quickly identify the best alternative. These two correlations are brought together for the first time in a highly sequential knowledge-gradient sampling algorithm, which chooses points to sample using a Bayesian value of information (VOI) criterion. We provide almost sure convergence guarantees as the number of samples grows without bound when parameters are known and provide approximations that allow practical implementation including a novel use of the VOI’s gradient rather than the response surface’s gradient. We demonstrate that CRN leads to improved optimization performance for VOI-based algorithms in sequential sampling environments with a combinatorial number of alternatives and costly samples.", "e:keyword": ["Discrete optimization via simulation", "Value of information", "Kriging", "Correlated samples"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1481", "e:abstract": "We study price and lead time quotation decisions in a make-to-order system with two customer classes: (1) contract customers whose orders are practically always accepted and fulfilled based on a contract price and lead time agreed on at the beginning of the time horizon, and (2) spot purchasers who arrive over time and are quoted a price and lead time pair dynamically. The objective is to maximize the long-run expected average profit per unit time, where profit from a customer is defined as revenues minus lateness penalties incurred because of lead time violations. We model the dynamic quotation problem of the spot purchasers as an infinite horizon Markov decision process, given a fixed price and lead time for contract customers. We analyze the impact of customer preferences (e.g., price and lead time sensitivity) on the optimal price and lead time decisions for spot purchasers and characterize the optimal policy. We explore the benefits of dynamic quotation compared to the use of fixed price and lead times, and provide recommendations for firms. Finally, we analyze the optimal contract terms given the dynamic quotation strategy for spot purchasers and discuss the profit improvements offered by the optimal mix of spot and contract customers.", "e:keyword": ["Due date management", "Pricing", "Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1482", "e:abstract": "Sparse process flexibility and the long chain have become important concepts in design flexible manufacturing systems. In this paper, we study the performance of the long chain in comparison to all designs with at most 2<i>n</i> edges over <i>n</i> supply and <i>n</i> demand nodes. We show that, surprisingly, long chain is not always optimal in this class of networks even for i.i.d. demand distributions. In particular, we present a family of instances where a disconnected network with 2<i>n</i> edges has a strictly better performance than the long chain under a specific class of i.i.d. demand distributions. This is quite surprising and contrary to the intuition that a connected design performs better than a disconnected one under <i>exchangeable</i> distributions. Although our family of examples disprove the optimality of the long chain in general, we observe that the empirical performance of the long chain is nearly optimal.To further understand the effectiveness of the long chain, we compare its performance to <i>connected</i> designs with at most 2<i>n</i> arcs. We show that the long chain is optimal in this class of designs for exchangeable demand distributions. Our proof is based on a coupling argument and a combinatorial analysis of the structure of maximum flow in directed networks. The analysis provides useful insights towards not just understanding the optimality of long chain but also toward designing more general sparse flexibility networks.", "e:keyword": ["Long chain", "Process flexibility design", "Stochastic max-flow", "Capacity pooling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1483", "e:abstract": "Robust optimization is a methodology that has gained a lot of attention in the recent years. This is mainly due to the simplicity of the modeling process and ease of resolution even for large scale models. Unfortunately, the second property is usually lost when the cost function that needs to be “robustified” is not concave (or linear) with respect to the perturbing parameters. In this paper we study robust optimization of sums of piecewise linear functions over polyhedral uncertainty set. Given that these problems are known to be intractable, we propose a new scheme for constructing conservative approximations based on the relaxation of an embedded mixed-integer linear program and relate this scheme to methods that are based on exploiting affine decision rules. Our new scheme gives rise to two tractable models that, respectively, take the shape of a linear program and a semidefinite program, with the latter having the potential to provide solutions of better quality than the former at the price of heavier computations. We present conditions under which our approximation models are exact. In particular, we are able to propose the first exact reformulations for a robust (and distributionally robust) multi-item newsvendor problem with budgeted uncertainty set and a reformulation for robust multiperiod inventory problems that is exact whether the uncertainty region reduces to a <i>L</i><sub>1</sub>-norm ball or to a box. An extensive set of empirical results will illustrate the quality of the approximate solutions that are obtained using these two models on randomly generated instances of the latter problem.", "e:keyword": ["Robust optimization", "Piecewise linear", "Linear programming relaxation", "Semidefinite program", "Tractable approximations", "Newsvendor problem", "Inventory problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1484", "e:abstract": "We propose, develop, and compare new stochastic models for the daily arrival rate in a call center. Following standard practice, the day is divided into time periods of equal length (e.g., 15 or 30 minutes), the arrival rate is assumed random but constant in time in each period, and the arrivals are from a Poisson process, conditional on the rate. The random rate for each period is taken as a deterministic base rate (or expected rate) multiplied by a random busyness factor having mean 1. Models in which the busyness factors are independent across periods, or in which a common busyness factor applies to all periods, have been studied previously. But they are not sufficiently realistic. We examine alternative models for which the busyness factors have some form of dependence across periods. Maximum likelihood parameter estimation for these models is not easy, mainly because the arrival rates themselves are never observed. We develop specialized techniques to perform this estimation. We compare the goodness-of-fit of these models on arrival data from three call centers, both in-sample and out-of-sample. Our models can represent arrivals in many other types of systems as well. Estimating a model for the vector of counts (the number of arrivals in each period) is generally easier than for the vector of rates, because the counts can be observed, but a model for the rates is often more convenient and natural, e.g., for simulation. We examine and provide insight on the relationship between these two types of modeling. In particular, we give explicit formulas for the relationship between the correlation between rates and that between counts in two given periods, and for the variance and dispersion index in a given period. These formulas imply that for a given correlation between the rates, the correlation between the counts is much smaller in low traffic than in high traffic.", "e:keyword": ["Arrival process", "Arrival rate", "Doubly stochastic Poisson process", "Input modeling", "Copula", "Correlation", "Call center"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1485", "e:abstract": "We develop the first approximation algorithm for periodic-review perishable inventory systems with setup costs. The ordering lead time is zero. The model allows for correlated demand processes that generalize the well-known approaches to model dynamic demand forecast updates. The structure of optimal policies for this fundamental class of problems is not known in the literature. Thus, finding provably near-optimal control policies has been an open challenge. We develop a randomized proportional-balancing policy (RPB) that can be efficiently implemented in an online manner, and we show that it admits a worst-case performance guarantee between 3 and 4. The main challenge in our analysis is to compare the setup costs between RPB and the optimal policy in the presence of inventory perishability, which departs significantly from the previous works in the literature. The numerical results show that the average performance of RPB is good (within 1% of optimality under i.i.d. demands and within 7% under correlated demands).", "e:keyword": ["Inventory", "Perishable products", "Setup costs", "Randomized algorithms", "Worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1486", "e:abstract": "Flow thinning (FT) is a concept of a traffic routing and protection strategy applicable to communication networks with variable capacity of links. In such networks, the links do not attain their nominal (maximum) capacity simultaneously, so in a typical network state only some links are fully available whereas on each of the remaining links only a fraction of its maximum capacity is usable. Every end-to-end traffic demand is assigned a set of logical tunnels whose total capacity is dedicated to carry the demand’s traffic. The nominal (i.e., maximum) capacity of the tunnels, supported by the nominal (maximum) link capacity, is subject to state-dependent thinning to account for variable capacity of the links fluctuating below the maximum. Accordingly, the capacity available on the tunnels is also fluctuating below their nominal levels and hence the instantaneous traffic sent between the demand’s end nodes must accommodate to the current total capacity available on its dedicated tunnels. The related multi-commodity flow optimization problem is <i>(N-script)(P-script)</i>-hard and its noncompact linear programming formulation requires path generation. For that, we formulate an integer programming pricing problem, at the same time showing the cases when the pricing is polynomial. We also consider an important variant of FT, affine thinning, that may lead to practical FT implementations. We present a numerical study illustrating traffic efficiency of FT and computational efficiency of its optimization models. Our considerations are relevant, among others, for wireless mesh networks utilizing multiprotocol label switching tunnels.", "e:keyword": ["Survivable network design", "Multiple partial link failures", "Mixed-integer programming", "Multicommodity flows", "Path generation", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1487", "e:abstract": "We consider a seller’s problem of determining revenue-maximizing prices for an assortment of products that exhibit network effects. Customers make purchase decisions according to a multinomial logit choice model, modified—to incorporate network effects—so that the utility each individual customer gains from purchasing a particular product depends on the market’s total consumption of that product. In the setting of homogeneous products, we show that if the network effect is comparatively weak, then the optimal pricing decision of the seller is to set identical prices for all products. However, if the network effect is strong, then the optimal pricing decision is to set the price of one product low and to set the prices of all other products to a single high value. This boosts the sales of the single low-price product in comparison to the sales of all other products. We also obtain comparative statics results that describe how optimal prices and sales levels vary with a parameter that determines the strength of the network effects. We extend our analysis to settings with heterogeneous products and establish that optimal solutions have a structure similar to that found in the homogeneous case: either maintain a semblance of balance among all products or boost the sales of just one product. Based on this structure, we propose an effective computational algorithm for such general heterogeneous settings.", "e:keyword": ["Pricing", "Choice models", "Network effects", "Revenue management"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1488", "e:abstract": "We analyze a variant of the whereabouts search problem, in which a searcher looks for a target hiding in one of <i>n</i> possible locations. Unlike in the classic version, our searcher does not pursue the target by actively moving from one location to the next. Instead, the searcher receives a stream of intelligence about the location of the target. At any time, the searcher can engage the location he thinks contains the target or wait for more intelligence. The searcher incurs costs when he engages the wrong location, based on insufficient intelligence, or waits too long in the hopes of gaining better situational awareness, which allows the target to either execute his plot or disappear. We formulate the searcher’s decision as an optimal stopping problem and establish conditions for optimally executing this search-and-interdict mission.", "e:keyword": ["Whereabouts search", "Optimal stopping", "Multinomial selection."]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1489", "e:abstract": "This paper proposes three strong second order cone programming (SOCP) relaxations for the AC optimal power flow (OPF) problem. These three relaxations are incomparable to each other and two of them are incomparable to the standard SDP relaxation of OPF. Extensive computational experiments show that these relaxations have numerous advantages over existing convex relaxations in the literature: (i) their solution quality is extremely close to that of the standard SDP relaxation (the best one is within 99.96% of the SDP relaxation on average for all the IEEE test cases) and consistently outperforms previously proposed convex quadratic relaxations of the OPF problem, (ii) the solutions from the strong SOCP relaxations can be directly used as a warm start in a local solver such as IPOPT to obtain a high quality feasible OPF solution, and (iii) in terms of computation times, the strong SOCP relaxations can be solved an order of magnitude faster than the standard SDP relaxation. For example, one of the proposed SOCP relaxations together with IPOPT produces a feasible solution for the largest instance in the IEEE test cases (the 3375-bus system) and also certifies that this solution is within 0.13% of global optimality, all this computed within 157.20 seconds on a modest personal computer. Overall, the proposed strong SOCP relaxations provide a practical approach to obtain feasible OPF solutions with extremely good quality within a time framework that is compatible with the real-time operation in the current industry practice.", "e:keyword": ["Optimal power flow", "Semidefinite programming", "Second order cone programming", "Cutting planes"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1490", "e:abstract": "We study decentralized markets involving producers and consumers that are facilitated by middlemen. We do this by analyzing a noncooperative networked bargaining game. We assume a complete information setup wherein all the agents know the structure of the network, the values of the consumers, and the transaction costs involved but allow for some search friction when either producers or consumers trade with middlemen. In such a setting, we show that sunk cost problems and a heterogeneous network can give rise to delay or failure in negotiation, and therefore reduce the total trade capacity of the network. In the limiting regime of extremely patient agents, we provide a sharp characterization of the trade pattern and the segmentation of these markets.", "e:keyword": ["Noncooperative bargaining", "Supply chain networks", "Trade volume"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1491", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1493", "e:abstract": "We propose a new and systematic reformulation and algorithmic approach to solve a complex class of stochastic programming problems involving a joint chance constraint with random technology matrix and stochastic quadratic inequalities. The method is general enough to apply to nonconvex as well as nonseparable quadratic terms. We derive two new reformulations and give sufficient conditions under which the reformulated problem is equivalent. The second reformulation provides a much sparser representation of the feasible set of the chance constraint and offers tremendous computational advantages. This new reformulation method can be used for linear stochastic inequalities and will also significantly improve the solution of such joint chance-constrained problems. We provide general and easily identifiable conditions under which the base reformulations can be linearized. We show that the size of the reformulated problems, in particular, their number of binary variables and quadratic mixed-integer terms, does not grow linearly with the number of scenarios used to represent uncertainty. We propose two new nonlinear branch-and-bound algorithms for the nonconvex quadratic integer reformulations. We present detailed empirical results, comparing the various reformulations and several algorithmic ideas that improve the performance of the mixed-integer nonlinear solver <sc>Couenne</sc> for solving these problems. Guidelines on how to tune the solver and to select reformulations are presented. The test instances are epidemiology and disaster management facility location models and cover the three types of stochastic quadratic inequalities, namely, product of two decision variables that are both binary, binary and continuous, or both continuous.", "e:keyword": ["Stochastic programming", "Mixed-integer nonlinear programming", "Boolean programming", "Nonlinear branch-and-bound algorithm", "Quadratic stochastic inequality", "Joint probabilistic constraint", "Random technology matrix", "Epidemiology", "Facility location"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1494", "e:abstract": "We consider a ranking and selection problem with independent normal observations, and we analyze the asymptotic sampling rates of expected improvement (EI) methods in this setting. Such methods often perform well in practice, but a tractable analysis of their convergence rates is difficult because of the nonlinearity and nonconvexity of the EI calculations. We present new results indicating that, for known sampling noise, variants of EI produce asymptotic simulation allocations that are essentially identical to those chosen by the optimal computing budget allocation (OCBA) methodology, which is known to yield near-optimal asymptotic performance in ranking and selection. This is the first general equivalence result between EI and OCBA, and it provides insight into the good practical performance of EI. We also derive the limiting allocation for EI under unknown sampling variance.", "e:keyword": ["Optimal learning", "Ranking and selection", "Expected improvement", "Optimal computing budget allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1495", "e:abstract": "This paper is concerned with optimal maintenance decision making in the presence of model misspecification. Specifically, we are interested in the situation where the decision maker fears that a nominal Bayesian model may be miss-specified or unrealistic, and would like to find policies that work well even when the underlying model is flawed. To this end, we formulate a robust dynamic optimization model for condition-based maintenance in which the decision maker explicitly accounts for distrust in the nominal Bayesian model by solving a worst-case problem against a second agent, “nature,” who has the ability to alter the underlying model distributions in an adversarial manner. The primary focus of our analysis is on establishing structural properties and insights that hold in the face of model miss-specification. In particular, we prove (i) an explicit characterization of nature’s optimal response through an analysis of the robust dynamic programming equation, (ii) convexity results for both the robust value function and the optimal robust stopping region, (iii) a general robustness result for the preventive maintenance paradigm, and (iv) the optimality of a robust control limit policy for the important subclass of Bayesian change point detection problems. We illustrate our theoretical result on a real-world application from the mining industry.", "e:keyword": ["Failure models", "Partially observable systems", "Dynamic programming", "Model ambiguity", "Robust control", "Relative entropy", "Stochastic dynamic games"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1496", "e:abstract": "We consider the <i>conflict-resolution</i> problem arising in the allocation of commercial advertisements to television program breaks. Because of the competition-avoidance requirements issued by advertisers, broadcasters aim to allocate any pairs of commercials promoting highly conflicting products to different breaks. Hence, the problem consists of assigning commercials to breaks, subject to time capacity constraints, with the aim of maximizing a total measure of the conflicts among commercials assigned to different breaks.Since the existing formulation can hardly be solved via exact methods, we introduce three new and efficient (mixed-)integer programming formulations of the problem. Our computational study is based on two sets of test problems, one from the literature and another more challenging data set that we generate. Numerical results show the excellent performance of the proposed formulations in terms of solution quality and computation times, when compared against an existing formulation and an effective heuristic approach. In particular, for the existing data set, all three formulations significantly outperform the existing formulation and heuristic, and moreover, for the new data set, our best formulation outperforms the heuristic on 76% of the test examples in terms of solution quality. We also provide theoretical evidence to demonstrate why some of our new formulations should outperform the existing formulation.", "e:keyword": ["Television advertising", "Conflict resolution problem", "Integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1497", "e:abstract": "Financial institutions are interconnected directly by holding debt claims against each other (the network channel), and they are also bound by the market when selling assets to raise cash in distressful circumstances (the liquidity channel). The goal of our study is to investigate how these two channels of risk interact to propagate individual defaults to a systemwide catastrophe. We formulate a constrained optimization problem that incorporates both channels of risk, and exploit the problem structure to generate the solution (to the clearing payment vector) via a partition algorithm. Through sensitivity analysis, we are able to identify two key contributors to financial systemic risk, the network multiplier and the liquidity amplifier, and to discern the qualitative difference between the two, confirming that the market liquidity effect has a great potential to cause systemwide contagion. We illustrate the network and market liquidity effects—in particular, the significance of the latter—in the formation of systemic risk with data from the European banking system. Our results contribute to a better understanding of the effectiveness of certain policy interventions. In addition, our algorithm can be used to pin down the changes of the net worth (marked to market) of each bank in the system as the spillover effect spreads, so as to estimate the extent of contagion, and to provide a metric of financial resilience as well. Our framework can also be easily extended to incorporate the effect of bankruptcy costs.", "e:keyword": ["Systemic risk", "Financial network", "Contagion", "Market liquidity"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1498", "e:abstract": "We consider a single stock-point for a repairable item facing Markov modulated Poisson demand. Repair of failed parts may be expedited at an additional cost to receive a shorter lead time. Demand that cannot be filled immediately is backordered and penalized. The manager decides on the number of spare repairables to purchase and on the expediting policy. We characterize the optimal expediting policy using a Markov decision process formulation and provide closed-form necessary and sufficient conditions that determine whether the optimal policy is a type of threshold policy or a no-expediting policy. We derive further asymptotic results as demand fluctuates arbitrarily slowly. In this regime, the cost of this system can be written as a weighted average of costs for systems facing Poisson demand. These asymptotics are leveraged to show that approximating Markov modulated Poisson demand by stationary Poisson demand can lead to arbitrarily poor results. We propose two heuristics based on our analytical results, and numerical tests show good performance with average optimality gaps of 0.11% and 0.33% respectively. Naive heuristics that ignore demand fluctuations have average optimality gaps of more than 11%. This shows that there is great value in leveraging knowledge about demand fluctuations in making repairable expediting and stocking decisions.", "e:keyword": ["Expediting", "Markov decision process", "Optimal policies", "Markov modulated Poisson process", "Heuristics", "Inventory", "Dual-sourcing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1499", "e:abstract": "Servers in many real queueing systems do not work at a constant speed. They adapt to the system state by speeding up when the system is highly loaded or slowing down when load has been high for an extended time period. Their speed can also be constrained by other factors, such as geography or a downstream blockage. We develop a state-dependent queueing model in which the service rate depends on the system “load” and “overwork.” Overwork refers to a situation where the system has been under a heavy load for an extended time period. We quantify load as the number of users in the system, and we operationalize overwork with a state variable that is incremented with each service completion in a high-load period and decremented at a rate that is proportional to the number of idle servers during low-load periods. Our model is a quasi-birth-and-death process with a special structure that we exploit to develop efficient and easy-to-implement algorithms to compute system performance measures. We use the analytical model and simulation to demonstrate how using models that ignore adaptive server behavior can result in inconsistencies between planned and realized performance and can lead to suboptimal, unstable, or oscillatory staffing decisions.", "e:keyword": ["State-dependent queues", "Quasi-birth-and-death", "Service operations", "Behavioral operations", "Load", "Overwork", "Staffing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1500", "e:abstract": "A <i>k</i>-club is a subset of vertices of a graph that induces a subgraph of diameter at most <i>k</i>, where <i>k</i> is a positive integer. By definition, 1-clubs are cliques and the model is a distance-based relaxation of the clique definition for larger values of <i>k</i>. The <i>k</i>-club model is particularly interesting to study from a polyhedral perspective as the property is not hereditary on induced subgraphs when <i>k</i> is larger than one. This article introduces a new family of facet-defining inequalities for the 2-club polytope that unifies all previously known facets through a less restrictive combinatorial property, namely, independent (distance) 2-domination. The complexity of separation over this new family of inequalities is shown to be NP-hard. An exact formulation of this separation problem and a greedy separation heuristic are also proposed. The polytope described by the new inequalities (and nonnegativity) is then investigated and shown to be integral for acyclic graphs. An additional family of facets is also demonstrated for cycles of length indivisible by three. The effectiveness of these new facets as cutting planes and the difficulty of solving the separation problem in practice are then investigated via computational experiments on a test bed of benchmark instances.", "e:keyword": ["k-club", "Clique relaxations", "2-club polytope", "Social network analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1501", "e:abstract": "We consider a class of Nash games, termed as aggregative games, being played over a networked system. In an aggregative game, a player’s objective is a function of the aggregate of all the players’ decisions. Every player maintains an estimate of this aggregate, and the players exchange this information with their local neighbors over a connected network. We study distributed synchronous and asynchronous algorithms for information exchange and equilibrium computation over such a network. Under standard conditions, we establish the almost-sure convergence of the obtained sequences to the equilibrium point. We also consider extensions of our schemes to aggregative games where the players’ objectives are coupled through a more general form of aggregate function. Finally, we present numerical results that demonstrate the performance of the proposed schemes.", "e:keyword": ["Noncooperative games", "Network applications", "Systems solution"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1502", "e:abstract": "We model the influence of sharing large exogeneous losses to the reinsurance market by a bipartite graph. Using Pareto-tailed claims and multivariate regular variation we obtain asymptotic results for the value-at-risk and the conditional tail expectation. We show that the dependence on the network structure plays a fundamental role in their asymptotic behaviour. As is well known in a nonnetwork setting, if the Pareto exponent is larger than 1, then for the individual agent (reinsurance company) diversification is beneficial, whereas when it is less than 1, concentration on a few objects is the better strategy.An additional aspect of this paper is the amount of uninsured losses that are covered by society. In our setting of networks of agents, diversification is never detrimental to the amount of uninsured losses. If the Pareto-tailed claims have finite mean, diversification is never detrimental, to society or individual agents. By contrast, if the Pareto-tailed claims have infinite mean, a conflicting situation may arise between the incentives of individual agents and the interest of some regulator to keep the risk for society small. We explain the influence of the network structure on diversification effects in different network scenarios.", "e:keyword": ["Bipartite graph", "Diversification", "Market risk", "Micro- vs. macro-prudential risk", "Multivariate regular variation", "Pareto-tail", "Reinsurance", "Risk measures"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1503", "e:abstract": "We develop a new approach for solving the optimal retirement problem for an individual with an unhedgeable income risk. The income risk stems from a forced unemployment event, which occurs as an exponentially distributed random shock. The optimal retirement problem is to determine an individual’s optimal consumption and investment behaviors and optimal retirement time simultaneously. We introduce a new convex-duality approach for reformulating the original retirement problem and provide an iterative numerical method to solve it. Reasonably calibrated parameters say that our model can give an explanation for lower consumption and risky investment behaviors of individuals, and for relatively higher stock holdings of the poor. We also analyze the sensitivity of an individual’s optimal behavior in changing her wealth level, investment opportunity, and the magnitude of preference for postretirement leisure. Finally, we find that our model explains a countercyclical pattern of the number of unemployed job leavers.", "e:keyword": ["Dynamic programming/optimal control", "Investment", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1504", "e:abstract": "We consider optimization problems whose parameters are known only approximately, based on noisy samples. In large-scale applications, the number of samples one can collect is typically of the same order of (or even less than) the dimensionality of the problem. This so-called high-dimensional statistical regime has been the object of intense recent research in machine learning and statistics, primarily due to phenomena inherent to this regime, such as the fact that the noise one sees here often dwarfs the magnitude of the signal itself. While relevant in numerous important operations research and engineering optimization applications, this setup falls far outside the traditional scope of robust and stochastic optimization. We propose three algorithms to address this setting, combining ideas from statistics, machine learning, and robust optimization. Our algorithms are motivated by three natural optimization objectives: minimizing the number of grossly violated constraints; maximizing the number of exactly satisfied constraints; and, finally, developing algorithms whose running time scales with the <i>intrinsic</i> dimension of a problem, as opposed to its <i>observed</i> dimension—a mismatch that, as we discuss in detail, can be dire in settings where constraints are meant to describe preferences of behaviors. The key ingredients of our algorithms are dimensionality reduction techniques from machine learning, robust optimization, and concentration of measure tools from statistics.", "e:keyword": ["Programming", "Stochastic", "Statistics", "Nonparametric"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1505", "e:abstract": "Assortment planning is an important problem that arises in many industries such as retailing and airlines. One of the key challenges in an assortment planning problem is to identify the “right” model for the substitution behavior of customers from the data. Error in model selection can lead to highly suboptimal decisions. In this paper, we consider a Markov chain based choice model and show that it provides a simultaneous approximation for all random utility based discrete choice models including the multinomial logit (MNL), the probit, the nested logit and mixtures of multinomial logit models. In the Markov chain model, substitution from one product to another is modeled as a state transition in the Markov chain. We show that the choice probabilities computed by the Markov chain based model are a good approximation to the true choice probabilities for any random utility based choice model under mild conditions. Moreover, they are exact if the underlying model is a generalized attraction model (GAM) of which the MNL model is a special case. We also show that the assortment optimization problem for our choice model can be solved efficiently in polynomial time. In addition to the theoretical bounds, we also conduct numerical experiments and observe that the average maximum relative error of the choice probabilities of our model with respect to the true probabilities for any offer set is less than 3% where the average is taken over different offer sets. Therefore, our model provides a tractable approach to choice modeling and assortment optimization that is robust to model selection errors. Moreover, the state transition primitive for substitution provides interesting insights to model the substitution behavior in many real-world applications.", "e:keyword": ["Choice modeling", "Assortment optimization", "Model selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1506", "e:abstract": "Traditionally, research focusing on the design of routing and staffing policies for service systems has modeled servers as having fixed (possibly heterogeneous) service rates. However, service systems are generally staffed by people. Furthermore, people respond to workload incentives; that is, how hard a person works can depend both on how much work there is and how the work is divided between the people responsible for it. In a service system, the routing and staffing policies control such workload incentives; and so the rate servers work will be impacted by the system’s routing and staffing policies. This observation has consequences when modeling service system performance, and our objective in this paper is to investigate those consequences.We do this in the context of the <i>M</i>/<i>M</i>/<i>N</i> queue, which is the canonical model for large service systems. First, we present a model for “strategic” servers that choose their service rate to maximize a trade-off between an “effort cost,” which captures the idea that servers exert more effort when working at a faster rate, and a “value of idleness,” which assumes that servers value having idle time. Next, we characterize the symmetric Nash equilibrium service rate under any routing policy that routes based on the server idle time (such as the longest idle server first policy). We find that the system must operate in a quality-driven regime, in which servers have idle time, for an equilibrium to exist. The implication is that to have an equilibrium solution the staffing must have a first-order term that strictly exceeds that of the common square-root staffing policy. Then, within the class of policies that admit an equilibrium, we (asymptotically) solve the problem of minimizing the total cost, when there are linear staffing costs and linear waiting costs. Finally, we end by exploring the question of whether routing policies that are based on the service rate, instead of the server idle time, can improve system performance.", "e:keyword": ["Service systems", "Staffing", "Routing", "Scheduling", "Routing", "Strategic servers"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1507", "e:abstract": "Dynamic spectrum access is a new paradigm of secondary spectrum utilization and sharing. It allows unlicensed secondary users (SUs) to exploit the opportunistically underutilized licensed spectrum. Market mechanism is a widely used promising means to regulate the consuming behaviours of users and, hence, achieve the efficient allocation and consumption of limited resources. In this paper, we propose and study a <i>hybrid</i> secondary spectrum market consisting of both the <i>futures market</i> and the <i>spot market</i>, in which SUs (buyers) purchase underutilized licensed spectrum from a spectrum regulator (SR), either through predefined contracts via the futures market, or through spot transactions via the spot market. We focus on the optimal spectrum allocation among SUs in an exogenous hybrid market that maximizes the secondary spectrum utilization efficiency. The problem is challenging because of the stochasticity and asymmetry of network information. To solve this problem, we first derive an off-line optimal allocation policy that maximizes the ex ante expected spectrum utilization efficiency based on the stochastic distribution of network information. We then propose an online Vickrey-Clarke-Groves (VCG) auction that determines the real-time allocation and pricing of every spectrum based on the realized network information and the prederived off-line policy. We further show that with the spatial frequency reuse, the proposed VCG auction is NP-hard; hence, it is not suitable for online implementation, especially in a large-scale market. To this end, we propose a heuristics approach based on an online VCG-like mechanism with polynomial-time complexity, and further characterize the corresponding performance loss bound analytically. We finally provide extensive numerical results to evaluate the performance of the proposed solutions.", "e:keyword": ["Secondary spectrum market", "Dynamic spectrum access", "Asymmetric network information", "Game theory", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1508", "e:abstract": "Existing approaches to designing recommendation systems with user feedback focus on settings where the number of items is small and/or admit some underlying structure. It is unclear, however, if these approaches extend to applications like social network news feeds and content-curation platforms, which have large and unstructured content pools and constraints on user-item recommendations. To this end, we consider the design of recommendation systems in content-rich setting—where the number of items and the number of item-views by users are of a similar order and an access graph constrains which user is allowed to see which item. In this setting, we propose recommendation algorithms that effectively exploit the access graph, and characterize how their performance depends on the graph topology. Our results demonstrate the importance of <i>serendipity</i> in exploration and how recommendation improves when the access graph has higher expansion; they also suggest reasons behind the success of simple algorithms like Twitter’s latest-first policy. From a technical perspective, our model presents a framework for studying explore-exploit trade-offs in large-scale settings, with potentially infinite number of items. We present algorithms with competitive-ratio guarantees under both finite-horizon and infinite-horizon settings; conversely, we demonstrate that naive policies can be highly suboptimal and also that in many settings, our results are orderwise optimal.", "e:keyword": ["Online recommendation", "Social networks", "Competitive analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1509", "e:abstract": "We present an exact characterization of utilities in competitive equilibria of two-sided matching markets in which the utility of each agent depends on the choice of partner and the terms of the partnership, potentially including monetary transfer. Examples of such markets include sellers and buyers or jobs and workers. Demange and Gale showed that the set of competitive equilibria in this type of market forms a complete lattice with each extreme point of the lattice representing an equilibrium with the highest utilities for the agents on one side and the lowest utilities for the agents on the opposite side.Our characterization is based on establishing a connection between the competitive equilibria of a market and the competitive equilibria of certain strict subsets of that market—each obtained by removing exactly one agent. This characterization captures the effect of competition when agents are added to the market or removed from the market. It gives a precise procedure for constructing competitive equilibria and provides a constructive proof of existence of such equilibria; in contrast, previous proofs have been based on fixed point theorems.", "e:keyword": ["Matching", "Competitive equilibria", "Stability", "Nonquasilinear utilities"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1510", "e:abstract": "Sustainable and efficient management of a distribution system requires coordination between transportation planning and inventory control decisions. In this context, we consider a one warehouse multi-retailer inventory system with a time-based shipment consolidation policy at the warehouse. This means that there are fixed costs associated with each shipment, and retailer orders are consolidated and shipped periodically to groups of retailers sharing the same delivery routes. Customer demand is compound Poisson distributed and unsatisfied demand at each stock point is backordered and allocated on a first-come first-served basis. The system is centralized and inventory levels are reviewed continuously. The warehouse has access to real-time inventory information from the retailers, and uses a (<i>R</i>, <i>nQ</i>) policy to replenish from an outside supplier/manufacturer. We derive the exact probability distributions for the inventory levels at the retailers, and use these to obtain exact expressions for the system’s expected shipment, holding and backorder costs, its average inventory levels, and fill rates. Based on the analytical properties of the objective function, we construct an optimization procedure by deriving bounds on the optimal reorder levels and shipment intervals both for single-item and multi-item systems.", "e:keyword": ["Inventory", "Multiechelon", "Multi-item", "Stochastic", "Shipment consolidation", "Continuous review", "Compound Poisson demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1511", "e:abstract": "Patrolling games were recently introduced to model the problem of protecting the nodes of a network from an attack. Time is discrete and in each time unit the Patroller can stay at the same node or move to an adjacent node. The Attacker chooses when to attack and which node to attack and needs <i>m</i> consecutive time units to carry it out. The Attacker wins if the Patroller does not visit the chosen node while it is being attacked; otherwise, the Patroller wins. This paper studies the patrolling game where the network is a line graph of <i>n</i> nodes, which models the problem of guarding a channel or protecting a border from infiltration. We solve the patrolling game for any values of <i>m</i> and <i>n</i>, providing an optimal Patroller strategy, an optimal Attacker strategy, and the value of the game (optimal probability that the attack is intercepted).", "e:keyword": ["Search and surveillance", "Patrolling"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1512", "e:abstract": "Resource selection games provide a model for a diverse collection of applications where a set of resources is matched to a set of demands. Examples include routing in traffic and in telecommunication networks, service of requests on multiple parallel queues, and acquisition of services or goods with demand-dependent prices. In reality, demands are often submitted by selfish entities (players) and congestion on the resources results in negative externalities for their users. We consider a policy maker that can set a priori rules to minimize the inefficiency induced by selfish players. For example, these rules may assume the form of scheduling policies or pricing decisions. We explore the space of such rules abstracted as cost-sharing methods. We prescribe desirable properties that the cost-sharing method should possess and prove that, in this natural design space, the cost-sharing method induced by the Shapley value minimizes the worst-case inefficiency of equilibria.", "e:keyword": ["Resource selection", "Cost sharing", "Shapley value", "Price of anarchy", "Network routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1513", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1514", "e:abstract": "Inventory models with lost sales and large lead times have traditionally been considered intractable due to the curse of dimensionality. Recently, Goldberg and coauthors laid the foundations for a new approach to solving these models, by proving that as the lead time grows large, a simple constant-order policy is asymptotically optimal. However, the bounds proven there require the lead time to be very large before the constant-order policy becomes effective, in contrast to the good numerical performance demonstrated by Zipkin even for small lead time values. In this work, we prove that for the infinite-horizon variant of the same lost sales problem, the optimality gap of the same constant-order policy actually converges <i>exponentially fast</i> to zero, with the optimality gap decaying to zero at least as fast as the exponential rate of convergence of the expected waiting time in a related single-server queue to its steady-state value. We also derive simple and explicit bounds for the optimality gap, and demonstrate good numerical performance across a wide range of parameter values for the special case of exponentially distributed demand. Our main proof technique combines convexity arguments with ideas from queueing theory.", "e:keyword": ["Inventory", "Lost sales", "Constant-order policy", "Lead time", "Convexity", "Queueing theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1515", "e:abstract": "We present a new partition-and-bound method for multistage adaptive mixed-integer optimization (AMIO) problems that extends previous work on finite adaptability. The approach analyzes the optimal solution to a static (nonadaptive) version of an AMIO problem to gain insight into which regions of the uncertainty set are restricting the objective function value. We use this information to construct partitions in the uncertainty set, leading to a finitely adaptable formulation of the problem. We use the same information to determine a lower bound on the fully adaptive solution. The method repeats this process iteratively to further improve the objective until a desired gap is reached. We provide theoretical motivation for this method, and characterize its convergence properties and the growth in the number of partitions. Using these insights, we propose and evaluate enhancements to the method such as warm starts and smarter partition creation. We describe in detail how to apply finite adaptability to multistage AMIO problems to appropriately address nonanticipativity restrictions. Finally, we demonstrate in computational experiments that the method can provide substantial improvements over a nonadaptive solution and existing methods for problems described in the literature. In particular, we find that our method produces high-quality solutions versus the amount of computational effort, even as the problem scales in the number of time stages and the number of decision variables.", "e:keyword": ["Adaptive optimization", "Robust optimization", "Integer optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1516", "e:abstract": "The open-pit mine block sequencing problem (OPBS) models a deposit of ore and surrounding material near the Earth’s surface as a three-dimensional grid of blocks. A solution in discretized time identifies a profit-maximizing extraction (mining) schedule for the blocks. Our model variant, a mixed-integer program (MIP), presumes a predetermined destination for each extracted block, namely, processing plant or waste dump. The MIP incorporates standard constructs but also adds not-so-standard lower bounds on resource consumption in each time period and allows fractional block extraction in a novel fashion while still enforcing pit-wall slope restrictions. A new extension of nested Benders decomposition, “hierarchical” Benders decomposition (HBD), solves the MIP’s linear-programming relaxation. HBD exploits time-aggregated variables and can recursively decompose a model into a master problem and two subproblems rather than the usual single subproblem. A specialized branch-and-bound heuristic then produces high-quality, mixed-integer solutions. Medium-sized problems (e.g., 25,000 blocks and 20 time periods) solve to near optimality in minutes. To the best of our knowledge, these computational results are the best known for instances of OPBS that enforce lower bounds on resource consumption.", "e:keyword": ["Industries", "Mining", "Production scheduling", "Deterministic", "Sequencing", "Integer programming", "Benders decomposition", "Heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1517", "e:abstract": "We consider dynamic asset allocation problems where the agent is required to pay capital gains taxes on her investment gains. These are very challenging problems because the tax owed whenever a security is sold depends on the cost basis, and this results in high-dimensional problems, which cannot be solved exactly except in the case of very stylized problems with just one or two securities and relatively few time periods. In this paper, we focus on exact and average cost-basis problems, make the <i>limited use</i> of <i>losses</i> (LUL) assumption and develop simple heuristic trading policies for these problems when there are differential tax rates for long- and short-term gains and losses. We use information relaxation-based duality techniques to assess the performance of these trading policies by constructing unbiased lower and upper bounds on the (unknown) optimal value function. In numerical experiments with as many as 80 time periods and 25 securities we find our best suboptimal policy is within 3–10 basis points of optimality on a certainty equivalent (CE) annualized return basis. The principal contribution of this paper is in demonstrating that while the primal problem remains very challenging to solve exactly, we can easily solve very large dual problem instances. Moreover, dual tractability extends to standard problem variations, including problems with random time horizons, no wash sales constraints, intertemporal consumption and recursive utility, as well as the step-up feature of the U.S. tax code, among others.", "e:keyword": ["Dynamic asset allocation", "Taxes", "Suboptimal control", "Duality", "Information relaxations"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1519", "e:abstract": "We develop a framework for the analysis of large-scale ad auctions where adverts are assigned over a continuum of search types. For this pay-per-click market, we provide an efficient mechanism that maximizes social welfare. In particular, we show that the social welfare optimization can be solved in separate optimizations conducted on the time scales relevant to the search platform and advertisers. Here, on each search occurrence, the platform solves an assignment problem and, on a slower time scale, each advertiser submits a bid that matches its demand for click-throughs with supply. Importantly, knowledge of global parameters, such as the distribution of search terms, is not required when separating the problem in this way. Exploiting the information asymmetry between the platform and advertiser, we describe a simple mechanism that incentivizes truthful bidding, has a unique Nash equilibrium that is socially optimal, and thus implements our decomposition. Further, we consider models where advertisers adapt their bids smoothly over time and prove convergence to the solution that maximizes social welfare. Finally, we describe several extensions that illustrate the flexibility and tractability of our framework.", "e:keyword": ["Sponsored search", "VCG mechanism", "Decomposition", "Auction", "Social welfare optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1520", "e:abstract": "With the increasing penetration of renewable energy into the power grid system, the volatility of real-time electricity prices increases significantly. This brings challenges for independent power producers to provide optimal bidding strategies. The traditional approaches of only attending the day-ahead market might not be profitable enough without taking advantage of real-time price volatility. In this paper, we study the optimal bidding strategies for the independent power producers utilizing self-scheduling strategies to participate in the real-time market considering real-time electricity price volatility, with the objective of maximizing the total expected profit. Considering the correlations of renewable energy generation outputs among different time periods, the correlations of real-time prices are captured in our modeling framework, in which we explore a multistage stochastic scenario tree to formulate the price uncertainties. Accordingly, the derived multistage stochastic self-scheduling unit commitment problem is transformed as a deterministic equivalent mixed-integer linear programming formulation. To overcome the curse of dimensionality, we develop strong valid inequalities for the derived stochastic unit commitment polytope to speed up the algorithms to solve the problem. In particular, we derive strong valid inequalities that can provide the convex hull descriptions for the two-period case and a special class of the three-period cases with rigorous proofs provided. Furthermore, strong valid inequalities, including facet-defining proofs, for multistage cases are proposed to further strengthen the model. Finally, numerical experiments verify the effectiveness of our derived strong valid inequalities by incorporating them in a branch-and-cut framework.", "e:keyword": ["Stochastic integer programming", "Cutting planes", "Self-scheduling unit commitment"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1521", "e:abstract": "We investigate risk-averse stochastic optimization problems with a risk-shaping constraint in the form of a stochastic-order relation. Both univariate and multivariate orders are considered. We extend the notion of a linear multivariate order, adding flexibility with respect to the controlled portion of the distributions. We propose several methods for the numerical solution of these problems based on augmented Lagrangian framework and analyze their convergence. The methods construct finite-dimensional risk-neutral approximations of the optimization problem whose solutions converge to the solution of the original problem. In the case of univariate stochastic dominance, we explore augmented Lagrangian functionals based on inverse formulations of the stochastic-order constraint. The performance of the methods is compared to other extant numerical methods and shows the numerical advantage of the augmented Lagrangian framework. The proposed numerical approach is particularly successful when applied to problems with multivariate stochastic dominance constraints.", "e:keyword": ["Stochastic order", "Risk", "Multivariate dominance relation", "Increasing convex order", "Duality", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1522", "e:abstract": "We study the design of mechanisms for a sequencing problem where the types of job-agents consist of processing times and waiting costs that are private to the jobs. In the Bayes-Nash setting, we seek to find a sequencing rule and incentive compatible payments that minimize the total expected payments that have to be made to the agents. It is known that the problem can be efficiently solved when jobs have single-dimensional types. Here, we address the problem with two-dimensional types. We show that the problem can be solved in polynomial time by linear programming techniques, answering an open problem formulated by Heydenreich et al. Our implementation is randomized and truthful in expectation. Remarkably, it also works when types are correlated across jobs. The main steps are a compactification of an exponential size linear programming formulation, and a convex decomposition algorithm that allows us to implement the optimal linear programming solution. In addition, by means of computational experiments, we generate some new insights into the implementability in different equilibria.", "e:keyword": ["Scheduling", "Mechanism design", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1523", "e:abstract": "We study joint replenishment and clearance sales of perishable goods under a general finite lifetime and a last-in-first-out (LIFO) issuing rule, a problem common in retailing. We show that the optimal policies can be characterized by two thresholds for each age group of inventory: a lower one and a higher one. For an age group of inventory with a remaining lifetime of two periods or longer, if its inventory level is below its lower threshold, then there is no clearance sales; if it is above its higher threshold, then it will be cleared down to the higher threshold. The optimal policy for the age group of inventory with a one-period remaining lifetime is different. Clearance sales may occur if its inventory level is above its higher threshold or below its lower threshold. The phenomenon that a clearance sale happens when the inventory is low is driven by the need to segregate the newest inventory from the oldest inventory and is unique to the LIFO issuing rule. The optimal policy requires a full inventory record of every age group and its computation is challenging. We consider two myopic heuristics that require only partial information. The first requires only the information about the total inventory and the second requires the information about the total inventory as well as the information about the inventory with a one-period remaining lifetime. Our numerical studies show that the second outperforms the first significantly and its performance is consistently very close to that of the optimal policy.", "e:keyword": ["Stochastic inventory control", "Perishable inventory", "LIFO", "Clearance sales", "Retail operations", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1524", "e:abstract": "We analyze the joint inventory and pricing decisions of a firm when demand depends on not only the current selling price but also a memory-based reference price and customers are loss averse. The presence of reference price effect leads to a nonconcave one-period expected revenue in price and reference price. We introduce a transformation technique that allows us to prove under some mild assumptions the optimality of a reference-price-dependent base-stock list-price policy, which is characterized by a base-stock level and a target reference price. In addition, we show that the target reference price is increasing in the reference price, but except in the loss-neutral case, the base-stock level is not monotone in the reference price. We also show that in the steady state of the model with the reference price effect, the optimal price is lower while the optimal base-stock level is higher than their counterparts in the model without the reference price effect.", "e:keyword": ["Dynamic pricing", "Stochastic inventory", "Reference price effects", "Loss averse", "Base-stock list-price policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1525", "e:abstract": "There has been growing interest in increasing the amount of radio spectrum available for unlicensed broadband wireless access. That includes “prime” spectrum at lower frequencies, which is also suitable for wide area coverage by licensed cellular providers. While additional unlicensed spectrum would allow for market expansion, it could influence competition among providers and increase congestion (interference) among consumers of wireless services. We study the value (social welfare and consumer surplus) obtained by adding unlicensed spectrum to an existing allocation of licensed spectrum among incumbent service providers. We assume a population of customers who choose a provider based on the minimum <i>delivered price</i>, given by the weighted sum of the price of the service and a congestion cost, which depends on the number of subscribers in a band. We consider models in which this weighting is uniform across the customer population and where the weighting is high or low, reflecting different sensitivities to latency. For the models considered, we find that the social welfare depends on the amount of additional unlicensed spectrum, and can actually decrease over a significant range of unlicensed bandwidths. Furthermore, with nonuniform weighting, introducing unlicensed spectrum can also reduce consumer welfare.", "e:keyword": ["Wireless service competition", "Network pricing", "Spectrum policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1526", "e:abstract": "Stochastic Programming (SP) has long been considered a well-justified yet computationally challenging paradigm for practical applications. Computational studies in the literature often involve approximating a large number of scenarios by using a small number of scenarios to be processed via deterministic solvers, or running Sample Average Approximation on some genre of high performance machines so that statistically acceptable bounds can be obtained. In this paper we show that for a class of stochastic linear programming problems, an alternative approach known as Stochastic Decomposition (SD) can provide solutions of similar quality in far less computational time using ordinary desktop or laptop machines of today. In addition to these compelling computational results, we provide a stronger convergence result for SD, and introduce a new solution concept that we call the compromise decision. This new concept is attractive for algorithms that call for multiple replications in sampling-based convex optimization algorithms. For such replicated optimization, we show that the difference between an average solution and a compromise decision provides a natural stopping rule. We discuss three stopping criteria that enhance the reliability of the compromise decision, reducing bias and variance associated with the result. Finally our computational results cover a variety of instances from the literature, including a detailed study of SONET Switched Network (SSN), a network planning instance known to be more challenging than other test instances in the literature.", "e:keyword": ["Stochastic linear programming", "Stochastic decomposition", "Computational experiments"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1527", "e:abstract": "This paper focuses on revealed preference tests of the collective model of household consumption. We start by showing that the decision problems corresponding to testing collective rationality are NP-complete. This makes the application of these tests problematic for (increasingly available) large(r)-scale data sets. We then present two approaches to overcome this negative result. First, we introduce exact algorithms based on mixed-integer programming (MIP) formulations of the collective rationality tests, which can be usefully applied to medium-sized data sets. Next, we propose simulated annealing heuristics, which allow for efficient testing of the collective model in the case of large data sets. We illustrate our methods by a number of computational experiments based on Dutch labor supply data.", "e:keyword": ["Revealed preference axioms", "Rationality", "Mixed-integer programming", "Global optimization", "Simulated annealing"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1528", "e:abstract": "This paper is motivated by the following question: How to construct good approximation for the distribution of the solution value to linear optimization problem when the random objective coefficients follow a multivariate normal distribution? Using Stein’s Identity, we show that the least squares normal approximation of the random optimal value can be computed by estimating the persistency values of the corresponding optimization problem. We further extend our method to construct a least squares quadratic estimator to improve the accuracy of the approximation; in particular, to capture the skewness of the objective. Computational studies show that the new approach provides more accurate estimates of the distributions of project completion times compared to existing methods.", "e:keyword": ["Distribution approximation", "Persistency", "Stein’s identity", "Project management", "Statistical timing analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1529", "e:abstract": "We study a supply chain involving a supplier–retailer relationship. When production lead-time is long and the selling season is short, the retailer has to place an order ahead of the season, which resembles the classical Newsvendor model. However, we consider the situation when the supplier agrees to deliver the order in multiple shipments in the season, and then the retailer needs to determine the quantity and/or timing of each shipment. Under a centralized setting, we derive the optimal quantity and/or timing decisions of the retailer. Under a decentralized setting, incentive misalignment arises from ineffective allocation of inventory costs between the parties, in addition to the well- known double marginalization effect. Hence, we devise an incentive contract, which involves a risk-sharing mechanism at the end of the season and an inventory subsidizing scheme for the entire season; in practice, the inventory subsidizing scheme can be implemented in different ways, such as a direct subsidizing scheme or a delayed-payment scheme. The proposed contract can achieve channel coordination and Pareto optimality. Furthermore, we can show that the inventory subsidizing scheme plays a key role in channel coordination because without the inventory subsidizing scheme, the loss of supply chain efficiency is almost always significant.", "e:keyword": ["Policies", "Multi-shipment", "Inventory/production", "Incentive contracts"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1530", "e:abstract": "Many procedures have been proposed in the literature to select the simulated alternative with the best mean performance from a finite set of alternatives. Among these procedures, frequentist procedures are typically designed under either the subset-selection (SS) formulation or the indifference-zone (IZ) formulation. Both formulations may encounter problems when the goal is to select the unique best alternative for any configuration of the means. In particular, SS procedures may return a subset that contains more than one alternative, and IZ procedures hinge on the relationship between the chosen IZ parameter and the true mean differences that is unknown to decision makers a priori. In this paper, we propose a new formulation that guarantees to select the unique best alternative with a user-specified probability of correct selection (PCS), as long as the means of alternatives are unique, and we design a class of fully sequential procedures under this formulation. These procedures are parameterized by the PCS value only, and their continuation boundaries are determined based on the Law of the Iterated Logarithm. Furthermore, we show that users can add a stopping criterion to these procedures to convert them into IZ procedures, and we argue that these procedures have several advantages over existing IZ procedures. Lastly, we conduct an extensive numerical study to show the performance of our procedures and compare their performance to existing procedures.", "e:keyword": ["Selection of the best", "Indifference-zone-free", "Fully sequential procedures"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1531", "e:abstract": "Decomposable Markov decision processes (MDPs) are problems where the stochastic system can be decomposed into multiple individual components. Although such MDPs arise naturally in many practical applications, they are often difficult to solve exactly due to the enormous size of the state space of the complete system, which grows exponentially with the number of components. In this paper, we propose an approximate solution approach to decomposable MDPs that is based on re-solving a fluid linear optimization formulation of the problem at each decision epoch. This formulation tractably approximates the problem by modeling transition behavior at the level of the individual components rather than the complete system. We prove that our fluid formulation provides a tighter bound on the optimal value function than three state-of-the-art formulations: the approximate linear optimization formulation, the classical Lagrangian relaxation formulation, and a novel, alternate Lagrangian relaxation that is based on relaxing an action consistency constraint. We provide a numerical demonstration of the effectiveness of the approach in the area of multiarmed bandit problems, where we show that our approach provides near optimal performance and outperforms state-of-the-art algorithms.", "e:keyword": ["Dynamic programming", "Optimal control", "Probability", "Markov processes", "Programming", "Linear", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1533", "e:abstract": "One of the fundamental concerns in the analysis of logistical systems is the trade-off between localized, independent provision of goods and services versus provision along a centralized infrastructure such as a backbone network. One phenomenon in which this trade-off has recently been made manifest is the transition of businesses from traditional brick-and-mortar stores to retail sales facilitated via e-commerce, such as grocery delivery services. Conventional wisdom would dictate that such services ought to be more efficient—say from the perspective of the overall carbon footprint—because of the economy of scale achieved by aggregating demand through a delivery van, as opposed to the many separate trips that customers would otherwise take using their own means of transport.In this paper, we quantify the changes in overall efficiency due to such services by looking at “household-level” economies of scale in transportation: a person might perform many errands in a day (such as going to the bank, grocery store, and post office), and that person has many choices of locations at which to perform these tasks (e.g., a typical metropolitan region has many banks, grocery stores, and post offices). Thus, the total driving distance (and therefore the overall carbon footprint) that that person traverses is the solution to a <i>generalized travelling salesman problem</i> (GTSP) in which they select both the best locations to visit and the sequence in which to visit them. We perform a probabilistic analysis of the GTSP under the assumption that all relevant locations are independently and identically distributed uniformly in a region and then determine the amount of adoption of such services that is necessary, under our model, in order for the overall carbon footprint of the region to decrease.", "e:keyword": ["Continuous location", "Facilities/equipment planning", "Stochastic networks/graphs", "Travelling salesman"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1534", "e:abstract": "The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g., MSR’s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining a ranking, finding ‘scores’ for each object (e.g., player’s rating) is of interest for understanding the intensity of the preferences.In this paper, we propose <sc>Rank</sc> <sc>Centrality</sc>, an iterative rank aggregation algorithm for discovering scores for objects (or items) from pairwise comparisons. The algorithm has a natural <i>random walk</i> interpretation over the graph of objects with an edge present between a pair of objects if they are compared; the score, which we call Rank Centrality, of an object turns out to be its stationary probability under this random walk.To study the efficacy of the algorithm, we consider the popular Bradley-Terry-Luce (BTL) model (equivalent to the Multinomial Logit (MNL) for pairwise comparisons) in which each object has an associated score that determines the probabilistic outcomes of pairwise comparisons between objects. In terms of the pairwise marginal probabilities, which is the main subject of this paper, the MNL model and the BTL model are identical. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. In particular, the number of samples required to learn the score well with high probability depends on the structure of the comparison graph. When the Laplacian of the comparison graph has a strictly positive spectral gap, e.g., each item is compared to a subset of randomly chosen items, this leads to dependence on the number of samples that is nearly order optimal.Experimental evaluations on synthetic data sets generated according to the BTL model show that our algorithm performs as well as the maximum likelihood estimator for that model and outperforms other popular ranking algorithms.", "e:keyword": ["Rank aggregation", "Rank centrality", "Markov chain", "Random walk"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1535", "e:abstract": "Effective route planning for battery electric commercial vehicle (ECV) fleets has to take into account their limited autonomy and the possibility of visiting recharging stations during the course of a route. In this paper, we consider four variants of the electric vehicle-routing problem with time windows: (i) at most a single recharge per route is allowed, and batteries are fully recharged on visit of a recharging station; (ii) multiple recharges per route, full recharges only; (iii) at most a single recharge per route, and partial battery recharges are possible; and (iv) multiple, partial recharges. For each variant, we present exact branch-price-and-cut algorithms that rely on customized monodirectional and bidirectional labeling algorithms for generating feasible vehicle routes. In computational studies, we find that all four variants are solvable for instances with up to 100 customers and 21 recharging stations. This success can be attributed to the tailored resource extension functions (REFs) that enable efficient labeling with constant time feasibility checking and strong dominance rules, even if these REFs are intricate and rather elaborate to derive. The studies also highlight the superiority of the bidirectional labeling algorithms compared to the monodirectional ones. Finally, we find that allowing multiple as well as partial recharges both help to reduce routing costs and the number of employed vehicles in comparison to the variants with single and with full recharges.", "e:keyword": ["Vehicle routing", "Electric vehicles", "Recharging decisions", "Branch price and cut", "Labeling algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1537", "e:abstract": "We consider the problem of optimally selecting a large portfolio of risky loans, such as mortgages, credit cards, auto loans, student loans, or business loans. Examples include loan portfolios held by financial institutions and fixed-income investors as well as pools of loans backing mortgage- and asset-backed securities. The size of these portfolios can range from the thousands to even hundreds of thousands. Optimal portfolio selection requires the solution of a high-dimensional nonlinear integer program and is extremely computationally challenging. For larger portfolios, this optimization problem is intractable. We propose an approximate optimization approach that yields an asymptotically optimal portfolio for a broad class of data-driven models of loan delinquency and prepayment. We prove that the asymptotically optimal portfolio converges to the optimal portfolio as the portfolio size grows large. Numerical case studies using actual loan data demonstrate its computational efficiency. The asymptotically optimal portfolio’s computational cost does not increase with the size of the portfolio. It is typically many orders of magnitude faster than nonlinear integer program solvers while also being highly accurate even for moderate-sized portfolios.", "e:keyword": ["Loan portfolio", "Approximate optimization", "Weak convergence", "Asymptotic approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1538", "e:abstract": "We study a continuous-review, infinite-horizon inventory system with compound Poisson demand and dual sourcing/delivery modes. Ordering from either source/mode incurs a fixed cost and the expedited mode provides a shorter lead time than the regular mode. As the optimal ordering policy is unknown, while expected to be very complicated, we propose a class of simple policies called single-index (<i>R, nQ</i>) policies—when ordering from each mode, based on the inventory position, the system follows an (<i>R, nQ</i>)-type of policy. We provide an exact procedure to compute the expected long-run average cost. Specifically, we first analyze the steady-state distribution of the inventory position, which is found to be no longer uniform in general as in the classic (<i>R, nQ</i>) inventory system where only one delivery mode is available. We then develop a recursive procedure, which overcomes the order-crossing effect, to determine the steady-state distribution of the inventory level. Two simple heuristics for computing near-optimal policy parameters are provided. For a special case where ordering from the regular mode incurs no fixed cost and follows a base-stock policy, we derive closed-form solution bounds by applying normal approximation. To assess the performance of the single-index (<i>R, nQ</i>) policy, we further study a more complicated class of policies called dual-index (<i>R, nQ</i>) policies and numerically illustrate that the simpler single-index policy performs close to the dual-index policy. Finally, the performance of the single-index policy is also shown comparable to the policy computed via dynamic programming.", "e:keyword": ["Batch ordering", "Cost evaluation", "Dual delivery modes", "Fixed order cost", "Lead time"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1539", "e:abstract": "This paper attempts to provide a decision-theoretic foundation for the measurement of economic tail risk, which is not only closely related to utility theory but also relevant to statistical model uncertainty. The main result is that the only risk measures that satisfy a set of economic axioms for the Choquet expected utility and the statistical property of general elicitability (i.e., there exists an objective function such that minimizing the expected objective function yields the risk measure) are the mean functional and value-at-risk (VaR), in particular the median shortfall, which is the median of tail loss distribution and is also the VaR at a higher confidence level. We also discuss various approaches of backtesting and their relations to elicitability and co-elicitability; in particular, we show that the co-elicitability of VaR and expected shortfall does not lead to a reliable backtesting method for expected shortfall and there have been only indirect backtesting methods for expected shortfall. Furthermore, we extend the result to address model uncertainty by incorporating multiple scenarios. As an application, we argue that median shortfall is a better alternative than expected shortfall for setting capital requirements in Basel Accords.", "e:keyword": ["Comonotonic independence", "Model uncertainty", "Robustness", "Elicitability", "Backtest", "Value-at-risk", "Expected shortfall"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1540", "e:abstract": "We propose a framework for testing the possibility of large cascades in financial networks. This framework accommodates a variety of specifications for the probabilities of emergence of “contagious links” conditional on a macroeconomic shock, where a contagious link leads to the default of a bank following the default of its counterparty. Under general contagion mechanisms and incomplete information, the financial network is modeled as an inhomogeneous random graph, where the conditional probabilities of having contagious links depend on banks’ characteristics. We give different bounds on the size of the cascade through contagious links and derive testable conditions for this cascade to be small.", "e:keyword": ["Systemic risk", "Default contagion", "Financial stability", "Interbank network", "Phase transitions", "Sharp threshold", "Complex networks", "Inhomogeneous random graph"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1541", "e:abstract": "We consider a nonstationary, stochastic, multistage supply system with a general assembly structure, in which customers can place orders in advance of their future demand requirements. This advance demand information is now recognized in both theory and practice as an important strategy for managing the mismatch between supply and demand. In conjunction, we allow expediting of components and partially completed subassemblies in the system to provide the supply chain with the means to manage the stockout risk and significantly enhance cost savings realized through advance demand information. To solve the resulting assembly system, we develop a new method based on identifying local properties of optimal decisions. This new method allows us to solve assembly systems with multiple product flows. We derive the structure of the optimal policy, which represents a double-tiered echelon basestock policy whose basestock levels depend on the state of advance demand information. This form of the optimal policy allows us to: (i) provide actionable policies for firms to manage large-scale assembly systems with expediting and advance demand information; (ii) prove that advance demand information and expediting of stock both reduce the amount of inventory optimally held in the system; and (iii) numerically solve such assembly systems, and quantify the savings realized. In contrast to the conventional wisdom, we discover that advance demand information and expediting of stock are complementary under short demand information horizons. They are substitutes only under longer information horizons.", "e:keyword": ["Multiechelon inventory", "Assembly system", "Advance demand information", "Expediting", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1542", "e:abstract": "We undertake an empirical study of the impact of delay announcements on callers’ abandonment behavior and the performance of a call center with two priority classes. A Cox regression analysis reveals that in this call center, callers’ abandonment behavior is affected by the announcement messages heard. To account for this, we formulate a structural estimation model of callers’ (endogenous) abandonment decisions. In this model, callers are forward-looking utility maximizers and make their abandonment decisions by solving an optimal stopping problem. Each caller receives a reward from service and incurs a linear cost of waiting. The reward and per-period waiting cost constitute the structural parameters that we estimate from the data of callers’ abandonment decisions as well as the announcement messages heard. The call center performance is modeled by a Markovian approximation. The main methodological contribution is the definition of an equilibrium in steady state as one where callers’ expectation of their waiting time, which affects their (rational) abandonment behavior, matches their actual waiting time in the call center, as well as the characterization of such an equilibrium as the solution of a set of nonlinear equations. A counterfactual analysis shows that callers react to longer delay announcements by abandoning earlier, that less patient callers as characterized by their reward and cost parameters react more to delay announcements, and that congestion in the call center at the time of the call affects caller reactions to delay announcements.", "e:keyword": ["Delay announcement", "Abandonment", "Structural estimation", "Equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1545", "e:abstract": "We consider the dynamic casino gambling model initially proposed by Barberis (2012) and study the optimal stopping strategy of a precommitting gambler with cumulative prospect theory (CPT) preferences. We illustrate how the strategies computed in Barberis (2012) [Barberis N (2012) A model of casino gambling. <i>Management Sci.</i> 58(1): 35–51.] can be strictly improved by reviewing the betting history or by tossing an independent coin, and we explain that the improvement generated by using randomized strategies results from the lack of quasi-convexity of CPT preferences. Moreover, we show that any path-dependent strategy is equivalent to a randomization of path-independent strategies.", "e:keyword": ["Casino gambling", "Cumulative prospect theory", "Path dependence", "Randomized strategies", "Quasi-convexity", "Optimal stopping"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1546", "e:abstract": "The majority of approaches to product line design that have been proposed by marketing scientists assume that the underlying choice model that describes how the customer population will respond to a new product line is known precisely. In reality, however, marketers do not precisely know how the customer population will respond and can only obtain an estimate of the choice model from limited conjoint data. In this paper, we propose a new type of optimization approach for product line design under uncertainty. Our approach is based on the paradigm of robust optimization where, rather than optimizing the expected revenue with respect to a single model, one optimizes the worst-case expected revenue with respect to an uncertainty set of models. This framework allows us to account for parameter uncertainty, when we may be confident about the type of model structure but not about the values of the parameters, and structural uncertainty, when we may not even be confident about the right model structure to use to describe the customer population. Through computational experiments with a real conjoint data set, we demonstrate the benefits of our approach in addressing parameter and structural uncertainty. With regard to parameter uncertainty, we show that product lines designed without accounting for parameter uncertainty are fragile and can experience worst-case revenue losses as high as 23%, and that the robust product line can significantly outperform the nominal product line in the worst case, with relative improvements of up to 14%. With regard to structural uncertainty, we similarly show that product lines that are designed for a single model structure can be highly suboptimal under other structures (worst-case losses of up to 37%), while a product line that optimizes against the worst of a set of structurally distinct models can outperform single model product lines by as much as 55% in the worst case and can guarantee good aggregate performance over structurally distinct models.", "e:keyword": ["Product line design", "Robust optimization", "Parameter uncertainty", "Structural uncertainty", "Model uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1548", "e:abstract": "We consider the pricing problem faced by a monopolist who sells a product to a population of consumers over a finite time horizon. Customers’ types differ along two dimensions: (i) their willingness-to-pay for the product and (ii) their arrival time during the selling season. We assume that the seller knows only the support of the customers’ valuations and do not make any other distributional assumptions about customers’ willingness-to-pay or arrival times. We consider a robust formulation of the seller’s pricing problem that is based on the minimization of her worst-case regret. We consider two distinct cases of customers’ purchasing behavior: myopic and strategic customers. For both of these cases, we characterize optimal price paths. For myopic customers, the regret is determined by the price at a critical time. Depending on the problem parameters, this critical time will be either the end of the selling season or it will be a time that equalizes the worst-case regret generated by undercharging customers and the worst-case regret generated by customers waiting for the price to fall. The optimal pricing strategy is not unique except at the critical time. For strategic consumers, we develop a robust mechanism design approach to compute an optimal policy. Depending on the problem parameters, the optimal policy might lead some consumers to wait until the end of the selling season and might price others out of the market. Under strategic customers, the optimal price equalizes the regrets generated by different customer types that arrive at the beginning of the selling season. We show that a seller that does not know if the customers are myopic should price as if they are strategic. We also show there is no benefit under myopic consumers to having a selling season longer than a certain uniform bound, but that the same is not true with strategic consumers.", "e:keyword": ["Demand uncertainty", "Strategic consumers", "Robust optimization", "Prior-free", "Worst-case regret"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1549", "e:abstract": "For a polytope <i>P</i>, the <i>Chvátal closure</i> <i>P</i>′ ⊆ <i>P</i> is obtained by simultaneously strengthening all feasible inequalities <i>cx</i> ⩽ <i>β</i> (with integral <i>c</i>) to <i>cx</i> ⩽ ⌊<i>β</i>⌋. The number of iterations of this procedure that are needed until the integral hull of <i>P</i> is reached is called the Chvátal rank. If <i>P</i> ⊆ [0, 1]<sup><i>n</i></sup>, then it is known that <i>O</i>(<i>n</i><sup>2</sup> log <i>n</i>) iterations always suffice and at least (1 + 1/<i>e</i> − <i>o</i>(1))<i>n</i> iterations are sometimes needed, leaving a huge gap between lower and upper bounds.We prove that there is a polytope contained in the 0/1 cube that has Chvátal rank Ω(<i>n</i><sup>2</sup>), closing the gap up to a logarithmic factor. In fact, even a superlinear lower bound was mentioned as an open problem by several authors. Our choice of <i>P</i> is the convex hull of a semi-random Knapsack polytope and a single fractional vertex. The main technical ingredient is linking the Chvátal rank to <i>simultaneous Diophantine approximations</i> w.r.t. the ‖·‖<sub>1</sub>-norm of the normal vector defining <i>P</i>.", "e:keyword": ["Integer programming", "Chvátal-gomory cuts"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1550", "e:abstract": "Properties of two large families of scale-free forecast accuracy measures that include popular measures such as mean absolute percentage error, relative error, and squared percentage error, are examined in this paper. We describe the optimal reports when forecasts are evaluated using these measures. We also provide analytic expressions for the optimal Bayes’ act associated with these measures under a general power transformation for several well-known probability distributions. We then show that using measures from these two families may inadvertently provide incentives for either pessimism or optimism among forecasters, i.e., rewarding underforecasts or overforecasts relative to some reference measure of central tendency. As an illustration of these concepts, we examine the use of these measures for model selection in a forecast aggregation example using stock price forecasts derived from the Thomson Reuters Institutional Brokers’ Estimate System. This example illustrates how aggregation methods that always yield lower estimates relative to the mean or median generally exhibit better scores using percentage error-based measures, while those that yield higher estimates compared to the mean or median will effectively rank higher when relative error-based measures are used.", "e:keyword": ["Forecast evaluation", "Forecast accuracy", "Scale-free measures", "Scale-independence", "Pessimism", "Optimism"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1552", "e:abstract": "We explore the procurement of influenza vaccines by a government whose objective is to minimize the expected social costs (including vaccine, vaccine administration, and influenza treatment costs) when a for-profit vaccine supplier has production yield uncertainty, private information about its productivity (adverse selection), and potentially unverifiable production effort (moral hazard). Timeliness is important—costs for both the supplier and the government procurer may increase if part of the vaccine order is delivered after a scheduled delivery date. We theoretically derive the optimal menu of output-based contracts. Next, we present a menu that is optimal within a more restricted set of practically implementable contracts, and numerically show that such a menu leads to near-optimal outcomes. Finally, we present a novel way to eliminate that information rent if the manufacturer’s effort is also verifiable, a counterintuitive result because the manufacturer has private productivity information. This provides an upper bound for the government on how much it should spend to monitor the manufacturer’s effort.", "e:keyword": ["Mechanism design", "Principal-agent modeling", "Adverse selection", "Moral hazard", "Epidemiology", "Influenza vaccine supply chain"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1553", "e:abstract": "This study provides an approach to manage an ongoing Internet ad campaign that substantially improves the number of clicks and the revenue earned from clicks. The problem we study is faced by an Internet advertising firm (Chitika) that operates in the Boston area. Chitika contracts with publishers to place relevant advertisements (ads) over a specified period on publisher websites. Ad revenue accrues to the firm and the publisher only if a visitor clicks on an ad (i.e., we are considering the cost-per-click model in this study). This might imply that all visitors to the publisher’s website be shown ads. However, this is not the case if the publisher imposes a click-through-rate constraint on the advertising firm. This performance constraint captures the publisher’s desire to limit ad clutter on the website and hold the advertising firm responsible for the publisher’s opportunity cost of showing an ad that did not result in a click. We develop a predictive model of a visitor clicking on a given ad. Using this prediction of the probability of a click, we develop a decision model that uses a threshold to decide whether or not to show an ad to the visitor. The decision model’s objective is to maximize the advertising firm’s revenue subject to a click-through-rate constraint. A key contribution of this paper is to characterize the structure of the optimal solution. We study and contrast two competing solutions: (1) a <i>static</i> solution, and (2) a <i>rolling-horizon</i> solution that resolves the problem at certain points in the planning horizon. The static solution is shown to be optimal when accurate information on the input parameters to the problem is known. However, when the parameters to the model can only be estimated with some error, the rolling-horizon solution can perform better than the static solution. When using the rolling-horizon solution, it becomes important to choose the appropriate resolving frequency. The implemented models operate in real time in Chitika’s advertising network. Implementation challenges and the business impact of our solution are discussed. To present a head-to-head comparison of our implemented approach with the past practice at Chitika, we implemented our solution in parallel to the past practice.", "e:keyword": ["Internet advertising", "Performance constraints", "Visitor profiling", "Revenue optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1554", "e:abstract": "We develop a framework for determining the optimal resource capacity of each station composing a stochastic network, motivated by applications arising in computer capacity planning and business process management. The problem is mathematically intractable in general and therefore one typically resorts to either simplistic analytical approximations or time-consuming simulation-based optimization methods. Our solution framework includes an iterative methodology that relies only on the capability of observing the queue lengths at all network stations for a given resource capacity allocation. We theoretically investigate this proposed methodology for single-class Brownian tree networks and illustrate the use of our framework and the quality of its results through computational experiments.", "e:keyword": ["Capacity allocation", "Capacity planning", "Queueing networks", "Resource capacity management", "Stochastic networks", "Stochastic approximation", "Simulation optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1555", "e:abstract": "Supply function equilibrium models are used to study electricity market auctions with uncertain demand. We study the effects on the supply function equilibrium of a tax, levied by the system operator, on the observed surplus of producers. Such a tax provides an incentive for producers to alter their offers to avoid the tax. We consider these incentives under both strategic and price-taking assumptions. The model is extended to a setting in which producers are taxed on the benefits accruing to them from a transmission line expansion (a beneficiaries-pay transmission charge). In this setting, we show how this tax may lead to lower prices in equilibrium.", "e:keyword": ["Supply function equilibrium", "Energy policies", "Auctions"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1556", "e:abstract": "We propose a novel approach to cross-selling investment products that considers both the customers’ and the bank’s interests. Our goal is to improve the risk–return profile of the customer’s portfolio and the bank’s profitability concurrently, essentially creating a win-win situation, while deepening the relationship with an acceptable product. Our cross-selling approach takes the customer’s status quo bias into account by starting from the existing customer portfolio, rather than forming an efficient portfolio from scratch. We estimate a customer’s probability of accepting a product offer with a predictive model using readily available data. Then, we model the investment product cross-selling problem as a nonlinear mixed-integer program that maximizes a customer’s expected return from the proposed portfolio, while ensuring that the bank’s profitability improves by a certain factor. We implemented our methodology at the private banking division of Yapı Kredi, the fourth-largest private bank in Turkey. Empirical results from this application illustrate that (1) a traditional mean-variance portfolio optimization approach does not increase portfolio returns and reduces overall bank profits, (2) a standard cross-selling approach increases bank profits at the expense of the customers’ portfolio returns, and (3) our win-win approach increases the expected portfolio returns of customers without increasing their variances, while simultaneously improving bank profits substantially.", "e:keyword": ["Portfolio optimization", "Cross selling", "Acceptance probability", "Predictive model", "Private banking"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1558", "e:abstract": "We study the mechanism design problem for a seller of an indivisible good in a setting where privately informed buyers can acquire additional information and refine their valuations for the good at a cost. For this setting, we propose optimal (revenue-maximizing) and efficient (welfare-maximizing) mechanisms that induce a right level of investment in information acquisition. We show that because information is costly, in the optimal and even the efficient mechanisms, not all buyers would obtain the additional information. In fact, these mechanisms incentivize buyers with higher initial valuations to acquire information.", "e:keyword": ["Mechanism design", "Auctions", "Online advertising"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1560", "e:abstract": "Public transit systems in major urban areas usually operate under deficits and therefore require significant subsidies. An important cause of this deficit, particularly in the developing world, is the high fare evasion rate mainly due to an ineffective control policy or the lack of it. In this paper we study new models for optimizing fare inspection strategies in transit networks based on bilevel programming. In the first level, the leader (the network operator) determines probabilities for inspecting passengers at different locations, while in the second level, the followers (the fare-evading passengers) respond by optimizing their routes given the inspection probabilities and travel times. To model the followers’ behavior we study both a nonadaptive variant, in which passengers select a path a priori and continue along it throughout their journey, and an adaptive variant, in which they gain information along the way and use it to update their route. For these problems, which are interesting in their own right, we design exact and approximation algorithms, and we prove a tight bound of 3/4 on the ratio of the optimal cost between adaptive and nonadaptive strategies. For the leader’s optimization problem, we study a fixed-fare and a flexible-fare variant, where ticket prices may or may not be set at the operator’s will. For the latter variant, we design an LP-based approximation algorithm. Finally, employing a local search procedure that shifts inspection probabilities within an initially determined support set, we perform an extensive computational study for all variants of the problem on instances of the Dutch railway and the Amsterdam subway network. This study reveals that our solutions are within 5% of theoretical upper bounds drawn from the LP relaxation. We also derive exact nonlinear programming formulations for all variants of the leader’s problem and use them to obtain exact solutions for small instance sizes.The e-companion is available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/opre.2016.1560\">https://doi.org/10.1287/opre.2016.1560</ext-link>.", "e:keyword": ["Transit networks", "Bilevel optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1561", "e:abstract": "We study a firm’s optimal strategy to adjust its capacity using demand information. The capacity adjustment is costly and often subject to managerial hurdles, which sometimes make it difficult to adjust capacity multiple times. To clearly analyze the impact of demand learning on the firm’s decision, we study two scenarios. In the first scenario, the firm’s capacity adjustment cost increases significantly with respect to the number of adjustments because of significant managerial hurdles, and resultantly the firm has a single opportunity to adjust capacity (<i>single adjustment scenario</i>). In the second scenario, the capacity adjustment cost does not change with respect to the number of adjustments because of little managerial hurdles, and therefore the firm has multiple opportunities to adjust capacity (<i>multiple adjustment scenario</i>). For both scenarios, we first formulate the problem as a stochastic dynamic program, and then characterize the firm’s <i>optimal</i> policy: when to adjust and by how much. We show that the optimal decision on when and by how much to change the capacity is not monotone in the likelihood of high demand in the single adjustment scenario, while the optimal decision is monotone under mild conditions, and the optimal policy is a control band policy in the multiple adjustment scenario. The sharp contrast reflects the impact of demand learning on the firm’s optimal capacity decision. Since computing and implementing the optimal policy is not tractable for general problems, we develop a data-driven heuristic for each scenario. In the single adjustment scenario, we show that a two-step heuristic, which explores demand for an appropriately chosen length of time and adjusts the capacity based on the observed demand is asymptotically optimal, and show the convergence rate. In the multiple adjustment scenario, we also show that a multistep heuristic under which the firm adjusts its capacity at a predetermined set of periods with an exponentially increasing gap between two consecutive decisions is asymptotically optimal and shows its convergence rate. We finally apply our heuristics to a numerical study and demonstrate the performance and robustness of the heuristics.The online appendix is available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/opre.2016.1561\">https://doi.org/10.1287/opre.2016.1561</ext-link>.", "e:keyword": ["Capacity management", "Managerial hurdles", "Exploration-exploitation", "Bayesian updating", "Data driven"]}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1562", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.2016.1564", "e:abstract": "In this paper we describe the Traveler’s Route Choice Problem (TRCP). This is the problem of a traveler in a railway system who plans to take the fastest route to a destination but is faced with a disruption of unknown length on this route. In that case, he can wait until the disruption is over or take a detour route as an alternative. Since the duration of the disruption is not known in advance, he is left with a decision problem under uncertainty. In this paper we model the problem and describe the strategies that may be used in such a situation. Instead of finding optimal strategies for a specialized quality measure, we consider dominance relations between strategies and show that dominated strategies are nonoptimal for the common quality measures. We then analyze which strategies for the TRCP are dominated. In general, the set of nondominated strategies is strongly reduced. We also show that, under certain assumptions, only a small set of strategies is nondominated and conclude that in this case the TRCP can be solved by enumeration for any of the quality measures.The e-companion is available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.128/opre.2016.1564\">https://doi.org/10.128/opre.2016.1564</ext-link>.", "e:keyword": ["Public transport", "Disruption", "Uncertainty", "Strategies", "Dominance"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.110", "e:abstract": "A searcher and target move among a finite set of cells <i>C</i> = 1, 2, …,<i>N</i> in discrete time. At the beginning of each time period, one cell is searched. If the target is in the selected cell <i>j</i>, it is detected with probability <i>q<sub>j</sub></i>. If the target is not in the cell searched, it cannot be detected during the current time period. After each search, a target in cell <i>j</i> moves to cell <i>k</i> with probability <i>p<sub>jk</sub></i>. The target transition matrix, <i>P</i> = [<i>p<sub>jk</sub></i>] is known to the searcher. The searcher's path is constrained in that if the searcher is currently in cell <i>j</i>, the next search cell must be selected from a set of neighboring cells <i>C<sub>j</sub></i>. The object of the search is to minimize the probability of not detecting the target in <i>T</i> searches.", "e:keyword": ["Military", "Search/surveillance: moving target and constrained search path", "Programming", "Integer algorithms", "Branch-and-bound: computing optimal search path"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.115", "e:abstract": "Assume that we wish to generate two samples of <i>n</i> independent identically distributed random variables, (<i>X</i><sub>1</sub>,…, <i>X<sub>n</sub></i>) and (<i>Y</i><sub>1</sub>,…, <i>Y<sub>n</sub></i>), where <i>X</i><sub>1</sub> and <i>Y</i><sub>1</sub> have densities <i>f</i> and <i>g</i>, respectively. If these samples are used in a simulation, and <i>f</i> is <i>close</i> to <i>g</i>, it is sometimes desirable to have close simulation results. This can be achieved by insisting that both samples agree in most of their components, that is, <i>X<sub>i</sub></i>=<i>Y<sub>i</sub></i> for as many <i>i</i> as possible under the given distributional constraints. Samples with this property are said to be optimally coupled. In this paper, we propose and study various methods of coupling two samples, a sequence of samples and an infinite family of samples.", "e:keyword": ["Probability: Kolmogorov entropy", "Simulation", "Random variable generation: generating dependent samples", "Statistics", "Correlation: coupled samples"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.127", "e:abstract": "Many interesting combinatorial problems can be optimized efficiently using recursive computations often termed <i>discrete dynamic programming</i>. In this paper, we develop a paradigm for a general class of such optimizations that yields a polyhedral description for each model in the class. The elementary concept of dynamic programs as shortest path problems in acyclic graphs is generalized to one seeking a least cost solution in a directed hypergraph. Sufficient conditions are then provided for binary integrality of the associated hyperflow problem. Given a polynomially solvable dynamic program, the result is a linear program, in polynomially many variables and constraints, having the solution vectors of the dynamic program as its extreme-point optima. That is, the linear program provides a succinct characterization of the solutions to the underlying optimization problem expressed through an appropriate change of variables. We also discuss projecting this formulation to recover constraints on the original variables and illustrate how some important dynamic programming solvable models fit easily into our paradigm. A classic multiechelon lot sizing problem in production and a host of optimization problems on recursively defined classes of graphs are included.", "e:keyword": ["Programming: dynamic programming and polyhedral theory", "Cutting planes"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.139", "e:abstract": "We solve the queueing system C<i><sub>k</sub></i>/C<i><sub>m</sub></i>/s, where <i>C<sub>k</sub></i> is the class of Coxian probability density functions (pdfs) of order <i>k</i>, which is a subset of the pdfs that have a rational Laplace transform. We formulate the model as a continuous-time, infinite-space Markov chain by generalizing the method of stages. By using a generating function technique, we solve an infinite system of partial difference equations and find closed-form expressions for the system-size, general-time, prearrival, post-departure probability distributions and the usual performance measures. In particular, we prove that the probability of <i>n</i> customers being in the system, when it is <i>saturated</i> is a linear combination of geometric terms. The closed-form expressions involve a solution of a system of nonlinear equations that involves only the Laplace transforms of the interarrival and service time distributions. We conjecture that this result holds for a more general model. Following these theoretical results we propose an exact algorithm for finding the system-size distribution and the system's performance measures. We examine special cases and apply this method for numerically solving the C<sub>2</sub>/C<sub>2</sub>/s and E<i><sub>k</sub></i>/C<sub>2</sub>/s queueing systems.", "e:keyword": ["Queues: multichannel", "Markovian queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.15", "e:abstract": "For a school board with several schools in its territory, the School Districting Problem is to specify the groups of children attending each school. A decision support system used to help administrators is described in this paper. It includes several heuristic procedures to assign edges of the network to the schools. The color graphics display is extensively used to assess the quality of the solution and to provide interactive functions for modifying the solution.", "e:keyword": ["Decision support system: school districting problem", "Education systems", "Planning: school districting"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.156", "e:abstract": "We investigate when <i>Arrivals See Time Averages</i> (ASTA) in a stochastic model; i.e., when the stationary distribution of an embedded sequence, obtained by observing a continuous-time stochastic process just prior to the points (arrivals) of an associated point process, coincides with the stationary distribution of the observed process. We also characterize the relation between the two distributions when ASTA does not hold. We introduce a <i>Lack of Bias Assumption</i> (LBA) which stipulates that, at any time, the conditional intensity of the point process, given the present state of the observed process, be independent of the state of the observed process. We show that LBA, without the Poisson assumption, is necessary and sufficient for ASTA in a stationary process framework. Consequently, LBA covers known examples of non-Poisson ASTA, such as certain flows in open Jackson queueing networks, as well as the familiar Poisson case (PASTA). We also establish results to cover the case in which the process is observed just after the points, e.g., when departures see time averages. Finally, we obtain a new proof of the Arrival Theorem for product-form queueing networks.", "e:keyword": ["Probability", "Distributions: stationary and Palm distributions", "Queues", "Limit theorems: on arrivals that see time averages", "Queues", "Networks: the Arrival Theorem"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.173", "e:abstract": "PASTA (Poisson Arrivals See Time Averages) is a term coined by R. Wolff in his well known 1982 paper. In keeping with Wolff's terminology, we use the term anti-PASTA to refer to the following converse of PASTA. Given that arrivals do indeed see time averages, when must the arrival process necessarily be Poisson? We show that anti-PASTA is satisfied in a pure-jump Markov process, provided that the arrival process corresponds to a subset of the Markov process jumps.", "e:keyword": ["Probability", "Stochastic model applications", "Point processes", "Queues", "Networks: customer and time averages"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.176", "e:abstract": "The result <i>Poisson Arrivals See Time Averages</i> (PASTA) is well established in the queueing literature. Although there are some well known examples where non-Poisson arrivals see time averages, arrival and time averages usually are different when arrivals are not Poisson. There are also situations where it has been shown that arrivals see time averages <i>only if</i> arrivals are Poisson, a property called <i>Anti-PASTA</i>. We present a simple and direct proof of Anti-PASTA for continuous-time Markov chains.", "e:keyword": ["Probability", "Markov processes properties of the Poisson process", "Queues: relations between time and arrivals averages"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.178", "e:abstract": "We consider problems on networks that are captured by two performance measures. One performance measure is any general cost function of a solution; the other is a bottleneck measure that describes the worst (maximum cost) component of the solution. The paper contains algorithms to solve three problems. In one problem, we minimize the bottleneck subject to a constraint on the generalized cost. In the second problem, we minimize the generalized cost subject to a constraint on the bottleneck. In the third problem, we consider the two criteria simultaneously and find all the Pareto optimum solutions. The major result is that the introduction of the bottleneck measure changes the complexity of the original (general cost) problem by a factor which is at most linear in the number of links.", "e:keyword": ["Algorithms", "Multiple criteria algorithms for multiple criteria problems on networks", "Networks: multiple criteria problems on networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.2", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.22", "e:abstract": "We consider the problem of scheduling <i>n</i> jobs to minimize the total earliness and tardiness penalty. We review the literature on this topic, providing a framework to show how results have been generalized starting with a basic model that contains symmetric penalties, one machine and a common due date. To this base we add such features as parallel machines, complex penalty functions and distinct due dates. We also consolidate many of the existing results by proving general forms of two key properties of earliness/tardiness models.", "e:keyword": ["Production/scheduling: deterministic sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.37", "e:abstract": "We consider a multistage, multi-item distribution system in which each of a number of items is stocked at each of a number of locations. The cost of placing an order at a location depends on the set of items ordered there. Demand for the items is constant, and there is a linear holding cost for each item at each location. Only nested policies are considered. A known heuristic is guaranteed to find a policy that is within 2% of optimal. However, if there are <i>M</i> items and <i>L</i> locations the running time of the heuristic is <i>O</i>(<i>M</i><sup>4</sup><i>L</i><sup>4</sup>). We improve the running time to <i>O</i>(<i>MLD</i> log(<i>ML</i>)) where <i>D</i> is at most the maximum of the depth of the location and family arborescences.", "e:keyword": ["Production/inventory: heuristics", "Multi-item multistage", "Deterministic approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.53", "e:abstract": "Two of the basic assumptions of economic order quantity models are an infinite horizon and static costs. In this paper, both assumptions are relaxed and results on the form of the optimal inventory policy are developed for both finite and infinite horizon models with changes in any or all costs. These results are used to develop short procedures for the computation of the optimal policy.", "e:keyword": ["Inventory/production: ordering strategies for price increases"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.64", "e:abstract": "We consider an inventory system with one warehouse and <i>N</i> retailers. Lead times are constant and the retailers face independent Poisson demand. Replenishments are one-for-one. We provide simple recursive procedures for determining the holding and shortage costs of different control policies.", "e:keyword": ["Inventory", "Multi-echelon: one-for-one replenishments", "Inventory", "Stochastic: Poisson demand", "Continuous review"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.7", "e:abstract": "Modeling is a principal tool for studying complex systems. Since models may be used for predictions, for analysis, or for prescription, we must ask what our goals are before we build our models. Historically, predictive numerical models have dominated our practice. Since the world we are modeling is orders of magnitude more complex than even the largest models our computers can handle, we must conserve computational power, first, by asking how much temporal detail we need and how much can be supported by available data and theories, second, by asking whether knowledge of steady states may not be more important than knowledge of temporal paths, third, by using the hierarchical properties of systems to aggregate and thereby simplify them, and, fourth, by substituting symbolic modeling, where appropriate, for numerical modeling.", "e:keyword": ["Philosophy of modeling: modeling complex systems", "Professional OR/MS philosophy"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.70", "e:abstract": "This paper considers a version of the economic lot sizing problem for a single product produced in a facility of finite capacity over a finite time horizon with specifiable start and end conditions. A set of algorithms is presented that will approximate the optimal production schedule to a given allowable error (ε). Algorithms with computation time bounds of <i>O</i>(1/ε<sup>2</sup>) are presented which allow for setups of finite length, setups with or without direct cash flow, quite general cost and demand functions, and a wide variety of production policy constraints. The procedures make no a priori assumptions about the form of the optimal solution. Numerical results are included.", "e:keyword": ["Production/scheduling: lot sizing approximations and production planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.84", "e:abstract": "Part-orienting systems are employed in many manufacturing environments to feed work parts in correct orientation for proper operations. A part-orienting system is a serial arrangement of orienting devices; the selection and ordering of these devices is an important design aspect that is addressed in this paper. In addition, a parallel configuration that consists of individual orienting subsystems is briefly investigated. Numerical examples and extensive computational results are reported.", "e:keyword": ["Production/scheduling: scheduling heuristics for part-orienting systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.1.99", "e:abstract": "Use of consolidation terminals to transport products from various sources to various destinations can take advantage of economies of scale in transportation costs. Instead of making direct shipments, each source can ship in bulk to one or more consolidation terminals. There, shipments can be broken down, and material bound for the same destination can be combined. We consider such a freight transport problem (FTP). For each source–destination pair, it must be decided whether to ship the product directly or via a consolidation terminal. Shipping costs are piecewise linear concave functions of the volume shipped. Shipping via a terminal can also incur a linear inventory holding cost. We seek a minimum cost pattern of direct and indirect (i.e., via a terminal) shipments. This is a type of concave cost multiproduct network flow problem. We can solve this problem optimally if either the source-to-terminal or the terminal-to-destination shipping costs are linear. In this case, FTP decomposes into a set of concave cost facility location problems (CFLP). In more general cases, heuristic methods that solve sequences of linear problems can be used. Some computational results are presented.", "e:keyword": ["Facilities/equipment planning", "Location concave-cost facility location problem", "Networks/graphs", "Multicommodity: concave-cost multicommodity network flow problem", "Transportation: use of consolidation terminals"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.189", "e:abstract": "Through two case examples this paper illustrates that senior public managers at even the highest levels can be, and regularly are, helped by OR/MS people and/or their tools, approaches, and ways of thinking. Thus, it questions the continuing belief evidently held by some in the OR/MS community that it does not, or cannot, happen for a variety of reasons, which they then proceed to spell out.", "e:keyword": ["Government: support to senior public managers", "Military: management of large programs", "Technology: decision making under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.193", "e:abstract": "This paper shows how a visual, interactive, dynamic multiple objective linear programming decision support system can effectively be used for analyzing input-output models. We have applied our approach to studying the quantitative effects of economic or political crises to the Finnish economy. Examples of such crises are nuclear power plant accidents, trade embargoes, and international conflicts. An input-output model of the Finnish economy with 17 industries (sectors) is employed. Our system has been implemented on a microcomputer and is being used by the National Board of Economic Defense. Several typical case situations are discussed.", "e:keyword": ["Decision analysis: multiple criteria", "Economics: input–output analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.202", "e:abstract": "We address the problem of moving a collection of objects from one subset of <i>Z<sup>m</sup></i> to another at minimum cost. We show that under fairly natural <i>rules for movement</i> assumptions, if the origin and destination are far enough apart, then a near optimal solution with special structure exists: Our trajectory from the origin to the destination accrues almost all of its cost repeating at most <i>m</i> different patterns of movement. Directions for related research are identified.", "e:keyword": ["Military", "Logistics", "Optimal maneuvering", "Networks/graphs", "Optimal maneuvering by graph theory approach", "Transportation", "Mode/route choice", "Optimal maneuvering by turnpike theorems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.217", "e:abstract": "This paper is concerned with a new linearization strategy for a class of zero-one mixed integer programming problems that contains quadratic cross-product terms between continuous and binary variables, and between the binary variables themselves. This linearization scheme provides an equivalent mixed integer linear programming problem which yields a tighter continuous relaxation than that obtainable via the alternative linearization techniques available in the literature. Moreover, the proposed technique provides a unifying framework in the sense that all the alternate methods lead to formulations that are accessible through appropriate surrogates of the constraints of the new linearized formulation. Extensions to various other types of mixed integer nonlinear programming problems are also discussed.", "e:keyword": ["Programming: linear reformulations of nonlinear integer programs", "Programming", "Integer", "Nonlinear: mixed integer programming problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.227", "e:abstract": "In a previous paper (1988), the author proposed methods to obtain sharp lower and upper bounds for probabilities that at least one out of <i>n</i> events occurs, based on the knowledge of some of the binomial moments of the number of events which occur and linear programming formulations. This paper presents further results concerning other and more general logical functions of events: We give sharp lower and upper bounds for the probabilities that: a) exactly <i>r</i> events, b) at least <i>r</i> events occur, using linear programming. The basic facts are expressed by the dual feasible basis characterization theorems which are interpreted in terms of the vertices of the dual problems. We mention some linear inequalities, among the binomial moments, generalize the theory for the case of nonconsecutive binomial moments and present numerical examples.", "e:keyword": ["Probability", "Distributions: bounds on distributions", "Programming", "Linear", "Algorithms: application of dual method", "Reliability", "Multistate systems: R out of N probability"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.240", "e:abstract": "KORBX<sup>®</sup> (a registered trademark of AT&T) is AT&T's new system for solving large-scale linear programs. The system consists of both hardware, which uses parallel processing technology configured with 256 MB of memory, and software which exploits the design and resources of this modern hardware. The KORBX linear programming software system contains four algorithms which are variations of the interior point method of Narendra Karmarkar. The primal, dual, primal-dual, and power series algorithms were empirically evaluated on a set of linear programming application models being used by the staff of the Military Airlift Command at Scott Air Force Base. For calibration purposes, a set of smaller test problems were also run using MPSX and XMP; and some pure network problems were solved using NETFLO, MPSX, and XMP.", "e:keyword": ["Programming: evaluating algorithms for military airlifts", "Programming", "Nonlinear: linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.249", "e:abstract": "Employment scheduling is the process whereby U. S. Navy ships, submarines, aircraft and other units are assigned to major operations, exercises, maintenance periods, inspections and other events; the employment schedule directly influences fleet combat readiness. Currently, this process is largely manual requiring several full-time scheduling officers and additional personnel at various levels of management. We introduce an optimization model that automates a substantial part of the employment scheduling problem. The model is formulated as a generalized set partitioning problem and is applied to the annual planning schedule for naval surface combatants of the Atlantic Fleet. For the calendar year 1983, 111 ships engage in 19 primary events yielding a model with 228 constraints and 10,723 binary variables. This model is solved optimally in about 1.6 minutes producing a schedule that is significantly better than the corresponding published schedule.The optimized peacetime employment schedule which has as its objective maximizing combat readiness should always be the goal and guide.U.S. Navy, <i>NWP</i> − 1He knew the things that were, the things that would be, and the things that had come before.Homer, <i>The Iliad</i>", "e:keyword": ["Military: force readiness", "Programming: generalized set partitioning"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.260", "e:abstract": "Some 30 years ago, O. G. Haywood, Jr. published a pioneering paper relating game theory considerations to historical cases of World War II (“Military Decision and Game Theory”). The paper has become one of the most cited in military operations research literature. Facts revealed long after its publication have shed new light upon the historical case and bear upon the core of that paper. We present the story and consider its implications.", "e:keyword": ["Games/group decision: game theory", "Military: military history", "Professional: OR implementation"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.265", "e:abstract": "This paper designs prediction models to estimate the chance of the most severe nuclear accidents (such as complete core melts) for the population of U.S. and worldwide nuclear reactors. We formally introduce the notion of random escalation of incident severity. We then develop a class of models that views accidents of high severity as members of a subpopulation of incidents of lower severity; a random escalation model (<b>REM</b>) uses Bayesian methods to update unobservable failure rates and other model parameters. The priors for failure rates are based on extensive engineering judgment about the probabilities of core melt. Predictive distributions for time to time to core melt are calculated from the model, based on operational experience and accident data accumulated to date; the results are compared with those of N. C. Rasmussen, H. W. Lewis, P. C. Groer and others. The paper includes three theorems that reveal the structure of separable densities for parameter updating, the invariance of <b>REM</b>s under severity level classification and the reproducibility of Poisson-Binomial <b>REM</b>s. In an appendix, we examine the special assumptions that are required to specify the current U.S. Nuclear Regulatory Commission risk model.", "e:keyword": ["Decision analysis", "Risk: influence diagrams for safety analysis", "Forecasting: Bayesian predictions and inference", "Probability", "Stochastic model: application to prediction of nuclear incidents"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.278", "e:abstract": "This paper examines the effects on the optimal ordering policy (and its cost) of allowing for interactions among retail outlets. Specifically, transshipments are allowed as recourse actions occurring after demands are realized but before they must be satisfied. The resultant savings in holding and shortage costs are balanced against the costs of transshipment. A base stock ordering policy is optimal for this model. If the final period base stock order-up-to point is nonnegative, then it will be the base stock order-up-to point for all periods; unfortunately, it can be found analytically only for two special cases, where either the cost parameters are equal at every outlet or there are only two outlets. These two special cases are used to validate a heuristic solution technique employing Monte Carlo integration, which is then compared to an easily calculated base policy in order to gauge the contribution of this model. The additional savings from using this heuristic policy are significant, particularly for problems with many retail outlets and low transshipment costs.", "e:keyword": ["Inventory/production", "Multi-item/echelon/stage: multilocation models with lateral transshipments"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.296", "e:abstract": "Competitive pressures and technological improvements are leading many firms to consider centralized information systems to manage inventories and schedule production. We propose a simple model to explore the potential benefits of such coordination. The model represents two products competing for a single production facility. Simple Markovian behavior is assumed throughout. The key step in the analysis is the explicit solution of a queueing model with a novel priority discipline: Serve a customer from the class having the largest number of customers in the system.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic: centralized inventory information", "Queues", "Nonstationary: longest-queue priority discipline"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.308", "e:abstract": "This paper considers a single server queue that operates in a random environment defined by an alternating renewal process with states 1 and 2. When the random environment is in state <i>i</i> (<i>i</i> = 1, 2), the arrivals occur at a mean rate of λ<i><sub>i</sub></i> and the distribution of service-time for these arrivals is <i>B<sub>i</sub></i>(<i>t</i>). The server is working when the random environment is in state 1 and not working when the state is 2. This model is applicable to situations in manufacturing, computer and telecommunications problems when the server is subject to random breakdown. It is also useful in modeling some priority and cyclic server queues. We analyze the problem by first examining the steady-state distribution of work in the system. We show that the work in the system is closely related to the waiting time in a special <i>GI</i>/<i>G</i>/1 queue. For the special case when the off-period is exponentially distributed, exact closed-form expressions are obtained for the performance measures of interest. For other cases, we propose an approximation and show that it works well when compared with simulations.", "e:keyword": ["Computers", "Communications", "Queues: manufacturing priority"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.319", "e:abstract": "A system must operate for t units of time. A certain component is essential for operation of the system and must be replaced by a new component whenever it fails. The price of this component changes over time. The problem of providing the proper number of spares for this component so as to minimize the total expected cost of maintaining the system operative for <i>t</i> units of time is studied. In particular, we show that when the component lives are exponentially distributed and the price is strictly increasing, it is optimal to provide <i>n</i> spares when the time remaining until termination is between <i>t<sub>n</sub></i> and <i>t<sub>n + 1</sub></i>, where 0 = <i>t</i><sub>0</sub> < <i>t</i><sub>1</sub> < <i>t</i><sub>2</sub> < …. This result is then extended to the case where the price change is arbitrary.", "e:keyword": ["Maintenance/replacement: keeping a system operating by replacing failing components", "Spares: optimal number of spares to be purchased"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.330", "e:abstract": "We consider a single-server queueing system with Poisson arrivals and general service times. While the server is up, it is subject to breakdowns according to a Poisson process. When the server breaks down, we need to repair the server immediately by initiating one of two available repair operations. The operating costs of the system include customer holding costs, repair costs and running costs. The objective is to find a corrective maintenance policy that minimizes the long-run average operating costs of the system. The problem is formulated as a semi-Markov decision process. Under some mild conditions on the repair time and service time distributions and the customer holding cost rate function, we prove that there exists an optimal stationary policy which is monotone, i.e., which is characterized by a single threshold parameter: The stochastically faster repair is initiated if and only if the number of customers in the system exceeds this threshold. We also present an efficient algorithm for the determination of an optimal monotone policy and its average cost. We then extend the problem to allow the system to postpone the repair until some future point in time. We provide a partial characterization of an optimal policy and show that monotone policies are, in general, not optimal. The latter problem also extends the authors' previous work.", "e:keyword": ["Dynamic programming: semi-Markov", "Queues", "Optimization: optimal maintenance", "Monotone policies", "Reliability", "Maintenance/repairs: queueing systems subject to breakdowns"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.344", "e:abstract": "The exchangeable-item repair systems considered here are characterized by customers who bring failed items to the system, and are prepared to take in return any serviceable item of the same kind. This exchangeability feature enables the system to keep spare items, either those repaired by the facilities the system operates or newly acquired ones. Rather than only considering the common number of backorders as a measure, the focus of this research is on the delay experienced by customers in an “ample-service” repair system, where repair of a failed item commences upon arrival at the system. The stationary and the nonstationary distributions of this delay are obtained in an analytic closed form in terms of the basic model parameters—the (Poisson) arrival rate, the (arbitrary) repair distribution of an item, and the initial number of spares in the system. The results are applicable to a variety of models that incorporate factors such as item scrapping, replenishment of new items, different modes of failure, and customers bringing several failed items. Successive use of the formulas can extend the spectrum of potential applications to multiechelon systems.", "e:keyword": ["Inventory/production: spares provisioning", "Queues: customer delays in repair systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.349", "e:abstract": "We compute the delay time distribution for a customer who delivers a failed item to a repair system, when the items are exchangeable and the customer accepts any repaired item that is offered when he is at the head of the customer queue.", "e:keyword": ["Queues", "Networks: exchangeable customers", "Reliability", "Maintenance/repairs: repair time distribution"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.355", "e:abstract": "T. W. Wall, D. Greening and R. E. D. Woolsey produced a geometric programming based technique for solving the system of nonlinear equations that arise from the kinetic approach to finding the composition of a chemical system at equilibrium. I argue that one should consider using nonlinear programming to solve these equations. This approach is illustrated by solving the example given by Wall, Greening and Woolsey. A pencil and paper solution to that problem is also given.", "e:keyword": ["Mathematics", "Systems solutions: nonlinear programming for solving simultaneous equations", "Programming", "Nonlinear", "Applications: solving simultaneous equations"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.359", "e:abstract": "This note extends the model of unemployment insurance reported by D. Zuckerman in 1985. The following extensions are incorporated into the model: The government's objective function is now formulated in a more economically appealing fashion; the search process is allowed to continue beyond the coverage period; the time value of money is explicitly incorporated; and the individual's search strategy is compared with the socially optimal policy. Using techniques and concepts from game theory, we investigate the socially and individually optimal strategies and derive an unemployment insurance strategy that best suits the government's objective.", "e:keyword": ["Games/group decisions", "Bargaining", "Stackelberg solution concept", "Government", "Programs: unemployment insurance system", "Labor: labor market and job search"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.362", "e:abstract": "The minimum discrimination information weights generalizes the entropy-based “nearly-equal” weights for additive multiattribute value models.", "e:keyword": ["Probability", "Entropy: procedure for allocation of weights", "Statistics: minimum discrimination information approach", "Utility/preference", "Multiattribute: sensitivity analysis of multiattribute models"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.2.364", "e:abstract": "We consider infinitesimal perturbation analysis estimates for the derivative of moments of the system time in an M/M/1 queue in steady state. The unbiasedness of these estimates is established for derivatives with respect to the mean service time and the arrival rate. A similar result is obtained for the system time of the <i>i</i>th customer in a GI/G/1 queue with fixed initial conditions.", "e:keyword": ["Queues", "Output process: perturbation analysis estimates", "Queues", "Tandem: perturbation analysis for the M/M/1 queue", "Queues", "Statistical analysis: unbiased perturbation analysis estimates"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.377", "e:abstract": "This paper, which is based on the remarks offered during a plenary address at the May 1989 CORS/TIMS/ORSA meeting in Vancouver, discusses the analogy between economic institutions and algorithms for solving mathematical programming problems. The simplex method for solving linear programs can be interpreted as a search for market prices that equilibrate the demand for factors of production with their supply. A possible interpretation in terms of the internal organization of a large firm is offered for Lenstra's integer programming algorithm.", "e:keyword": ["Programming", "Mathematical: the relationship between algorithms and economic institutions"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.386", "e:abstract": "This paper describes a decision support system developed to prioritize oil and gas exploration activities. During each budgeting cycle, the exploration division of a major oil company must review a number of potential oil and gas <i>plays</i>, and assign personnel to the most promising ones. These personnel are teams of geologists and geophysicists who are responsible for the exploration activities on the site. The number of interesting plays always exceeded the available exploration budget and manpower, so management was concerned about the development of a consistent ranking system of oil and gas plays that could be used to guide the budgeting and manpower decision. The development of the ranking system was based on familiar concepts from decision theory, specifically, multiattribute models and the value of information. Linear approximations to more complex mathematical functions were developed to simplify the model. However, this system was designed to be used on a routine basis by personnel unfamiliar with these technical issues, so they were masked from the user. The successful implementation of the proposed decision support system is described.", "e:keyword": ["Decision analysis: applications in petroleum/natural gas", "Utility/preference: multiattribute"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.397", "e:abstract": "This paper describes an end-use energy forecasting model for the residential sector. The technology-choice component dynamically optimizes capital and energy inputs to achieve economic efficiency within policy constraints. A policy example contrasts the marginal social costs—from both a private and a social perspective—of a building conservation standard with marginal social costs for investments in electric-power generation alternatives. The standard reduces electric space heat consumption in new houses to approximately one-third of the level predicted under current building codes. The policy analysis assumes that the builder and homeowner bear the full cost and risks of the conservation measure. The standard is shown not to be cost effective relative to power generation alternatives available for the Bonneville Power Administration, a U.S. government power distributor in the Pacific Northwest. However, because the policy is a cost effective social investment, the paper recommends an alternative financing scheme.", "e:keyword": ["Economics: building standard policy impacts on the new house market", "Forecasting: residential energy use by appliance and fuel", "Programming: Everett's method variant with conserved energy shadow price"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.412", "e:abstract": "The problem of multilocation facility modernization is described. An integer multicommodity flow formulation is presented. Because of the problem size, exact solutions are not practical, and approximate solution methods are examined. Upper bounds are obtained by LP and Lagrangian relaxation. Near-optimal feasible solutions are obtained by heuristic procedures. The relaxation methods and the heuristics are compared in terms of the computation cost and the quality of the modernization policies produced, where quality is measured by the net present value of cash flows for a number of test problems. Also included is an evaluation of an aggregate modeling approach to multilocation planning, where average costs are employed in a single-location model to determine a homogeneous multilocation modernization policy. This assessment is important, as aggregate models have been employed widely in practice.", "e:keyword": ["Facilities/equipment planning", "Maintenance/replacement: multilocation facility modernization"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.426", "e:abstract": "We develop general methodologies for the minimal time routing problem of a vessel moving in stationary or time dependent environments, respectively. Local optimality considerations, combined with global boundary conditions, result in piecewise continuous optimal policies. In the stationary case, the velocity of the traveling vessel within each subregion depends only on the direction of motion. Variational calculus is used to derive the geometry of piecewise linear extremals. For the time dependent problem, the speed of the vessel within each subregion is assumed to be a known function of time and the direction of motion. Optimal control theory is used to reveal the nature of piecewise continuous optimal policies.", "e:keyword": ["Dynamic programming: optimal control", "Transportation: route selection", "Water transportation"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.439", "e:abstract": "The problem of adjusting the entries of a large matrix to satisfy prior consistency requirements occurs in economics, urban planning, statistics, demography, and stochastic modeling; these problems are called <i>Matrix Balancing Problems</i>. We describe five applications of matrix balancing and compare the algorithmic and computational performance of balancing procedures that represent the two primary approaches for matrix balancing—matrix scaling and nonlinear optimization. The algorithms we study are the <b>RAS</b> algorithm, a diagonal similarity scaling algorithm, and a truncated Newton algorithm for network optimization. We present results from computational experiments with large-scale problems based on producing consistent estimates of Social Accounting Matrices for developing countries.", "e:keyword": ["Economics", "Input-output analysis: estimating social accounting matrices", "Networks/graphs", "Applications: network models in economics", "Statistics and urban planning", "Programming", "Nonlinear algorithms/applications", "Scaling and nonlinear optimization for matrix balancing"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.456", "e:abstract": "Changeover costs (and times) are central to numerous manufacturing operations. These costs arise whenever work centers capable of processing only one product at a time switch from the manufacture of one product to another. Although many researchers have contributed to the solution of scheduling problems that include changeover costs, due to the problem's combinatorial explosiveness, optimization-based methods have met with limited success. In this paper, we develop and apply polyhedral methods from integer programming for a dynamic version of the problem. Computational tests with problems containing one to five products (and up to 225 integer variables) show that polyhedral methods based upon a set of facet inequalities developed in this paper can effectively reduce the gap between the value of an integer programming formulation of the problem and its linear programming relaxation (by a factor of 94 to 100%). These results suggest the use of a combined cutting plane/branch-and-bound procedure as a solution approach. In a test with a five product problem, this procedure, when compared with a standard linear programming-based branch-and-bound approach, reduced computation time by a factor of seven.", "e:keyword": ["Inventory/production: production planning with product changeover", "Production/scheduling: integer programming for product changeover", "Programming", "Integer algorithms: facets for production planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.474", "e:abstract": "In this paper, we consider the lot size model for the production and storage of a single commodity with limitations on production capacity and the possibility of not meeting demand, i.e., stockouts, at a penalty. The stockout option means that horizons can exist and permits the use of horizons to develop a forward algorithm for solving the problem. The forward algorithm is shown in the worst case to be asymptotically linear in computational requirements, in contrast to the case for the classical lot size model which has exponential computing requirements. Two versions of the model are considered: first, in which the upper bound on production is the same for every time period; and second, in which the upper bound on production is permitted to vary each time period. In the first case, the worst case computational difficulty increases in a cubic fashion initially, and then becomes linear. In the second case, the initial increase is exponential before becoming linear. Besides the forward algorithm, a number of necessary conditions are derived that reduce the computational burden of solving the integer programming problem posed by the model.", "e:keyword": ["Dynamic programming", "Deterministic forward algorithm for the capacitated lot size model", "Inventory/production", "Planning horizons: horizon techniques for the capacitated lot size model", "Networks/graphs", "Flow algorithms: flows in concave cost networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.487", "e:abstract": "We give an <i>O</i>(<i>n</i> log <i>n</i>) time algorithm to preemptively schedule <i>n</i> equal-length jobs with release times on two identical, parallel machines so as to minimize the mean flow time. The complexity of the general problem of minimizing the mean weighted flow time is also reviewed, both for nonpreemptive and preemptive scheduling.", "e:keyword": ["Production/scheduling: deterministic job shop scheduling", "Production/scheduling line balancing: mean weighted flow time"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.495", "e:abstract": "<i>N</i> jobs are to be processed sequentially on a single machine. While waiting for processing, jobs deteriorate, causing the random processing requirement of each job to grow at a job-specific rate. Under such conditions, the <i>actual</i> processing times of the jobs are no longer exchangeable random variables and the expected makespan is no longer invariant under any scheduling strategy that disallows idleness. In this paper, we analyze the effects of different deterioration schemes and derive optimal scheduling policies that minimize the <i>expected</i> makespan, and, for some models, policies that minimize the <i>variance</i> of the makespan. We also allow for random setup and detaching times. Applications to optimal inventory issuing policies are discussed and extensions are considered.", "e:keyword": ["Inventory/production", "Policies", "Maintenance/replacement: scheduling deteriorating jobs", "Probability", "Renewal processes", "Modeling probabilistic deterioration", "Production/scheduling: scheduling deteriorating jobs"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.499", "e:abstract": "We prove that the throughput of the M/G/x/x system is jointly concave in the arrival and service rates. We also show that the fraction of customers lost in the M/G/x/x system is convex in the arrival rate, if the traffic intensity is below some Ρ<sup>*</sup> and concave if the traffic intensity is greater than Ρ<sup>*</sup>. For 18 or less servers, Ρ<sup>*</sup> is less than one. For 19 or more servers, Ρ<sup>*</sup> is between 1 and 1.5. Also, the fraction lost is convex in the service rate, but not jointly convex in the two rates. These results are useful in the optimal design of queueing systems.", "e:keyword": ["Queues: loss system", "Design of queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.506", "e:abstract": "We study the stationary distribution of the number of customers in M/G/2 queueing systems. The two servers are allowed to have different service time distributions. We include the elapsed service times of the customers presently served as supplementary variables and obtain the forward equations satisfied by the joint stationary distribution of the number of customers and the elapsed service times. Using a sequence of transformations, we reduce the problem of determining the marginal probabilities of the number of customers present to the solution of a pair of coupled integral equations. When the servers are identical, only a single integral equation must be solved. The solution of the integral equation(s), and with it the stationary distribution of the number of customers, is constructed for several specific service time densities (e.g., Erlang, hyperexponential, and deterministic).", "e:keyword": ["Queues: length distribution in M/G/2"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.519", "e:abstract": "Consider a finite set of independent phase-type random variables. Suppose that we construct a random vector, each component of which is a total over a subset of this set. In 1984, D. Assaf et al. conjectured that such a vector need not have a multivariate phase-type distribution, and they provided a sufficient condition for the distribution to be of that type. We confirm their conjecture, and show that their sufficient condition is also essentially necessary. An open question of V. G. Kulkarni is whether or not his extension of the multivariate phase-type family is closed under familiar operations arising in reliability. This question is answered here in the negative.", "e:keyword": ["Probability", "Distributions: phase-type distributions", "Probability", "Markov processes: joint distribution of hitting times", "Reliability", "Coherent structures: joint distribution of failure times"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.527", "e:abstract": "In this paper, we consider the minimum cost transshipment problem in a directed network, having a single source node <i>s</i> with supply <i>S</i> and multiple demand nodes. The arc lengths are independent and exponentially distributed random variables. The cost of shipping is $1/unit flow/unit length, and <i>T</i> is the minimum cost of shipping the <i>S</i> units so as to satisfy all the demands. We construct a continuous time Markov chain with an upper triangular generator matrix such that <i>T</i> equals a particular first passage time in this chain. This fact is used to derive numerically stable algorithms for computing the exact distribution and moments of <i>T</i>.", "e:keyword": ["Networks", "Stochastic: minimum cost routing", "Probability", "Markov processes", "First passage times as routing costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.537", "e:abstract": "In this paper, we provide and study a new aspect in the classical problem of the asymptotic behavior of a nonhomogeneous Markov system (NHMS). The set of asymptotically attainable structures is determined as the convex hull of <i>k</i> vertices, which are given. Also, the rate of convergence to asymptotically attainable structures is studied and conditions are provided for the convergence to be geometrically fast. Finally, an illustration of the results with data from manpower systems is provided.", "e:keyword": ["Organizational studies", "Manpower planning: asymptotic control of population systems", "Probability", "Markov processes: nonhomogeneous Markov systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.546", "e:abstract": "We discuss various coverage-surface plots and scatter plots for evaluating and comparing confidence-interval procedures.", "e:keyword": ["Simulation", "Graphical methods: statistical analysis", "Statistics", "Estimation: confidence-interval procedures"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.553", "e:abstract": "The efficient point algorithm proposed by J. F. Bard for the computation of the solution of the Linear Two-Stage Optimization Problem does not always converge to the desired solution. A counterexample is provided and the reasons for this lack of convergence are discussed.", "e:keyword": ["Programming: nonlinear"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.556", "e:abstract": "We show, using small examples, that two algorithms previously published for the Bilevel Linear Programming problem (BLP) may fail to find the optimal solution and thus must be considered to be heuristics. A proof is given that solving BLP problems is NP-hard, which makes it unlikely that there is a good, exact algorithm.", "e:keyword": ["Analysis of algorithms", "Computational complexity: bilevel linear programming is NP-hard", "Games/group decisions", "Noncooperative: algorithms for solving Stackelberg games", "Programming", "Multiple criteria: bilevel linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.3.560", "e:abstract": "This paper analyzes a queueing system consisting of a single server which dispenses service to jobs of <i>K</i> ≥ 1 priority classes. Jobs are assumed to arrive to the queue according to a Poisson point process with a class dependent rate and to have class dependent service demands that are generally distributed. A linear priority function of the time spent in the system is specified for each job class and is used to schedule jobs. Specifically, the server, when available for service, nonpreemptively selects the job having the highest priority value to be the next job scheduled. Previous work in analyzing a system of this type has concentrated on providing bounds on the expected class response time. We extend these results by providing a closed form expression for the mean class response time and an expression for the ratio of expected response times in the limiting case of heavy traffic. We also provide a closed form expression for the mean class response time under certain light load conditions.", "e:keyword": ["Queues", "Limit theorems: heavy traffic response times", "Queues", "Priority: linear priority times"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.571", "e:abstract": "I have 20 years of experience in solving practical routing and scheduling problems, as well as the associated problems of crew scheduling. In this paper, I offer some reflections on the history and practice in this field and project some future directions for it.", "e:keyword": ["Routing", "Scheduling", "History of routing and scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.580", "e:abstract": "The uncertainty of the Northwest electric system was the subject of a recent study for the Bonneville Power Administration. System uncertainty was described along two dimensions over a twenty year planning period. One dimension was uncertainty in the demand for electricity; the second dimension was uncertainty in the price of electricity. The study focused on the impact of efficiency standards that would reduce the electricity used in new buildings and appliances. Many planners expect that the standards would reduce the long-term uncertainty in electricity demand. Some planners have come to realize that the standards could also reduce the long-term uncertainty in the price of electricity. This paper explains the case study approach to estimating the magnitude of these reductions in uncertainty. I describe the analytical approach and the key findings of the study, and conclude with a discussion of the study's impact on decision making in the region.", "e:keyword": ["Government", "Energy policy: the impact of efficiency standards", "Simulation", "Statistical analysis: estimating tolerance intervals", "Simulation", "System dynamics: a model of the Northwest electric system"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.598", "e:abstract": "In industries with capacity constraints and nonstorable outputs, priority service is a form of market organization in which customers subscribe in advance to the order in which they will be served from scarce supplies. The optimal duration of priority service contracts depends on a tradeoff between transaction costs and efficiency gains that, in turn, depends on the serial correlations of customers' service valuations. Using a stationary Markov process to characterize the distribution of customers' valuations, we present several simple methods that illustrate the principal determinants of the optimal contract period.", "e:keyword": ["Decision analysis", "Applications: optimal contract period", "Dynamic programming", "Applications: optimal contract period", "Industries", "Electric: priority service of electric power"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.607", "e:abstract": "Physician manpower requirements of most developing countries can often be met by developing local training facilities, sending indigenes to train abroad, and/or by recruiting expatriate physicians. Foreign training and having expatriates often involve expenditures of scarce foreign exchange in competition with other development projects. This paper develops a manpower planning model which coordinates physician manpower requirements of a developing country with its capacity to train such physicians along with national objectives to contain costs. The paper presents computational results from a dynamic programming model using the best data available from Nigeria.", "e:keyword": ["Dynamic programming: capacity planning for training of physicians", "Government", "Services: manpower planning in developing countries", "Planning", "Government: physician planning requirements in developing countries"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.619", "e:abstract": "Column aggregation of 0/1 programming problems is considered. By relaxing the integer requirements in the aggregated problem, an easy problem results, the solution of which can be used to compute bounds on the loss of accuracy in the objective function value. It is shown how these bounds can be improved by identification of valid inequalities and by exploiting primal degeneracy. A small example is given to illustrate the procedure.", "e:keyword": ["Programming", "Integer: integer algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.624", "e:abstract": "We consider the specialization of the primal simplex algorithm to the problem of finding a tree of directed shortest paths from a given node to all other nodes in a network of <i>n</i> nodes or finding a directed cycle of negative length. Two efficient variants of this shortest path simplex algorithm are analyzed and shown to require at most (<i>n</i> − 1)(<i>n</i> − 2)/2 pivots and <i>O</i>(<i>n</i><sup>3</sup>) time.", "e:keyword": ["Networks/graphs", "Distance algorithms: shortest path algorithms", "Programming", "Integer applications: simplex algorithms for shortest path problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.629", "e:abstract": "<i>Fixed Cycle Scheduling</i> (FCS) is a class of job scheduling problems in which all scheduling decisions (assignment of tasks to processors) are made on a cycle. That is, if task <i>t</i> is assigned to processor <i>j</i> at period <i>h</i> < <i>n</i>, then the same assignment is made at period <i>h</i> + <i>in</i>, for all integers <i>i</i>, where <i>n</i> is the cycle length. Possible applications of FCSs are given and a special case with uniform job arrivals and empty precedence structure is shown to be solvable analytically.", "e:keyword": ["Production/scheduling: fixed cycle with empty precedence structure"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.634", "e:abstract": "This work provides a comprehensive analysis of a general periodic review production/inventory model with random (variable) yield. Existence of an order point whose value does not depend on yield being random is proved in the single period case without specifying the yield model and using a very general cost structure. When yield is a random multiple of lot size, the nonorder-up-to optimal policy is characterized for a finite-horizon model. The finite-horizon value functions are shown to converge to the solution of an infinite-horizon functional equation, and the infinite-horizon order point is shown to be no smaller than when yield is certain.", "e:keyword": ["Dynamic programming: periodic review with random yield", "Inventory/production: periodic review with random yield"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.644", "e:abstract": "This paper considers the dynamic lot sizing problem of H. M. Wagner and T. M. Whitin with the assumption that the total cost of <i>n</i> setups is a concave nondecreasing function of <i>n</i>. Such setup costs could arise from the worker learning in setups and/or technological improvements in setup methods. An efficient dynamic programming algorithm is developed to solve a finite horizon problem and results are presented to find decision/forecast horizons. Several new results presented in the paper have potential use in solving other related problems.", "e:keyword": ["Inventory/production", "Planning horizons: dynamic lot size model", "Production/scheduling", "Learning: setups"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.656", "e:abstract": "This paper analyzes a multiperiod production planning model which incorporates pricing decisions and learning effects. We provide a partial characterization of the optimal policy. This result, combined with a simple algorithm, is used to find the optimal solution in special cases and near-optimal solutions for the general case.", "e:keyword": ["Inventory/production", "Policies: marketing and pricing", "Marketing: pricing", "Production/scheduling: learning"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.667", "e:abstract": "Queueing analysis is conducted for M/G/1-type systems with multiple classes of service requests that need a setup time prior to each busy period. We consider a variety of service disciplines: FCFS (first-come, first-served), LCFS (last-come, first-served), nonpreemptive priority, preemptive resume priority, and gated batch priority. This study has been motivated by the performance evaluation of multiprocessor bus arbitration protocols. The Laplace–Stieltjes transform of the distribution function, the mean, and the second moment of the waiting time for each class are derived explicitly. As a limit of continuous priorities, the shortest-job-first discipline with setup times is also considered.", "e:keyword": ["Queues", "Priority: priority queues with setup times"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.678", "e:abstract": "In this paper we consider the M<i><sup>x</sup></i>/G/1 queueing system with batch arrivals. We give simple approximations for the waiting-time probabilities of individual customers. These approximations are checked numerically and they are found to perform very well for a wide variety of batch-size and service-time distributions.", "e:keyword": ["Queues", "Approximations: approximations based on asymptotic analysis", "Queues", "Batch/bulk: approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.686", "e:abstract": "In this paper, we study multi-item inventory systems under a single resource capacity constraint. In contrast to most previous approaches, we treat the capacity as a decision variable, and not as given data. This is consistent with many practical cases where additional capacities/resources can be acquired/rented at some cost/profit. Two solution procedures are developed for deriving an optimal policy within the class of policies that has a fixed cycle for all items with phasing of orders within the cycle. These solution procedures can be applied to various types of cost functions, as illustrated by an example.", "e:keyword": ["Inventory/production", "Multi-item: single resource capacity problem in inventory systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.694", "e:abstract": "In this paper, properties are derived that are useful for characterizing optimal allocations of servers and workloads in single-class, multiserver closed queueing networks (CQNs). The problem is as follows: Suppose a particular workload is allocated to a set of servers within a subnetwork of a CQN. This set of servers is to be partitioned into several multiserver stations. The number of stations, the number of servers, and the workload allocation to each station define a configuration of this subnetwork. Thus, the problem is to determine the best configuration of each subnetwork to maximize the throughput in the original CQN. Decomposition is used to address this problem. Results are obtained for subnetworks in isolation. These results are used to solve the optimal-configuration problem. Applications of the results to design and planning problems of flexible manufacturing are also described.", "e:keyword": ["Queues: closed queueing networks", "Inventory/production: flexible manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.704", "e:abstract": "Network design problems concern flows over networks in which a fixed charge must be incurred before an arc becomes available for use. The uncapacitated, multicommodity network design problem is modeled with aggregate and disaggregate forcing constraints. (Forcing constraints ensure logical relationships between the fixed charge-related and the flow-related decision variables.) A new lower bound for this problem—referred to as the capacity improvement (CI) bound—is presented; and an efficient implementation scheme using shortest path and linearized knapsack programs is described. A key feature of the CI lower bound is that it is based on the LP relaxation of the aggregate version of the problem. A numerical example illustrates that the CI lower bound can converge to the optimal objective function value of the IP formulation.", "e:keyword": ["Networks/graphs: network design problems", "Programming", "Integer", "Theory: lower bound for fixed charged problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.711", "e:abstract": "The joint replenishment problem involves the lot sizing of several items with nonstationary demand in discrete time. The items have individual ordering costs and linear inventory holding costs. In addition, a joint ordering cost is incurred whenever one or more items is ordered together. This problem often arises when economies can be affected by coordinated ordering or setup of the items, both in distribution and in manufacturing environments. This problem is known to be NP-complete. In this paper, we analyze the worst case performance of an existing multipass heuristic for the problem. Then a new single pass forward heuristic is proposed, and it is proved that it has a uniformly bounded worst case performance. Furthermore, a lower bound on the cost of the optimal solution is obtained once the heuristic has been used. We then discuss a number of related heuristic algorithms and their worst case performance. The behavior of our heuristics for a randomly generated set of problems is also studied.", "e:keyword": ["Inventory/production: joint replenishment problem", "Production/scheduling: heuristics and performance bounds"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.724", "e:abstract": "In this article the M/G/1 queue with server vacations is considered with the assumption that the decision whether or not to take a new vacation, when the system is empty, depends on the number of vacations already taken through a random outcome. Both descriptive and optimization issues are considered, where the latter is done under the expected long-run average cost criterion with linear holding costs, fixed setup costs and a concave piecewise linear reward function for being on vacation. The optimization problem results in an infinite dimensional fractional program of which the solution yields a (deterministic) policy of the control limit type.", "e:keyword": ["Queues: vacation models", "Queues", "Multichannel: optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.4.728", "e:abstract": "This paper studies how sampling variation in component reliability estimates affects the computation of system reliability that uses these estimates as input. If these components are in parallel they lead to an understatement of system reliability as the number of components <i>k</i> increases. In series, they lead to an overstatement as <i>k</i> increases. For a sample component reliability based on <i>n</i> observations, our results show that lim<i><sub>k → ∞</sub></i> (<i>k</i><sup>2</sup>/<i>n</i>) = 0 is required for the sample system reliability to converge in probability to the true system reliability.", "e:keyword": ["Networks/graphs", "Stochastic: networks", "Reliability: simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.747", "e:abstract": "Several recent comments published by leading operations researchers leave the impression that OR does not address strategic business or government issues and, furthermore, that the field would have to develop new analysis methods to address these issues. In fact, however, many OR practitioners are addressing strategic issues these days, and the failure of OR commentators to recognize this explicitly carries dangers for the future development of the field.", "e:keyword": ["Professional: OR/MS philosophy", "Role of operations research in strategy analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.752", "e:abstract": "The placement of text on a map to maximize map legibility, while avoiding graphic overplotting, is a problem in combinatorial optimization. This particular optimization problem can be formulated as a multiple choice integer programming problem, and the integer programming problem has a structure that can be exploited, using a Lagrangian heuristic, to obtain a cost effective solution, even when tens of thousands of variables and thousands of constraints are involved. This paper presents the mathematical formulation of the map label placement problem and describes a computer program implemented to solve it.", "e:keyword": ["Programming", "Integer", "Heuristic: automated cartography"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.760", "e:abstract": "Controlled rounding is a procedure that perturbs tabular data collected from respondents in such a way as to preserve the anonymity of the respondents while maintaining the integrity of the data. The three-dimensional controlled rounding problem is NP-complete. This paper describes a binary tree search algorithm for solving three-dimensional problems based on linear programming techniques. Computational results obtained from tests with over 31,000 randomly generated tables and 292 real-life tables have shown that this algorithm can effectively find controlled roundings when they exist or determine that no solution exists. The computational results also demonstrate that the running time of the search algorithm can be cut in half by using a heuristic for initializing the simplex basis for the linear programming problems.", "e:keyword": ["Programming", "Integer", "Algorithms", "Heuristics: LP guided search", "Programming", "Integer", "Applications: controlled rounding", "Statistics", "Censoring: data perturbation methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.773", "e:abstract": "This paper presents a model and a two-phase algorithm for incorporating nondispatchable technologies or energy sources as decision variables in long-range electric utility capacity expansion plans. The first phase uses a deterministic model to quickly obtain a near-optimal expansion plan. This model employs a step approximation to the time-dependent availabilities of the nondispatchable technologies, and a derating technique to represent forced outages of conventional units. Specialized algorithms that exploit the structure of this deterministic problem are developed. The second phase refines the resulting solution toward the probabilistic optimum. It employs accurate probabilistic production costing techniques to account for forced outages of conventional generators, and uses hour-by-hour simulation to represent the contributions of nondispatchable technologies. This second phase requires few additional iterations in our computational experience. The proposed approach suggests a modification for the existing state-of-the-art methodology electric generation expansion analysis system (EGEAS), particularly for situations in which the cumulant approximation used in EGEAS may be inadequate, and it suggests an improved technique for accelerating Bloom's Generalized Benders' Decomposition algorithm for the conventional equipment capacity expansion planning problem.", "e:keyword": ["Facility planning: capacity expansion", "Industries: electric utilities", "Natural resources: nondispatchable/renewable sources of energy"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.792", "e:abstract": "This paper addresses the problem of a single military transport aircraft that must carry cargo from an origin base to a destination base. Refueling is required and performed by a tanker aircraft that originates from and returns to a third base. The objective is to determine the initial fuel required by each aircraft and the location of the refueling point so as to minimize the total fuel consumed subject to restrictions on the range of the transport and the tanker. Based on U.S. Air Force data, analytical relationships are derived which allow the problem to be formulated similar to a constrained spherical Weber problem with two main differences: 1) the objective function is nonlinear, and 2) some of the constraints are a function of the decision variables. Spherical convexity of both the objective function and the feasible region is shown and used to develop a solution procedure. Computational experience is given.", "e:keyword": ["Military", "Logistics", "Programming: nonlinear algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.801", "e:abstract": "This paper addresses the bias characteristics of estimators produced from Monte Carlo simulations. If the computer time allocated to the simulation is <i>t</i>, then <i>N</i>(<i>t</i>), the number of replications completed by time <i>t</i>, is a renewal process. The simulation implications of a known exact expression for the expected value of a sample mean based on <i>N</i>(<i>t</i>) replications are explored and a similar exact expression for a sample mean based on <i>N</i>(<i>t</i>) + 1 replications is derived. Bias expansions for a sample mean based on <i>N</i>(<i>t</i>) or <i>N</i>(<i>t</i>) + 1 replications are obtained. The bias in the sample mean based on <i>N</i>(<i>t</i>) replications is at most of order o(1/<i>t</i>). Under suitable moment conditions, the bias decreases at a much faster rate than o(1/<i>t</i>); on the other hand, the estimator based on <i>N</i>(<i>t</i>) + 1 replications has bias of order 1/<i>t</i>. The exact expressions also lead to simple and totally unbiased estimators. Using Taylor series, the bias expansions of a general function of means based on <i>N</i>(<i>t</i>) or <i>N</i>(<i>t</i>) + 1 replications are determined. The leading terms in these expansions are of order 1/<i>t</i>, although the coefficients are different. Based on these expansions, a Tin-style adjusted estimator is proposed to reduce the bias. These expansions are specialized to the case of ratio estimation in regenerative simulation. Due to a cancellation effect, the ratio estimator based on <i>N</i>(<i>t</i>) + 1 cycles is biased only to order o(1/<i>t</i>) providing confirmation and reinterpretation of a result of M. Meketon and P. Heidelberger.", "e:keyword": ["Probability: regenerative and renewal processes", "Simulation: statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.815", "e:abstract": "There are many ways to assess the benefits and costs of risk-reduction policies. This paper shows that discounted longevity as a measure of benefits and the negative of net output as a measure of costs can be deduced from the Shepard and Zeckhauser utility-maximization model.", "e:keyword": ["Cost analysis: cost/benefit analysis", "Decision analysis: risk", "Utility/preference: multiattribute utility theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.820", "e:abstract": "We consider an investor who wants to allocate funds among several projects. Each project is expected to yield a certain reward, and the objective is that a total reward will achieve a certain given amount, called the target. This problem is relatively easy to solve when rewards are deterministic, but may be hard in a more realistic setting when the rewards are stochastic and the investor wants to maximize the probability of attaining the target. We show that, by combining dynamic programming with a search procedure, the stochastic version of the problem can be solved relatively fast when rewards are normally distributed. The procedure is also useful for other risk criteria, which involve both the mean and the variance of the total reward.", "e:keyword": ["Dynamic programming: risk criteria", "Networks", "Stochastic: shortest path"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.826", "e:abstract": "We consider single server M/G/1 and GI/G/1 queues in the limit of heavy traffic. We develop a procedure for obtaining the full asymptotic series of the stationary distribution of unfinished work in powers of one minus the traffic intensity. The leading term in this series is the (exponential) density obtained from the heavy traffic limit theorem. We show that the correction terms have different forms in different regions of the state space. These corrections are constructed using the method of matched asymptotic expansions. We assume that the method of matched asymptotic expansions is valid.", "e:keyword": ["Queues", "Diffusion models: refining diffusion approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.838", "e:abstract": "This paper outlines a new approach to the problem of demand uncertainty. It deals with setting optimal supply levels when demand is unknown. The novel feature of this approach is that information is obtained by observing sales. This information is used to determine future supply levels. Thus, supply levels are chosen with a view to current profit and future information. The technical apparatus is based on an extension of the theory of high-low search. We derive an algorithm to determine a sequence of supply quantities which minimizes total costs of over- and undersupply in the most adverse demand conditions. We calculate the value of perfect information, indicating how much a rational risk averse decision maker would be willing to pay to know demand exactly.", "e:keyword": ["Decision analysis", "Sequential: supply quantities", "Inventory/production", "Uncertainty: minmax analysis", "Production/scheduling", "Learning: demand uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.847", "e:abstract": "New sufficient conditions are presented for a dynamic process to have a myopic optimum and for a sequential game to possess a myopic equilibrium point. An optimum (or an equilibrium point) is said to be <i>myopic</i> if it can be specified by solving a static optimization problem (or a static Nash game). The results are applied to an aquaculture harvesting model and a dynamic oligopoly model with advertising decisions.", "e:keyword": ["Dynamic programming: myopic solutions of affine dynamic models", "Games/group decisions: noncooperative sequential games", "Marketing: dynamic oligopoly with advertising decisions"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.854", "e:abstract": "Consider a single server loss system in which the server, being idle, may reject or accept an arriving customer for service depending on the state at the arrival epoch. It is assumed that at every arrival epoch the server knows the service time of the arriving customer, the arrival time of the next customer and the service time. The server gets a fixed reward for every customer admitted to the system. The form of an optimal stationary policy is investigated for the discounted and average reward cases.", "e:keyword": ["Dynamic programming", "SemiMarkov: admission control to a loss system", "Queues", "Optimization: input control to a single server loss system"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.863", "e:abstract": "Recently P. G. Harrison derived a closed form solution for the normalizing constant of a closed network of single server queues. In the present paper, we describe an alternative method of obtaining Harrison's result. The new method can usefully be applied to other closed networks of queues, particularly those in which irregular constraints bound the state space. Examples of the method are provided, and several new results are obtained.", "e:keyword": ["Queues/networks: calculation of normalizing constants"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.870", "e:abstract": "Consider a system that is modeled as an M/M/1 queueing system with multiple user classes. Each class is characterized by its delay cost per unit of time, its expected service time and its demand function. This paper derives a pricing mechanism which is optimal and incentive-compatible in the sense that the arrival rates and execution priorities jointly maximize the expected net value of the system while being determined, on a decentralized basis, by individual users. A closed-form expression for the resulting price structure is presented and studied.", "e:keyword": ["Information systems", "Management: transfer pricing for computer services", "Organizational studies", "Information: systems design under asymmetric information", "Queues", "Priority: priority pricing of service operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.884", "e:abstract": "We are concerned with a discrete-time undiscounted dynamic lot size model in which demand and cost parameters are constant for an initial few periods. As our main result, we obtain an upper bound on the number of these periods which guarantees the optimality of the Economic Order Quantity (EOQ) as the size of the production lot to be produced in the first period. The upper bound is given and the optimality holds for every problem with an horizon not less than the upper bound and for the infinite horizon problem. The data beyond the upper bound are allowed to be specified arbitrarily. In the context of forecast horizon theory, we obtain conditions for a finite forecast horizon to exist in the undiscounted dynamic lot size model. Furthermore, existence results for forecast horizons in an undiscounted optimization problem are obtained for the first time in this paper.", "e:keyword": ["Dynamic programming", "Deterministic: dynamic lot size model", "Inventory/production", "Dynamic extension with constant initial demand", "Inventory/production: planning horizons", "Existence of forecast horizon", "Rolling horizon procedures"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.893", "e:abstract": "We examine a periodically reviewed cellular manufacturing system where parts are dispatched from a central control station to <i>n</i> manufacturing cells. After being processed at a cell, parts are routed back to the control station for inspection. An inspection failure will result in a feedback job order. Optimal dispatching policies are pursued to minimize the expected in-process inventory costs over a finite horizon. A dynamic programming formulation is developed for optimal dispatching. We show that the dynamic recursive functions (i.e., cost-to-go functions) are convex and monotonic under the condition of low defect rates and relative low cost material handling. From the convex and monotonic properties, it is shown that optimal dispatching calls for a combination of zero-inventory and nonzero-inventory policies. The optimal input control is proved to be in the form of a pulling system.", "e:keyword": ["Dynamic programming", "Optimal control: Markovian decision models", "Production/scheduling: dispatching in flexible manufacturing systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.902", "e:abstract": "We consider problems of allocating resources to activities where the allocation to each activity is restricted to a general set of admissible values, the objective function is additively-separable but not necessarily concave nor differentiable, and each activity uses at most one resource. We develop a simple algorithm, based on a nonsmooth convex relaxation, that generates a near-optimal solution whenever each allocation is a small fraction of resource capacity.", "e:keyword": ["Programming: nonlinear algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.911", "e:abstract": "A two-person, noncooperative game in which the players move in sequence can be modeled as a bilevel optimization problem. In this paper, we examine the case where each player tries to maximize the individual objective function over a jointly constrained polyhedron. The decision variables are variously partitioned into continuous and discrete sets. The leader goes first, and through his choice may influence but not control the responses available to the follower. For two reasons the resultant problem is extremely difficult to solve, even by complete enumeration. First, it is not possible to obtain tight upper bounds from the natural relaxation; and second, two of the three standard fathoming rules common to branch and bound cannot be applied fully. In light of these limitations, we develop a basic implicit enumeration scheme that finds good feasible solutions within relatively few iterations. A series of heuristics are then proposed in an effort to strike a balance between accuracy and speed. The computational results suggest that some compromise is needed when the problem contains more than a modest number of integer variables.", "e:keyword": ["Games: noncooperative", "Programming", "Integer: branch-and-bound algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.922", "e:abstract": "In LP problems with an extremely large number of possible structural variables, solution to optimality is not always required. A bound can be used to terminate such LPs. Such a bound is presented for a class of problem that includes most cutting stock formulations.", "e:keyword": ["Production/scheduling", "Cutting stock: cutting stock problems", "Programming", "Linear theory: bounding linear programming solutions"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.5.924", "e:abstract": "Stochastic permutation flow shops of <i>m</i> identical machines and <i>n</i> jobs are considered. There exist buffers of arbitrary size between two consecutive machines. For particular types of job processing times, we present optimal schedules which minimize the makespan in the stochastic sense. Moreover, it will be shown that the optimal schedules are closely related to those of the no buffer case. Finally, our results are recaptured in the context of tandem queues.", "e:keyword": ["Production/scheduling: stochastic minimization of the makespan", "Programming", "Stochastic: minimization of the makespan in flow shops"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1006", "e:abstract": "We show that a variant of Karmarkar's projective algorithm for linear programming can be viewed as following the approach of Dantzig-Wolfe decomposition. At each iteration, the current primal feasible solution generates <i>prices</i> which are used to form a simple subproblem. The solution to the subproblem is then incorporated into the current feasible solution. With a suitable choice of stepsize a constant reduction in potential function is achieved at each iteration. We also use our analysis to motivate a new primal simplex pivot rule that is closely related to rules used by E. Klotz and L. Schrage.", "e:keyword": ["Programming", "Linear", "Algorithms: Dantzig-Wolfe decomposition", "Interior-point method", "Simplex pivot rule"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1019", "e:abstract": "Consider a complete graph <i>G</i> = (<i>V</i>, <i>E</i>) in which each node is present with probability <i>p</i>. We are interested in solving combinatorial optimization problems on subsets of nodes which are present with a certain probability. We introduce the idea of a priori optimization as a strategy competitive to the strategy of reoptimization, under which the combinatorial optimization problem is solved optimally for every instance. We consider four problems: the traveling salesman problem (TSP), the minimum spanning tree, vehicle routing, and traveling salesman facility location. We discuss the applicability of a priori optimization strategies in several areas and show that if the nodes are randomly distributed in the plane the a priori and reoptimization strategies are very close in terms of performance. We characterize the complexity of a priori optimization and address the question of approximating the optimal a priori solutions with polynomial time heuristics with provable worst-case guarantees. Finally, we use the TSP as an example to find practical solutions based on ideas of local optimality.", "e:keyword": ["Networks/graphs: stochastic heuristics", "Traveling salesman", "Probability: stochastic model applications", "Transportation: vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1034", "e:abstract": "In this paper, we introduce a general class of single-server network location models which includes the median, center, stochastic queue median, stochastic queue center, L<i><sub>p</sub></i> norm, and other location problems in a single parametric framework. The model incorporates both queueing effects and a cost function (similar to a disutility function), which is nonlinear in response time. We develop a number of properties of this class of models, and present a method for evaluating a range of different objective functions for any given problem. The robustness of the median and center objectives as solution concepts is highlighted.", "e:keyword": ["Facilities/equipment planning: facility location", "Networks/graphs: theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1045", "e:abstract": "We provide an algorithm for computing Cournot-Nash equilibria in a market that involves finitely many producers. The algorithm amounts to following a certain dynamical system all the way to its steady state, which happens to be a noncooperative equilibrium. The dynamics arise quite naturally as follows. Let each producer continuously adjust the planned production, if desired, as a response to the current aggregate supply. In doing so, the producer is completely guided by myopic profit considerations. We show, under broad hypothesis, that this adjustment process is globally, asymptotically convergent to a Nash equilibrium.", "e:keyword": ["Games/group theory", "Noncooperative: equilibria in Cournot oligopolies", "Programming", "Nonlinear: algorithm for computing equilibria with nonsmooth data"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1052", "e:abstract": "We consider a multiclass closed queueing network with two single-server stations. Each class requires service at a particular station, and customers change class after service according to specified probabilities. There is a general service time distribution for each class. The problem is to schedule the two servers to maximize the long-run average throughput of the network. By assuming a large customer population and nearly balanced loading of the two stations, the scheduling problem can be approximated by a dynamic control problem involving Brownian motion. A reformulation of this control problem is solved exactly and the solution is interpreted in terms of the queueing network to obtain a scheduling rule. (We conjecture, quite naturally, that the resulting scheduling rule is asymptotically optimal under <i>heavy traffic</i> conditions, but no attempt is made to prove that.) The scheduling rule is a static priority policy that computes an index for each class and awards higher priority at station 1 (respectively, station 2) to classes with the smaller (respectively, larger) values of this index. An analytical comparison of this rule to any other static policy is also obtained. An example is given that illustrates the procedure and demonstrates its effectiveness.", "e:keyword": ["Production/scheduling: priority sequencing in a stochastic job shop", "Queues: Brownian models of network scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1065", "e:abstract": "Motivated by a factory scheduling problem, we consider the problem of input control, subject to a specified product mix, and priority sequencing in a two-station multiclass queueing network with general service time distributions and a general routing structure. The objective is to minimize the long-run expected average number of customers in the system subject to a constraint on the long-run expected average output rate. Under balanced heavy loading conditions, this scheduling problem is approximated by a control problem involving Brownian motion. A reformulation of this Brownian control problem was solved exactly in 1990 by L. M. Wein. In the present paper, this solution is interpreted in terms of the queueing network model in order to obtain an effective scheduling rule. The resulting sequencing policy dynamically prioritizes customers according to reduced costs calculated from a linear program. The input rule is a <i>workload regulating</i> input policy, where a customer is injected into the system whenever the expected total amount of work in the system for the two stations falls within a prescribed region. An example is presented that illustrates the procedure and demonstrates its effectiveness.", "e:keyword": ["Production/scheduling: sequencing in a stochastic job shop", "Queues: models of network scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1079", "e:abstract": "In this paper, we consider a version of the Economic Lot Scheduling Problem in which there are no costs specified. The usual form of this problem seeks a pattern of production that minimizes the sum of holding the setup costs. We investigate the problem of finding a feasible schedule which avoids a stockout given the initial stocks of the products being manufactured. It is shown that this problem is NP-hard, and an effective heuristic method for its solution is proposed. This heuristic method is compared with a more naive approach to the problem and some computational results are given.", "e:keyword": ["Analysis of algorithms", "Computational complexity: NP-hardness of a lot scheduling problem", "Production/scheduling", "Heuristic: economic lot scheduling problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1089", "e:abstract": "Consider a single-item, periodic review, stationary inventory model with stochastic demands, proportional ordering costs, and convex holding and shortage costs, where shortages are backordered and Veinott's well known terminal condition holds. Orders can be scheduled for any period, but the actual inventory level is determined every <i>T</i> periods through an audit. This leads to a dynamic programming model where stage <i>n</i> contains periods (<i>n</i> − 1)<i>T</i> + 1 through <i>nT</i>. For both discounted and averaging criteria, a simple rule optimally describes the orders for the <i>T</i> periods of a stage as a function of the state (beginning inventory level) and the cumulative <i>T</i>-period order. The latter is optimally determined by a base stock policy with two base stock levels: one for the final stage, another for the rest. (The horizon may be finite or infinite.) Methods are presented for computing optimal policies, together with bounds on the costs of (suboptimal) myopic policies. Models with proportional costs and continuous demands are studied in detail. Computational experiments indicate that myopic policies perform quite well for such models. The selection of a best review period <i>T</i> is covered briefly. Applications of our model include just in time settings where audit decisions play a negligible role.", "e:keyword": ["Dynamic programming", "Applications: dynamic programming model", "Inventory/production", "Policies", "Review: periodic auditing and inspection", "Inventory/production", "Uncertainty", "Stochastic: stochastic inventory model"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1104", "e:abstract": "Substantial simplification of existing processes and designs may be required before the potential benefits of modern manufacturing technology can be realized. This paper analyzes implementation problems associated with a flexible system that produces flat sheet-metal parts with interior holes. The paper makes three main contributions. First, we formulate the problem of selecting tooling and design standards as an optimization model and demonstrate that the model yields insight by applying it to one manufacturer's problem, thereby reducing substantially the required tooling. Second, we show that the model has a totally balanced constraint matrix, and hence, there are polynomial time algorithms for various versions of the problem. Third, we provide new algorithms with substantially improved performance bounds for two important versions of the problem.", "e:keyword": ["Facilities/design: tooling", "Inventory/production: applications", "Manufacturing: automated systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1116", "e:abstract": "A production process exhibits a decreasing pattern in the process mean in the course of production. We address the decision problem of selecting the starting level of the process mean and the level at which the process mean should be adjusted back to the starting level. The costs to be minimized are the long-run average cost of production, adjustment and loss for defective items. In addition to the optimal solution, simple approximate solutions are developed and shown to be more cost effective than a simple linear policy often applied to this problem.", "e:keyword": ["Queues", "Multichannel: optimization of process adjustment model", "Reliability", "Failure models: inspection of production process", "Reliability", "Quality control: process control"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1123", "e:abstract": "An arbitrary configuration of an open queueing network with exponential service times and finite buffers is analyzed. We offer an iterative procedure for approximating the marginal occupancy probabilities for each queue of the system. The method decomposes the queueing network into individual queues and analyzes each in isolation using information from only its nearest neighbors. Based upon the SIMP approximation previously used for tandem queues, it replaces each server's service time with a clearance time, which includes blocking, and each server's arrival rate by an equivalent <i>acceptance</i> rate. The procedure is easy to implement and requires modest memory and computer time. Extensive numerical experiments, performed for various topologies, yield accurate results compared with those obtained by exact or simulation methods.", "e:keyword": ["Queues", "Applications: analysis of networks with blocking", "Queues", "Limit theorems: approximation method for networks with blocking"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.1135", "e:abstract": "We show that permutation schedules are always optimal for regular, symmetric measures of performance for the <i>m</i>-machine flow shop with precedence constraints provided that processing times are identical for noncomparable jobs.", "e:keyword": ["Production/scheduling: flow shops with precedence constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.937", "e:abstract": "Ford Whitman Harris first presented the familiar economic order quantity (EOQ) model in a paper published in 1913. Even though Harris's original paper was disseminated widely, it apparently was unnoticed for many years before its rediscovery in 1988. During this period much confusion developed over the origin of the EOQ model. This paper explores the early literature on this model and traces the evolution of the confusion. It also sketches the remarkable life of Harris, who made contributions as an engineer, inventor, author and patent attorney, even though he received no formal education beyond high school. Harris's original 1913 essay is reprinted following this paper.", "e:keyword": ["Inventory/production: EOQ model", "Professional: history of OR/MS", "Obituaries"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.947", "e:abstract": "Reprinted from <i>Factory, The Magazine of Management</i>, Volume 10, Number 2, February 1913, pp. 135–136, 152.Interest on capital tied up in wages, material and overhead sets a maximum limit to the quantity of parts which can be profitably manufactured at one time; “set-up” costs on the job fix the minimum. Experience has shown one manager a way to determine the economical size of lots.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.951", "e:abstract": "We describe the design and use of a program to schedule interviews for law firms and students at a job fair. It has been used to manage the Southeastern Public Interest Job Fair for the last six years. The program uses a new scheduling algorithm that produces particularly convenient schedules.", "e:keyword": ["Information systems: analysis and design", "Networks/graphs", "Matchings: edge-coloring", "Production/scheduling", "Applications: timetabling"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.961", "e:abstract": "In this paper, we develop and analyze a model to generate an equitable set of routes for hazardous material shipments. The objective is to determine a set of routes that will minimize the total risk of travel and spread the risk equitably among the zones of the geographical region in which the transportation network is embedded, when several trips are necessary from origin to destination. An integer programming formulation for the problem is proposed. We develop and test a heuristic that repeatedly solves single-trip problems: a Lagrangian dual approach with a gap-closing procedure is used to optimally solve single-trip problems. We report a sampling of our computational experience, based on a real-life routing scenario in the Albany district of New York State. Our findings indicate that one can achieve a high degree of equity by modestly increasing the total risk and by embarking on different routes to evenly spread the risk among the zones. Furthermore, it appears that our heuristic procedure is excellent in terms of computational requirements as well as solution quality. We also suggest some directions for future research.", "e:keyword": ["Environment: transportation of hazardous materials", "Networks/graphs: application", "Transportation: equity in route selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.974", "e:abstract": "Other than common random numbers, <i>control variates</i> is the most promising variance reduction technique in terms of its potential for widespread use: Control variates is applicable in single or multiple response simulation, it does not require altering the simulation run in any way, and any stochastic simulation contains potential control variates. A rich theory of control variates has been developed in recent years. Most of this theory assumes a specific probabilistic structure for the simulation output process, usually joint normality of the response and the control variates. When these assumptions are not satisfied, desirable properties of the estimator, such as unbiasedness, may be lost. A number of remedies for violations of the assumptions have been proposed, including jackknifing and splitting. However, there has been no systematic analytical and empirical evaluation of these remedies. This paper presents such an evaluation, including evaluation of the small-sample statistical properties of the proposed remedies.", "e:keyword": ["Simulation", "Efficiency: variance reduction", "Simulation", "Statistical analysis: point and interval estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.38.6.993", "e:abstract": "Implementation of certain algorithms on parallel computing architectures may involve partitioning contiguous elements into a fixed number of groups, each to be handled by a single processor. We wish to find an assignment of elements to processors that minimizes the sum of the maximum workloads experienced at each stage. This problem may be viewed as a multiobjective network optimization problem. Polynomially-bounded algorithms are developed for the case of two-stages, whereas the general problem, for an arbitrary number of stages, is shown to be NP-hard. Heuristic procedures are therefore proposed and analyzed for the general problem. Computational experience with one of the exact algorithms, incorporating certain pruning rules, is presented for a variety of test problems. Empirical results also demonstrate that one of the heuristic procedures is especially effective in practice.", "e:keyword": ["Computers: parallel algorithms and workload balancing", "Networks", "Applications: multiobjective optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.100", "e:abstract": "A research theme involving location on networks, since its inception, has been the identification of a <i>finite dominating set</i> (FDS), or a finite set of points to which an optimal solution must belong. We attempt to unify and generalize results of this sort. We survey the literature and then prove some theorems that subsume most previous results and that are, at the same time, more general than previous results. The paper is aimed primarily at investigators who wish to know whether an FDS exists for a specific problem.", "e:keyword": ["Networks/graphs: location on networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.119", "e:abstract": "Many transportation networks, e.g., networks of cooperating power systems, and hydrological networks involve a real-valued demand function, defined on the set of nodes, and it is said to be feasible if there exists a flow such that at each node the sum of the incoming flow values is greater than or equal to the demand assigned to this node. By the theorem of D. Gale and A. Hoffman, a system of linear inequalities involving the demand and the arc capacity functions, gives necessary and sufficient condition for the feasibility of the demand. If the demands and/or the arc capacities are random, then an important problem is to find the probability that all these inequalities are satisfied. This paper proposes a new method to eliminate all redundant inequalities for given lower and upper bounds of the demand function, and finds sharp lower and upper bounds for the probability that a feasible flow exists. The results can be used to support transportation network analysis and design.", "e:keyword": ["Networks/graphs: stochastic transportation networks", "Probability: applications of sharp probability inequalities", "Reliability: availability analysis in power systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.13", "e:abstract": "FORPLAN (FORest PLANning) is a large-scale linear programming system used to support national forest land management planning. It is available in two versions, and is used extensively to help interdisciplinary planning teams develop forest-wide plans as dictated by the National Forest Management Act of 1976. Nine years of experience clearly show that while the system is working in a technical sense, troublesome issues remain. This paper begins with an overview of how USDA Forest Service planning has evolved. We then give mathematical formulations for portions of FORPLAN models and examples of how the system is used to aid planners on national forests. We present an evaluation of the use of FORPLAN that addresses five criteria including, problems associated with large-scale models and systematic, comprehensive planning, Forest Service organizational issues, the role of foresters in national forest management, and conflicts over competing land uses. We then consider lessons for operations research practitioners. Finally, we discuss a number of conclusions and recommendations, the most important being the need for the Forest Service to more clearly specify the role of forest planning in the overall agency planning hierarchy and the role of FORPLAN in forest planning.", "e:keyword": ["Planning", "Government: comprehensive planning for national forests", "Programming", "Linear", "Applications: linear programming and forest planning", "Professional", "OR/MS implementation: forest service implementation of operations research effort"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.130", "e:abstract": "In many important combinatorial optimization problems, such as bin packing, allocating customer classes to queueing facilities, vehicle routing, multi-item inventory replenishment and combined routing/inventory control, an optimal partition into groups needs to be determined for a finite collection of objects; each is characterized by a single attribute. The cost is often separable in the groups and the group cost often depends on the cardinality and some aggregate measure of the attributes, such as the sum or the maximum element. An upper bound (<i>capacity</i>) may be specified for the cardinality of each group and the number of groups in the partition may either be fixed or variable. The objects are indexed in nondecreasing order of their attribute values and within a given partition the groups are indexed in nondecreasing order of their cardinalities. We identify easily verifiable analytical properties of the group cost function under which it is shown that an optimal partition exists of one of three increasingly special structures, thus allowing for increasingly simple solution methods. We give examples of all the above listed types of planning problems, and apply our results for the identification of efficient solution methods (wherever possible).", "e:keyword": ["Queues: structured set partitioning problems", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.150", "e:abstract": "We describe a branch-and-bound algorithm for solving the axial three-index assignment problem. The main features of the algorithm include a Lagrangian relaxation that incorporates a class of facet inequalities and is solved by a modified subgradient procedure to find good lower bounds, a primal heuristic based on the principle of minimizing maximum regret plus a variable depth interchange phase for finding good upper bounds, and a novel branching strategy that exploits problem structure to fix several variables at each node and reduce the size of the total enumeration tree. Computational experience is reported on problems with up to 78 equations and 17,576 variables. The primal heuristics were tested on problems with up to 210 equations and 343,000 variables.", "e:keyword": ["Networks/graphs", "Matchings: three-dimensional", "Programming", "Integer algorithms: primal heuristics", "Queues", "Optimization: subgradient optimization with facet defining cutting planes"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.162", "e:abstract": "A partially observed Markov decision process (POMDP) is a sequential decision problem where information concerning parameters of interest is incomplete, and possible actions include sampling, surveying, or otherwise collecting additional information. Such problems can theoretically be solved as dynamic programs, but the relevant state space is infinite, which inhibits algorithmic solution. This paper explains how to approximate the state space by a finite grid of points, and use that grid to construct upper and lower value function bounds, generate approximate nonstationary and stationary policies, and bound the value loss relative to optimal for using these policies in the decision problem. A numerical example illustrates the methodology.", "e:keyword": ["Dynamic programming: partially observed Markov decision processes", "Dynamic programming", "Markov: Bayesian programming and infinite state Markov models"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.28", "e:abstract": "Bidding at U.S. offshore oil and gas lease sales is characterized by high stakes, enormous uncertainties, and many interrelated decisions. The Lease Bidding Strategy System combined techniques from decision analysis, statistics, and nonlinear optimization to provide information and insights to management responsible for bidding at Gulf Oil Corporation. It was used prior to every major federal lease sale from September 1980 until Gulf was acquired in 1984, during which time Gulf's bids exceeded $1.5 billion. This paper describes the evolution and use of this system, emphasizing its impact on the organization. It describes efforts that gained acceptance for this system under difficult circumstances, and illustrates the importance of adapting methodology to problem changes over time. To our knowledge, this is the first public documentation of the long-term use by a major oil company of a system for constrained multiblock optimization of its bids and partnership shares at U.S. offshore lease sales.", "e:keyword": ["Decision analysis", "Applications: allocation of capital in bidding for leases", "Games/group decisions", "Bidding/auctions: bidding systems for offshore oil and gas leases", "Statistics: statistical models for winning in offshore lease bidding"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.42", "e:abstract": "This paper describes a line segmentation problem in a multistage, multimachine production system. The production facility can concurrently produce several types of circuit boards because each production stage consists of multiple machines. The items produced are categorized into families, and items belonging to the same family share the common major setup, while switching over from one family to another requires a major setup. The line segmentation problem determines an allocation of machines at each production stage to families so as to minimize the time to complete all jobs. As a result of segmenting the line, several minilines are formed which are dedicated to the production of items in each family. Forming dedicated minilines and producing the items in a family on the same line captures the benefits of group technology and focused factory. We first formalize the line segmentation problem as a quadratic integer programming problem, and establish its NP-completeness. Since the problem is NP-complete, we propose several heuristics to find a good solution. Lower bounding procedures are developed to show the quality of the feasible solution. We also provide bounds on the performance of the heuristic solutions, and then empirically evaluate their performance.", "e:keyword": ["Manufacturing", "Automated systems: populating printed circuit boards", "Production/scheduling: approximations/heuristics", "Production/scheduling", "Planning: formation of dedicated lines"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.5", "e:abstract": "The theme of the October 29–31, 1990, ORSA/TIMS meeting in Philadelphia was “Productivity and Global Competition.” This paper, which is an address given there, pursues this theme by emphasizing the importance of knowledge in contrast to natural resources as the critical ingredient in economic development in the future, and stresses the rapidity with which knowledge is transferred across national boundaries and the impossibility of restraining it, It forecasts that the competition among nations will lead to a situation in which no notation dominates international trade the way the United States did from the end of World War II until the early 1970s. Rather, the leadership will change in a pattern in which one nation innovates and others begin to follow and erode that innovation. Thus, in the long run, the critical ingredient will be the proportion of the GNP that a nation is willing to devote to research.", "e:keyword": ["Economics: addresses", "Industries: addresses", "Professional: addresses"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.56", "e:abstract": "This paper treats a version of the Economic Lot Scheduling Problem (ELSP) in which items may be produced several times in different amounts during a cycle. We show how to compute the optimal lot sizes and cycle length, given the sequence of items in a cycle. This requires solving a parametric quadratic program, plus a few EOQ calculations. Our procedure is designed to be used along with a heuristic for selecting the sequence of items in a cycle, such as the one proposed in 1987 by G. Dobson. The two algorithms together comprise a simple, plausible heuristic for the ELSP as a whole.", "e:keyword": ["Inventory/production", "Multi-item", "Economic lot scheduling", "Programming", "Quadratic: application to production scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.64", "e:abstract": "In this paper, we address the problem of scheduling a number of jobs on a bank of parallel machines to minimize the total weighted tardiness, under the assumption that the weight of each job is proportional to its processing time. The version of the problem that has general weights has been shown to be strongly NP-complete. We prove this version of the problem to be NP-complete, and give a pseudopolynomial time algorithm for solving it. We study a family of simple sequencing rules in which the jobs are sequenced in increasing order of γ<i><sub>i</sub></i> = <i>d<sub>i</sub></i> − θ<i>p<sub>i</sub></i>, where <i>d<sub>i</sub></i> is the due date of job <i>i</i>, <i>p<sub>i</sub></i> its processing time, <i>w<sub>i</sub></i> its weight, and 0 ≤ θ ≤ 1. This family of sequencing rules generalizes the earliest due date sequencing rule. We obtain bounds on the ratio [<i>C</i><sub>γ</sub> · <i>C</i><sup>*</sup>]/[Σ<i><sub>i</sub>w<sub>i</sub>p<sub>i</sub></i>], where <i>C</i><sub>γ</sub> and <i>C</i><sup>*</sup> are the costs of the heuristic and optimal schedules, respectively. The denominator is the cost of having each job be late by its own processing time. It is intended to measure what is or is not a large deviation from optimality, in absolute rather than relative terms, for the problem at hand. We also report on the results of computational experiments.", "e:keyword": ["Production/scheduling: weighted-tardiness scheduling with proportional weights"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.82", "e:abstract": "Recent developments in location systems technology for railroads provide a train dispatcher with the capability to improve the operations of a rail line by <i>pacing</i> trains over a territory; i.e., to permit trains to travel at less than maximum velocity to minimize fuel consumption while maintaining a given level of performance. Traditional railroad dispatching models assume that the velocities of the trains moving over a dispatcher's territory are fixed at their maximum value and, thus, are incapable of dealing with a pacing situation. This paper presents a mathematical programming model for the pacing problem and describes alternative solution procedures for this model. Analytical and numerical evidence are presented that confirm the applicability of a heuristic solution procedure for this problem, as well as providing evidence that a pacing approach versus the traditional dispatching approach is an efficient and potentially cost effective method for the control of train movements.", "e:keyword": ["Production/scheduling: scheduling of rail traffic", "Programming", "Nonlinear applications: heuristic for large-scale", "Mixed integer convex programs", "Transportation: control of freight railroad traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.1.9", "e:abstract": "This memorial tribute to the late Stella Dafermos summarizes her contributions to equilibrium modeling, analysis, and computation, and provides a list of her published papers.", "e:keyword": ["Economics: solution of equilibrium problems", "Networks/graphs: equilibrium modeling and computation", "Transportation: formulation and solution of multimodal problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.183", "e:abstract": "This paper, which is based on the Omega Rho lecture given at the 1990 ORSA/TIMS meeting in Philadelphia, states the author's necessary and sufficient conditions for effective OR/MS. Then it describes the changing character of this field, considers some opportunities for it, addresses some human resources concerns facing it, and offers a view of what it will do in the 21st century. After addressing some concerns about the higher education culture, it then provides some conclusions about the future of OR/MS.", "e:keyword": ["Professional: addresses"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.194", "e:abstract": "This paper describes a project to develop on optimal production plan for crops and livestock in Chang Qing County, People's Republic of China. The production plan was to increase net profit without adverse effects on the environment. We set up four large-scale linear programming models, each having more than 3,000 variables and 100 constraints, for different weather conditions and combinations of crops and livestock production. We obtained optimal solutions using computers. We used game theory to work out an optimal plan least sensitive to weather variations. We used sensitivity analysis to obtain the information for production planning as a function of market prices. The project took 1 year; 20 people were involved. More than 3 million data items were used in the development of the models. The optimal production plan involved a 2-year cycle that began in September 1983. By September 1985, the net profit from crops in the county increased by 12.33% and that from livestock by 53.77%. This success opened up the study of production planning and control of the macroscopic economy in agriculture in China. Optimal production plans are being developed and improved in many parts of China.", "e:keyword": ["Agriculture: optimization of production structure"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.206", "e:abstract": "In order to provide a guide to source material for practitioners interested in applying decision analysis methods, this paper surveys applications of decision analysis published from 1970 through 1989. In addition, it presents references for useful decision analysis methods that are often omitted from introductory textbooks. As used in this article, the term <i>decision analysis</i> refers to a set of quantitative methods for analyzing decisions which use expected utility as the criterion for identifying the preferred decision alternative. To be included in this survey, an application had to explicitly analyze alternatives for a decision problem using judgmental probabilities and/or subjectively assessed utility functions. The paper classifies the applications into five areas: energy, manufacturing and services, medical, public policy, and general. It further subclassifies energy applications into bidding, product and project selection, regulation, site selection, and technology choice. Those in manufacturing and services are subclassified into budget allocation, product planning, strategy, and miscellaneous. Applications in public policy are subclassified into standard-setting and miscellaneous. The paper notes articles that present significant detail about methodological and implementation issues, including problem structure/formulation, decision trees, probability and utility assessment, communication/facilitation, and group decision making.", "e:keyword": ["Decision analysis", "Applications: survey of applications", "Utility/preferences", "Applications", "Survey of applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.220", "e:abstract": "A new method for optimizing the operation of a single storage connected to a general linear memoryless system is presented. The model is shown to cover a wide variety of practical situations where a deterministic approximation is valid. The method combines linear and dynamic programming concepts to produce a fast but exact optimization.", "e:keyword": ["Dynamic programming", "Deterministic: rapid", "Exact dynamic programming", "Production/scheduling: optimal utilization of storage", "Programming", "Linear: fast optimization of linear systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.233", "e:abstract": "Automated warehouses are often faced with the problem of smoothing their stock volume over time in order to minimize the cost due to space acquisition. In this paper, we consider an infinite-horizon, multi-item replenishment problem: In addition to the usual setup and holding costs incurred by each item, an extra charge proportional to the peak stock volume at the warehouse is due. This last cost raises the need for careful coordination while making decisions on the individual item order policies. We restrict ourselves to the class of policies that follows a stationary rule for each item separately. We derive a lower bound on the optimal average cost over all policies in this class. Then we investigate the worst case of the Rotation Cycle policy. We show that depending on the problem's parameters, the Rotation Cycle policy may yield an extremely good solution but in other settings this heuristic may generate an extremely poor policy. We also develop a new heuristic whose performance is at least as good as that of the Rotation Cycle procedure, and moreover, it is guaranteed to come, independently of the problem's parameters, within no more than 41% of the optimal solution!", "e:keyword": ["Analysis of algorithms: worst case bounds", "Inventory/production: storage space requirement in automated warehouses", "Inventory/production", "Deterministic models: EOQ with annual linear cost for storage space"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.244", "e:abstract": "This paper presents a class of facet-defining inequalities for an assignment problem with the additional constraints that specified variables are required to be equal to each other. In a special case, the complete polyhedral description is given.", "e:keyword": ["Networks/graphs: matchings", "Programming", "Integer: applications and cutting plane algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.251", "e:abstract": "We present a method for approximating sojourn time distributions in open queueing systems based on light and heavy traffic limits. The method is consistent with and generalizes the interpolation approximations for moments previously presented by M. I. Reiman and B. Simon. The method is applicable to the class of systems for which both light and heavy traffic limits can be computed, which currently includes Markovian networks of priority queues with a unique bottleneck node. We illustrate the method of generating closed-form analytic approximations for the sojourn time distribution of the M/M/1 queue with Bernoulli feedback, the M/M/1 processor sharing queue, a priority queue with feedback and the M/E<i><sub>k</sub></i>/1 queue. Empirical evidence suggests that the method works well on a large and identifiable class of priority queueing models.", "e:keyword": ["Queues", "Approximations: interpolation approximations", "Queues", "Limit theorems: light traffic and heavy traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.261", "e:abstract": "A system subject to catastrophic failure deteriorates according to a delayed Markov process and is subjected to a series of binary tests that may yield false negative and false positive outcomes. A corrective action is carried out when a true positive is observed, thereby reducing the chance of system failure. Costs of inspections, false positives, the corrective action, and failure are incurred, and dynamic programming is used to compute the optimal inspection schedule. Two tractable computational methods are developed. The model, which is suited for medical screening, is applied to the problems of post-operative periumbilical pruritis and breast cancer.", "e:keyword": ["Dynamic programming/optimal control: semi-Markov inspection scheduling model", "Health care", "Diagnosis: stochastic model for optimal medical screening", "Probability", "Stochastic model", "Applications: optimal inspection schedule model"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.274", "e:abstract": "A finite number of candidates appear one-by-one in random order with all permutations equally likely. We are able, at any time, to rank the candidates that have so far appeared according to some order of preference. Each candidate may be classified into one of two types independent of the other candidates: available or unavailable. An unavailable candidate does not accept an offer of employment. The goal is to find a strategy that maximizes the probability of employing the best among the available candidates based on both the relative ranks and the availabilities observed so far. According to when the availability of a candidate can be ascertained, two models are considered. The availability is ascertained only by giving an offer of employment (MODEL 1), while the availability is ascertained just after the arrival of the candidate (MODEL 2).", "e:keyword": ["Dynamic programming", "Applications", "The secretary problem", "The marriage problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.285", "e:abstract": "We examine an allocation problem in which limited resources are allocated among competing activities. Certain substitutions among resources are possible. The substitutional relations are formulated using tree structures, where a node (resource) can substitute for all its descendants. Potential applications with such resources are found, for example, in the manufacturing of high technology products. The objective is to minimize the maximum weighted relative deviation of the activity levels from given demands. Our formulation of this problem involves a large number of possible resource constraints. We develop an efficient minimax algorithm that is based on an iterative solution of relaxed problems, where each such problem considers at most one aggregated constraint from each tree. The algorithm is extended to minimize lexicographically the nonincreasingly sorted vector of all terms in the objective, each of which represents an activity's deviation. Computational results show that the algorithm solves large problems using relatively small computation time.", "e:keyword": ["Production/scheduling", "Applications: allocation of manufacturing resources", "Programming", "Linear", "Algorithm: minimax algorithm for substitutable resources"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.296", "e:abstract": "Planning and scheduling of bottleneck operations is a fundamental problem in production management because bottleneck facilities control the output and profitability of the entire system. We develop optimal solutions for the product-mix planning, lot sizing, and scheduling of such bottleneck facilities. The integrated product-mix planning, lot sizing and scheduling problem yields a nonconvex, mixed-integer nonlinear program which is difficult to solve for global optimality. Our strategy is to characterize the optimal solution and reduce the overall problem to a series of smaller, single variable, constrained optimization problems which can be solved relatively easily. We provide results for the two-product case for both the single-stage and two-stage production systems and for two types of production environments. Optimal solutions are also provided for the single-stage multiproduct case when all products are produced under a common-cycle restriction. Our work represents an extension of classical product-mix models to include the economic lot sizing and scheduling model.", "e:keyword": ["Inventory/production: product-mix planning", "Production/scheduling: bottleneck facilities"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.308", "e:abstract": "In this paper, we develop an approximate model of an inventory control system in which there exist two options for resupply, with one having a shorter lead-time. We assume that demand and the fixed ordering costs are small relative to the holding cost so that a one-for-one ordering policy is appropriate. We consider a policy for placing emergency orders that uses information about the age of outstanding orders. We derive the steady-state behavior of this policy and present some computational results.", "e:keyword": ["Inventory: emergency replenishment", "Inventory", "Policies: order for order policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.322", "e:abstract": "Motivated by scheduling problems that arise in flexible manufacturing systems, we develop a heuristic procedure to obtain effective flow control (sequencing, routing, and input control) policies for multiclass queueing networks. The procedure employs a Brownian model of J. M. Harrison that approximates a multiclass queueing network with dynamic scheduling capability, if the total load imposed on each station in the queueing network is approximately equal to that station's capacity. In this paper, each customer class may be served at any of several different stations, and thus dynamic routing decisions are added to the sequencing and input control decisions already present in Harrison's model. Using previous heavy traffic results as a guide, we observe that, under heavy traffic conditions, a queueing network routing its customers to the queue where they will incur the shortest expected delay behaves very much like the reduced queueing network formed by pooling the appropriate servers. This observation leads to a proposed reduction of a Brownian network with discretionary routing to a simpler Brownian network without discretinary routing. Computational results indicate that combining this reduction with previous analysis of Brownian networks without discretionary routing leads to effective flow control policies for many moderately sized queueing network scheduling problems.", "e:keyword": ["Production/scheduling: routing and sequencing in a stochastic job shop", "Queues: Brownian models of network scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.2.341", "e:abstract": "The problem of data verification may be described as two sides that have concluded a contract stipulating that one side (the inspectee) is to report a set of data to the other (the inspector). The inspector has to decide on the basis of his own measurements whether to accept the data reported by the inspectee as correct or to assume they have been falsified. This situation is modeled as a statistical game, and practical solutions are supplied. We consider the verification of <i>n</i> data with a sample of size <i>k</i>, and prove that the traditional <i>D</i>-test is optimal for both maximum and minimum sample sizes (<i>k</i> = <i>n</i> and <i>k</i> = 1, respectively). These outcomes are, of course, to be expected; however, optimal falsification strategies are also obtained in these cases. In the special case in which two out of three data sets are verified, strong numerical evidence indicates that the <i>D</i>-test is no longer optimal if the total falsification is high. By applying the results obtained for <i>k</i> = 1 to an arbitrary sample size <i>k</i>, however, we show that the <i>D</i>-test is optimal in the case of low total falsification.", "e:keyword": ["Accounting: inspector leadership game", "Decision analysis: data verification occurring in accounting", "Noncooperative games: arms control and others", "Is formulated as"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.355", "e:abstract": "Many models used in policy or systems analysis either cannot be validated in any fully adequate sense, such as by comparing them with actual data, or could adequately be validated but have not been. For example, in the area of combat analysis, the central models are arguably almost entirely unvalidated and most will never be susceptible to adequate validation. Nevertheless, such models are often used and can be used fruitfully, even though we have no theory for how to use them or how to interpret and place value on the results they produce. This paper takes a step toward providing such a theory by focusing on the logic that should govern the use of inadequately validated models and the costs and benefits of using them. To this end, it identifies and evaluates six legitimate uses to which such models can be put.", "e:keyword": ["Professional: comments on", "Military: warfare models"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.366", "e:abstract": "This paper reports on a decision support system (DSS) for computer capacity planning. The system was developed for Sandia National Laboratories (SNL) to support planning in an environment characterized by large-scale scientific computing needs, governmental budgetary limitations and specific planning documentation required by the funding source. The implementation of the system is based directly on the concepts of relational model management and logic-based model representation and manipulation. The implementation vehicle for this system is an extended version of Prolog, which allows interactions with externally defined computational routines and sources of data. This vehicle provides for the integration of various data sources with a set of models for estimating capacity requirements and optimizing different objective functions subject to budgetary constraints.", "e:keyword": ["Artificial intelligence", "Government", "Information systems: management"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.378", "e:abstract": "This is the second in a series of three papers that empirically examine the competitiveness of simulated annealing in certain well-studied domains of combinatorial optimization. Simulated annealing is a randomized technique proposed by S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi for improving local optimization algorithms. Here we report on experiments at adapting simulated annealing to graph coloring and number partitioning, two problems for which local optimization had not previously been thought suitable. For graph coloring, we report on three simulated annealing schemes, all of which can dominate traditional techniques for certain types of graphs, at least when large amounts of computing time are available. For number partitioning, simulated annealing is not competitive with the differencing algorithm of N. Karmarkar and R. M. Karp, except on relatively small instances. Moreover, if running time is taken into account, natural annealing schemes cannot even outperform multiple random runs of the local optimization algorithms on which they are based, in sharp contrast to the observed performance of annealing on other problems.", "e:keyword": ["Mathematics", "Combinatorics: number partitioning heuristics", "Networks/graphs", "Heuristics: graph coloring heuristics", "Simulation", "Applications: optimization by simulated annealing"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.407", "e:abstract": "This paper deals with the problem of assigning components to parallel-series (PS) and series-parallel (SP) systems so as to maximize the system's reliability. We assume that any component can be assigned to any position of the system and the reliability of component <i>j</i> is <i>r<sub>i</sub>p<sub>j</sub></i> if it is assigned to position <i>i</i>. Exploiting the nature of Schur-convex functions, an algorithm is developed to obtain an optimal assignment for PS systems. Also, a simple optimal assignment rule is derived for SP systems with only two positions in each minimal cutset and with component reliabilities invariant of positions.", "e:keyword": ["Reliability: optimal allocation of components in reliability systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.415", "e:abstract": "In these games the searcher has a sequence of looks in which to detect the target, while the target chooses a new cell after each look in the knowledge of what cells have been searched so far. Since time is of the essence and the searcher's speed is bounded, the target has a tendency to choose cells far away from the most recent look. A theory for such games is developed and used to approximate a game involving a dipping sonar.", "e:keyword": ["Games/group decisions", "Gambling: active sonar search", "Military", "Antisubmarine warfare: active sonar search", "Search and surveillance: active sonar search"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.423", "e:abstract": "In the Dynamic Capacitated Plant Location Problem (DCPLP) the task is to find a time schedule and sizes for installing facilities at plant locations to minimize the discounted cost of capital expenditures over the planning horizon. The costs include setup costs for establishing facilities, volume dependent operational costs, and transportation costs for distributing demand from facilities to customers. We consider a class of the DCPLP in which the available facilities have finite capacities and the number of facility types is relatively small so that the expansion sizes cannot be modeled by continuous variables. The DCPLP is formulated as a combinatorial optimization problem that allows consideration of more than one fatality type and finds the optimum mix of facilities in each location. We describe an optimization algorithm for solving the DCPLP based on the Lagrangian relaxation technique. Algorithms for converting infeasible optimal solutions of the Lagrangian to a feasible solution of the DCPLP are presented. The procedure has been tested on both randomly generated and real-life based problems. Computational results indicate that the algorithm produces solutions within 3% of the lower bounds for a wide range of input data.", "e:keyword": ["Facilities/equipment planning: multiperiod facilities location planning", "Capacity expansion", "Programming: dynamic programming", "Lagrangian relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.437", "e:abstract": "It is a well observed fact that in minisum multifacility location problems the optimal locations of several facilities often tend to coincide. Some sufficient conditions for this phenomenon, involving only the weights and applicable to any metric, have been published previously. The objective of this paper is to show how these conditions may be extended further and to obtain a more complete description of their implications, in particular, in the case of certain locational constraints.", "e:keyword": ["Facilities", "Continuous location: coincidence properties in minisum multifacility ocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.443", "e:abstract": "We analyze a continuous-time, two-stage production/inventory system. In the first stage, a common intermediate product is produced in batches, and possibly stored. In the second phase, the intermediate product is fabricated into <i>n</i> distinct finished products. Several finished products may be included in a single production batch of <i>limited capacity</i> to exploit economies of scale. We propose a planning methodology to address the <i>combined</i> problem of joint setup costs and capacity limits (per setup). We restrict ourselves to a class of replenishment strategies with the following properties: a replenishment strategy specifies a collection of families (subsets of items) covering all end-items, if an item belongs to several families a specific fraction of its sales is assigned to each family. Each time the inventory of one item in a family is replenished, the inventories of all other items in the family are replenished as well. We derive a simple (roughly 0(<i>n</i> log <i>n</i>)) algorithm that results in a strategy whose long-run average cost comes within a few percentage points of a lower bound for the minimum achievable cost (within the above class of strategies).", "e:keyword": ["Inventory/production: multi-item", "Echelon stage", "Policies for two-stage capacitated multi-item models"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.456", "e:abstract": "The delivery problem consists of finding a set of routes for a fleet of capacitated vehicles to satisfy the cargo delivery requirements of customers. The vehicles are located in a central depot, and have to fulfill the delivery requirements in a sequence that minimizes total delivery costs. Each vehicle tour starts and terminates at the central depot, and each node is supplied by exactly one vehicle. All vehicles have the same cargo carrying capacity. The paper presents parallel savings algorithms (PSAs) for generating feasible solutions to this problem. The new algorithms combine the savings approach, with matching based procedures. In computational tests the heuristic produces better solutions than the best known solutions for six problems out of a standard set of 14 difficult test problems. Augmented Lagrangian based lower bounding procedures are developed, and used to evaluate the quality of the solutions generated by PSAs. The lower bounds generated by the augmented Lagrangian are the tightest bounds known for delivery problems. The performance of the PSAs is also compared to tour partitioning based heuristics which have better worst case error bounds. The average quality of solutions generated by PSAs is shown to be significantly superior on large sets of test problems.", "e:keyword": ["Integer programming: Lagrangian relaxation for delivery problems", "Vehicle routing: heuristics for vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.470", "e:abstract": "This paper considers the rescheduling of operations with release dates and multiple resources when disruptions prevent the use of a preplanned schedule. The overall strategy is to follow the preschedule until a disruption occurs. After a disruption, part of the schedule is reconstructed to match up with the preschedule at some future time. Conditions are given for the optimality of this approach. A practical implementation is compared with the alternatives of preplanned static scheduling and myopic dynamic scheduling. A set of practical test problems demonstrates the advantages of the matchup approach. We also explore the solution of the matchup scheduling problem and show the advantages of an integer programming approach for allocating resources to jobs.", "e:keyword": ["Production/scheduling: approximations", "Heuristic", "Applications: programming", "Infinite dimensional"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.484", "e:abstract": "A work force includes workers of <i>m</i> types. The worker categories are ordered, with type-1 workers the most highly qualified, type-2 the next, and so on. If the need arises, a type-<i>k</i> worker is able to substitute for a worker of any type <i>j</i> > <i>k</i> (<i>k</i> = 1, …, <i>m</i> − 1). For 7-day-a-week operation, daily requirements are for at least <i>D<sub>k</sub></i> workers of type-<i>k</i> or better, of which at least <i>d<sub>k</sub></i> must be precisely type-<i>k</i>. Formulas are given to find the smallest number and most economical mix of workers, assuming that each worker must have 2 off-days per week and a given fraction of weekends off. Algorithms are presented which generate a feasible schedule, and provide work stretches between 2 and 5 days, and consecutive weekdays off when on duty for 2 weekends in a row, without additional staff.", "e:keyword": ["Labor: manpower scheduling", "Organizational studies", "Manpower planning: scheduling", "Production/scheduling: manpower scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.496", "e:abstract": "We consider a machine that works constantly and produces two types of fluid products, one type of product at a time. Whenever there is a positive amount of both types of products, the products are mixed immediately and shipped away. Whenever mixing is infeasible the product that is currently being produced accumulates inventory. The goal is to minimize the long-run average as well as the discounted costs of running such an operation. Inventory incurs a linear holding cost for each type of product. Also, whenever switching from the production of one type of product to the other, a fixed switching cost is incurred. It is found that a stationary periodic policy is optimal. The optimal solution for the long-run average cost criterion is described by closed form formulas. As a special case, we show that the (<i>s</i>, <i>S</i>) policy for the textbook EOQ inventory model is indeed optimal over the class of all policies. Also, a theorem that proves the optimality of cyclic policies within a particular set of policies is introduced in a very general setting.", "e:keyword": ["Inventory/production: deterministic", "Single machine", "Production/scheduling: deterministic", "Single machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.502", "e:abstract": "We examine the effects of nonstationarity on the performance of multiserver queueing systems with exponential service times and sinusoidal Poisson input streams. Our primary objective is to determine when and how a stationary model may be used as an approximation for a nonstationary system. We focus on a particular question: How nonstationary can an arrival process be before a simple stationary approximation fails? Our analysis reveals that stationary models can seriously underestimate delays when the actual system is only modestly <i>nonstationary</i>. Other findings include confirmation and elaboration of S. M. Ross's conjecture that expected delays increase with nonstationarity, and the identification of easily computed and tight lower and upper bounds for expected delay and the probability of delay. These empirical results are based on a series of computer experiments in which the differential equations governing system behavior are solved numerically.", "e:keyword": ["Queues", "Multichannel Markovian: effects of nonstationarity", "Queues", "Nonstationary: behavior as a function of nonstationarity"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.512", "e:abstract": "In a multi-echelon assembly system, each production stage has inputs from a number of preceding stages, and supplies at most one succeeding stage. The product of the final stage is used to satisfy external demand in discrete time. There are fixed setup costs and linear inventory holding costs at each stage. The objective is to minimize the cost of operating the system over a finite time horizon. We consider the common case of nonstationary demands, and propose an extremely fast and simple single-pass approximation algorithm for this problem. We prove that in the worst case the performance of the algorithm is uniformly bounded.", "e:keyword": ["Inventory/production: multi-echelon assembly systems", "Production/scheduling: heuristics with error bounds"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.3.519", "e:abstract": "How to combine expert opinions is an issue that has many aspects and even more “answers.” The problem addressed here is the incremental information an additional expert brings when he or she is correlated with the existing experts. R. T. Clemen and R. L. Winkler derive an algebraic formula that gives the surprising answer—usually very little. In this paper, we provide the geometrical intuition behind the Clemen and Winkler result. We also show how the Clemen and Winkler formula breaks down (i.e., gives bizarre results) as the error covariance structure approaches singularity.", "e:keyword": ["Decision analysis: theory", "Forecasting: applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.531", "e:abstract": "During the birth of operations research in World War II, some of the greatest OR successes occurred when events were happening so fast that people did not have time to learn from experience. Then skilled scientific analysis often provided a critical advantage. Today an analogous situation is unfolding on a society-wide scale. Rapid technological advances are shrinking the globe, speeding social change, and offering the prospect of an improved quality of life. The potential for increased effectiveness arises amid stiff global competition. Opportunities for OR analyses that will produce organizational and societal benefits are everywhere evident. Indeed, the complexity of today's industry virtually requires OR/MS models. This paper, based on the Morse Lecture given in October 1990 at the ORSA/TIMS meeting in Philadelphia, maintains that the challenge of contributing to industrial productivity will lead OR/MS in new directions. These will include: finding methods for extracting decision making information out of massive amounts of automatically collected data; helping to shrink the time required to develop and install new products and processes; and empowering front-line workers in organizations to improve their effectiveness.", "e:keyword": ["Industries: general", "Marketing: measurement", "Professional: address"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.543", "e:abstract": "This paper reports an analysis of multiresource production capabilities of the National Forest System for the purpose of national planning in the USDA Forest Service. The analysis identifies previously unrecognized limits to the productive capabilities of the National Forest System. The analysis was used by Forest Service planners to define the long-run planning problem and to develop alternative strategies for addressing that problem. We test a multilevel optimization approach and apply it to all forest system regions of the coterminous United States. We conclude that it is not feasible for the National Forest System to supply a constant proportion of all projected resource needs. Furthermore, we found that even more conservative levels of outputs require significant increases in investment and management intensity. These results have led to Forest Service program alternatives involving more reliance on other forest and rangelands and more intense multiresource management of the National Forest System. Finally, these conclusions are limited by the range of alternatives generated and outputs modeled in the National Forest planning effort.", "e:keyword": ["Natural resources: multilevel analysis of the National Forest System"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.553", "e:abstract": "A fundamental issue in the use of optimization models is the tradeoff between the level of detail and the ease of using and solving the model. Aggregation and disaggregation techniques have proven to be valuable tools for manipulating data and determining the appropriate policies to employ for this tradeoff. Furthermore, aggregation and disaggregation techniques offer promise for solving large-scale optimization models, supply a set of promising methodologies for studying the underlying structure of both univariate and multivariate data sets, and provide a set of tools for manipulating data for different levels of decision makers. In this paper, we develop a general framework for aggregation and disaggregation methodology, survey previous work regarding aggregation and disaggregation techniques for optimization problems, illuminate the appropriate role of aggregation and disaggregation methodology for optimization applications, and propose future research directions.", "e:keyword": ["Programming", "Linear integer: aggregation/disaggregation and surrogate programming", "Production/scheduling", "Approximations/heuristics: large-scale models", "Statistics", "Cluster analysis: aggregation process"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.583", "e:abstract": "This paper considers the determination of the relative merits of two or more system designs via stochastic simulation experiments by constructing simultaneous interval estimates of certain differences in expected performance. Tukey's all-pairwise-comparisons procedure, Hsu's multiple-comparisons-with-the-best procedure, and Dunnett's multiple-comparisons-with-a-control procedure are standard methods for making such comparisons. We propose refinements for all three procedures through the use of two variance reduction techniques: common random numbers and control variates. We show that the proposed procedures are better than the standard multiple-comparison procedures in the sense that they have a larger probability of containing the true difference and, at the same time, not containing zero when a difference exists.", "e:keyword": ["Simulation", "Statistical analysis: variance reduction", "Statistics: multiple comparisons"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.592", "e:abstract": "A necessary condition for the widely used additive value function is total preferential independence, or somewhat equivalently, total substitutability among the decision criteria. We consider cases where total substitutability is absent, and study the value functions that are applicable to such cases. First we take the case of total nonsubstitutability, and prove that the maximin value function is appropriate for it. This result easily extends to the closely related maximax value function. Next we consider the case where there is neither total substitutability nor total nonsubstitutability, and show how a <i>minsum</i> value function can be applicable. A minsum function is one that uses only addition and minimum extraction operations. We explain how the structure of a minsum function can be inferred from substitutability information. In the process, we encounter certain subsets of criteria which we call chains and cuts.", "e:keyword": ["Utility/preference", "Multiattribute: maximin and minsum value functions", "Utility/preference", "Value theory: axiomatic derivation of value functions"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.601", "e:abstract": "We propose and analyze a generic mathematical model for dynamic, stochastic vehicle routing problems, the dynamic traveling repairman problem (DTRP). The model is motivated by applications in which the objective is to minimize the wait for service in a stochastic and dynamically changing environment. This is a departure from classical vehicle routing problems where one seeks to minimize total travel time in a static, deterministic environment. Potential areas of application include repair, inventory, emergency service and scheduling problems. The DTRP is defined as follows: Demands for service arrive in time according to a Poisson process, are independent and uniformly distributed in a Euclidean service region, and require an independent and identically distributed amount of on-site service by a vehicle. The problem is to find a policy for routing the service vehicle that minimizes the average time demands spent in the system. We propose and analyze several policies for the DTRP. We find a provably optimal policy in light traffic and several policies with system times within a constant factor of the optimal policy in heavy traffic. We also show that the waiting time grows much faster than in traditional queues as the traffic intensity increases, yet the stability condition does not depend on the system geometry.", "e:keyword": ["Networks/graphs: stochastic", "Traveling salesman", "Probability", "Stochastic model", "Transportation: vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.616", "e:abstract": "T = (<i>V</i>, <i>E</i>) is a tree with nonnegative weights associated with each of its vertices. A fleet of vehicles of capacity <i>Q</i> is located at the depot represented by vertex <i>v</i><sub>1</sub>. The Capacitated Vehicle Routing Problem on Trees (TCVRP) consists of determining vehicle collection routes starting and ending at the depot such that: the weight associated with any given vertex is collected by exactly one vehicle; the sum of all weights collected by a vehicle does not exceed <i>Q</i>; a linear combination of the number of vehicles and of the total distance traveled by these vehicles is minimized. The TCVRP is shown to be NP-hard. This paper presents lower bounds for the TCVRP based on the solutions of associated bin packing problems. We develop a linear time heuristic (upper bound) procedure and present a bound on its worst case performance. These lower and upper bounds are then embedded in an enumerative algorithm. Numerical results follow.", "e:keyword": ["Transportation: vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.623", "e:abstract": "This paper presents an optimal approach for the asymmetric Generalized Traveling Salesman Problem (GTSP). The GTSP is defined on a directed graph in which the nodes are grouped into <i>m</i> predefined, mutually exclusive and exhaustive sets with the arc set containing no intraset arcs. The problem is to find a minimum cost <i>m</i>-arc directed cycle which includes exactly one node from each set. Our approach employs a Lagrangian relaxation to compute a lower bound on the total cost of an optimal solution. The lower bound and a heuristically determined upper bound are used to identify and remove arcs and nodes which are guaranteed not to be in an optimal solution. Finally, we use an efficient branch-and-bound procedure which exploits the multiple choice structure of the node sets. We present computational results for the optimal approach tested on a series of randomly generated problems. The results show success on a range of problems with up to 104 nodes.", "e:keyword": ["Networks/graphs", "Traveling salesman: generalized TSP model", "Programming", "Relaxation/subgradient: efficient relaxation", "Transportation", "Vehicle routing: routing with alternatives"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.633", "e:abstract": "We consider the multiserver queueing system studied originally by L. Green (1980) in which customers request service from a random number of identical servers. We provide a matrix-geometric formulation of the problem, present a simple means for computing the stationary probability vector, and propose an algorithm based on randomization for computing the waiting time distribution. We also give a numerical example and discuss issues involved in extending the formulation to include customer priority classes.", "e:keyword": ["Queues: random number of servers", "Waiting time distribution"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.639", "e:abstract": "New heuristic dominance rules and a flexible decomposition heuristic are developed for the problem of minimizing weighted tardiness on a single processor. Extensive computational experience demonstrates that, when our new heuristic dominance rules were incorporated into an optimal algorithm, optimal or nearly optimal solutions were obtained quickly. In fact, solution times were orders of magnitude faster than those using the optimal algorithm alone. On larger problems, our decomposition heuristic obtained better solutions than previous heuristics. Furthermore, on 50-job problems our decomposition heuristic obtained an optimal solution over ten times more often on the average than the best competing heuristic (22% versus 2% of the time). Since both our approaches are basically relaxations of optimal solution algorithms, they could easily be adapted for use in the solution of other scheduling problems.", "e:keyword": ["Production/scheduling", "Deterministic: dominance and decomposition heuristics", "Production/scheduling", "Approximations", "Heuristic: single machine sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.648", "e:abstract": "A high multiplicity scheduling problem consists of many jobs which can be partitioned into relatively few groups, where all the jobs within each group are identical. Polynomial, and even strongly polynomial, algorithms for the standard scheduling problem, in which all jobs are assumed to be distinct, become exponential for the corresponding high multiplicity problem. In this paper, we study various high multiplicity problems of scheduling unit-time jobs on a single machine. We provide strongly polynomial algorithms for constructing optimal schedules with respect to several measures of efficiency (completion time, lateness, tardiness, the number of tardy jobs and their weighted counterparts). The algorithms require a number of operations that are polynomial in the number of groups rather than in the total number of jobs. As a by-product, we identify a new family of <i>nxn</i> transportation problems which are solvable in <i>O</i>(<i>n</i> log <i>n</i>) time by a simple greedy algorithm.", "e:keyword": ["Computers/computer science: strongly polynomial algorithms", "Networks/graphs", "Flow algorithms: greedy algorithms for transportation", "Production/scheduling: sequencing", "Deterministic", "Single machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.654", "e:abstract": "In this paper, a new algorithm for computing optimal (<i>s</i>, <i>S</i>) policies is derived based upon a number of new properties of the infinite horizon cost function <i>c</i>(<i>s</i>, <i>S</i>) as well as a new upper bound for optimal order-up-to levels <i>S</i>* and a new lower bound for optimal reorder levels <i>s</i>*. The algorithm is simple and easy to understand. Its computational complexity is only 2.4 times that required to evaluate a (specific) single (<i>s</i>, <i>S</i>) policy. The algorithm applies to both periodic review and continuous review inventory systems.", "e:keyword": ["Inventory/production", "Policies: efficient algorithm for optimal (s", "S) policy", "Inventory/production", "Stochastic: efficient algorithm for optimal (s", "S) policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.666", "e:abstract": "Loading problems of Flexible Manufacturing Systems (FMSs) have usually been formulated as an integer program with nonlinear constraints for tool magazine capacities. The nonlinearity and integer nature of the problem results in the loading problem being difficult to solve. Conventional branch-and-bound methods have been proposed, but again the solution time can easily be excessive for moderate sized problems. In this paper, we present an alternative formulation of the FMS loading problem. Such a formulation defines more decision variables, but is able to avoid nonlinearity of the constraints. It also includes the time availabilities of the machines as additional constraints. The main feature of the formulation is that it exhibits a block angular structure. By exploiting this special structure, an efficient branch-and-bound algorithm can be developed. This algorithm has the attractive feature that the bounds of the branches can be computed through simple procedures based on the solution of linear knapsack problems. Some computational results of the algorithm are also presented.", "e:keyword": ["Integer programming: algorithms", "Production/scheduling: flexible manufacturing", "Planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.677", "e:abstract": "The application of commonality in a system represents an attempt to reduce costs by reducing the number of unique components. A formal method for conducting commonality analysis has not been established. This paper characterizes commonality analysis as a partitioning problem for which the solution may be approximated by the application of clustering methods. A clustering algorithm is developed and applied to a commonality analysis of Space Station water tanks. The success in applying a clustering method in this problem indicates a significant potential for application of clustering methods to commonality analysis.", "e:keyword": ["Engineering: systems optimization", "Statistics", "Cluster analysis: variance methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.4.680", "e:abstract": "This paper addresses a machine scheduling problem that arises in the case of scheduling tasks over an idealized distributed multiprocessor. Precedence constraints with small communication delays have to be taken into account and task duplication is allowed. A critical path-like algorithm is presented, which is shown to construct an optimal schedule in polynomial time.", "e:keyword": ["Computers: task allocation on distributed memory processors", "Production/scheduling: scheduling with interprocessor communication delays"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.701", "e:abstract": "This paper describes a course taught in the College of Business and Administration at Drexel University in which students undertake real-world projects in an inner-city neighborhood. The course is an elective for students who have been exposed to the traditional techniques for problem solving in such courses as management science, operational research, statistics, organizational behavior, marketing, and economics. They address such public sector problems as people moving, traffic flow, trash removal, market promotion, and surveys. Typically, the students discover that these problems are complex and have many stakeholders with competing interests, so that they do not fit neatly into one of the problem types encountered in traditional course work. Thus, the students are encouraged to bring to bear a variety of traditional and nontraditional techniques from many disciplines. The work on large-scale problems is pursued in directed project teams that simultaneously offer the students a unique learning experience and real service to the community.", "e:keyword": ["OR/MS education: community OR/MS"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.710", "e:abstract": "The objective of this study was to develop and test a method for evaluating simulation models and to illustrate how it can provide insights into a simulation's strengths and weaknesses, especially in terms of identifying areas for improvements. To this end, our resulting evaluative methodology was systematically applied to three Army simulation models that were used in the acquisition of air defense systems. We describe the evaluative framework and the results of our analysis.", "e:keyword": ["Government", "Defense", "Use of combat models", "Simulation: assessment of models"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.724", "e:abstract": "In recent years, there has been a surge of research into methods for estimating derivatives of performance measures from sample paths of stochastic systems. In the case of queueing systems, typical performance measures are mean queue lengths, throughputs, etc., and the derivatives estimated are with respect to system parameters, such as parameters of service and interarrival time distributions. Derivative estimates potentially offer a general means of optimizing performance, and are useful in sensitivity analysis. This paper concerns one approach to derivative estimation, known as <i>infinitesimal perturbation analysis</i>. We first develop a general framework for these types of estimates, then give simple sufficient conditions for them to be unbiased. The key to our results is identifying conditions under which certain finite-horizon performance measures are almost surely continuous functions of the parameter of differentiation throughout an interval. The sufficient conditions we introduce are formulated in the setting of generalized semi-Markov processes, but translate into readily verifiable conditions for queueing systems. These results substantially extend the domain of problems in which infinitesimal perturbation analysis is provably applicable.", "e:keyword": ["Queues: sample path analysis and optimization", "Simulation: gradient estimation techniques"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.739", "e:abstract": "This paper examines convergence criteria of an algorithm for the computation of Cournot-Nash economic equilibria. The method is based on formulating the equilibrium problem as that of finding a solution to a nonlinear complementarity problem, solved by sequential linearization and Lemke's algorithm. Conditions for local and global convergence are developed and the technique is applied to homogeneous, segmented and differentiated product markets.", "e:keyword": ["Games/group decisions: noncooperative", "Differential"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.749", "e:abstract": "We consider a two-stage location and allocation game involving two competing firms. The firms first select the location of their facility on a network. Then the firms optimally select the quantities each wishes to supply to the markets, which are located at the vertices of the network. The criterion for optimality for each firm is maximizing its profit, which is the total revenue minus the production and transportation costs. Under reasonable assumptions regarding the revenue, the production cost and the transportation cost functions, we show that there is a Nash equilibrium for the quantities offered at the markets by each firm. Furthermore, if the quantities supplied (at the equilibrium) by each firm at each market are positive, then there is also a Nash locational equilibrium, i.e., no firm finds it advantageous to change its location.", "e:keyword": ["Facilities/equipment planning: competitive location", "Games/group decisions", "Noncooperative: two-stage game", "Networks/graphs: location"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.757", "e:abstract": "This paper describes an approach for modeling two-stage stochastic programs that yields a form suitable for interior point algorithms. A staircase constraint structure is created by replacing first stage variables with sparse “split variables” in conjunction with side-constraints. Dense columns are thereby eliminated. The resulting model is larger than traditional stochastic programs, but computational savings are substantial—over a tenfold improvement for the problems tested. A series of experiments with stochastic networks drawn from financial planning demonstrates the attained efficiencies. Comparisons with MINOS and the dual block angular stochastic programming model are provided as benchmarks. The split variable approach is applicable to general two-stage stochastic programs and other dual block angular models.", "e:keyword": ["Programming", "Linear: techniques for stochastic linear programs", "Programming", "Stochastic: solution by interior point methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.771", "e:abstract": "We review the Kiefer–Wolfowitz conditions for finite delay moments at a <i>GI</i>/<i>G</i>/<i>s</i> queue, which are independent of moment conditions on the arrival process. Through examples, we show that for nonrenewal arrivals, some additional conditions on the arrival process are necessary. We discuss the question of whether some known sufficient conditions for finite delay moments at “downstream” stations in a tandem queue are really necessary. We then show how to <i>decouple</i> conditions on the arrival process from conditions on service times. Finally, we obtain sufficient conditions for queues with regenerative arrivals.", "e:keyword": ["Queues: effect of arrival process on delay moments", "Regeneration"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.776", "e:abstract": "One approach for solving linear programs with random coefficients is chance constrained programming. For the case where the technical coefficients are normally distributed, we present a convergent cutting plane algorithm to solve the equivalent nonlinear program, which takes advantage of the characteristics of the problem. The algorithm requires a moderate computational effort and compares favorably with a general nonlinear code and other approaches proposed for solving this problem.", "e:keyword": ["Programming", "Nonlinear convex: chance constrained programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.786", "e:abstract": "The paper deals with the organization of decision making for multidivision enterprises. If decisions can be represented by linear programming models with divisions sharing resources, an organization is proposed with one division or a combination of these setting resource prices, while the remaining ones determine quantities. The role of each division is determined by the numerical coefficient values as well as the models' structure. This approach is related to, but quite different from, the Dantzig–Wolfe decomposition principle. Interactions between divisions are like ordinary commercial transactions with one party setting the price and the other the quantity traded. The usefulness of this organizational structure depends on its stability for expected variations of model data, which can be determined by parametric variations and simulations. These concepts are applied to a model by R. M. Burton and B. Obel to decide whether the M-form or the U-form organization is preferable. For central values of the model data the M-form turns out to be appropriate, and remains valid for large variations of the coefficients. A comparison with Dantzig–Wolfe decomposition indicates that the proposed approach has significant advantages.", "e:keyword": ["Organizational studies: decision making", "Design"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.798", "e:abstract": "This study presents a model that integrates the supply planning of potential cogenerating industries with that of the host electric utility suppliers. It provides a tool for electric utilities and potential cogenerators to analyze the effect of cogeneration on their energy supply plans. The model can analyze how much the potential benefits of cogeneration might be and how the benefits might be distributed among the participating industries.", "e:keyword": ["Industries", "Electric industry: joint optimal planning with industrial cogenerators", "Facilities", "Equipment planningL: industrial cogeneration and conventional electricity systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.807", "e:abstract": "We describe and analyze a discipline called <i>kanban</i> for the control and coordination of cells in large-scale manufacturing facilities. There are many cells in tandem and also a process of arriving consumers; each consumer queues as long as necessary before departing with a product finished by the last cell. The coordination scheme relies on the circulation of a fixed number of cards (or kanbans) in each cell. This paper differs from our earlier paper in that here the process of consumers modulates the production process. Our results are in three parts. First, the kanban is compared to the classical production discipline and its (sample path) dominance in terms of the consumers' waiting time is proven. Second, we give an analytic scheme for approximating the performance of the stochastic kanban system by examining a single cell in isolation and then combining these through fixed-point equations. Ergodicity is equivalent in our approximation to the existence and uniqueness of a solution to the fixed-point equations and a complete, finite procedure for its resolution is obtained. Third, we report on experiments involving simulations and our method of analysis.", "e:keyword": ["Inventory/production: approximations", "Heuristics and policies", "Mathematics: fixed points", "Probability: stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.824", "e:abstract": "This paper presents an operation partitioning problem (OPP) that arises from the design of an automated assembly system. To reduce the traffic flow of the system, the OPP assigns operations to machines so that the total number of movements of jobs between machines is minimized. This problem has applications in flexible manufacturing and VLSI design. In flexible manufacturing, OPP relates to a part grouping problem in which different parts are grouped into families. In VLSI design, this problem is related to a VLSI design problem in which a large circuit is partitioned into layers of small circuits. In this paper, we develop a simulated annealing heuristic that finds a near-optimal solution. Random problems are generated for examining the effectiveness of this heuristic.", "e:keyword": ["Inventory/production: manufacturing and automated systems", "Networks/graphs: heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.836", "e:abstract": "This paper and its companion (Part II) concern the scheduling of jobs with cost penalties for both early and late completion. In Part I, we consider the problem of minimizing the weighted sum of earliness and tardiness of jobs scheduled on a single processor around a common due date, <i>d</i>. We assume that <i>d</i> is not early enough to constrain the scheduling decision. The weight of a job does not depend on whether the job is early or late, but weights may vary between jobs. We prove that the recognition version of this problem is NP-complete in the ordinary sense. We describe optimality conditions, and present a computationally efficient dynamic programming algorithm. When the weights are bounded by a polynomial function of the number of jobs, a fully polynomial approximation scheme is given. We also describe four special cases for which the problem is polynomially solvable. Part II provides similar results for the unweighted version of this problem, where <i>d</i> is arbitrary.", "e:keyword": ["Dynamic programming", "Deterministic: earliness and lateness costs", "Production/scheduling: deterministic sequencing", "Single machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.847", "e:abstract": "A companion paper (Part I) considers the problem of minimizing the weighted earliness and tardiness of jobs scheduled on a single machine around a common due date, <i>d</i>, which is unrestrictively late. This paper (Part II) considers the problem of minimizing the unweighted earliness and tardiness of jobs, allowing the possibility that <i>d</i> is early enough to constrain the scheduling decision. We describe several optimality conditions. The recognition version of the problem is shown to be NP-complete in the ordinary sense, confirming a well known conjecture. Moreover, this complexity definition is precise, since we describe a dynamic programming algorithm which runs in pseudopolynomial time. This algorithm is also extremely efficient computationally, providing an improvement over earlier procedures, of almost two orders of magnitude in the size of instance that can be solved. Finally, we describe a special case of the problem which is polynomially solvable.", "e:keyword": ["Production/scheduling: deterministic", "Single machine sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.5.857", "e:abstract": "Counterintuitive optimal buffer capacity allocation patterns appeared in a 1988 paper by J. M. Smith and S. Daskalaki. This comment explains those patterns with examples and a simple diagram. Intuition gained from this explanation should be of use for researchers and practitioners alike.", "e:keyword": ["Manufacturing", "Automatic: buffer storage capacity allocation", "Queues", "Simulation: finite storage capacity in tandem queues", "Queues", "Tandem: finite storage capacity allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.1005", "e:abstract": "Partitioning methods lend themselves very well to implementation on parallel computers. In recent years, branch-and-bound algorithms have been tested on various types of architectures. In this paper, we develop a queueing network model for the analysis of a class of branch-and-bound algorithms on a master–slave architecture. The analysis is based on a fluid flow approximation. Numerical examples illustrate the concepts developed. Finally, related branch-and-bound algorithms are studied using a machine repair queueing model.", "e:keyword": ["Computers/computer science: master–slave architecture", "Programming: analysis of parallel branch-and-bound algorithms", "Queues: fluid flow approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.1018", "e:abstract": "State-space truncation is frequently demanded for computation of large or infinite Markov chains. Conditions are given that guarantee an error bound or rate of convergence. Roughly, these conditions apply either when probabilities of large states are sufficiently small, or when transition probabilities (rates) for state increases become small in sufficiently large states. The verification of these conditions is based on establishing bounds for bias terms of reward structures. The conditions and their verification are illustrated by two nonproduct form queueing examples: an overflow model and a tandem queue with blocking. A concrete truncation and explicit error bound are obtained. Some numerical support is provided.", "e:keyword": ["Queues", "Markovian", "Truncations", "Queues", "Tandem", "Finite approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.869", "e:abstract": "This paper offers an introductory view of the concept of habitual domains. It describes how they affect behavior, discusses their formation, dynamics, stability, and application, and points out how to expand and enrich them. Finally, this concept is related to the operations research profession to suggest how its habitual domains can be expanded and enriched to make OR workers more effective, both individually and collectively.", "e:keyword": ["Decision analysis: high-stake decision making and conflict resolution", "Philosophy of modeling: habitual domains in decision making and problem solving", "Professional", "OR/MS philosophy: expanding and enriching OR habitual domains for success"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.877", "e:abstract": "A new methodology provides military decision makers with information on the aggregate effectiveness of large-scale conventional (nonnuclear) and combined nuclear and conventional force structures against a wide range of targets. This methodology coupled a generic data base of representative targets, to which real-world targets can be mapped, with a nuclear force-exchange model that uses goal programming techniques to determine optimal allocations of weapons to targets. This methodology enabled military strategists to analyze the effectiveness of existing and proposed weapon systems against a variety of targets and to study a number of force structures constrained by proposed force reduction treaties and tight defense budgets.", "e:keyword": ["Military", "Cost effective: force tradeoff analyses", "Military", "Force effectiveness: conventional and nuclear force capability assessment", "Military", "Targeting: generic targets for large-scale conventional analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.886", "e:abstract": "This paper proposes a classification scheme for discrete manufacturing systems. It also includes a brief discussion of modeling, analysis, and optimization techniques that have been applied to manufacturing. This taxonomy is intended to be a first step in fitting existing environments to models. This will indicate directions for future manufacturing modeling and analysis.", "e:keyword": ["Manufacturing: taxonomy of systems", "Review of models"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.903", "e:abstract": "In this paper we present two iterative methods for solving a model to evaluate busy probabilities for Emergency Medical Service (EMS) vehicles. The model considers location dependent service times and is an alternative to the mean service calibration method; a procedure, used with the Hypercube Model, to accommodate travel times and location-dependent service times. We use monotonicity arguments to prove that one iterative method always converges to a solution. A large computational experiment suggests that both methods work satisfactorily in EMS systems with low ambulance busy probabilities and the method that always converges to a solution performs significantly better in EMS systems with high busy probabilities.", "e:keyword": ["Health care", "Ambulance service", "Evaluating busy probabilities", "Mathematics", "Fixed points: fixed point methods for nonlinear equations", "Queues", "Approximations: nonlinear equation approximation models"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.917", "e:abstract": "Frequency domain methodology has been applied to discrete-event simulations to identify terms in a polynomial model of the simulation output. In this paper, the problem of optimally selecting input frequencies is studied. A fast algorithm is presented that closely approximates the optimal solution. The results obtained from the algorithm are compared to known optimal solutions. Tables of input frequencies for various experiments are presented in an appendix.", "e:keyword": ["Simulation: design of experiments", "Factor screening", "Frequency domain methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.925", "e:abstract": "The function of a digital telecommunications network is to transport demand of digital signals between pairs of locations. To achieve this economically, multiplex equipment packs lower rate digital signals into higher rate signals for routing over transmission facility links. Given a multiperiod demand forecast and demand routing plan, the multiplex bundling problem minimizes equipment and transmission costs by demultiplexing the higher rate signals into their lower rate components at various nodes along paths to allow lower rate signals from different sources and sinks to be combined. However, exact solution of the multiplex bundling problem is intractable, and therefore this paper presents a heuristic two-phased approach. Phase 1 formulates a single-period, capacitated routing model. The Phase 1 solution method is to first solve a problem with relaxed capacity constraints and then solve a 0–1 multiconstraint knapsack problem to obtain feasibility. Computational results show that the Phase 1 heuristic produces near-optimal results. Phase 2 exactly solves a multiperiod problem for each demand re-routing chosen in Phase 1 by using a single-stage dynamic programming algorithm. The Phase 2 algorithm reduces the state space by taking advantage of state conditions at optimality.", "e:keyword": ["Communications: telecommunications transmission networks", "Facilities/equipment planning: capacity expansion of telecommunications equipment", "Networks/graphs", "Heuristics: network design optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.945", "e:abstract": "We present a class of shake-and-bake algorithms for generating (asymptotically) uniform points on the boundary of full-dimensional bounded polyhedra. We also report results of simulations for some elementary test problems.", "e:keyword": ["Simulation", "Random variable generation: generating asymptotically uniform points on the boundary of a polytope"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.955", "e:abstract": "We extend the list of linear programming problems that are known to be solvable in strongly polynomial time to include a class of LPs which contains special cases of the generalized transshipment problem. The result is facilitated by exploiting some special properties associated with Leontief substitution systems and observing that a feasible solution to the system, <i>Ax</i> = <i>b</i>, <i>x</i> ≥ 0, in which no variable appears in more than two equations, can be found in strongly polynomial time for <i>b</i> belonging to some set Ω.", "e:keyword": ["Programming", "Linear: strong polynomality", "Exploiting special structure"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.961", "e:abstract": "This paper considers tradeoffs between cost and perturbations of the weights in a single facility, minisum location problem over a finite set of feasible points. Specifically, the paper characterizes a tradeoff curve of variability in the weights versus the degree of optimality of a solution. The general theoretical framework includes problems of location on a tree network and location using block norms; special results are given for these cases.", "e:keyword": ["Facility/location", "Location models: minisum facility locations", "Programming: sensitivity analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.970", "e:abstract": "A hierarchical approach to control a manufacturing system, subject to multiple machine states modeled by a Markov process with weak and strong interactions, is suggested. The idea is to aggregate strongly interacting or high transition probability states within a group of states and consider only the transition between these groups for the analysis of the system in the long run. We show that such an aggregation results in a problem of reduced size, whose solution can be modified in a simple way to obtain an asymptotically optimal feedback solution to the original problem. Also, an example is solved to illustrate the results developed in the paper.", "e:keyword": ["Dynamic programming", "Optimal control: stochastic", "Continuous time", "Probability", "Markov processes: hierarchical control of Markov process driven systems", "Production/scheduling", "Hierarchical planning: manufacturing with unreliable machines"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.979", "e:abstract": "A set of <i>N</i> jobs has to be processed on a single machine. Jobs have the same basic processing time, but the actual processing time of each job grows linearly with its starting time. A (possibly) different rate of growth is associated with each job. We show that the optimal sequence to minimize flow time is V-shaped: Jobs are arranged in descending order of growth rate if they are placed before the minimal growth rate job, and in ascending order if placed after it. Efficient (0(<i>N</i> log <i>N</i>)) asymptotically optimal heuristics are developed. Their average performance is shown to be extremely good: The average relative error over a set of 20-job problems is on the order of 10<sup>−5</sup>.", "e:keyword": ["Production/scheduling: deterministic sequencing", "Single machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.39.6.992", "e:abstract": "The process planning problem is described for a class of flexible assembly systems for printed circuit cards. The general problem of minimizing the number of station visits is shown to be NP-complete, and two classes of heuristics are shown to have arbitrarily bad worst case performance. Implications for design and operating discipline are discussed.", "e:keyword": ["Analysis of algorithms: complexity", "Bounds", "Heuristic performance", "Manufacturing", "Performance/productivity: process planning for circuit card assembly"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.104", "e:abstract": "Service options for the electric utility industry are increasingly including provisions to dynamically dispatch prices or service interruption calls to customers. When the number of such calls is contractually limited, then the issue of optimally dispatching the calls must be addressed. In this paper, an algorithm for optimally dispatching interruptible and curtailable (I/C) service options is developed within a stochastic dynamic programming (SDP) framework. The solution yields thresholds for calling load curtailments as a function of: 1) the number of days remaining in the period, 2) the available number of interruptions, and 3) the level of uncertainty regarding future price and cost fluctuations. The SDP approach also provides a framework for optimally designing the I/C program parameters. An illustration of the dispatching routine is provided using data from Niagara Mohawk Power Corporation (NMPC). The algorithm developed in the paper is currently being employed by NMPC as part of its I/C program for industrial and commercial customers.", "e:keyword": ["Economics: maximizing program value using marginal operating and outage costs", "Industries", "Electric: dispatching interruptible and curtailable service"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.113", "e:abstract": "We describe an approximation algorithm for the problem of finding the minimum makespan in a job shop. The algorithm is based on simulated annealing, a generalization of the well known iterative improvement approach to combinatorial optimization problems. The generalization involves the acceptance of cost-increasing transitions with a nonzero probability to avoid getting stuck in local minima. We prove that our algorithm asymptotically converges in probability to a globally minimal solution, despite the fact that the Markov chains generated by the algorithm are generally not irreducible. Computational experiments show that our algorithm can find shorter makespans than two recent approximation approaches that are more tailored to the job shop scheduling problem. This is, however, at the cost of large running times.", "e:keyword": ["Analysis of algorithms", "Suboptimal algorithms", "Simulated annealing", "Production/scheduling", "Sequencing", "Deterministic", "Multiple machine", "Job shop scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.126", "e:abstract": "An assembly production system with <i>n</i> facilities has a constant external demand occurring at the end facility. Production rates at each facility are finite and nonincreasing along any path in the assembly network. Associated with each facility are a setup cost and positive echelon holding cost rate. The formulation of the lot sizing problem is developed in terms of integer-ratio lot size policies. This formulation provides a unification of the integer-split policies formulation of L. B. Schwarz and L. Schrage and the integer-multiple policies formulation of J. P. Moily, allowing either assumption to be operative at any point in the system. A relaxed solution to this unified formulation provides a lower bound to the cost of any feasible policy. The derivation of this Lower Bound Theorem is novel and relies on the notion of path holding costs, which is a generalization of echelon holding costs. An optimal power-of-two lot size policy is found by an <i>O</i>(<i>n</i><sup>5</sup>) algorithm and its cost is within 2% of the optimum in the worst case.", "e:keyword": ["Inventory/production", "Approximation/heuristics: finite production rate assembly systems", "Inventory/production", "Multi-item/echelon/stage: finite production rate assembly systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.14", "e:abstract": "We describe an interactive optimization system for multiperiod exhaust relief planning in the local loop of a public telephone network. In exhaust relief planning in the local loop one seeks the minimum cost capacity expansion plan that meets projected demand over a given planning horizon. The problem can be modeled as an integer programming problem. However, due to cost structures and varying transmission technologies, the single-period exhaust relief planning problem is NP-complete. The size of the problem precludes the use of general purpose integer programming. Based on the mathematical structure and complexity of the problem, we decompose the optimization problem into a single-period dynamic programming problem, and a multiperiod greedy heuristic. A software system surrounds the optimization algorithm and provides interactive planning capabilities, before and after creation of the optimized plan. Important aspects of the system are the model assumptions made to keep the problem tractable, and their effect on the standardization of input data and methodology. The system is in use by several hundred outside plant planners in a major U.S. telephone company. An overview of major elements of the package is given as well as a summary of important implementation issues that arose during the first three years of the on-going project.", "e:keyword": ["Facilities/equipment planning: capacity expansion", "Design", "Discrete location"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.142", "e:abstract": "In this paper, we construct a model of the steady-state values of on-hand inventory and backorders for each facility of a serial inventory system in which each facility follows a (<i>Q</i>, <i>R</i>) policy based on installation stock. Such policies are represented by the popular kanban systems as well as more conventional applications of (<i>Q</i>, <i>R</i>) policies. The descriptive model presented here is intended for optimizing the parameters of such a policy and for obtaining theoretical results about the behavior of the system. We also illustrate the potential of the model for providing insights into system performance.", "e:keyword": ["Inventory/production", "Multi-echelon: descriptive model of policy performance", "Probability", "Stochastic model applications: steady-state inventory performance under stochastic demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.157", "e:abstract": "An iterative numerical technique for the evaluation of queue length distribution is applied to multiserver systems with queues in parallel in which customers join (one of) the shortest queues upon arrival. The technique is based on power-series expansions of the state probabilities as functions of the load of the system. The convergence of the series is accelerated by applying a modified form of the epsilon algorithm. The shortest-queue model lends itself particularly well to a numerical analysis by means of the power-series algorithm due to a specific property of this model. Numerical values for the mean and the standard deviation of the total number of customers and the waiting times in stationary symmetrical systems are obtained for practically all values of the load for systems with up to ten queues and for a load not exceeding 75% for systems with up to 30 queues. Data are also presented for systems with four queues and unequal service rates.", "e:keyword": ["Queues", "Algorithms: power-series expansions", "Queues", "Multichannel: join the shortest queue"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.168", "e:abstract": "In many contexts in which resource allocation takes place in a stochastic environment, new jobs arrive over time. Incorporation of an arrivals process into the scheduling model significantly complicates the problem of determining optimal strategies. Earlier computational studies suggest that for a large class of single machine problems often little is lost by adopting a heuristic that (essentially) ignores the arrivals process. Cases are described in which the heuristic yields an optimal strategy and analytical tools are developed that enable its evaluation. The heuristic performs well, both when arrivals are rare and when arrivals of good jobs are frequent.", "e:keyword": ["Production/scheduling", "Stochastic: models with arrivals", "Queues: optimal resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.178", "e:abstract": "The objective of this study is to develop a shortest augmenting path algorithm for solving the semi-assignment problem and conduct an extensive computational comparison with the best alternative approaches. The algorithm maintains dual feasibility and complementary slackness and works toward satisfying primal feasibility. Effective heuristics arc used to achieve an excellent advanced start, and convergence is assured via the use of the shortest augmenting path procedure using reduced costs for arc lengths. Unlike other algorithms, such as the primal simplex or the auction algorithm, each iteration during the final phase of the procedure (also known as the end-game) achieves one additional assignment. The software implementations of our algorithm are fastest for the semi-assignment problems that we tested. Our dense code is also faster than the best software for assignment problems. In addition, the algorithm has the best polynomial worst-case running time bound that we have seen; and the memory requirements are relatively small.", "e:keyword": ["Networks/graphs: algorithm for the semi-assignment problem", "Flow algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.188", "e:abstract": "For the Euclidean single facility location problem, E. Weiszfeld proposed a simple iterative algorithm in 1937. Later, it was proved by numerous authors that it is a convergent descent algorithm. W. Miehle extended Weiszfeld's algorithm to solve the Euclidean multifacility location problem. Then, L. M. Ostresh proved that Miehle's algorithm is a descent algorithm. Recently, F. Rado modified Miehle's algorithm and provided several sets of sufficient conditions for the modified algorithm to converge. He also indicated that the convergence of Miehle's algorithm was an open problem. In this paper, the relationship between Miehle's multifacility location algorithm and Weiszfeld's single facility location algorithm is analyzed. Counterexamples show that Miehle's algorithm may converge to nonoptimal points for well structured problems.", "e:keyword": ["Facilities/equipment planning: multifacility location algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.192", "e:abstract": "Correction to Zheng, Y.-S., A. Federgruen. 1991. Finding optimal (<i>s</i>, <i>S</i>) policies is about as simple as evaluating a single policy. <i>Opns. Res.</i> <b>39</b> 654--665.", "e:keyword": ["Inventory/production: errata", "Inventory/production", "Stochastic: errata"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.26", "e:abstract": "It is common practice for airlines to charge several different fares for a common pool of seats. This paper presents a model that has been used to address the problem of when to refuse booking requests for a given fare level to save the seat for a potential request at a higher fare level. This model assumes that lower fare passengers book before higher fare passengers book. This occurs quite frequently in practice, where lower fare passengers are usually vacationers and higher fare passengers are usually business travelers. In fact, many airlines mandate this practice by imposing advance booking requirements on lower fare classes to prevent business travelers from using them. We present an algorithm that finds a seat management policy that maximizes mean revenue by establishing a critical value for each fare class. Booking requests for a particular fare level are accepted if the number of empty seats is strictly greater than its critical value and rejected otherwise. This critical value is a decreasing function of the fare price and is equal to zero for the class with the highest fare.", "e:keyword": ["Probability", "Stochastic model applications: optimize given fare class demand distribution", "Transportation", "Costs: airline revenue maximization"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.38", "e:abstract": "This is the first of two articles on the principal features of SML, a language for expressing structured models. SML is presented in terms of four “levels” of increasing expressive power; this article covers the first two levels, while the sequel covers levels 3 and 4. The lower levels, at least, are easy to learn. Both articles rely entirely on examples and give special attention to the characteristics of SML that, collectively, make it unique. The intended audience includes evaluators of other modeling languages, designers of modeling languages and systems, and those who follow the development of structured modeling.", "e:keyword": ["Computers/computer science: modeling language design", "Computer/computer science", "Data bases: semantic data modeling", "Information systems", "Decision support systems: structured modeling"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.58", "e:abstract": "This is the second of two articles on the principal features of SML, a language for expressing structured models. The prior article covered levels 1 and 2. The present article covers the remaining levels, with special attention to the characteristics of SML that, collectively, make it unique. The intended audience includes evaluators of other modeling languages, designers of modeling languages and systems, and those following the development of structured modeling.", "e:keyword": ["Computers/computer science: modeling language design", "Computer/computer science", "Data bases: semantic data modeling", "Information systems", "Decision support systems: structured modeling"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.7", "e:abstract": "This paper reviews the flowshop-sequencing research since 1954 that has been devoted to the static deterministic case in which <i>n</i> jobs are to be processed through a shop in which the processing times at each stage are fixed. The paper then comments on NP-completeness, the selection of criteria for optimization, and the lack of applications of this work in industry. Finally, it draws some conclusions about the possible future of sequencing research and the lessons that this area's work has to teach the rest of operations research.", "e:keyword": ["Production/scheduling", "Flowshop sequencing: review of research lessons for researchers"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.76", "e:abstract": "In a <i>game of boldness</i> a player chooses one from a group of tasks, each having a known probability of success. The player attempts the task, and wins if it succeeds and has the lowest probability of any that succeed. Y. Gerchak and M. Henig (1986) showed that sports competitions that take this form and have players perform sequentially with full information are biased, in that the order of play helps later competitors. We discuss six game variants embodying different types of information possessed by the players before they choose, and different rules about the sequence of choice. Our results suggest that contests would be fairer if players were told others' task difficulties but not told their successes. Y. Gerchak and M. Kilgour's proposal to make later players choose tasks no easier than previous ones, also does well in regard to fairness and practicality.", "e:keyword": ["Games/group decisions: continuous games of bidding and timing", "Recreation and sports: fairness of contest rules"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.87", "e:abstract": "To analyze simulation experiments performed under the Schruben-Margolin strategy for assigning random number streams to individual runs, A. Nozari, S. Arnold, and C. Pegden developed special statistical methods for estimating a general linear metamodel (that is, a regression model) of a selected response variable expressed in terms of design variables (regressors) relevant to the target system. This paper describes a three-stage procedure for validating the use of these simulation analysis methods. Each stage of the validation procedure tests a key assumption about the behavior of the response across all points in the experimental design. The first stage checks for multivariate normality in the overall set of responses, the second stage checks for the Schruben-Margolin covariance structure among those responses, and the third stage checks for adequacy (goodness of fit) of the user-specified metamodel. To handle simulation experiments that display significant departures from the Schruben-Margolin covariance structure, we present alternative versions of the goodness-of-fit test and the follow-up analysis for the postulated metamodel that merely requires jointly normal responses. A numerical example illustrates the application of this validation procedure.", "e:keyword": ["Simulation", "Design of experiments: design of simulation experiments", "Simulation", "Statistical analysis: analysis of simulation experiments", "Simulation", "Systems dynamics: variance reduction techniques"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s109", "e:abstract": "Several techniques of linearization have appeared in the literature. The technique of F. Glover, which seems to be the most efficient, linearizes a binary quadratic integer problem of <b>n</b> variables by introducing <b>n</b> new continuous variables and <b>4n</b> auxiliary linear constraints. The new technique proposed in this paper is not only useful in linearizing binary quadratic and cubic integer problems, but also applicable to the case of quadratic and to a certain class of cubic “mixed-integer” problems. It is shown that the new technique further reduces the number of auxiliary linear constraints from <b>4n</b> to <b>n</b>, while keeping the number of new continuous variables at <b>n</b> for the binary quadratic integer problem of <b>n</b> variables. And, it requires, in the case of a certain class of cubic mixed-integer problems having <b>2n</b> of 0–1 variables, only <b>3n</b> auxiliary linear constraints and the same number of new continuous variables. The analytical superiority of the new linearization technique has also been observed, in terms of the number of iterations and execution times, through a computational experiment conducted on a set of randomly generated binary quadratic integer problems.", "e:keyword": ["Mathematics", "Combinatorics: equivalent formulations", "Programming", "Integer: linearization"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s117", "e:abstract": "We study discrete infinite horizon optimization problems without the common assumption of a unique optimum. A method based on solution set convergence is employed for finding optimal initial decisions by solving finite horizon problems. This method is applicable to general discrete decision models that satisfy a weak reachability condition. The algorithm, together with a stopping rule, is applied to production planning and capacity expansion, and computational results are reported.", "e:keyword": ["Dynamic programming", "Deterministic: infinite horizon optimization", "Facilities/equipment planning", "Capacity expansion: infinite horizon models"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s127", "e:abstract": "The problem of selecting processes and capacity expansion policies for a chemical complex consisting of continuous chemical processes can be formulated as a multiperiod, mixed integer linear programming (MILP) problem. Based on a variable disaggregation technique which exploits lot sizing substructures, we propose two reformulations of the conventional MILP model. The first one is an NLP reformulation which very quickly yields good suboptimal solutions. The second is an MILP reformulation for exact solutions which leads to up to an order of magnitude faster computational results for large problems due to its tighter linear programming relaxation.", "e:keyword": ["Facilities/equipment planning", "Capacity expansion: capacity expansion of chemical processes", "Production/scheduling", "Planning: long-range planning of chemical processes", "Programming", "Integer: reformulation of multiperiod MILP by variable disaggregation"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s14", "e:abstract": "We present a model for forest planning with habitat dispersion constraints. The problem is reduced to a linear program that is solved by a column generation approach. Generating one column reduces to a stable set problem in a graph; this is solved with linear programming techniques based on a partial description of the stable set polytope. We report computational experience with medium sized problems.", "e:keyword": ["Natural resources: forest planning and the stable set problem", "Programming", "Integer: cutting plane", "Facet generation", "Planning: applications to forest planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s145", "e:abstract": "We consider the <i>n</i>-period economic lot sizing problem, where the cost coefficients are not restricted in sign. In their seminal paper, H. M. Wagner and T. M. Whitin proposed an <i>O</i>(<i>n</i><sup>2</sup>) algorithm for the special case of this problem, where the marginal production costs are equal in all periods and the unit holding costs are nonnegative. It is well known that their approach can also be used to solve the general problem, without affecting the complexity of the algorithm. In this paper, we present an algorithm to solve the economic lot sizing problem in <i>O</i>(<i>n</i> log <i>n</i>) time, and we show how the Wagner-Whitin case can even be solved in linear time. Our algorithm can easily be explained by a geometrical interpretation and the time bounds are obtained without the use of any complicated data structure. Furthermore, we show how Wagner and Whitin's and our algorithm are related to algorithms that solve the dual of the simple plant location formulation of the economic lot sizing problem.", "e:keyword": ["Analysis of algorithms", "Computational complexity: economic lot sizing problem", "Dynamic programming", "Applications: economic lot sizing by dynamic programming", "Inventory/production: complexity of economic lot sizing"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s157", "e:abstract": "Over the past decade, optimization models have been widely used to help select bond portfolios. Several different formulations are popular. The purposes of this paper are to clarify the basic structures of the models, to explain the relationships among them, and to assess their strengths and weaknesses.", "e:keyword": ["Finance", "Portfolio: model formulations", "Programming", "Linear applications: portfolio optimization models"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s170", "e:abstract": "The <i>k</i>-partition problem seeks to cluster the vertices of an edge-weighted graph, <i>G</i> = (<i>V</i>, <i>E</i>), into <i>k</i> sets of |<i>V</i>|/<i>k</i> vertices each, such that the total weight of the edges having both endpoints in the same cluster is maximized. Bottom-up type heuristics based on matchings are presented for this problem. These heuristics are shown to yield solutions that are at least one-half the weight of the optimal solution for <i>k</i> equal to |<i>V</i>|/3 and |<i>V</i>|/4.", "e:keyword": ["Networks/graphs", "Heuristics: approximation algorithms for k-partitions"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s22", "e:abstract": "We introduce a new lower bound for the quadratic assignment problem based on a sequence of equivalent formulations of the problem. We present a procedure for obtaining tight bounds by sequentially applying our approach in conjunction with the A. Assad and W. Xu bound and the N. Christofides and M. Gerrard bound.", "e:keyword": ["Facilities", "Location: applications", "Programming", "Quadratic: assignment", "Lower bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s28", "e:abstract": "The classroom assignment (or hotel room or interval scheduling) problem is to assign classes, which meet at different time intervals, to rooms. Two classes may not meet simultaneously in the same room, nor may a class meet in two different rooms. Thousands of colleges and secondary schools face this problem every semester. There has been some confusion as to how hard this problem is. Many colleges claim that it is easy, while others complain that it is next to impossible. In the literature, some authors claim or conjecture polynomial time algorithms, while others develop heuristic approaches. The goal of this paper is to resolve the confusion by identifying cases where the problem will be easy and others where it will be hard. We focus on the kinds of cases that schedulers are apt to encounter in practice.", "e:keyword": ["Education systems", "Operation: classroom assignment", "Course timetabling", "Mathematics: computational complexity", "Production/scheduling: applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s40", "e:abstract": "We introduce an effective branch-and-bound algorithm for solving the set partitioning problem. The new algorithm employs a new multiplier-adjustment-based bounding procedure, and a complementary branching strategy which results in relatively small search trees. Computational results based on 20 moderately sized crew scheduling problems indicate that our new algorithm is on average 16.6 times faster than the popular code, SETPAR. The improvements are mainly due to the bounding procedure, which is fast, easy to use, and provides tight lower bounds. On average, the bounds are 97.6% of the optimal objective value of the linear programming relaxation after only five iterations, and 98.5% after ten iterations. Moreover, the lower bounds are observed to be monotonically nondecreasing. We also apply the technique of variable elimination, which is very effective in reducing the size of the problems. On average, 89% of the variables are eliminated from the problem at the root node.", "e:keyword": ["Programming", "Algorithms", "Heuristic: multiplier adjustment method with variable elimination", "Programming", "Integer: branch-and-bound algorithm for the set partitioning problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s48", "e:abstract": "Previous research on the multicommodity minimum cost flow problem (MMCFP) has assumed that there are two types of values associated with an arc. The first is the capacity of the arc and the second is the unit flow cost along the arc. This paper adds the third attribute—the degree of difficulty—into the conventional model of the MMCFP. In this way, a new problem, which is more general and difficult than the conventional MMCFP, is formed. However, we show that these two problems are indeed polynomially equivalent. With the equivalence, the techniques developed for solving the conventional MMCFP can be used to solve the new but more general problem.", "e:keyword": ["Networks/graphs", "Flow algorithms: polynomial equivalence of multicommodity network flow problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s5", "e:abstract": "In this paper, we present a new primal simplex pivot rule and analyze the worst case complexity of the resulting simplex algorithm for the minimum cost flow, the assignment, and the shortest path problems. We consider networks with <i>n</i> nodes, <i>m</i> arcs, integral arc capacities bounded by an integer number <i>U</i>, and integral arc costs whose magnitudes are bounded by an integer number <i>C</i>. Our pivot rule may be regarded as a scaling version of Dantzig's pivot rule. Our pivot rule defines a threshold value Δ, which is initially at most 2<i>C</i>, and the rule permits any arc with a violation of at least Δ/2 to be the editing variable. We select the leaving arc so that strong feasibility of the basis is maintained. When there is no arc satisfying this rule, then we replace Δ by Δ/2 and repeat the process. The algorithm terminates when Δ < 1. We show that the simplex algorithm using this rule performs <i>O</i>(<i>nmU</i> log <i>C</i>) pivots and can be implemented to run in <i>O</i>(<i>m</i><sup>2</sup><i>U</i> log <i>C</i>) time. Specializing these results for the assignment and shortest path problems we show that the simplex algorithm solves these problems in <i>O</i>(<i>n</i><sup>2</sup> log <i>C</i>) pivots and <i>O</i>(<i>nm</i> log <i>C</i>) time.", "e:keyword": ["Networks/graphs: improved simplex algorithms for network flow problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s56", "e:abstract": "In this paper, we consider the network version of the <i>m</i>-median problem with mutual communication (MMMC). We reformulate this problem as a graph theoretic <i>node selection problem</i> defined on a special graph. We give a polynomial time algorithm to solve the node selection problem when the flow graph (graph that denotes the interaction between pairs of new facilities in MMMC) has a special structure. We also show that with some modification in the algorithm for MMMC, the <i>m</i>-center problem with mutual communication can also be solved when the flow graph has a special structure.", "e:keyword": ["Networks/graphs", "Location analysis: m-median and m-center problems", "Networks/graphs", "Theory: series-parallel graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s67", "e:abstract": "This paper presents new results on the problem of scheduling jobs on <i>K</i> ≥ 1 parallel processors to stochastically minimize the makespan. The jobs are subject to out-forest precedence constraints, i.e., each job has at most one immediate predecessor, and job running times are independent samples from a given exponential distribution. We define a class of <i>uniform</i> out-forests in which all subtrees are ordered by an embedding relation. We prove that an intuitive greedy policy is optimal for <i>K</i> = 2, and that if out-forests satisfy an additional, uniform <i>root-embedding</i> constraint, then the greedy policy is optimal for all <i>K</i> ≥ 2.", "e:keyword": ["Network/graphs: tree algorithms", "Production/scheduling: stochastic sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s76", "e:abstract": "We consider a scheduling problem that involves two types of processors, but three types of jobs. Each job has a fixed start time and a fixed completion time, and falls into one of three types. Jobs of type 1 can be done only by type-1 processors, type-2 jobs only by type-2 processors, and type-0 jobs by either type of processors. We present a polynomial algorithm for finding the minimal cost combination of the two types of processors required to complete all jobs. The steps of the algorithm consist of constructing a job schedule network, transforming it into a single-commodity flow problem and finding the maximal flow through it.", "e:keyword": ["Networks/graphs", "Flow algorithms", "Flows and arc-disjoint paths of networks", "Production/scheduling", "Sequencing", "Assigning two types of processors to three types"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s86", "e:abstract": "One measure of the effectiveness of institutional trauma and burn management based on collected patient data involves the computation of a standard normal <i>Z</i> statistic. A potential weakness of the measure arises from incomplete patient data. In this paper, we apply methods of fractional programming and global optimization to efficiently calculate bounds on the computed effectiveness of an institution. The measure of effectiveness (i.e., the trauma outcome function) is briefly described, the optimization problems associated with its upper and lower bounds are defined and characterized, and appropriate solution procedures are developed. We solve an example problem to illustrate the method.", "e:keyword": ["Health care: trauma care evaluation", "Programming", "Fractional: nonconvex ratio optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.1.s96", "e:abstract": "We consider two generalizations of the fixed job schedule problem, obtained by imposing a bound on the spread-time or on the working time of each processor. These NP-hard problems, studied by the authors in previous works, can arise in bus driver scheduling. We introduce several polynomial-time approximation algorithms, give efficient implementations for them, and analyze their complexity and worst case performance. The average behavior is also investigated through extensive computational experiments.", "e:keyword": ["Production/scheduling: fixed job scheduling", "Programming", "Algorithms: approximation algorithms", "Worst case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.199", "e:abstract": "This paper is based on the second Philip McCord Morse Lecture given May 14, 1991, at the TIMS/ORSA Joint National Meeting in Nashville. It traces the author's involvement in the development of vehicular traffic science over the last 35 years. After some historical background, the paper discusses highlights of this work: developing and testing car-following theory, traffic theory for multilane highways, the behavior of traffic in towns, the relationship of trip decisions to traffic-system dynamics, and fuel consumption in urban areas. Throughout these discussions, particular attention is paid to the role of complexity and collective effects. The paper continues with comments on the importance of viewing traffic in the context of the overall infrastructure as well as its technology and environment. It concludes with some reflections on the state of the scientific enterprise in our society.", "e:keyword": ["Professional: address", "Transportation: traffic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.213", "e:abstract": "Curtailable electricity service is a voluntary option in which customers receive credits for permitting the utility a certain number of discretionary interruptions per year. This paper describes a methodology that allows an electric utility to design and manage these service offerings to achieve maximal peak load reduction. This methodology was developed in a project jointly sponsored by the Electric Power Research Institute (EPRI) and New England Electric Service (NEES). A spreadsheet decision support tool was developed for and used by the NEES system dispatcher, who must decide on a daily basis, when to call for an interruption. The methodology also provides guidelines for designing new service offerings, by determining the service attributes and corresponding pricing that will lead to the most efficient use of customer interruptions. Simulated application of the spreadsheet model to NEES's historical loads indicated that significant cost savings can be achieved during years with high daily peak loads. Although the methodology was developed for NEES, it can be applied at other electric utilities, including those who use curtailable service offerings to reduce peak energy costs or to improve tight reserve margins.", "e:keyword": ["Forecasting", "Applications: peak load forecasting", "Industries", "Electric: credits for service interruptions", "Marketing", "Pricing: dispatching service interruptions"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.229", "e:abstract": "This model finds a minimum cost fuel tankering policy for an airline flight schedule based on fuel prices, station constraints and supplier constraints. A station constraint is an upper or lower bound on the amount of fuel that may be purchased at a particular station for all flights. A supplier constraint is an upper or lower bound on the amount of fuel that may be purchased from a particular supplier at all stations. The problem formulates as a linear program. However, if there are no station or supplier constraints it can be reduced to a pure network problem by a series of transformations on the constraints and variables. If there are either station or supplier constraints, but not both, it can be reduced to a generalized network problem. If both are present, the program is not a generalized network. However, the number of supplier constraints is likely to be small. When this is the case, other techniques may be used to decrease computation time. McDonnell Douglas uses this model to estimate the profit potential of various aircraft types under optimal fuel management policies. Cost savings of 5 to 6% are common.", "e:keyword": ["Networks/graphs", "Flow algorithms: network flow formulation", "Networks/graphs", "Generalized networks: network flow formulation", "Transportation", "Fuel: minimize fuel costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.238", "e:abstract": "We develop a heuristic for a problem motivated by the loading of aircraft or trucks: pack blocks into a bin so that their center-of-gravity is as close as possible to a target point. Our heuristic either produces good solutions or else signals that none is possible. It also works when loading nonhomogeneous blocks into a bin of nonzero and possibly nonhomogeneous mass.", "e:keyword": ["Analysis of algorithms", "Suboptimal algorithms: balancing cargo", "Combinatorial mechanics"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.247", "e:abstract": "The paper presents an interactive procedure to search the set of nondominated outcomes of a multiple criteria linear programming problem. The procedure requires the decision maker to specify the worst outcome. Then, the ideal outcome is identified, and the improvement direction from the worst to ideal outcomes is constructed. A trial solution is found by moving from a current solution along the improvement direction, while maximizing the step size. For a trial solution, the decision maker is requested to partition the set of objective functions into three categories: those to be improved, those which may remain unchanged, those which may be relaxed. Based on this partition, the procedure displaces the worst and ideal outcomes, calculates the new improvement direction, and the entire process is repeated. The method terminates when two successive trial solutions are reasonably similar.", "e:keyword": ["Decision analysis", "Multiple criteria: algorithm", "Interactive method"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.259", "e:abstract": "This paper presents a model of a facility that processes many jobs. Each job requires a sequence of activities. The sequence of activities required by a job is random because each activity ends with a test, and the outcome of that test determines the activity that must be performed next. Evolution from activity to activity is Markovian, i.e., is determined by transition probabilities. A few (e.g., six) different types of job can share this facility, each type having its own transition probabilities between the activities. Each activity takes place in a designated sector. Several activities can share a sector. Each sector has a capacity, and buffers of inventory serve to decouple the sectors. In this paper, we introduce a family of linear control rules that smooth the flow through each sector and regulate the amount of inventory in each buffer. The operating characteristics of each linear control rule are computed. These include the mean and variance of the flow into each sector, the inventory in each buffer, the throughput of the systems, and the cycle time. These operating characteristics are optimized by a convex nonlinear program.", "e:keyword": ["Inventory/production: linear rules that control uncertainty", "Inventory/production", "Uncertainty", "Stochastic: optimization via a convex NLP"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.279", "e:abstract": "The simulation run length required to achieve desired statistical precision for a sample mean in a steady-state stochastic simulation experiment is largely determined by the asymptotic variance of the sample mean and, to a lesser extent, by the second-order asymptotics of the variance and the asymptotic bias. The asymptotic variance, the second-order asymptotics of the variance, and the asymptotic bias of the sample mean of a function of an ergodic Markov process can be expressed in terms of solutions of Poisson's equation, as indicated by positive recurrent potential theory. We review this positive recurrent potential theory, giving special attention to continuous-time Markov chains. We provide explicit formulas for birth-and-death processes and diffusion processes, and recursive computational procedures for skip-free chains. These results can be used to help design simulation experiments after approximating the stochastic process of interest by one of the elementary Markov processes considered here.", "e:keyword": ["Probability", "Diffusion: the asymptotic variance of the sample mean", "Probability", "Markov processes: potential theory and Poisson's equation", "Simulation", "Design of experiments: required run lengths"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.292", "e:abstract": "Countable-state, continuous-time Markov chains are often analyzed through simulation when simple analytical expressions are unavailable. Simulation is typically used to estimate costs or performance measures associated with the chain and also characteristics like state probabilities and mean passage times. Here we consider the problem of estimating <i>derivatives</i> of these types of quantities with respect to a parameter of the process. In particular, we consider the case where some or all transition rates depend on a parameter. We derive derivative estimates of the <i>infinitesimal perturbation analysis</i> type for Markov chains satisfying a simple condition, and argue that the condition has significant scope. The unbiasedness of these estimates may be surprising—a “naive” estimator would fail in our setting. What makes our estimates work is a special construction of specially structured parameteric families of Markov chains. In addition to proving unbiasedness, we consider a variance reduction technique and make comparisions with derivative estimates based on likelihood ratios.", "e:keyword": ["Simulation", "Statistical analysis of derivation estimates", "Simulation", "Markov processes: sensitivity analysis for"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.309", "e:abstract": "We describe a cutting plane approach to the problem of designing survivable fiber optic communication networks. This problem can be formulated as a minimum cost network design problem with certain low-connectivity constraints. Computational results on real-world telephone network design problems demonstrate the effectiveness of our cutting plane method. The facet-inducing inequalities for the convex hull of the solutions to this problem on which our algorithm is based are studied in detail in a companion paper.", "e:keyword": ["Communications", "Designing optic-based survivable communication systems", "Programming", "Integer: cutting plane/facet generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.331", "e:abstract": "We characterize the trajectory of the Stochastic Queue Median (SQM) location problem in a planar region with discrete demands and a general <i>L<sub>p</sub></i> travel metric (1 < <i>p</i> < ∞). The location objective is to minimize expected response time to customers (i.e., travel time plus queue delay). We use an ε-perturbed version of the SQM objective function (to account for points of nondifferentiability) to show that for the ε-perturbed problem the optimal SQM location occurs in a region bounded by the point minimizing the first and second moments of service time (<i>s</i>*∣ε and <i>s</i>2*∣ε, respectively); all optimal locations can be characterized by a simple ratio condition relating the derivatives of the first and second moments of service time; and the trajectory as a function of the customer call rate moves monotonically along a path from <i>s</i>*∣ε toward <i>s</i>2*∣ε, then turns and retraces the same path back to <i>s</i>*∣ε. Finally, we establish convergence of the ε-optimal solution to an optimal SQM solution as ε approaches zero, as well as a general condition under which we can solve the SQM problem directly, with no perturbation.", "e:keyword": ["Facilities/equipment planning", "Location: continuous", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.342", "e:abstract": "The vehicle routing problem with time windows (VRPTW) is a generalization of the vehicle routing problem where the service of a customer can begin within the time window defined by the earliest and the latest times when the customer will permit the start of service. In this paper, we present the development of a new optimization algorithm for its solution. The LP relaxation of the set partitioning formulation of the VRPTW is solved by column generation. Feasible columns are added as needed by solving a shortest path problem with time windows and capacity constraints using dynamic programming. The LP solution obtained generally provides an excellent lower bound that is used in a branch-and-bound algorithm to solve the integer set partitioning formulation. Our results indicate that this algorithm proved to be successful on a variety of practical sized benchmark VRPTW test problems. The algorithm was capable of optimally solving 100-customer problems. This problem size is six times larger than any reported to date by other published research.", "e:keyword": ["Dynamic programming/optimal control: subproblem modeling", "Programming", "Integer: column generation algorithm", "Transportation: vehicle routing and scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.355", "e:abstract": "Field service support is important for certain products, such as computer systems and copiers, for which service performance has a significant impact on market share. In this paper, the problem of field service operations and its important characteristics are described. Models are developed to assist the field service manager in the evaluation of performance of a given service territory with heterogeneous machines scattered (typically, nonuniformly) over a region, repaired by a traveling serviceman. Given the probability distributions of machine uptimes and repair times, and travel times between machine locations, models are developed to approximate the overall travel time distribution. This is used in existing queueing models to determine the performance measures, such as mean downtime per machine and server utilization. The results of the models are tested by simulation and observed to provide good estimates for the measures of interest. These models are eventually intended to help in making decisions on territory design and staffing levels.", "e:keyword": ["Probability", "Stochastic model applications: evaluation of service territories", "Queues", "Applications: finite source model", "Reliability", "Maintenance/repairs: traveling repairman problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.367", "e:abstract": "This paper considers a stochastic system where a fixed number of nonpreemptive jobs (no new jobs arrive) are to be processed on multiple nonidentical processors. Each processor has an increasing hazard rate processing time distribution and the processors are ordered in ascending order of their expected processing times. It is shown that the policy which minimizes the total expected delay of all the jobs (flowtime) has a threshold structure. This policy would utilize the fastest available processor only if its mean processing time is less than a critical number. Furthermore, a previously rejected processor must never be utilized at later times. This policy is also individually optimal in the sense that it minimizes the delay of each job subject to the constraint that processor preference is given to jobs at the head of the buffer. This result proves the conjecture of P. R. Kumar and J. Walrand regarding socially and individually optimal policies in parallel routing systems.", "e:keyword": ["Production/scheduling", "Sequencing", "Stochastic: multiple", "Uniform processor scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.376", "e:abstract": "We consider single machine scheduling problems when the machine capacity varies stochastically over time. Suppose that <i>R</i>(<i>t</i>) ≥ 0 processing requirements can be processed per unit of time at time <i>t</i>. First, we treat the general “machine capacity” <i>R</i>(<i>t</i>) and then consider the machine breakdown models where <i>R</i>(<i>t</i>) = 0 or 1. Jobs may arrive according to an arbitrary process. We show that simple priority rules are optimal under some assumptions. Specifically, the <i>c</i>μ rule minimizes the expected weighted total number of jobs if the processing requirements are exponentially distributed. The SEPT rule minimizes the expected total number of jobs if the processing requirements of jobs are stochastically ordered. The SERPT rule also minimizes it if the processing requirements form an ICR family.", "e:keyword": ["Production/scheduling", "Sequencing", "Stochastic: capacity varying machine", "Queues", "Optimization: cμ rule"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.384", "e:abstract": "We consider inventory systems with several distinct items. Demands occur at constant, item specific rates. The items are interdependent because of jointly incurred fixed procurement costs: The joint cost structure reflects general economies of scale, merely assuming a monotonicity and concavity (submodularity) property. Under a power-of-two policy each item is replenished with constant reorder intervals which are power-of-two multiples of some fixed or variable base planning period. Our main results include a proof that, depending upon whether the base planning period is fixed or variable, the best among all power-of-two policies has an average cost which comes within either 6% or 2% of an easily computable lower bound for the <i>minimum cost value</i>. We also derive two efficient algorithms to compute an optimal power-of-two policy. The proposed algorithms generate as a by-product, a specific <i>cost allocation</i> of the joint cost structure to the individual items. With this specific allocation, the problem with separable costs is in fact equivalent to the original problem with nonseparable joint costs in the sense that the two problems share the same sets of optimal power-of-two policies with identical associated long-run average costs.", "e:keyword": ["Inventory/production", "Approximations: power-of-two policies", "Inventory/production", "Deterministic models: joint replenishment", "Mathematics", "Combinatorics: submodular function minimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.404", "e:abstract": "We consider a producer who turns a raw material into a product. Before embarking upon production, the producer has to consider the quantity of raw material to order and the finished product delivery commitments; the actual amount produced is a random multiple of the amount of raw material ordered. A concave expected profit function is introduced which gives rise to simple formulas for determining the optimal quantities to order and to commit for delivery. We also analyze the relations between the optimal quantities to order and to commit, the expected amount received and production capacity. We show that among several vendors of the raw material, there exists a preferred one, no matter what the producer's cost parameters, if and only if the random multiple associated with that vendor is dominant in the sense of the second-degree stochastic order.", "e:keyword": ["Invenstory/production: random yield", "Production/scheduling", "Stochastic: joint production planning and product delivery", "Probability: stochastic order"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.409", "e:abstract": "In an optimal stopping problem where bids on an asset are received, conditions are given that ensure the so-called reservation price property, namely, if a certain price is accepted, then any higher price would also have been accepted at that point in time. The approach followed in this paper is similar to that pursued by D. B. Rosenfield and R. D. Shapiro in 1981.", "e:keyword": ["Dynamic programming", "Bayesian: reservation prices", "Dynamic programming", "Optimal control: optimal price search"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.2.414", "e:abstract": "In this note, a simple procedure is given for determining the optimal ordering policy for an item for which an increase in the price has been announced. The procedure eliminates the need for evaluation and comparison of the total costs of alternate ordering policies for determining the optimal ordering policy.", "e:keyword": ["Inventory/production: ordering strategies for price increases"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.423", "e:abstract": "The purposes of this paper—a revised and extended version of the Omega Rho Lecture given at the November 1991 ORSA/TIMS Joint National Meeting—are to assess some important aspects of the current MS/OR situation and to draw some conclusions about desirable future emphases. To these ends, it identifies and discusses four <i>forces</i> of historic importance (the microcomputer and communications revolutions, the dispersion of MS/OR in industry, and academia's unbalanced reward structure), three major <i>trends</i> (rapidly disseminating MS/OR tools, declining enrollments of native-born students, and persisting management apathy toward MS/OR), and five outstanding <i>opportunities</i> (ride the computer and communications revolutions, support dispersed practitioners, focus on the service sector, stress embedded applications, and go into the quality business). An underlying theme is that the field will flourish in proportion to how astutely individuals, organizations, professional societies, and universities adapt to the changing realities within which MS/OR lives.", "e:keyword": ["Professional", "Addresses: 1991 Omega Rho Lecture"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.446", "e:abstract": "Natural gas utilities supply about a quarter of the energy needs of the United States. From wellhead to consumer, operations are governed by an astounding diversity of purchase, transport, and storage contract agreements which prepare a complex physical distribution system to meet future demands no more predictable than next year's weather. We present a decision support system based on a highly detailed optimization model used by utilities to plan operations which minimize cost while satisfying regulatory agencies. Applications at Southwest Gas Corporation are presented along with a case study at Questar Pipeline Corporation.“But thou, contracted to thine own bright eyes,Feed'st thy light's flame with self-substantial fuel”<i>William Shakespeare, First Sonnet</i>", "e:keyword": ["Industries", "Petroleum/natural gas: purchase", "Storage and transmission contract analysis: programming", "Linear: applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.463", "e:abstract": "This paper proposes a new method for representing and solving Bayesian decision problems. The representation is called a valuation-based system and has some similarities to influence diagrams. However, unlike influence diagrams which emphasize conditional independence among random variables, valuation-based systems emphasize factorizations of joint probability distributions. Also, whereas influence diagram representation allows only conditional probabilities, valuation-based system representation allows all probabilities. The solution method is a hybrid of local computational methods for the computation of marginals of joint probability distributions and the local computational methods for discrete optimization problems. We briefly compare our representation and solution methods to those of influence diagrams.", "e:keyword": ["Decision analysis", "Theory: representation", "And solution using local computation", "Dynamic programming", "Markov", "Finite state solution using local computation", "Networks/graphs: representation for decision", "Optimization and probabilistic inference problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.485", "e:abstract": "Empirical estimates of quantile residual life functions can be employed effectively to obtain properties of recidivism and to help screen parametric mixture models. In this manner, the Burr model is demonstrated to be an appropriate model for characterizing recidivism. When applied to certain data, the model suggests that while the observed declining recidivism rate can be explained by population heterogeneity, individual recidivism rates may in fact be increasing. The quantile residual life function approach to modeling recidivism is applied to two often-referenced data sets, as well as to an extensive data set obtained from the State of New York which is new to the criminal justice literature.", "e:keyword": ["Judicial/legal", "Crime: recidivism modeling", "Analysis of data", "Probability: mixture models and Burr distributions", "Statistics: data analysis", "Quantile residual life function plotting"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.505", "e:abstract": "A decision-theoretic framework is proposed for evaluating the efficiency of simulation estimators. The framework includes the cost of obtaining the estimate as well as the cost of acting based on the estimate. The cost of obtaining the estimate and the estimate itself are represented as realizations of jointly distributed stochastic processes. In this context, the efficiency of a simulation estimator based on a given computational budget is defined as the reciprocal of the risk (the overall expected cost). This framework is appealing philosophically, but it is often difficult to apply in practice (e.g., to compare the efficiency of two different estimators) because only rarely can the efficiency associated with a given computational budget be calculated. However, a useful practical framework emerges in a large sample context when we consider the limiting behavior as the computational budget increases. A limit theorem established for this model supports and extends a fairly well known efficiency principle, proposed by J. M. Hammersley and D. C. Handscomb: “The efficiency of a Monte Carlo process may be taken as inversely proportional to the product of the sampling variance and the amount of labour expended in obtaining this estimate.”", "e:keyword": ["Simulation", "Efficiency: definitions and asymptotic theory", "Simulation", "Statistical analysis: asymptotic efficiency", "Statistics", "Estimation: efficiency of simulation estimators"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.521", "e:abstract": "Concerns about American manufacturing competitiveness compel new interest in alternative production control strategies. In this paper, we examine the behavior of push and pull production systems in an attempt to explain the apparent superior performance of pull systems. We consider three conjectures: that pull systems have less congestion; that pull systems are inherently easier to control; and that the benefits of a pull environment owe more to the fact that WIP is bounded than to the practice of “pulling” everywhere. We examine these conjectures for analytically tractable models. In doing so, we not only find supporting evidence for our surmises but also identify a control strategy that has push and pull characteristics and appears to outperform both pure push and pure pull systems. This hybrid system also appears to be more general in its applicability than traditional pull systems such as Kanban.", "e:keyword": ["Inventory/production: Kanban and other pull systems", "Production/scheduling: stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.533", "e:abstract": "In this paper, we address the design of unidirectional loop network layouts for automated manufacturing environments. We show that the problem is NP-complete. Using a workstation interchange argument, we develop dominance relationships for easy identification of local optimal solutions. Our results suggest the need for simple heuristics to solve the problem. We identify cases in which the heuristics perform optimally, and we analyze the worst case behavior of the heuristics. We develop an optimal branch-and-bound procedure that is computationally efficient for medium-sized problems. We also present a decomposition principle helpful for dealing with large workflow matrices. We report computational results on the heuristics and the branch-and-bound procedure. We show that one of the heuristics performs impressively well in terms of solution quality and computational time requirements.", "e:keyword": ["Facilities/equipment planning: facilities planning for manufacturing systems", "Manufacturing", "Layout of automated systems", "Manufacturing", "Automated systems: layout of flexible manufacturing systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.551", "e:abstract": "This study is motivated by a make-to-order marketing environment where an order is met from a single production lot size. The objective of the study is to endogenize rework and scrap decisions in a multistage production process. A Markov decision process model is developed and solved using dynamic programming techniques. The model assumes that demand is given, and material, processing and rework costs are linear in the production lot size. Modeling random yield at each stage of the production process is of key interest. The solution to the problem is characterized and the sensitivity of the solution to the parameters of the model is examined.", "e:keyword": ["Dynamic programming/optimal control", "Applications: multistage manufacturing", "Inventory/production: rework and scrap policies in multistage stochastic systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.564", "e:abstract": "In this paper, we study how the uncapacitated facility location problem is transformed into a two-stage stochastic program with recourse when uncertainty on demand, selling prices, production and transportation costs are introduced. We then present a dual-based procedure and indicate how the dual-descent and primal-dual adjustment procedures proposed by D. Erlenkotter (1978) in the static case can be made monotonically improving in the stochastic case. Results of computer experiments are reported.", "e:keyword": ["Facilities/equipment planning", "Location", "Stochastic: a dual-based algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.574", "e:abstract": "We consider a natural probabilistic variation of the classical vehicle routing problem (VRP), in which demands are stochastic. Given only a probabilistic description of the demand we need to design routes for the VRP. Motivated by applications in strategic planning and distribution systems, rather than resolving the problem when the demand becomes known, we propose to construct an a priori sequence among all customers of minimal expected total length. We analyze the problem using a variety of theoretical approaches. We find closed-form expressions and algorithms to compute the expected length of an a priori sequence under general probabilistic assumptions. Based on these expressions we find upper and lower bounds for the probabilistic VRP and the VRP re-optimization strategy, in which we find the optimal route at every instance. We propose heuristics and analyze their worst case performance as well as their average behavior using techniques from probabilistic analysis. Our results suggest that our approach is a strong and useful alternative to the strategy of re-optimization in capacitated routing problems.", "e:keyword": ["Networks/graphs: stochastic applications", "Probability: stochastic model applications", "Transportation", "Vehicle routing: stochastic vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.586", "e:abstract": "In the problem of scheduling a single machine to minimize total late work, there are <i>n</i> jobs to be processed for which each has an integer processing time and a due date. The objective is to minimize the total late work, where the late work for a job is the amount of processing of this job that is performed after its due date. For the preemptive total late work problem, an <i>O</i>(<i>n</i> log <i>n</i>) algorithm is derived. The nonpreemptive total late work problem is shown to be NP-hard, although efficient algorithms are derived for the special cases in which all processing times are equal and all due dates are equal. A pseudopolynomial dynamic programming algorithm is presented for the general nonpreemptive total late work problem; it requires <i>O</i>(<i>n</i>UB) time, where UB is any upper bound on the total late work. Computational results for problems with up to 10,000 jobs are given.", "e:keyword": ["Production/scheduling: single machine scheduling", "Programming", "Integer", "Algorithms: dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.596", "e:abstract": "The problem of simultaneous determination of optimal due dates and optimal sequence for <i>N</i>-job single machine problem with multiple due dates is considered in this paper. The penalty for a job is assumed to be a linear function of the due date and the earliness/tardiness for the job. The objective is to minimize the total penalty for all jobs. An efficient optimal algorithm to solve the problem is developed and several results are provided.", "e:keyword": ["Production/scheduling: single machine scheduling", "Deterministic"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.603", "e:abstract": "A base-stock policy is shown to be optimal when a dynamic version of the “news vendor” model is optimized with respect to an exponential utility criterion.", "e:keyword": ["Dynamic programming/optimal control: risk-sensitive inventory model", "Inventory/production", "Uncertainty", "Stochastic: exponential utility", "Utility/preference: exponential utility in inventory model"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.609", "e:abstract": "In the combination of forecasts, weighted averages that attempt to take into account the accuracy of the forecasts and any dependence among forecasts tend to perform poorly in practice. An important factor influencing this performance is the sensitivity, or instability, of the estimated weights used to generate the combined forecast. The intent of this paper is to look at this instability via graphs and the sampling distribution of the weights. Results are developed for the combination of two forecasts and extended to the <i>m</i>-forecast case by viewing the <i>m</i>-forecast case as a sequence of two-forecast combinations.", "e:keyword": ["Forecasting: combining forecasts"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.614", "e:abstract": "This paper considers the Economic Lot Scheduling Problem with Reducible Setup Times (ELSP-RS), that is, determining a multiple product, single facility cyclic schedule to minimize holding and setup costs when setup times can be reduced, at the expense of setup costs, by externalizing setup operations. We develop an efficient algorithm that finds an optimal or near-optimal production schedule. Computational results indicate that dramatic savings are possible for highly utilized facilities.", "e:keyword": ["Production/scheduling: economic lot sizing problem", "Production/scheduling", "Approximations/heuristic: setup time reduction"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.620", "e:abstract": "Discrete optimal control theory is used to develop an efficient noniterative algorithm for solving the multiproduct production and work force planning problems with a quadratic cost function. The quadratic cost models allow uncertainties to be handled directly because they minimize the expected cost if unbiased expected demand forecasts are given. A real-world problem may involve as many as 200,000 variables. The noniterative algorithm makes the computations, irrespective of the number of products, not only feasible but also extremely easy and efficient.", "e:keyword": ["Inventory/production", "Production smoothing", "And linear decision rules", "Programming", "Nonlinear programming algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s183", "e:abstract": "By far the most common planning procedure found in practice is to approximate the solution to an infinite horizon problem by a series of rolling finite horizon solutions. Although many empirical studies have been done, this so-called rolling horizon procedure has been the subject of few analytic studies. We provide a cost error bound for a general rolling horizon algorithm when applied to infinite horizon nonhomogeneous Markov decision processes, both in the discounted and average cost cases. We show that a Doeblin coefficient of ergodicity acts much like a discount factor to reduce this error. In particular, we show that the error goes to zero for any <i>fixed</i> rolling horizon as this Doeblin measure of control over the future decreases. The theory is illustrated through an application to vehicle deployment.", "e:keyword": ["Dynamic programming", "Markov infinite state: rolling horizon solution of nonhomogeneous MDPs"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s195", "e:abstract": "We consider network optimization problems in which the weights of the edges are random variables. We develop conditions on the combinatorial structure of the problem which guarantee that the objective function value is a first passage time in an appropriately constructed continuous time Markov chain. The arc weights must be distributed exponentially, the method of solution of the deterministic problem must be greedy in a general sense, and the accumulation of objective function value during the greedy procedure must occur at a constant rate. We call these structures <i>constant access systems</i> after the third property. Examples of constant access systems include the shortest path system, the longest path system, the time until disconnection in a network of failing components, and some bottleneck optimization problems. For each system, we give the distribution of the objective function, the distribution of the solution of the problem, and the probability that a given arc is a member of the optimal solution. We also provide easily implementable formulas for the moments of each optimal objective function value, as well as criticality indices for each arc.", "e:keyword": ["Networks/graphs", "Flow algorithms: stochastic flow networks", "Networks/graphs", "Stochastic: networks with random arc lengths"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s210", "e:abstract": "We consider the problem of optimally meeting a stochastically growing demand for capacity over an infinite horizon. Under the assumption that demand for product follows either a nonlinear Brownian motion or a non-Markovian birth and death process, we show that this stochastic problem can be transformed into an equivalent deterministic problem. Consistent with earlier work by A. Manne, the equivalent problem is formed by replacing the stochastic demand by its deterministic trend and discounting all costs by a new interest rate that is smaller than the original, in approximate proportion to the uncertainty in the demand.", "e:keyword": ["Facilities/equipment planning: capacity expansion", "Probability: stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s217", "e:abstract": "R. Larson proposed a method to statistically infer the expected transient queue length during a busy period in <i>O</i>(<i>n</i><sup>5</sup>) solely from the <i>n</i> starting and stopping times of each customer's service during the busy period and assuming the arrival distribution is Poisson. We develop a new <i>O</i>(<i>n</i><sup>3</sup>) algorithm which uses these data to deduce transient queue lengths as well as the waiting times of each customer in the busy period. We also develop an <i>O</i>(<i>n</i>) on-line algorithm to dynamically update the current estimates for queue lengths after each departure. Moreover, we generalize our algorithms for the case of a time-varying Poisson process and also for the case of i.i.d. interarrival times with an arbitrary distribution. We report computational results that exhibit the speed and accuracy of our algorithms.", "e:keyword": ["Queues: statistical inference", "Algorithms", "Transient results"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s229", "e:abstract": "We present an algorithm for numerically finding the limiting distribution of the number in the system for the bulk-arrival, multiserver queueing system <i>M<sup>X</sup></i>/<i>D</i>/<i>c</i>. Sample numerical results and graphs of various quantities of interest are also presented. In all cases, the proposed method is computationally efficient, accurate and reliable for both high and low values of the model parameters. The procedure is adaptable to other queueing models in discrete and continuous time, to problems in inventory control, the theory of dams, etc. Exact results found here will also be found useful by people dealing with inequalities, bounds, and approximations.", "e:keyword": ["Queues: algorithms", "Approximations", "Bulk", "Exact solution", "Multiserver", "Roots"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s239", "e:abstract": "Motivated by modeling manufacturing systems in which job arrivals and processing times are essentially deterministic, but the environment is typically random, we develop a fluid model with random disruptions. The quality and relevance of such a model are supported by the following facts which we establish in this study. The fluid model is more susceptible to analysis: Its (dynamical) sample paths are continuous and piecewise linear, and its stationary behavior can be studied using standard approaches in random walk; the model is a limit of corresponding queueing systems with random disruptions that are usually difficult to analyze; there exist pathwise bounds between the fluid model and <i>D</i>/<i>D</i>/1 queues with random disruptions, and the bounds are simple and tight.", "e:keyword": ["Probability", "Stochastic model applications: systems with random disruptions", "Production", "Applications: manufacturing in a random environment", "Queues: fluid model for queues with random disruptions"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s248", "e:abstract": "We develop iterative computational formulas for the steady-state probabilities of an exponential single-channel repair facility with multiple Poisson sources and a dynamic return policy. Such facilities occur as part of multi-echelon repairable item provisioning systems in which backorders are filled according to need instead of FIFO or SIRO policies. Some computational examples are given which show the difference in system performance. Our analysis is an example of the state-reduction computational method for Markov chains. Our implementation makes use of taboo sets and taboo probabilities which have physically meaningful interpretations.", "e:keyword": ["Inventory/production", "Theory", "Multi-echelon with backorder policies", "Probability", "Computational: algorithm for structured Markov chain", "Queues", "Theory: priority machine repair problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s257", "e:abstract": "Motivated by queues with service interruptions, we consider an infinite-capacity storage model with a two-state random environment. The environment alternates between “up” and “down” states. In the down state, the content increases according to one stochastic process; in the up state, the content decreases according to another stochastic process. We describe the steady-state behavior of this system under assumptions on the component stochastic elements. For the special case of deterministic linear flow during the up and down states, we show that the steady-state content is directly related to the steady-state workload or virtual waiting time in an associated G/G/1 queue, thus supplementing the results of D. P. Gaver, Jr., and R. G. Miller, Jr. (1962), R. G. Miller, Jr. (1963) and H. Chen and D. D. Yao (1992).", "e:keyword": ["Inventory/production", "Operating characteristics: a fluid model with alternating up and down times", "Probability", "Regenerative processes: a storage model in a two-state random environment", "Queues", "Approximations: a fluid model with random disruptions"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s263", "e:abstract": "In this paper, we consider the problem of scheduling <i>n</i> tasks on two processors. The processing times of the <i>n</i> tasks are i.i.d. exponential random variables. The precedence constraints among the <i>n</i> tasks form an in-tree. The two processors are subject to failure and repair in a completely arbitrary manner, but are independent of the task processing times. We introduce the concept of stochastic partial ordering on random in-trees and show that among all policies, the highest level first (HLF) policy produces the smallest in-tree of unfinished tasks under the stochastic partial ordering. This implies that the HLF policy stochastically minimizes the makespan even when the two processors are subject to failures and repairs. As a special case, we also show that the HLF policy minimizes the dynamic failure probability when the processors are subject to failure, but no repairs can be done.", "e:keyword": ["Production/scheduling", "Stochastic: scheduling with in-tree precedence constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s272", "e:abstract": "We analyze an M/G/1 queue with generalized vacations and exhaustive service. This system has been shown to possess a stochastic decomposition property. That is, the customer waiting time in this system is distributed as the sum of the waiting time in a regular M/G/1 queue with no vacations and the additional delay due to vacations. In this paper, a general formula for the additional delay is derived for a wide class of vacation policies. The formula is also extended to cases with multiple types of vacations. Using these new formulas, existing results for certain vacation models as well as head-of-line priority queues are easily rederived and unified. More importantly, they enable us to obtain the waiting times for many complex vacation policies, which would otherwise be difficult to analyze. These new results are also applicable to other related queueing models, if they conform with the basic model considered in this paper.", "e:keyword": ["Queues: busy period analysis", "Stochastic decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s284", "e:abstract": "The cost of producing electric power at a given time depends on the demand and the set of generating units that are available. We present a Markovian model of the generation system together with a deterministic, time-varying demand function that yields a stochastic integral for the production cost over a time interval. The variance of this integral may be computed exactly by enumerating states. An expression for the integrand is developed in which the deterministic time variation is decoupled from the stochastic variation. This expression is amenable to an asymptotic approximation that reduces the computation. When system state transition rates are high, little accuracy is lost. The exact and approximate results are compared for several parameter values in a small example system.", "e:keyword": ["Industries", "Electric: variance of production costs", "Probability", "Stochastic model applications: variance of a stochastic integral with a deterministic input"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s293", "e:abstract": "In many multiclass queueing systems, certain performance measures of interest satisfy <i>strong conservation laws</i>. That is, the total performance over all job types is invariant under any nonidling service control rule, and the total performance over any subset (say <i>A</i>) of job types is minimized or maximized by offering absolute priority to the types in <i>A</i> over all other types. We develop a formal definition of strong conservation laws, and show that as a necessary consequence of these strong conservation laws, the state space of the performance vector is a (base of a) polymatroid. From known results in polymatroidal theory, the vertices of this polyhedron are easily identified, and these vertices correspond to absolute priority rules. A wide variety of multiclass queueing systems are shown to have this polymatroidal structure, which greatly facilitates the study of the optimal scheduling control of such systems. When the defining set function of the performance space belongs to the class of generalized symmetric functions, additional applications are discussed, which include establishing convexity and concavity properties in various queueing systems.", "e:keyword": ["Production/scheduling", "Stochastic: combinatorial optimization", "Queues: conservation laws and polymatroids"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s300", "e:abstract": "Several researchers have reported difficulties in analyzing the behavior of single queues and networks of queues. This is so even in the case of closed product-form networks, for which an exact solution and efficient solution algorithms are known. The difficulty arises because the exact solution could not, by itself, be used for such analysis as proving properties of the network, relating performance measures to one another, and characterizing some interesting behavior. This paper proposes an approach to surmounting such difficulties. The idea is to analyze an approximate solution based on Schweitzer's approximation, and interpret the results as approximate relationships among the exact performance measures. This approach is applied to three problems concerning the interaction among job classes, the mean arrival and variance of queue length, and thrashing. The reliability of the approach is tested by applying it to an optimal routing problem, for which the exact solution is known. The results are illustrated with problems drawn from computer systems.", "e:keyword": ["Computers/computer science", "System operation: behavior analysis", "Queues", "Networks: closed product-form networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s312", "e:abstract": "Motivated by a factory scheduling problem, we consider the problem of input control (subject to a specified input mix) and priority sequencing in a multistation, multiclass queueing network with general service time distributions and a general routing structure. The objective is to minimize the long-run expected average number of customers in the system subject to a constraint on the long-run expected average output rate. Under balanced heavy loading conditions, this scheduling problem can be approximated by a control problem involving Brownian motion. Linear programming is used to reduce the workload formulation of this control problem to a constrained singular control problem for a multidimensional Brownian motion. The finite difference approximation method is then used to find a linear programming solution to the latter problem. The solution is interpreted in terms of the original queueing system to obtain an effective scheduling policy. The priority sequencing policy is based on dynamic reduced costs from a linear program, and the workload regulating input policy releases a customer into the system whenever the workload process enters a particular region. An example is provided that illustrates the procedure and demonstrates its effectiveness.", "e:keyword": ["Production/scheduling: priority sequencing in a stochastic job shop", "Queues: Brownian models of network scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s335", "e:abstract": "This paper considers optimal public policies regarding R&D programs in a random environment. For a stochastic R&D decision model without rivalry, we investigate and derive the privately and the socially optimal policies. The study focuses on the socially optimal R&D program and its inducement by governmental incentives. The appropriate instruments that should be employed in supporting R&D projects are examined. Our proposed R&D model provides a theoretical economic justification for public intervention in support of private R&D activities. Furthermore, some of the results shed light on practical issues in designing a functional and efficient R&D project support system.", "e:keyword": ["Government: subsidies for R&D programs", "Research and development: private and social program selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s338", "e:abstract": "Customers on an <i>S</i>-server queue with exponential service times face losses due to waiting that are proportional to waiting time, with different loss rates. Customers are otherwise identical. They have complete knowledge of each other's loss rates. Instead of bribing a queue manager for priority assignment, they buy and sell queue positions among themselves. It is shown that the resulting market in queue positions optimally allocates waiting time. The transactions that can occur are completely characterized, including balking and reneging rules.", "e:keyword": ["Queues", "Balking and reneging: bribing among customers", "Queues", "Optimization: market allocation of waiting time", "Queues", "Priority: customer trading in queue position"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s342", "e:abstract": "Let ρ be the traffic intensity of an open queueing system, and let <i>f</i>(ρ), 0 ≤ ρ < 1 be a function, such as an average queue length or sojourn time. A relationship between the light and heavy traffic limits of <i>f</i> is found that is asymptotically exact for high-order light traffic limits (derivatives at ρ = 0). This simple but unexpected result provides a natural method for approximating <i>f</i> based on partial information. The approximation it provides turns out to be identical to the “canonical” interpolation approximation based on the same partial information. Relationships are then derived that correspond to noncanonical interpolation approximations. These relationships may he useful for gauging the accuracy of interpolation approximations.", "e:keyword": ["Queues", "Approximations", "Interpolation approximations", "Queues", "Limit theorems", "Light traffic and heavy traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s346", "e:abstract": "We consider a continuous-time search model with <i>n</i> random offer streams forming independent renewal processes. The distribution of the offer size may depend on the arrival time and the type of the offer, and there is a constant search cost per unit time. The searcher wants to select an offer such that the expected discounted reward is maximized. We present a method for computing such a strategy and apply it to some numerical examples. In the case of Poisson offer streams a more direct approach is also given.", "e:keyword": ["Dynamic programming/optimal control", "Models: optimal stopping for superimposed renewal processes", "Search and surveillance: continuous-time job search model"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.3.s352", "e:abstract": "In establishing the seminal result PASTA (Poisson Arrivals See Time Averages), R. Wolff (1982) constructed a martingale, and demonstrated that PASTA was a consequence of a strong law of large numbers of this martingale. Here we establish a central limit theorem for the PASTA martingale, and characterize its asymptotic normality. The result can be used to construct confidence intervals for estimators of the difference between the arrival (event) and time averages.", "e:keyword": ["Queues: PASTA", "Martingale central limit theorem"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.633", "e:abstract": "As in any science, day-to-day work in operations research involves diverse craft skills. This informal essay discusses some early examples, based on the work of W. J. Youden, one of the unsung heroes of OR work during World War II. It is based on a banquet talk given September 5, 1991 at the Eighth International Symposium on Military Operational Research held at the Royal Military College of Science, Shrivenham, United Kingdom.", "e:keyword": ["Professional: craft practices", "Address", "History"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.640", "e:abstract": "This paper highlights the methodologies, organizations, and applications of military operations research in practice as evidenced by presentations at ORSA/TIMS meetings over the last seven and a half years. Cross-tabulations address the questions of who does what kind of military OR, what methodologies are used in what applications, and what trends are evident.", "e:keyword": ["Military: practice of military OR", "Professional", "OR/MS education: military OR practice", "Professional", "OR/MS implementation", "Military OR practice"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.647", "e:abstract": "A model reflecting the evolution of an engagement between an integrated air defense system (IADS) and a penetrating strike group is presented. The engagement is modeled as an optimization problem on a network with stochastic arc lengths. We produce the distribution of our measure of effectiveness, as well as calculating the importance of each IADS agent to the performance of the overall system. We demonstrate the effectiveness of several jamming plans against the network.", "e:keyword": ["Military: tactics and strategy", "Probability: Markov processes and stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.660", "e:abstract": "Variable and row aggregation as a technique of simplifying a mathematical program is utilized to develop bounds for two-stage stochastic convex programs with random right-hand sides. If one is able to utilize the problem structure along with only first moment information, a tighter bound than the usual mean model bound (based on Jensen's inequality) may be obtained. Moreover, it is possible to construct examples for which the mean model bound will be arbitrarily poor. Consequently, one can tighten Jensen's bound for stochastic programs when the distribution has a compact support. This bound may be improved further by partitioning the support using conditional first moments. With regard to first moment upper bounds, the Gassmann-Ziemba inequality is used for the stochastic convex program to seek a model which can be solved using standard convex programming techniques. Moreover, it allows one to easily construct upper bounds using the solution of the lower bounding problem. Finally, the results are extended to multistage stochastic convex programming problems.", "e:keyword": ["Probability", "Stochastic model: approximations in stochastic programming", "Programming", "Stochastic: bounds for stochastic convex programs"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.678", "e:abstract": "This paper addresses the problem of interconnecting a group of Local Area Networks (LANs) with bridges. The network designer would like to minimize the cost of connecting the LANs while maintaining acceptable traffic intensity levels on each of the LANs and the bridges. This problem belongs to the class of fixed charge network flow problems. A Lagrangian relaxation algorithm is proposed that incorporates two sets of constraints into the objective function. The relaxed problem has a special structure and can be solved as a minimum spanning tree problem and a set of shortest path problems. The subgradient algorithm is then used to maximize the Lagrangian dual. A Lagrangian heuristic algorithm is also proposed to find good primal feasible solutions. For problems with five LANs, the heuristic solutions are normally within 1% of the optimum. For larger problems the heuristic solutions are close to the lower bounds generated by the Lagrangian relaxation algorithm. This suggests that the quality of the heuristic solutions does not deteriorate as the dimension of the problem grows.", "e:keyword": ["Facilities/equipment planning: designing LAN internetworks", "Networks/graphs: modeling an LAN-internetwork as a graph", "Programming", "Integer: Lagrangian relaxation heuristic algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.689", "e:abstract": "The cost of transmission capacity constitutes a significant portion of the total investment cost of a backbone computer communications network. In this paper, we address the problem of deciding where, when and how much transmission capacity should be installed, over a <i>multiperiod horizon</i>, to meet increasing traffic requirements at minimum total discounted cost, while maintaining acceptable performance levels. The model allows traffic among existing nodes to increase, new nodes to be added to the network, and its topology to change, over time. It is formulated as an integer programming problem, and a Lagrangian relaxation based solution method is proposed. Capacity and routing decisions are made jointly over time in our multiperiod model. Furthermore, our method automatically provides numerical verification of solution quality through the Lagrangian lower bound. Computational experiments with several networks show that the method yields verifiably good solutions to this combinatorially explosive problem.", "e:keyword": ["Communications: backbone computer networks", "Facilities/equipment planning", "Capacity expansion: discrete capacity with joint routing over time", "Programming", "Integer: applications", "Relaxation/subgradient"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.706", "e:abstract": "In this paper, we approximate moments of the sojourn time distribution in open networks of priority queues via an interpolation approximation. The interpolation is constructed from five random variables (and their covariance matrix) that are simultaneously estimated from a single regenerative simulation of the system at any arrival rate. The random variables are consistent estimates of the zeroth and first-order light traffic limits, the heavy traffic limit, and the value of the function and its derivative at the arrival rate of the simulation. The data yield a least squares estimate of a normalized version of the original function, which, in turn, yields an estimate of the function of interest. This procedure is useful for complex systems because the light and heavy traffic limits needed for an interpolation are not easy to calculate analytically in those cases.", "e:keyword": ["Simulation", "Estimating gradients and limits", "Simulation", "Interpolation approximations: simulation based"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.724", "e:abstract": "Motivated by make-to-stock production systems, we consider a scheduling problem for a single server queue that can process a variety of different job classes. After jobs are processed, they enter a finished goods inventory that services customer demand. The scheduling problem is to dynamically decide which job class, if any, to serve next in order to minimize the long-run expected average cost incurred per unit of time, which includes linear costs (which may differ by class) for backordering and holding finished goods inventory. Under the heavy traffic condition that the server must be busy the great majority of the time in order to satisfy customer demand, the scheduling problem is approximated by a dynamic control problem involving Brownian motion. The Brownian control problem is solved, and its solution is interpreted in terms of the queueing system to obtain a scheduling policy. A simulation experiment is performed that demonstrates the policy's effectiveness.", "e:keyword": ["Inventory/production: multistage stochastic", "Production/scheduling: sequencing in a stochastic system", "Queues: Brownian models of scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.736", "e:abstract": "In this paper, we formulate the cyclic lot scheduling problem on a single facility with sequence-dependent setup costs and times and provide a heuristic solution procedure. A Lagrangian relaxation of the formulation leads to a partial separation of the embedded lot sizing and traveling salesman problems. The relaxation results in a new combinatorial problem related to the minimum spanning tree problem. The information about frequency of production, gained from the solution to this relaxation, is used to find heuristic solutions for the entire problem. Computational work on randomly generated problems shows that the problem becomes more difficult as the diversity in the sequence-dependent parameters increases.", "e:keyword": ["Manufacturing", "Performance/productivity: cyclic lot scheduling with sequence-dependent setups", "Production/scheduling", "Sequencing", "Deterministic: batching with sequence-dependent setups"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.750", "e:abstract": "We consider a situation in which the manufacturing system is equipped with batch and discrete processors. Each batch processor can process a batch (limited number) of jobs simultaneously. Once the process begins, no job can be released from the batch processor until the entire batch is processed. In this paper, we analyze a class of two-machine batching and scheduling problems in which the batch processor plays an important role. Specifically, we consider two performance measures: the makespan and the sum of job completion times. We analyze the complexity of this class of problems, present polynomial procedures for some problems, propose a heuristic, and establish an upper bound on the worst case performance ratio of the heuristic for the NP-complete problem. In addition, we extend our analysis to the case of multiple families and to the case of three-machine batching.", "e:keyword": ["Manufacturing: automated systems", "Production/scheduling: algorithms and heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.764", "e:abstract": "In this paper, we study the problem of scheduling semiconductor burn-in operations, where burn-in ovens are modeled as batch processing machines. A batch processing machine is one that can process up to <i>B</i> jobs simultaneously. The processing time of a batch is equal to the largest processing time among all jobs in the batch. We present efficient dynamic programming-based algorithms for minimizing a number of different performance measures on a single batch processing machine. We also present heuristics for a number of problems concerning parallel identical batch processing machines and we provide worst case error bounds.", "e:keyword": ["Dynamic programming/optimal control", "Applications: dynamic programming applications", "Production/scheduling", "Sequencing", "Deterministic: semiconductor manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.776", "e:abstract": "This paper concerns dynamic part dispatch decisions in electronic test systems with random yield. A discrete time, multiproduct, miltistage production system is used as a model for the test system with the objective to minimize the sum of inventory holding, backlogging, and overtime costs over a finite horizon. Exact results for such systems have been limited to either single-stage, multiple time period, or multistage, single time period problems with a single product. Here we develop two approximate policies: the linear decision rule, and the myopic resource allocation. The effectiveness of the two policies is evaluated through simulation under different operating conditions representative of those encountered in IBM and Tandem Computer facilities. The extensive computational study clearly demonstrates the overall superiority of the linear decision rule.", "e:keyword": ["Dynamic programming/optimal control", "Applications: linear decision rule", "Industries", "Computers/electronics: PCB testing", "Semi-conductor fabrication", "Inventory/production", "Multi-item", "Echelon", "Stage: dispatch policies for uncertain yield systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.790", "e:abstract": "We analyze the vehicle routing problem with constraints on the total distance traveled by each vehicle. Two objective functions are considered: minimize the total distance traveled by vehicles and minimize the number of vehicles used. We demonstrate a close relationship between the optimal solutions for the two objective functions and perform a worst case analysis for a class of heuristics. We present a heuristic that provides a good worst case result when the number of vehicles used is relatively small.", "e:keyword": ["Networks/graphs", "Heuristics: worst case analysis", "Transportation", "Vehicle routing: distance constrained vehicle routing problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.800", "e:abstract": "In a recent article, the author presented results for the optimum levels of inventory to dispose and keep in an excess inventory situation. This note shows that these levels do not change when additional disposal opportunities are presented in the future. In this sense, the optimal policy is myopic in that the decision can be made without any examination of future decisions.", "e:keyword": ["Inventory/production: perishable/aging items"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.804", "e:abstract": "We explore analytically cardinal effects of the extent of demand randomness on optimal inventory levels and the associated expected costs. To model changes in demand randomness, we make extensive use of a mean-preserving transformation commonly used in probabilistic microeconomics, as well as the notion of risk-pooling (aggregating independent demands). For the single period (news vendor) model, the order quantity and associated costs are shown to depend on the randomness parameter in a simple and highly interpretable manner. We show by a simple example that in some situations risk pooling might neither reduce inventories, nor move them closer to the mean or median demand. The managerial relevance of such analyses is discussed.", "e:keyword": ["Inventory/production: effects of demand randomness", "Risk pooling"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.808", "e:abstract": "The reorder point/reorder quantity policies, also referred to as (<i>r</i>, <i>Q</i>) policies, are widely used in industry and extensively studied in the literature. However, for a period of almost 30 years there has been no efficient algorithm for computing optimal control parameters for such policies. In this paper, we present a surprisingly simple and efficient algorithm for the determination of an optimal (<i>r</i>*, <i>Q</i>*) policy. The computational complexity of the algorithm is linear in <i>Q</i>*. For the most prevalent case of linear holding, backlogging and stockout penalty costs (in addition to fixed order costs), the algorithm requires at most (6<i>r</i>* + 13<i>Q</i>*) elementary operations (additions, comparisons and multiplications), and hence, no more than 13 times the amount of work required to do a single evaluation of the long-run average cost function in the point (<i>r</i>*, <i>Q</i>*).", "e:keyword": ["Inventory/production: stochastic policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.4.813", "e:abstract": "This note treats optimization problems that involve two criteria on set systems. One is a bottleneck criterion, and the other is general. In particular, recent algorithms for such problems on the edge-sets of networks are related to the fundamental work of J. Edmonds and D. R. Fulkerson involving more general set systems.", "e:keyword": ["Mathematics", "Combinatorics", "Clutters: networks/graphs", "Algorithms", "Programming: bottleneck objective with a side-constraint"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.1018", "e:abstract": "A criterion called “improving faster than” is proposed and characterized to compare the strength or preference of value functions. The result can be applied in decision models where no risk is involved or, at least, explicitly modeled. A bargaining theory example illustrates the criterion.", "e:keyword": ["Decision analysis: theory", "Games/group decisions: bargaining"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.831", "e:abstract": "This paper proposes the term perishable-asset revenue management to denote the field that combines the areas of yield management, overbooking, and pricing for perishable assets. After summarizing the characteristics common to problems in this field, the paper discusses the objectives and constraints faced by decision makers. Then it offers a comprehensive taxonomy with 14 different elements and reviews the research that has been done related to each element. Finally, it suggests some important areas of future research that can help bridge the gap between theory and application.", "e:keyword": ["Inventory/production", "Perishable/aging items: problem taxonomy yield including management and overbooking", "Marketing", "Pricing", "Profit optimization from perishable assets", "Marketing", "Segmentation: price-sensitive customers of perishable assets"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.845", "e:abstract": "The evaluation and the allocation of inspection resources is a problem faced by private sector firms as well as by several government agencies. This paper reports the development and the use by the Food and Drug Administration (FDA) of decision making tools for measuring the performance of their drug inspection activities in which data are collected only intermittently, and a multiobjective integer program to assist FDA decision makers in planning and evaluating the effectiveness of their drug inspection activities.", "e:keyword": ["Government", "Agencies: regulating inspections", "Programming", "Multiple criteria: multiobjective integer program", "Decomposed and heuristics", "Statistics", "Data analysis: multiple logistic regression and index"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.856", "e:abstract": "Analysis of data about the 58,000 Americans killed in Vietnam implies that affluent U.S. communities had only marginally lower casualty rates than the nation as a whole. Poor communities had only marginally higher rates. Data about the residential addresses of war casualties suggest that, within both large heterogeneous cities and wealthy suburbs, there was little relationship between neighborhood incomes and per capita Vietnam death rates. Such outcomes call into question a widespread belief that continues to influence U.S. policy discussions, namely, that American war deaths in Vietnam were overwhelmingly concentrated among the poor and working class.", "e:keyword": ["Military", "Personnel", "Organizational studies: manpower planning", "Statistics: data analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.867", "e:abstract": "Following an outline of dynamic Markov fields, we briefly describe some spatial models for contagious diseases and pose a prototype epidemic control problem. The notion of automatic learning is then introduced, and its relevance to epidemic control is described. In essence, once a contagion model is adopted and a domain of controls has been selected, learning can be used to obtain asymptotically optimal performance. (The learning algorithm is a synthesis of simulation and optimization, and is a suitable alternative to response surface methodology, in many applications.) The end product is the same optimal control as would be obtained by a conventional analysis. The point is that our current understanding of dynamic Markov fields does not permit conventional analysis; automatic learning has no computationally competitive alternative. The theory is illustrated by application to a spatial epidemic control problem.", "e:keyword": ["Computers/computer science: artificial intelligence", "Health care: epidemiology", "Probability: stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.877", "e:abstract": "An inspection agreement, contained within a treaty proposal, determines a two-person, zero sum game which we call the implicit game. The value of the implicit game, called the upper risk, is an important parameter of the agreement. The upper risk and other parameters in the solution of the implicit game are useful for evaluating the proposal and comparing it to other proposals. The purpose of this paper is to define the implicit game which arises from an inspection arrangement, define the upper risk and other parameters, and then to illustrate the theory with examples, several of which originated in the analysis of actual inspection proposals.", "e:keyword": ["Games/group decisions: evaluation of inspection agreements", "Military", "Search/surveillance: models of inspection"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.885", "e:abstract": "Experience with solving a 12.753.313 variable linear program is described. This problem is the linear programming relaxation of a set partitioning problem arising from an airline crew scheduling application. A scheme is described that requires successive solutions of small subproblems, yielding a procedure that has little growth in solution time in terms of the number of variables. Experience using the simplex method as implemented in <b>CPLEX</b>, an interior point method as implemented in <b>OBI</b>, and a hybrid interior point/simplex approach is reported. The resulting procedure illustrates the power of an interior point/simplex combination for solving very large-scale linear programs.", "e:keyword": ["Programming", "Linear: large-scale systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.898", "e:abstract": "We investigate the small-sample behavior and convergence properties of confidence interval estimators (CIEs) for the mean of a stationary discrete process. We consider CIEs arising from nonoverlapping batch means, overlapping batch means, and standardized time series, all of which are commonly used in discrete-event simulation. The performance measures of interest are the coverage probability, and the expected value and variance of the half-length. We use empirical and analytical methods to make detailed comparisons regarding the behavior of the CIEs for a variety of stochastic processes. All the CIEs under study are asymptotically valid; however, they are usually invalid for small sample sizes. We find that for small samples, the bias of the variance parameter estimator figures significantly in CIE coverage performance—the less bias the better. A secondary role is played by the marginal distribution of the stationary process. We also point out that some CIEs require fewer observations before manifesting the properties for CIE validity.", "e:keyword": ["Simulation", "Statistical analysis: batch means", "Overlapping batch means", "Standardized time series", "Statistics", "Estimation: confidence intervals on the mean", "Standard error", "Statistics", "Time series: confidence intervals on the mean", "Stationary time series"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.914", "e:abstract": "This work presents an upper bound on the expectation of sublinear polyhedral functions of multivariate random variables based on an inner linearization and domination by a quadratic function. The problem is formulated as a semi-infinite program which requires information on the first and second moments of the distribution, but without the need of an independence assumption. Existence of a solution and stability of this semi-infinite program are discussed. We show that an equivalent optimization problem with a nonlinear objective function and a set of linear constraints may be used to generate solutions.", "e:keyword": ["Programming", "Infinite dimensional", "Stochastic: upper bounds on the recourse function", "Functional approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.923", "e:abstract": "This paper describes a randomized algorithm for solving the maximum-flow maximum-cut problem on connected random graphs. The algorithm is very fast—it does not look up most vertices in the graph. Another feature of this algorithm is that it almost surely provides, along with an optimal solution, a proof of optimality of the solution. In addition, the algorithm's solution is, by construction, a collection of vertex-disjoint paths which is maximum. Under a restriction on the graph's density, an optimal solution to the NP-hard communication problem is provided as well, that is, finding a maximum collection of vertex-disjoint paths between sender-receiver pairs of terminals. The algorithm lends itself to a sublogarithmic parallel and distributed implementation. Its effectiveness is demonstrated via extensive empirical study.", "e:keyword": ["Analysis of algorithms", "Computational complexity: sublinear flow algorithms", "Communications: communication problem on random graphs", "Networks/graphs", "Flow algorithms: algorithms for communication and max-flow on random graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.936", "e:abstract": "This paper explores a natural generalization of the classic tandem-queue model, designed specifically to represent make- to-stock production processes. In such systems, intermediate and finished goods can be produced and stored in advance of demand. We consider the simplest version of the model, where demand is a Poisson process, and the unit production times are exponentially distributed. We propose and test a tractable approximation scheme. The approximation appears to be quite accurate.", "e:keyword": ["Inventory/production", "Multistage: queueing models of make-to-stock queues", "Queues", "Approximations", "Tandems with planned inventories"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.948", "e:abstract": "In this paper, we consider the issue of customer service in pull production systems. We first discuss the meaning of customer service in a pull system and contrast it to push systems. We also discuss both pure and hybrid pull systems that are found in the literature. We then investigate the effects of changing inventory levels and processing time characteristics on customer service in a pure kanban system. Finally, we show that a hybrid system known as CONWIP not only has better service than a pure kanban system, but also solves certain implementation problems.", "e:keyword": ["Inventory/production: operating characteristics", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.959", "e:abstract": "We study the problem of selecting the economic lot size for an unreliable manufacturing facility with a constant failure rate and general randomly distributed repair times. Safety stocks must be used to meet the managerially prescribed service level (the fraction of lost sales) because these stochastic interventions reduce the effective production capacity. We develop bounds on the range of feasible service levels and investigate the impact of several system parameters on this range. We introduce an easily implementable production control policy (but do not establish the optimality of its structure) and prove that under this policy the safety stock dynamics can be characterized fully by a renewal process analogous to the workload process of a special single server queueing system. This analogy is exploited in deriving exact and approximate expressions for the safety stock holding costs. Several operational insights are revealed by experimenting with the models developed here. We show how the results can be incorporated in a broader management framework for evaluating resource allocation decisions aimed at reducing the failure rate of the machines. A clear tradeoff is shown to exist between the overall investment in increasing the maintenance level and the resulting savings in safety stocks and repair costs.", "e:keyword": ["Inventory/production", "Uncertainty: impact of breakdowns on batching and safety stocks", "Inventoiy/production", "Lot sizing: with breakdowns and safety stocks", "Reliability", "Maintenance/repairs: batching and safety stocks"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.972", "e:abstract": "The paper presents a formulation of an <i>n</i>-job, <i>m</i>-machine flowshop problem whose objective is to determine a processing sequence of jobs that minimizes total job idleness subject to meeting job deadlines. A mirror image problem is defined with the property that there is a one-to-one correspondence between the feasible schedules of the original problem and the feasible schedules of the mirror image problem. The mirror image problem is a traditional scheduling problem with a regular performance measure, whereas the performance measure in the original problem is not regular. The equivalence of the original problem and its mirror image problem enables us to solve one by solving the other. One special case of the original problem is investigated. It concerns minimization of total job idleness in a 2-machine flowshop. For this NP-hard problem we study permutation schedules under sufficient conditions of feasibility. We present complexity results, dominance properties, bounding criteria, and computational experience with a branch-and-bound procedure.", "e:keyword": ["Inventory/production: inventory minimization through scheduling", "Production/scheduling: multiple machine deterministic sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.986", "e:abstract": "We consider the product design problem of allocating the chip sites on a semiconductor wafer to various types of chips. The manufacturing facility sells chips to its customers in sets (a specified number of several different types of chips), and the objective of the facility is to maximize the average production rate of sets. Variability in the wafer fabrication process, in particular random yield, poses a major obstacle in producing sets in a reliable fashion. A stochastic analysis is employed to develop an effective wafer design, to measure the improvement in performance of the multitype wafer over the traditional single-type wafer, and to determine the cumulative production of sets of nondefective chips as a function of the rate at which lots of wafers are released into the facility. The analysis reveals that multitype wafers regularize the production flow of nondefective chips of each type and cause these flows to be correlated positively, both of which help to improve performance. A numerical example is provided that illustrates the analysis and demonstrates the design's effectiveness.", "e:keyword": ["Inventory/production: random yields", "Probability", "Stochastic model applications: product design"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.5.999", "e:abstract": "In this paper, we model production problems where yields are stochastic, demands are substitutable, and several items are jointly produced. We formulate this problem as a profit maximizing convex program, and study two approximation procedures. The first method solves finite horizon stochastic programs on a rolling horizon basis. We develop a decomposition algorithm for solving the finite horizon problems. The finite horizon problems are linear programs. Our algorithm utilizes the network-like structure of the coefficient matrix of the linear programs. The second method is a heuristic procedure that is based on the structure of the optimal policy for two-period problems. The heuristic parallels the decision rules used by managers in practice. The computational results suggest that the performance of this heuristic is comparable to that of the rolling horizon approach.", "e:keyword": ["Industries: manufacturing of chips", "Inventory/production", "Approximations: approximations and heuristics for production", "Programming", "Stochastic: stochastic programming problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1031", "e:abstract": "Rapid changes in both hardware and software computer technology pose many challenges to quantitative applications, and especially to operations research. Much of the future success of OR will depend on its ability to meet these challenges by marrying its approaches to computer technology and then using this enhanced capability to reach out to important new problem areas. To support the contention that OR is moving in these directions, as it must, this paper begins by reviewing computing progress to date as it relates to OR. Then it views three facets of the expansion of OR: university enrollments (and those on nonresident aliens in particular), application opportunities, and the dispersion of OR people in large organizations. The paper is an outgrowth of a plenary lecture at the May 1991 TIMS/ORSA meeting in Nashville.", "e:keyword": ["Professional", "Addresses: Spring 1991 TIMS/ORSA plenary lecture", "Professional", "Comments on: Spring 1991 TIMS/ORSA plenary lecture", "Professional", "OR/MS education: Spring 1991 TIMS/ORSA plenary lecture"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1040", "e:abstract": "This paper describes the development of a model for making project funding decisions at The National Cancer Institute (NCI). The American Stop Smoking Intervention Study (ASSIST) is a multiple-year, multiple-site demonstration project, aimed at reducing smoking prevalence. The initial request for ASSIST proposals was answered by about twice as many states as could be funded. Scientific peer review of the proposals was the primary criterion used for funding decisions. However, a modified Delphi process made explicit several criteria of secondary importance. A structured questionnaire identified the relative importance of these secondary criteria, some of which we incorporated into a composite preference function. We modeled the proposal funding decision as a zero-one program, and adjusted the preference function and available budget parametrically to generate many suitable outcomes. The actual funding decision, identified by our model, offers significant advantages over manually generated solutions found by experts at NCI.", "e:keyword": ["Decision analysis", "Applications: incorporating decision makers' preferences into optimization model", "Health care: national anti-smoking campaign", "Programming", "Integer", "Applications: optimization model to support funding decisions"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1053", "e:abstract": "The PAIRS method developed in this paper introduces imprecise preference statements into value trees. The assessment of attribute weights in PAIRS extends the well known SMART technique so that in addition to exact statements the decision maker can enter interval judgments which indicate ranges for the relative importance of the attributes. The interval judgments and the possibly range-valued information about the outcomes of the alternatives are processed with linear programming into value intervals and dominance relations. As the decision maker refines the description of his preferences, either by entering new statements or by tightening his earlier judgments, these results become more detailed and convey more information about which alternatives are preferred. Throughout the interactive refinement process PAIRS supports the decision maker by deriving and displaying the consequences of his earlier judgments.", "e:keyword": ["Decision analysis", "Theory", "Ambiguous preferences in value trees", "Utility/preference", "Multiattribute", "Ambiguous preferences in value trees"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1062", "e:abstract": "Bootstrap confidence intervals for the actual cost of using a given nonparametric estimate of the optimal age replacement strategy are shown to have the claimed coverage probability. A numerical algorithm is given to obtain these confidence intervals in practice. The small sample behavior of these confidence intervals is illustrated by simulations. Finally, comparisons are made with the confidence interval obtained from asymptotic normal theory. We show that the bootstrap confidence interval is the one to use in age replacement problems.", "e:keyword": ["Reliability", "Replacement/renewal: confidence interval for the optimal cost in age replacement", "Statistics", "Estimation: confidence interval for the optimal cost in age replacement"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1074", "e:abstract": "A major design issue facing the designer of a distributed computing system involves the determination of the number of file copies and their locations in the distributed environment. This problem is commonly referred to as the file allocation problem (FAP). This paper considers two FAP models that seek to minimize operating costs (i.e., the total cost of file storage and query/update communication). The first model ensures the attainment of acceptable levels of communication delay during peak network traffic periods (worst-case scenario). The second model considers average communication delay. Unlike previous FAP research, the proposed models treat communication delay on a query-by-query basis, and not as a single, system-wide average delay constraint. For both models, a Lagrangian relaxation-based solution procedure is proposed for the resulting 0/1 integer programming problem. In the case of average delays, we utilize a hybrid model combining analytic and simulation procedures. The results of computational experiments with the proposed solution techniques are reported.", "e:keyword": ["Communications: effect of allocation on delays", "Computers/computer science", "Data bases: allocation of files and data base fragments", "Programming", "Relaxation/subgradient: allocation of files and data base fragments"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1086", "e:abstract": "This paper describes a new insertion procedure and a new postoptimization routine for the traveling salesman problem. The combination of the two methods results in an efficient algorithm (<b>GENIUS</b>) which outperforms known alternative heuristics in terms of solution quality and computing time.", "e:keyword": ["Networks/graphs", "Traveling salesman: insertion and postoptimization procedures"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1095", "e:abstract": "In the capacitated vehicle routing problem with unsplit demands, the demand of a customer may not be divided over more than one vehicle. The objective is to find tours for the vehicles such that the amount delivered by a vehicle does not exceed its capacity, each customer receives its demand, and the total distance traveled is as small as possible. We find the asymptotic optimal solution value of the capacitated vehicle routing problem with unsplit demands for any distribution of the demands when customers are independently and identically distributed in a given region. We also develop polynomial-time heuristics for the problem and show that, under certain assumptions on the distribution of customer locations and demands, the heuristics produce solutions that are asymptotically optimal. In addition, we give a tight bound on the rate of convergence for the case when customers are uniformly distributed in a rectangular region.", "e:keyword": ["Programming", "Integer", "Heuristic", "Application: probabilistic analysis of algorithms", "Transportation", "Vehicle routing: probabilistic analysis of algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1107", "e:abstract": "Dynamic allocation of stochastic capacity among competing activities in a just in time manufacturing environment is addressed by optimal flow control. Optimal policies are characterized by generally intractable Bellman equations. A near-optimal controller design technique is proposed. It provides an approximate numerical solution to the Bellman equation, a tight lower bound for the optimality gap of tractable, near-optimal controller designs, and a building block for improved, near-optimal controller designs that rely on the decomposition of a multiple part-type problem to smaller (two or three part-type) problems. Computational experience is reported for two and three part-type problems.", "e:keyword": ["Dynamic programming/optimal control: numerical solution through simulation", "Manufacturing", "Automated systems: production flow control"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1126", "e:abstract": "The system under consideration comprises two classes of customers to be served by two stations, with parallel servers in each station. While class-1 customers can only receive service from station 1, class-2 customers can be served by either station. Arrival processes of customers form two mutually independent Poisson processes. The service time of a customer at either station is exponentially distributed with a common rate. A class-<i>i</i> customer, while present in the system, will incur a holding cost <i>h<sub>i</sub></i> with <i>h</i><sub>1</sub> ≥ <i>h</i><sub>2</sub>. The objective is to dynamically assign customers to idle servers so that the expected discounted (or the long-run average) holding cost is minimized. We show that a class-<i>j</i> customer should be assigned to an idle server in station <i>j</i>, <i>j</i> = 1, 2, whenever possible, and a class-2 customer should be assigned to an idle server in station 1 only if (no class-1 customers are waiting, and) the length of queue 2 exceeds a critical number. Moreover, the critical number is monotonically increasing in the number of busy servers in station 1. The numerical results for some test cases are reported.", "e:keyword": ["Dynamic programming/optimal control: infinite state Markov model", "Manufacturing: strategy in automated manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1139", "e:abstract": "Little is known about the interaction among the various manufacturing subsystems in a factory. The output of one manufacturing subsystem is usually the input to one or more downstream subsystems in the plant. Examples include the output of one production line being the input to another manufacturing stage, to a shipping system, or to an automated part conveyor. In all these examples, the output process of the production line forms the arrival process to the next subsystem. This paper develops a technique to analytically describe the output process of a serial production line of <i>N</i> machines with exponential processing time distributions and finite buffer capacities. Extensive exact results are used to examine the effects of line length, buffer capacity, and buffer placement on the interdeparture distribution, correlation structure, and variability of the output process of the production line. These results are used to determine the extent to which buffer allocation can be used to control the variability of the output process (and thereby the amount of work-in-process required to downstream subsystems). In addition, insights are provided to help explain why small buffers in production lines are normally adequate.", "e:keyword": ["Manufacturing: design and analysis", "Production/scheduling", "Stochastic: model of serial production line", "Queues", "Output processes: tandem queues with blocking"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1148", "e:abstract": "We discuss a single-machine scheduling problem where the objective is to minimize the variance of job completion times. To date, the problem has not been solved in polynomial time. This paper presents a dynamic programming algorithm that is pseudopolynomial in complexity. We also propose a fully polynomial approximation scheme and derive a lower bound that is useful in its implementation. Furthermore, we show that the dynamic programming solution is easy to extend to a bicriteria version of the problem in which it is desired to simultaneously minimize the mean completion time.", "e:keyword": ["Dynamic programming/optimal control: exact and approximate algorithms", "Production/scheduling: minimizing completion time variance", "Programming: scheduling with multiple criteria"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1156", "e:abstract": "This paper describes and compares several methods for computing stationary probability distributions of Markov chains. The main linear algebra problem consists of computing an eigenvector of a sparse, nonsymmetric matrix associated with a known eigenvalue. It can also be cast as a problem of solving a homogeneous, singular linear system. We present several methods based on combinations of Krylov subspace techniques, single vector power iteration/relaxation procedures and acceleration techniques. We compare the performance of these methods on some realistic problems.", "e:keyword": ["Mathematics", "Matrices: direct and iterative methods", "Probability", "Stochastic model applications: numerical solution of", "Queues", "Markovian: large queueing network models"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1180", "e:abstract": "The two most commonly considered reward criteria for Markov decision processes are the discounted reward and the long-term average reward. The first tends to “neglect” the future, concentrating on the short-term rewards, while the second one tends to do the opposite. We consider a new reward criterion consisting of the weighted combination of these two criteria, thereby allowing the decision maker to place more or less emphasis on the short-term versus the long-term rewards by varying their weights. The mathematical implications of the new criterion include: the deterministic stationary policies can be outperformed by the randomized stationary policies, which in turn can be outperformed by the nonstationary policies; an optimal policy might not exist. We present an iterative algorithm for computing an ε-optimal nonstationary policy with a very simple structure.", "e:keyword": ["Decision analysis", "SequentialL: tradeoffs between discounted and long-term average objectives", "Dynamic programming", "Markov", "Finite state: new reward criteria for Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1188", "e:abstract": "We formulate a mixed integer program to determine whether a finite time horizon is a forecast horizon in a nonhomogeneous Markov decision process. We give a Bender's decomposition approach to solving this problem that evaluates the stopping rule, eliminates some suboptimal combinations of actions, and yields bounds on the maximum error that could result from the selection of a candidate action in the initial stage. The integer program arising from the decomposition has special properties that allow efficient solution. We illustrate the approach with numerical examples.", "e:keyword": ["Dynamic programming/optimal control: Markov", "Finite states", "Solving nonhomogeneous problems", "Programming", "Integer: Bender's decomposition", "Applications to forecast horizons"]}, {"@id": "http://dx.doi.org/10.1287/opre.40.6.1200", "e:abstract": "This note reports a multicriteria optimization approach to aircraft loading. Only cargo manifests or “standard loads” provided (or approved) by experienced loadmasters are employed in the optimization procedure. The familiarity of the load manifests that comprise the solution makes the model truly operable. In testing with actual data, the model reduced the number of aircraft loads required to complete an airlift by 9%, as compared to the traditional, manual method presently used. More importantly, the model provides timely planning and improves airlift support of combat operations.", "e:keyword": ["Military: logistics"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.102", "e:abstract": "The empty container allocation problem occurs in the context of the management of the land distribution and transportation operations of international maritime shipping companies. It involves dispatching empty containers of various types in response to requests by export customers and repositioning other containers to storage depots or ports in anticipation of future demands. We describe the problem and identify its basic structure and main characteristics. We then introduce two dynamic deterministic formulations for the single and multicommodity cases, which offer a general modeling framework for this class of problems, and which account for its specific characteristics: the space and time dependency of events, substitutions among container types, relationships with partner companies, imports and exports, massive equilibration flows, etc. Finally, we provide a mathematical formulation for handling, in the single commodity case, the uncertainty of demand and supply data that is characteristic of container allocation and distribution problems. Various modeling choices, data requirements, and algorithmic considerations related to the implementation of the models are also discussed.", "e:keyword": ["Networks/graphs", "Multicommodity: transportation of empty containers", "Transportation", "Freight: allocation of empty containers", "Transportation", "Network models: stochastic and dynamic flow models"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.11", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.127", "e:abstract": "This paper addresses the problem of determining optimal booking policies for multiple fare classes that share the same seating pool on one leg of an airline flight when seats are booked in a nested fashion and when lower fare classes book before higher ones. We show that a fixed-limit booking policy that maximizes expected revenue can be characterized by a simple set of conditions on the subdifferential of the expected revenue function. These conditions are appropriate for either the discrete or continuous demand cases. These conditions are further simplified to a set of conditions that relate the probability distributions of demand for the various fare classes to their respective fares. The latter conditions are guaranteed to have a solution when the joint probability distribution of demand is continuous. Characterization of the problem as a series of monotone optimal stopping problems proves optimality of the fixed-limit policy over all admissible policies. A comparison is made of the optimal solutions with the approximate solutions obtained by P. Belobaba using the expected marginal seat revenue (EMSR) method.", "e:keyword": ["Decision analysis", "Applications: stochastic", "Integer capacity allocation", "Transportation: airline seat inventory control", "Yield management"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.138", "e:abstract": "One of the most important functions of air traffic management systems is the assignment of ground-holding times to flights, i.e., the determination of whether and by how much the take-off of a particular aircraft headed for a congested part of the ATC system should be postponed to reduce the likelihood and extent of airborne delays. In this paper, we will present an analysis of the fundamental case in which flights from many origins must be scheduled for arrival at a single, congested airport. We will describe a set of approaches for addressing a deterministic and a stochastic version of the problem. A minimum cost flow algorithm can be used for the deterministic problem. Under a particular natural assumption regarding the functional form of delay costs, a very efficient, simple algorithm is also available. For the stochastic version, an exact dynamic programming formulation turns out to be impractical for typical instances of the problem and we present a number of heuristic approaches to it. The models and numerical results suggest the potential usefulness of formal decision support tools in developing effective ground-holding strategies. Many methodological and implementation issues, however, still require resolution.", "e:keyword": ["Dynamic programming", "Applications: limited lookahead and heuristics", "Transportation", "Models: air traffic control flow management"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.153", "e:abstract": "A major problem for the airline industry is the assignment of airplane capacity to flight schedules to meet fluctuating market needs. Demand Driven Dispatch (<i>D</i><sup>3</sup>) is an operating concept that addresses this problem. Utilizing a demand forecast which improves as flight departure approaches, aircraft are dynamically assigned to flights to better match the predicted final demands. The result, demonstrated in studies of actual airline systems, is an increase in passenger loads and revenues with simultaneously reduced costs for a net of 1–5% improvement in operating profits. Concept implementation is simplified by the prevalence of yield management systems which provide the forecasting capability, and the emergence of airplane families which provide the necessary operational flexibility. Implementation also requires frequent solution of extremely large aircraft assignment problems. These problems, which can be formulated in terms of a multicommodity network flow, can be solved with heuristic algorithms shown to exhibit an accuracy and efficiency essential to successful concept implementation.", "e:keyword": ["Networks/graphs: heuristic multicommodity flow algorithm", "Transportation", "Models: dynamic aircraft capacity assignment"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.169", "e:abstract": "Recent interest in stochastic traffic assignment models has been motivated by a need to determine the stationary probability distribution of a network's traffic volumes and by the possibility of using time-series of traffic counts to fit and test travel demand models. Because of the way traffic volumes are generated as the sum of path flows from different origin-destination pairs, and because of the nonlinear nature of the process relating traffic conditions to traveler route selection, most plausible assignment models tend to be intractable. In this paper, we first pose a general stochastic assignment model that includes as special cases most models which have appeared in the literature, and then verify that the probability distributions of an equivalent Markovian model converge to a stationary distribution. We next verify that as the number of individual travelers becomes large, the general model can be approximated by the sum of a nonlinear deterministic process and a time-varying linear Gaussian process. The stationary distribution of this approximation is readily characterized, and the approximation also provides a means for employing linear system methods to estimate model parameters from a set of observed traffic counts. For the case where the route choice probabilities are given by the multinomial logit function, computationally feasible procedures for implementing the approximate model exist.", "e:keyword": ["Probability", "Markov processes: street traffic generation and assignment", "Transportation", "Models", "Assignment: Markov assignment process"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.179", "e:abstract": "In the present paper we are concerned with developing more realistic dynamic models of route choice and departure time decisions of transportation network users than have been proposed in the literature heretofore. We briefly review one class of models that is a dynamic generalization of the static Wardropian user equilibrium, the so-called Boston traffic equilibrium. In contrast, we then propose a new class of models that is also a dynamic generalization of the static Wardropian user equilibrium. In particular, we show for the first time that there is a variational inequality formulation of dynamic user equilibrium with simultaneous route choice and departure time decisions which, when appropriate regularity conditions hold, preserves the first in, first out queue discipline.", "e:keyword": ["Networks/graphs: theory", "Programming: infinite-dimensional", "Transportation: models", "Network"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.18", "e:abstract": "This article proposes a reliability model for emergency service vehicle location. Emergency services planners must solve the strategic problem of where to locate emergency services stations and the tactical problem of the number of vehicles to place in each station. We view the problem from a system reliability perspective, where system failure is interpreted as the inability of a vehicle to respond to a demand call within an acceptable amount of time. Our model handles the stochastic problem aspects in a more explicit way than previous models in the literature. Based on a reliability bound on the probability of system failure, we derive a 0-1 integer programming (<b>IP</b>) optimization model. We propose the augmentation of the <b>IP</b> using certain valid inequalities as a preprocessing technique, and solve the <b>IP</b> using a branch-and-bound procedure. Our computational results show that the preprocessing technique is highly effective. Also, sensitivity studies show that the planner can produce a variety of different desired solution characteristics by appropriate manipulation of parameters. We feel that the reliability perspective should have applications beyond this context and hope that it will lead to ideas for similar optimization models in the context of designing reliable systems.", "e:keyword": ["Facilities/equipment planning", "Location: facility and vehicle location", "Reliability: integer programming models with reliability constraints", "Transportation", "Models", "Location: facility and vehicle location"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.192", "e:abstract": "The instantaneous dynamic user-optimal (DUO) traffic assignment problem is to determine vehicle flows on each link at each instant of time resulting from drivers using instantaneous minimal-time routes. Instantaneous route time is the travel time incurred if traffic conditions remain unchanged while driving along the route. In this paper, we introduce a different definition of an instantaneous DUO state. Using the optimal control theory approach, we formulate two new DUO traffic assignment models for a congested transportation network. These models include new formulations of the objective function and flow propagation constraints, and are dynamic generalizations of the static user-optimal model. The equivalence of the solutions of the two optimal control programs with DUO traffic flows is demonstrated by proving the equivalence of the first-order necessary conditions of the two programs with the instantaneous DUO conditions. Since these optimal control problems are convex programs with linear constraints, they have unique solutions. A numerical example is presented indicating that this class of models yields realistic results.", "e:keyword": ["Dynamic programming/optimal control: deterministic application model", "Networks/graphs: multicommodity", "Transportation", "Models: network traffic assignment"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.203", "e:abstract": "This paper presents a method for finding optimal flows in a dynamic network with random inputs into the system and congestion limits on flow. This model has been used in deterministic settings to represent dynamic traffic assignment and job shop routing. This paper builds on the deterministic results to show that a globally optimal solution in the stochastic problem may be obtained by a sequence of linear optimizations. A decomposition algorithm for this procedure is presented that efficiently solves problems with large-scale deterministic equivalents of up to 66,000 variables.", "e:keyword": ["Production/scheduling", "Sequencing", "Stochastic: routing and scheduling in dynamic networks", "Programming", "Stochastic: multistage stochastic nonlinear programming", "Transportation", "Models", "Network: dynamic traffic assignment networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.217", "e:abstract": "For congested networks on which flows vary over time, we derive system marginal costs, user perceived costs and user externality costs, for each arc and path. We also obtain a set of optimal congestion tolls and flow controls which may be used to shift the user determined flows toward a socially preferred pattern. An important way in which our results differ from the usual static analysis is that the social cost externality depends not only on the level of congestion, but also on the rate of increase or decrease of congestion. This is intuitively explicable as follows. Consider users delayed on an arc. Their delays will be further compounded or multiplied if congestion has <i>increased</i> during the time they are delayed. On the other hand, their delays will be <i>reduced</i> if congestion has <i>declined</i> during the time they are delayed. This multiplier effect is such that the resultant dynamic externalities can easily be a few times larger, or smaller, than the externalities derived in the usual static analysis. As a result, the congestion tolls or tariffs which are usually proposed or advocated, based on static analysis, may be inappropriate. The results are illustrated with a numerical application to a small network example.", "e:keyword": ["Transportation", "Models", "Network: dynamic flows and externalities", "Transportation", "Models", "Traffic: dynamic flows and congestion"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.37", "e:abstract": "We consider distribution systems with a single depot and many retailers each of which faces external demands for a single item that occurs at a specific deterministic demand rate. All stock enters the systems through the depot where it can be stored and then picked up and distributed to the retailers by a fleet of vehicles, combining deliveries into efficient routes. We extend earlier methods for obtaining low complexity lower bounds and heuristics for systems without central stock. We show under mild probabilistic assumptions that the generated solutions and bounds come asymptotically within a few percentage points of optimality (within the considered class of strategies). A numerical study exhibits the performance of these heuristics and bounds for problems of moderate size.", "e:keyword": ["Inventory/production: approximations and heuristics", "Multi-item", "Multi-echelon", "Multi-stage", "Transportation: vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.48", "e:abstract": "We consider the problem of determining optimal purchasing and shipping quantities over a finite planning horizon for arborescent, multi-echelon physical distribution systems with deterministic, time-varying demands. We assume that the inventory holding cost at a given warehouse of the distribution network is a linear function of the inventory level, and that the total procurement cost (i.e., ordering, plus purchasing, plus transportation and reception costs) is a general piecewise-linear function of the quantities shipped to and from the warehouse. We formulate a mixed integer linear programming model of the problem and develop a Lagrangian relaxation-based procedure to solve it. We show computational results for problems with 12 periods, up to 15 warehouses, and 3 transportation price ranges.", "e:keyword": ["Inventory/production: multi-echelon lot sizing", "Programming: integer", "Branch and bound", "Relaxation/subgradient"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.60", "e:abstract": "In 1991, D. J. Bertsimas and G. van Ryzin introduced and analyzed a model for stochastic and dynamic vehicle routing in which a single, uncapacitated vehicle traveling at a constant velocity in a Euclidean region must service demands whose time of arrival, location and on-site service are stochastic. The objective is to find a policy to service demands over an infinite horizon that minimizes the expected system time (wait plus service) of the demands. This paper extends our analysis in several directions. First, we analyze the problem of <i>m</i> identical vehicles with unlimited capacity and show that in heavy traffic the system time is reduced by a factor of 1/<i>m</i><sup>2</sup> over the single-server case. One of these policies improves by a factor of two on the best known system time for the single-server case. We then consider the case in which each vehicle can serve at most <i>q</i> customers before returning to a depot. We show that the stability condition in this case depends strongly on the geometry of the region. Several policies that have system times within a constant factor of the optimum in heavy traffic are proposed. Finally, we discuss extensions to mixed travel cost and system time objectives.", "e:keyword": ["Networks: stochastic", "Traveling salesman", "Probability: stochastic model", "Transportation: vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.77", "e:abstract": "We describe a dynamic and stochastic vehicle dispatching problem called the delivery dispatching problem. This problem is modeled as a Markov decision process. Because exact solution of this model is impractical, we adopt a heuristic approach for handling the problem. The heuristic is based in part on a decomposition of the problem by customer, where customer subproblems generate penalty functions that are applied in a master dispatching problem. We describe how to compute bounds on the algorithm's performance, and apply it to several examples with good results.", "e:keyword": ["Dynamic programming", "Markov", "Finite state: Markov decision model for vehicle dispatching", "Transportation", "Route selection: route selection for inventory replenishment"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.1.91", "e:abstract": "We examine shortest path problems in acyclic networks in which arc costs are known functions of certain <i>environment variables</i> at network nodes. Each of these variables evolves according to an independent Markov process. The vehicle can wait at a node (at a cost) in anticipation of more favorable arc costs. We first develop two recursive procedures for the individual arc case, one based on successive approximations, and the other on policy iteration. We also solve the same problem via parametric linear programming. We show that the optimal policy essentially classifies the state of the environment variable at a node into two categories: <i>green</i> states for which the optimal action is to immediately traverse the arc, and <i>red</i> states for which the optimal action is to wait. We then extend these concepts for the entire network by developing a dynamic programming procedure that solves the corresponding problem. The complexity of this method is shown to be <i>O</i>(<i>n</i><sup>2</sup><i>K</i> + <i>nK</i><sup>3</sup>), where <i>n</i> is the number of network nodes and <i>K</i> is the number of Markov states at each node. We present examples and discuss possible research extensions.", "e:keyword": ["Networks/graphs", "Distance algorithms: Markovian cost structure", "Networks/graphs", "Stochastic: dynamic shortest paths", "Transportation", "Models", "Network: dynamic and stochastic problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.241", "e:abstract": "While building complex models is an important part of operations research practice, OR workers have focused too often on modeling's technical aspects instead of making the models manageable, that is, designing them around the ways people will operate them. The issues raised for complex models are different from those most widely discussed for decision support systems because the focus is on models that require a staff to maintain and operate them and on how the staff functions. Operations management provides the tools for thinking about the operations of large complex models; this paper examines some of these tools and shows how they relate to the design and operation of large complex models.", "e:keyword": ["Philosophy of modeling: managing large models", "Production/scheduling: applying operations management techniques to model operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.253", "e:abstract": "Many manufacturing firms that use Material Requirements Planning (MRP) cannot deliver products on schedule and within budget. Faced with bewildering bottlenecks, erratic process flows, and unrealistic due dates, they are unable to develop accurate schedules for their raw material acquisitions, workforce, and equipment. Their MRP plans must be translated into a workable schedule, one which determines when individual tasks will be performed by workers at work centers. There is a clear need for such an enhancement to MRP, a means to operate on detailed task data, yet capable of producing a schedule that directly relates to the MRP plan, the master production schedule, and the resource plan. We describe a method for determining feasible and cost-effective schedules for both labor and machines in a job shop. The method first sequences tasks at resources and then minimizes overall earliness and lateness cost by solving a series of maximum flow problems. By using the model as an enhancement to a company's MRP system, we simulated the cost effects of redeploying its workforce. Although the model was not used for real-time scheduling, it served a strategic role in workforce expansion and deployment decisions.", "e:keyword": ["Information systems", "Management: coordination of a fabrication and assembly process", "Inventory/production", "Parametric analysis: minimizing material", "Labor", "Inventory carrying and lateness costs", "Inventory/production", "Policies", "Ordering: scheduling job shop orders with accurate lead times"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.269", "e:abstract": "The resident scheduling problem is a specific case of the multiperiod staff assignment problem where individuals are assigned to a variety of tasks over multiple time periods. As in many staffing and training situations, numerous limitations and requirements may be placed on those assignments. This paper presents a procedure for addressing two major problems inherent in the determination of a solution to this type of problem: infeasibilities that naturally occur in the scheduling environment but are obscured by complexity; and the intractable nature of large-scale models with this structure. The procedure developed describes a systematic approach that allows decision makers to resolve system-inherent infeasibilities, and a heuristic based on rounding to develop good feasible solutions to the model. The procedure is illustrated via a case example of resident assignments for teaching and training modules in a university affiliated teaching hospital.", "e:keyword": ["Education systems", "Operations: scheduling training experiences", "Health care", "Hospitals: scheduling medical residents", "Programming", "Integer", "Heuristics: multiperiod staff assignment problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.280", "e:abstract": "An influence diagram is a graphical representation of a decision problem that is at once a formal description of a decision problem that can be treated by computers and a representation that is easily understood by decision makers who may be unskilled in the art of complex probabilistic modeling. The power of an influence diagram, both as an analysis tool and a communication tool, lies in its ability to concisely summarize the structure of a decision problem. However, when confronted with highly asymmetric problems in which particular acts or events lead to very different possibilities, many analysts prefer decision trees to influence diagrams. In this paper, we extend the definition of an influence diagram by introducing a new representation for its conditional probability distributions. This extended influence diagram representation, combining elements of the decision tree and influence diagram representations, allows one to clearly and efficiently represent asymmetric decision problems and provides an attractive alternative to both the decision tree and conventional influence diagram representations.", "e:keyword": ["Decision analysis", "Influence diagrams: representing and solving decision problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.298", "e:abstract": "This paper presents a quadratic programming model for allocating a global budget to individual hospitals. In the model, allocation weights are estimated for Diagnosis Related Groups (DRGs) which reflect the average historical costs of treating an additional case in each of nine DRG systems. The model is illustrated using data from the Department of Defense (DoD). Allocated budgets and DoD specific DRG marginal cost weights are estimated using information on three factors; the facility designated functions in the military health care system; hospital output; and sets of price structures used in other systems. The model's weights are used to allocate the DoD's global budget to its treatment facilities under constraints. The model minimizes the difference (in a least squares sense) between the facilities' current budgets and their computed budgets, while constraints are designed to produce budgets representing differences in the type and volume of patients and to preserve certain specialized institutional capabilities.", "e:keyword": ["Cost analysis: adjustment of payment for medical need", "Health care", "Hospitals: output based models for reimbursement", "Programming", "Quadratic: optimization under covariate constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.310", "e:abstract": "This paper considers rational methods for the design of search densities when the intended target is stationary among stationary, Poisson-distributed false contacts. Attention is restricted to situations in which the searcher must immediately decide, based on sensed information, whether a contact is the intended target and, if the answer is yes, end the search and commit to the contact. Discrimination of false contact from target is modeled as reliable only with given probabilities. We seek to allocate search effort to maximize the probability of finding the intended target in a fixed time. Modeling space and time as discrete, we apply the Kuhn-Tucker theorem to obtain a set of necessary conditions for an optimum. For the particular case of a discrete space and time approximation of search for a normally distributed target in <i>R</i><sup>2</sup> and a uniform density of false contacts, we develop a computer algorithm that converges to a solution of the necessary conditions. Numerical results are presented.", "e:keyword": ["Search and surveillance: stationary target among random", "Stationary false targets"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.319", "e:abstract": "We develop an algorithm for solving nonlinear, two-stage stochastic problems with network recourse. The algorithm is based on the framework of <i>row-action</i> methods. The problem is formulated by replicating the first-stage variables and then adding <i>nonanticipativity</i> side constraints. A series of (independent) deterministic network problems are solved at each step of the algorithm, followed by an iterative step over the nonanticipativity constraints. The solution point of the iterates over the nonanticipativity constraints is obtained analytically. The row-action nature of the algorithm makes it suitable for parallel implementations. A data representation of the problem is developed that permits the massively parallel solution of <i>all</i> the scenario subproblems concurrently. The algorithm is implemented on a Connection Machine CM-2 with up to 32K processing elements and achieves computing rates of 276 MFLOPS. Very large problems—8,192 scenarios with a deterministic equivalent nonlinear program with 868,367 constraints and 2,474,017 variables—are solved within a few minutes. We report extensive numerical results regarding the effects of stochasticity on the efficiency of the algorithm.", "e:keyword": ["Computers/computer science: massively parallel", "Networks/graphs: stochastic generalized networks", "Programming: stochastic", "Nonlinear algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.338", "e:abstract": "In this paper, we present a new strongly polynomial time algorithm for the minimum cost flow problem, based on a refinement of the Edmonds-Karp scaling technique. Our algorithm solves the <i>uncapacitated</i> minimum cost flow problem as a sequence of <i>O</i>(<i>n</i> log <i>n</i>) shortest path problems on networks with <i>n</i> nodes and <i>m</i> arcs and runs in <i>O</i>(<i>n</i> log <i>n</i>(<i>m</i> + <i>n</i> log <i>n</i>)) time. Using a standard transformation, this approach yields an <i>O</i>(<i>m</i> log <i>n</i>(<i>m</i> + <i>n</i> log <i>n</i>)) algorithm for the <i>capacitated</i> minimum cost flow problem. This algorithm improves the best previous strongly polynomial time algorithm, due to Z. Galil and E. Tardos, by a factor of <i>n</i><sup>2</sup>/<i>m</i>. Our algorithm for the capacitated minimum cost flow problem is even more efficient if the number of arcs with finite upper bounds, say <i>m</i>′, is much less than <i>m</i>. In this case, the running time of the algorithm is <i>O</i>((<i>m</i>′ + <i>n</i>) log <i>n</i>(<i>m</i> + <i>n</i> log <i>n</i>)).", "e:keyword": ["Networks/graphs", "Flow algorithms: a faster strongly polynomial minimum cost flow algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.351", "e:abstract": "We present an inventory model, where the demand rate varies with an underlying state-of-the-world variable. This variable can represent economic fluctuations, or stages in the product life-cycle, for example. We derive some basic characteristics of optimal policies and develop algorithms for computing them. In addition, we show that certain monotonicity patterns in the problem data are reflected in the optimal policies.", "e:keyword": ["Inventory/production: fluctuating demand environment"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.371", "e:abstract": "We consider a multistage, multiproduct production/inventory system in discrete time. When an order is placed for a component it is instantly delivered, and the required amounts of the components consumed in producing the given component are simultaneously withdrawn from their respective inventories. External demand occurs for a single component. We assume that the external demand for the component is nonconstant, deterministic, and must be met without backlogging. We propose two new cluster-based heuristics for this problem. We will show that the first of these heuristics has a worst-case relative cost that is between 1.44 and 2, and the second of these heuristics has a worst-case relative cost of 2. Computational tests indicate that, on the average, these heuristics are within 0.7% and 1.3% of optimal, respectively, and that their performance is very insensitive to the size of the system and to other input parameters.", "e:keyword": ["Inventory/production: deterministic lot sizing", "Multi-item", "Multistage heuristics", "Production/scheduling: production planning and lot sizing"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.386", "e:abstract": "Queueing networks with blocking are useful for modeling and analyzing discrete event systems, especially manufacturing systems. Most analysis methods for queueing networks with blocking are approximation methods that involve a decomposition of the network into a set of subsystems. This paper presents some insight into these decomposition methods as well as new results. Attention is mainly restricted to the case of tandem queueing networks with exponential service times and blocking-after-service. This type of blocking is especially encountered in manufacturing systems. The first aim of this paper is to improve the understanding and present a unified view of the decomposition methods. We show that decomposition methods can be classified according to three main approaches. One of these approaches is of special interest because it offers a symmetrical view of the decomposition. The second aim of the paper is to provide properties pertaining to these decomposition methods in the case of exponential characterizations of subsystems. We prove the existence and uniqueness of the solution. Moreover, we prove the convergence of the computational algorithm associated with the symmetrical approach.", "e:keyword": ["Manufacturing: modeling and performance evaluation", "Queues", "Approximations: decomposition methods", "Queues", "Tandem: tandem queueing networks with blocking"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.400", "e:abstract": "In this paper we establish a joint central limit theorem for customer and time averages by applying a martingale central limit theorem in a Markov framework. The limiting values of the two averages appear in the translation terms. This central limit theorem helps to construct confidence intervals for estimators and perform statistical tests. It thus helps determine which finite average is a more asymptotically efficient estimator of its limit. As a basis for testing for PASTA (Poisson arrivals see time averages), we determine the variance constant associated with the central limit theorem for the difference between the two averages when PASTA holds.", "e:keyword": ["Queues", "Limit theorems: central limit theorems for customer and time averages", "Queues", "Statistical inference: estimating customer and time averages", "Statistics", "Estimation: averages over time and at embedded points"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.409", "e:abstract": "When <i>n</i> independent identical renewal processes are superimposed, the number of events in (0, <i>s</i>) is approximately Poisson distributed for large <i>n</i>. For small to moderate <i>n</i>, this approximation is inaccurate. If no component process has more than one event in (0, <i>s</i>), the probability of exactly <i>r</i> events for the superimposed process is given by a binomial-like expression where the <i>p</i> and <i>q</i> do not sum to one. Several approximations have been derived from this observation and compared to the exact probability for small <i>n</i>, when the underlying distribution of time between events is gamma. One of these binomial approximations gives excellent results for small <i>n</i>. The following application is discussed. When <i>k</i> new series systems, each consisting of <i>m</i> identical components, are tested for time <i>s</i> and failed components are replaced upon failure, a superimposed renewal process results. To design an acceptance sampling scheme for new series systems using exact probability computations is cumbersome. The Poisson approximation is convenient, but may give misleading estimates of the O.C. curve for small <i>n</i>. The new binomial approximation is a compromise: It is easier to use for test design than the exact computation and more accurate than the simpler Poisson approximation.", "e:keyword": ["Probability", "Distributions: new approximations for event count of superimposed renewals", "Reliability", "Quality control: acceptance tests of new series systems", "Probability", "Renewal processes: superposition of identical processes at the time origin"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.2.419", "e:abstract": "The paper considers the aggregation of similar products in the lot size scheduling problems for single-stage systems with limited upper bounds on inventory levels. The necessary and sufficient conditions of consistent aggregation, which guarantee that the optimal aggregate solution can be disaggregated into an optimal solution of the original problem, are presented.", "e:keyword": ["Inventory/production: a single-stage lot size model", "Production/scheduling: feasibility of aggregate production plans"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.435", "e:abstract": "Exploratory modeling is using computational experiments to assist in reasoning about systems where there is significant uncertainty. While frequently confused with the use of models to consolidate knowledge into a package that is used to predict system behavior, exploratory modeling is a very different kind of use, requiring a different methodology for model development. This paper distinguishes these two broad classes of model use describes some of the approaches used in exploratory modeling, and suggests some technological innovations needed to facilitate it.", "e:keyword": ["Computers/computer science: system design", "Philosophy of modeling: exploratory modeling"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.450", "e:abstract": "Measuring performance in a multiproduct firm is difficult because of the complex nature of the firm's output. We present an approach to measuring performance, called <i>operating efficiency</i> which is the ratio of aggregate output to aggregate resource use. We describe how to calculate operating efficiency for a multiproduct organization and demonstrate how it can be used to understand why performance varies across plants and/or through time. Our application of operating efficiency to the U.S. Postal Service is discussed along with the reactions of potential users.", "e:keyword": ["Economics", "Econometrics: using econometric models", "Manufacturing", "Productivity: performance measurement system"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.484", "e:abstract": "This paper demonstrates that the computational effort required to develop numerical solutions to continuous-state dynamic programs can be reduced significantly when cubic piecewise polynomial functions, rather than tensor product linear interpolants, are used to approximate the value function. Tensor product cubic splines, represented in either piecewise polynomial or B-spline form, and multivariate Hermite polynomials are considered. Computational savings are possible because of the improved accuracy of higher-order functions and because the smoothness of higher-order functions allows efficient quasi-Newton methods to be used to compute optimal decisions. The use of the more efficient piecewise polynomial form of the spline was slightly superior to the use of Hermite polynomials for the test problem and easier to program. In comparison to linear interpolation, use of splines in piecewise polynomial form reduced the CPU time to obtain results of equivalent accuracy by a factor of 250–330 for a stochastic 4-dimensional water supply reservoir problem with a smooth objective function, and factors ranging from 25–400 for a sequence of 2-, 3-, 4-, and 5-dimensional problems. As a result, a problem that required two hours to solve with linear interpolation was solved in a less than a minute with spline interpolation with no loss of accuracy.", "e:keyword": ["Dynamic programming", "Applications: numerical solution of continuous-state dynamic programs", "Dynamic programming", "Markov", "Infinite state: higher-order approximation of the value function"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.501", "e:abstract": "Many commonly used estimators of the variance of the sample mean from a covariance-stationary process can be written as quadratic forms. We study the class of quadratic-form estimators algebraically and graphically, including five specific types of estimators, some from the literature and some that are new. Finite and asymptotic bias, variance, and covariance are derived and examined, with emphasis on developing intuition and insight by interpreting these properties graphically. The graphs depict the nonoptimal statistical behavior of some of the simulation literature estimators such as nonoverlapping batch means, as well as the better behavior of estimators obtained by overlapping batches.", "e:keyword": ["Simulation: statistical analysis", "Statistics: estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.518", "e:abstract": "This is the first of two papers dealing with the establishment of shadow prices for measures of effectiveness in an optimization-based combat model. In this paper we explain how the requirement for the analysis arose, and we show how to build a simple linear model that produces shadow, prices for kill requirements. When the model is further specialized, these shadow prices become the classical eigenvalue weights familiar from Lanchester theory.", "e:keyword": ["Military", "Cost effectiveness: shadow pricing", "Military", "Force effectiveness: force-on-force models"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.536", "e:abstract": "This is the second of a pair of papers describing a two-sided game model of combat. In this paper, each side attempts to develop a force structure attaining the maximum of a prescribed merit function, subject to certain constraints expressed by a set of prescribed measures of effectiveness. These measures can be different for the two sides: furthermore, those of each side can depend on the other side's actions. A solution of the model is a generalized Nash equilibrium of this game, and such a solution also yields shadow prices that reveal the cost in merit paid by each side for requiring the specified level of performance on each measure of effectiveness. The first paper examines a special case in which the model has a linear structure, and shows that in a restricted case the shadow prices produced by that model are the classical eigenvalue weights familiar from Lanchester theory.", "e:keyword": ["Military", "Cost effectiveness: shadow pricing", "Military", "Force effectiveness: force-on-force models"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.549", "e:abstract": "Many problems in inventory control, production planning, and capacity planning can be formulated in terms of a simple economic lot size model proposed independently by A. S. Manne (1958) and by H. M. Wagner and T. M. Whitin (1958). The Manne-Wagner-Whitin model and its variants have been studied widely in the operations research and management science communities, and a large number of algorithms have been proposed for solving various problems expressed in terms of this model, most of which assume concave costs and rely on dynamic programming. In this paper, we show that for many of these concave cost economic lot size problems, the dynamic programming formulation of the problem gives rise to a special kind of array, called a Monge array. We then show how the structure of Monge arrays can be exploited to obtain significantly faster algorithms for these economic lot size problems. We focus on uncapacitated problems, i.e., problems without bounds on production, inventory, or backlogging; capacitated problems are considered in a separate paper.", "e:keyword": ["Analysis of algorithms: optimal and suboptimal algorithms", "Computers/computer science: faster lot sizing using Monge arrays", "Inventory/production: faster algorithms for various Wagner-Whitin models"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.572", "e:abstract": "A simple, <i>O</i>(|<i>V</i>|<sup>2</sup>) time algorithm is presented that reduces a connected two-terminal, undirected, planar graph to a single edge, by way of series and parallel reductions and delta-wye transformations. The method is applied to a class of optimization/equilibnum problems which includes max flow, shortest path, and electrical resistance problems.", "e:keyword": ["Analysis of algorithms", "Data structures: efficient structure for solving many types of network problems", "Networks/graphs", "Flow algorithms: solves a class of flow equilibrium problems", "Networks/graphs", "Theory: reduction method for 2-terminal planar graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.583", "e:abstract": "A parameter adaptive decision process is a sequential decision process where some parameter or parameter set impacting the rewards and/or transitions of the process is not known with certainty. Signals from the performance of the system can be processed by the decision maker as time progresses, yielding information regarding which parameter set is operative. Active learning is an essential feature of these processes, and the decision maker must choose actions that simultaneously guide the system in a preferred direction, as well as yield information that can be used to better prescribe future actions. If the operative parameter set is known with certainty, the parameter adaptive problem reduces to a conventional stochastic dynamic program, which is presumed solvable. Previous authors have shown how to use these solutions to generate suboptimal policies with performance bounds for the parameter adaptive problem. Here it is shown that some desirable characteristics of those bounds are shared by a larger class of functions than those generated from fully observed problems, and that this generalization allows for iterative tightening of the bounds in a manner that preserves those attributes. An example inventory stocking problem demonstrates the technique.", "e:keyword": ["Decision analysis: Bayesian dynamic programming", "Dynamic programming: parameter adaptive decision processes", "Inventory/production: policies under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.600", "e:abstract": "For the <i>GI</i>/<i>G</i>/1 queue with finite capacity we derive the time-dependent distributions of the number of customers in the system and of the duration of a busy period and a busy cycle under an arbitrary initial condition at time zero.", "e:keyword": ["Queues", "Transient results: explicit results for finite capacity queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.3.608", "e:abstract": "In some flexible manufacturing systems, tool magazine capacity naturally divides parts into families so that substantial setups occur only when switching production between families. For this situation (positive setup times and costs only between families), we show that the production planning problem decomposes into two simpler problems: an aggregate scheduling problem, and a disaggregation. First, we derive a disaggregation that determines the optimal intrafamily product schedules corresponding to a given family schedule in polynomial time. Second, we show how to aggregate so that the decomposition maintains optimality for family schedules satisfying the property that production does not begin until inventory is zero (the Zero-Switch rule).", "e:keyword": ["Production/scheduling: decomposition for hierarchical production planning", "Production/scheduling", "Flexible manufacturing: scheduling families of parts"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.625", "e:abstract": "“Crisis? What crisis?” could also have been an appropriate title for this paper. The OR/MS literature contains more than enough papers addressing the crisis in OR/MS to take the matter seriously, but it is not always clear exactly what is meant by crisis. The complaints usually concern the perceived gap between theory and practice, pointing out that there are too many theoretical and too few practice-oriented papers. This may well be true, but we suggest a slightly different view of the crisis, by hypothesizing that a ‘natural drift’ has occurred, i.e., that old-style OR has remained underdeveloped relative to its more purely theoretical and practical counterparts. To explain how this hypothesis arose, we provide an overview of the debate on professional concerns in OR/MS, and contrast it with <i>Harvard Business Review</i> papers providing a managerial perspective. We also explore the extent to which such a natural drift would be truly natural, by comparing the development of OR/MS to that of other professions. We arrive at a mixed conclusion. All is not well, but all is not lost either.", "e:keyword": ["Professional: comments on", "OR/MS philosophy"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.641", "e:abstract": "This paper describes a scheduling support system for plants that produce multiply paper bags. The machine environment in these plants may be described as a flexible flow shop. That is, there are a number of stages in series with a number of machines in parallel at each stage. The jobs have specified shipping dates as well as priorities; their processing times and setup times are known in advance. We developed a five-step algorithm that schedules all the jobs at the various stages. This algorithm has been incorporated in a system, the Bagpak Production Scheduling System (BPSS), which also contains a data base management module and a user interface module. The system has been implemented in two plants, and is used on a regular basis. It has increased considerably the accuracy of the available information regarding the production process. The scheduler, in conjunction with the system, is able to produce better schedules; he now always knows where given performance measures stand. In addition, the available information enables the scheduler to provide better service to customers.", "e:keyword": ["Information systems: decision support systems", "Production/scheduling: applications", "Sequencing", "Multiple machines"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.649", "e:abstract": "In recent years, the National Hockey League (NHL) expanded from 21 to 24 teams. In order to accommodate its scheduling process, the league had to determine new game allocations, that is, the number of times the teams play against each other. This paper introduces a procedure based on integer linear programming that generates several game allocation scenarios that have been presented to the league managers. The model takes into account the constraints specified by the NHL, and the objective function allows the league managers to specify their preference on the distribution of the games.", "e:keyword": ["Programming", "Integer", "Applications: games allocation for a sport league", "Recreation and sports: games allocation for a sport league"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.655", "e:abstract": "Methods are proposed for the valuation of strategic offensive and defensive force structures, with emphasis on the consideration of incentives for the formation of coalitions. Coalitions consist of subsets of the nuclear weapons states, together with the nonnuclear weapons states taken as components of the total value target inventory. The basic approach is to formulate and solve two worldwide nuclear coalition games. In the first game, the first striking coalition is retaliated against by the surviving weapons of the second striking coalition, minimizing the objective function of the first striking coalition. In the second game, the surviving weapons of the second striking coalition are used to maximize its own objective function. The objective function in both models is the percent of surviving value. The games differ substantially. Computational results are presented for all possible coalitions of nuclear weapons states and neutrals. Offensive weapons, defensive weapons, and value target data bases are varied. Results are very sensitive to coalition composition, the number of value targets, and the number of offensive and defensive weapons.", "e:keyword": ["Games/group decisions: incentives to initiate nuclear warfare", "Military", "Warfare models: strategic offensive and defensive nuclear warfare"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.669", "e:abstract": "We present a new solution approach for the multicommodity network flow problem (MCNF) based upon both primal partitioning and decomposition techniques, which simplifies the computations required by the simplex method. The partitioning is performed on an arc-chain incidence matrix of the MCNF, similar within a change of variables to the constraint matrix of the master problem generated in a Dantzig-Wolfe decomposition, to isolate a very sparse, near-triangular working basis of greatly reduced dimension. The majority of the simplex operations performed on the partitioned basis are simply additive and network operations specialized for the nine possible pivot types identified. The columns of the arc-chain incidence matrix are generated by a dual network simplex method for updating shortest paths when link costs change.", "e:keyword": ["Industries", "Transportation/shipping: transshipment and consolidation problems", "Networks/graphs", "Multicommodity: primal partitioning solution", "Programming", "Linear", "Large-scale systems: multicommodity network flow problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.694", "e:abstract": "A mean square error analysis of finite-difference sensitivity estimators for stochastic systems is presented and an expression for the optimal size of the increment is derived. The asymptotic behavior of the optimal increments, and the behavior of the corresponding optimal finite-difference (FD) estimators are investigated for finite-horizon experiments. Steady-state estimation is also considered for regenerative systems and in this context a convergence analysis of ratio estimators is presented. The use of variance reduction techniques for these FD estimates, such as common random numbers in simulation experiments, is not considered here. In the case here, direct gradient estimation techniques (such as perturbation analysis and likelihood ratio methods) whenever applicable, are shown to converge asymptotically faster than the optimal FD estimators.", "e:keyword": ["Probability: stochastic model applications", "Simulation: simulation efficiency", "Statistical analysis of simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.704", "e:abstract": "We propose a randomized strategy for selecting/combining forecasts that is better than the forecasts used to produce it in a sense made precise in this paper. Unlike traditional methods this approach requires that no assumptions be made about the distribution of the event being forecasted or the error distribution and stationarity of the constituent forecasts. The method is simple and easy to implement.", "e:keyword": ["Forecasting: combining forecasts"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.710", "e:abstract": "The Department of Transportation (DOT) rates commercial airlines' on-time performance every month, but its ratings may unfairly penalize airlines that disproportionately fly into airports at which it is inherently more difficult to land in on-time. We propose several rating methods that we consider more equitable and apply them to 36 months of DOT data. Rating airlines' promptness is an example of the more general problem of evaluating players who compete in overlapping but not identical sets of tournaments, where there is no exogenous knowledge about the quality of the players or the “difficulty” of the tournaments.", "e:keyword": ["Government", "Agencies: DOT on-time statistics", "Organizational studies", "Effectiveness/performance: rating on-time performance", "Transportation: aviation", "Commercial airlines' punctuality"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.721", "e:abstract": "In this paper, we develop a stochastic, multiclass network equilibrium model of airline passenger transportation. The model explicitly incorporates the behavior of the passengers in regard to the decision whether or not to travel, and route selection, and formalizes the passenger's decision as an abstract network. The equilibrium between the realized demand for the routes of travel and the supply of the seats is shown to satisfy a system of nonlinear equations. A Gauss-Seidel algorithm is then proposed for the computation of the equilibrium and conditions for convergence are established. Finally, the algorithm is applied to compute the solution to the national Air Canada airline passenger network, and the output is compared to the actual observed flows.", "e:keyword": ["Networks/graphs: airline transportation", "Transportation: stochastic", "Dynamic network equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.731", "e:abstract": "We establish some general structural results and derive some simple formulas describing the time-dependent performance of the <i>M<sub>t</sub></i>/<i>G</i>/∞ queue (with a nonhomogeneous Poisson arrival process). We know that, for appropriate initial conditions, the number of busy servers at time <i>t</i> has a Poisson distribution for each <i>t</i>. Our results show how the time-dependent mean function <i>m</i> depends on the time-dependent arrival-rate function λ and the service-time distribution. For example, when λ is quadratic, the mean <i>m</i>(<i>t</i>) coincides with the pointwise stationary approximation λ(<i>t</i>)<i>E</i>[<i>S</i>], where <i>S</i> is a service time, except for a time lag and a space shift. It is significant that the well known insensitivity property of the stationary <i>M</i>/<i>G</i>/∞ model does not hold for the nonstationary <i>M<sub>t</sub></i>/<i>G</i>/∞ model; the time-dependent mean function <i>m</i> depends on the service-time distribution beyond its mean. The service-time stationary-excess distribution plays an important role. When λ is decreasing before time <i>t</i>, <i>m</i>(<i>t</i>) is increasing in the service-time variability, but when λ is increasing before time <i>t</i>, <i>m</i>(<i>t</i>) is decreasing in service-time variability. We suggest using these infinite-server results to approximately describe the time-dependent behavior of multiserver systems in which some arrivals are lost or delayed.", "e:keyword": ["Queues", "Approximations: infinite-server queues with time-dependent arrival rates", "Queues", "Nonstationary: simple formulas for infinite-server models", "Queues", "Transient results: the magnitude and timing of peak congestion"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.743", "e:abstract": "We consider the problem of finding an optimal dynamic priority sequencing policy to maximize the mean throughput rate in a multistation, multiclass closed queueing network with general service time distributions and a general routing structure. Under balanced heavy loading conditions, this scheduling problem can be approximated by a control problem involving Brownian motion. Although a unique, closed-form solution to the Brownian control problem is not derived, an analysis of the problem leads to an effective static sequencing policy, and to an approximate means of comparing the relative performance of arbitrary static policies. Several examples are provided that illustrate the effectiveness of our procedure.", "e:keyword": ["Production/scheduling: priority sequencing in a stochastic job shop", "Queues: Brown models of network scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.759", "e:abstract": "We consider a new family of search directions for the standard form variant of Karmarkar's projective linear programming algorithm. The family includes the usual projected gradient direction, and also a direction first proposed by Mike Todd. We prove that any choice from the family preserves the algorithm's polynomial-time complexity. We then examine the computational behavior of the algorithm using different choices of directions. Although the theoretical complexity is the same for the different directions, in practice we find wide variations in algorithm performance. One particular choice consistently requires about 20% fewer iterations than the usual direction, while another requires a number of iterations which grows rapidly with problem size. Our computational results also demonstrate that a small number of monotonic steps on early iterations may considerably improve the performance of the algorithm.", "e:keyword": ["Programming", "Linear", "Algorithms: directions for projective algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.768", "e:abstract": "Best-first search is a widely used problem solving technique in the field of artificial intelligence. The method has useful applications in operations research as well. Here we describe an application to constrained two-dimensional cutting stock problems of the following type: A stock rectangle <i>S</i> of dimensions (<i>L</i>, <i>W</i>) is supplied. There are <i>n</i> types of demanded rectangles <i>r</i><sub>1</sub>, <i>r</i><sub>2</sub>, …, <i>r<sub>n</sub></i>, with the <i>i</i>th type having length <i>l<sub>i</sub></i>, width <i>w<sub>i</sub></i>, value <i>v<sub>i</sub></i>, and demand constraint <i>b<sub>i</sub></i>. It is required to produce, from the stock rectangle <i>S</i>, <i>a<sub>i</sub></i> copies of <i>r<sub>i</sub></i>, 1 ≤ <i>i</i> ≤ <i>n</i>, to maximize <i>a</i><sub>1</sub><i>v</i><sub>1</sub> + <i>a</i><sub>2</sub><i>v</i><sub>2</sub> + · + <i>a<sub>n</sub>v<sub>n</sub></i> subject to the constraints <i>a<sub>i</sub></i> ≤ <i>b<sub>i</sub></i>. Only orthogonal guillotine cuts are permitted. All parameters are integers. A best-first tree search algorithm based on Wang's bottom-up approach is described that guarantees optimal solutions and is more efficient than existing methods.", "e:keyword": ["Computers/computer science", "Artificial intelligence: search methods", "Production/scheduling", "Cutting stock/trim: rectangular stock sheets"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.777", "e:abstract": "We consider a two-level inventory system with one warehouse and <i>N</i> identical retailers. Lead times (transportation times) are constant and the retailers face independent Poisson demand. In a previous paper, we derived a recursive procedure for determining the policy costs for an average item in case of one-for-one replenishment policies. In this paper, we show how these results can be used for the exact or approximate evaluation of more general policies where both the retailers and the warehouse order in batches. We compare our methods to the method recently suggested by A. Svoronos and P. Zipkin.", "e:keyword": ["Inventory/production", "Multi-echelon: batch-ordering policies", "Identical retailers", "Inventory/production", "Stochastic: Poisson demand", "Continuous review"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.786", "e:abstract": "We address the problem of scheduling <i>n</i> jobs on a single machine, which is subject to random breakdowns, to minimize an expected sum of nonregular penalty functions. A simple recourse model is considered when the penalty function is the squared deviation of job completion times from a common due date, and a deterministic equivalent objective function is developed. Characterizations of optimal schedules for this quadratic objective function are established both when the common due date is a decision variable and when it is given and fixed. Most importantly, the V-shaped nature of optimal schedules is investigated for a class of Poisson processes, {<i>N</i>(<i>t</i>), <i>t</i> > 0}, describing the number of breakdowns in the interval (0, <i>t</i>). In addition, relationships to a class of bicriteria models are demonstrated.", "e:keyword": ["Production/scheduling", "Sequencing: early/tardy", "Quadratic penalty functions", "Production/scheduling", "Stochastic: random breakdown"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.797", "e:abstract": "When scheduling <i>n</i> jobs on <i>m</i> identical machines in parallel, two performance criteria are of particular interest: the makespan (the completion time of the last job) and the flowtime (the sum of the completion times of all <i>n</i> jobs). Whereas minimizing makespan is NP-hard, many schedules minimize flowtime, and they are easy to characterize. This paper considers the problem of minimizing the makespan among flowtime-optimal schedules. Heuristics have appeared in the literature that result in flowtime-optimal schedules with makespans which are always less than or equal to 5/4 times the minimum flowtime-optimal makespan. In this note, we present new heuristics for this problem, one of which yields a worst-case bound of 28/27 for the two-machine case. The new heuristics have a running time of <i>O</i>(<i>n</i> log <i>n</i>). Extensions are discussed for general <i>m</i>.", "e:keyword": ["Production/scheduling", "Sequencing", "Deterministic", "Worst-case analysis of parallel machines scheduling problem", "Programming", "Fractional worst-case analysis of parallel machines scheduling problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.801", "e:keyword": ["Forecasting: combining forecasts"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.4.802", "e:keyword": ["Forecasting: combining forecasts"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.819", "e:abstract": "Many firms with self-funded medical insurance administered by outside parties fail to review the administrators' performance effectively. Often the firms do not know the extent of overpayments because they lack the expert knowledge to evaluate the paid claims. We examine the use of a simple expert system in conjunction with optimization methods for identifying claims payment errors. The knowledge base is constructed using expertise from the areas of claims processing, auditing, medical diagnosis, and procedure coding practices. Once potential errors are identified, a mathematical program is used to select claims for audit based on maximizing expected benefits subject to various firm-specific processing limitations. We use an efficient Lagrangian relaxation to solve instances of this pure binary programming problem with as many as 150,000 variables. We report on our experience in the use of this system on behalf of a Fortune 100 firm's claims data base.", "e:keyword": ["Health care: health care insurance coverage", "Information systems: used in auditing health care claims", "Programming", "Integer", "Applications: Lagrangian relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.835", "e:abstract": "A supply chain is a network of facilities that performs the functions of procurement of material, transformation of material to intermediate and finished products, and distribution of finished products to customers. Often, organizational barriers between these facilities exist, and information flows can be restricted such that complete centralized control of material flows in a supply chain may not be feasible or desirable. Consequently, most companies use decentralized control in managing the different facilities at a supply chain. In this paper, we describe what manufacturing managers at Hewlett-Packard Company (HP) see as the needs for model support in managing material flows in their supply chains. These needs motivate our initial development of such a model for supply chains that are not under complete centralized control. We report on our experiences of applying such a model in a new product development project of the DeskJet printer supply chain at HP. Finally, we discuss avenues to develop better models, as well as to fully exploit the power of such models in application.", "e:keyword": ["Inventory/production: inventory applications and multistage systems", "Manufacturing: supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.848", "e:abstract": "This paper introduces a model that describes how local drug markets might respond to intensive local enforcement operations known as crackdowns. The model supports the intuition of some drug policy analysts and makes concrete suggestions for how crackdowns should be managed.", "e:keyword": ["Economics: markets for illicit drugs", "Judicial/legal: drug policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.864", "e:abstract": "An approach to the solution of decision analysis problems under uncertainty with imprecise and incomplete information is presented. The methodology is designed for cases in which payoffs (conditional on the state of nature) are known precisely, but only limited or imprecise probability and utility information is available regarding a decision maker's beliefs and tastes. A decision maker provides: conditional payoffs, (optionally) bounds on state probabilities, bounds on the certainty equivalent for a simple lottery, any known relationships between probabilities of states of nature, and a series of strict preferences between pairs of vectors of conditional payoffs. We assume an exponential utility function with unknown parameter. The method proceeds by sequentially eliciting preferences, new bounds on probabilities and/or the certainty equivalent, and new relationships among probabilities until the problem is solved. The methodology is demonstrated through a two-state and a three-state example which illustrate the effects of the progressive elicitation of additional information.", "e:keyword": ["Decision analysis", "Risk: decision analysis with imprecise", "Incomplete information"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.880", "e:abstract": "Construction of simulation designs for the estimation of response surface metamodels is often based on optimal design theory. Underlying such designs is the assumption that the postulated model provides the correct representation of the simulated response. As a result, the location of design points and the assignment of pseudorandom number streams to these experiments are determined through the minimization of some function of the covariance matrix of the model coefficient estimators. In contrast, we assume that the postulated model may be incorrect. Attention is therefore directed to the development of simulation designs that offer protection against the bias due to possible model misspecification as well as error variance. The particular situation examined is the estimation of first-order response surface models in the presence of polynomials of order two. Traditional two-level factorial plans combined with one of three pseudorandom number assignment strategies define the simulation designs. Specification of the factor settings for these experimental plans are based on two integrated mean squared error criteria of particular interest in response surface studies. For both design criteria, comparisons of the optimal designs across the three assignment strategies are presented to assist experimenters in the selection of an appropriate simulation design.", "e:keyword": ["Simulation: design of experiments", "Statistics: estimation of first-order response surfaces"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.903", "e:abstract": "We study a class of point processes generated by transitions in Markov chains. We are primarily concerned with approximating superposed phase renewal processes by these point processes. We identify a subclass of Markov chains that we call Super-Erlang chains. These chains have special properties that facilitate the development of approximations. We outline an approximation procedure and provide computational results that demonstrate the potential of the approach. The primary motivation for this study is the analysis of open queueing networks.", "e:keyword": ["Probability", "Markov processes: approximating nonrenewal processes by Markov chains", "Queues", "Approximations: approximating arrival processes to queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.924", "e:abstract": "In this paper, we consider a scheduling problem in which <i>m</i> classes, <i>J</i><sub>1</sub>, <i>J</i><sub>2</sub>, …, <i>J<sub>m</sub></i>, of independent jobs with ready time 0 are to be processed by a single machine. The number of jobs of class <i>J<sub>i</sub></i> is <i>n<sub>i</sub></i> and the processing times of these <i>n<sub>i</sub></i> jobs are independent and identically distributed exponentially distributed with unknown parameter θ<i><sub>i</sub></i>, which has a conjugate gamma prior. The objective is to minimize the expected (weighted) sum of flowtimes of all the jobs, where <i>R<sub>i</sub></i> is the weight for a job of class <i>J<sub>i</sub></i>. The problem is formulated as a dynamic program and optimal strategies are derived.", "e:keyword": ["Dynamic programming/optimal control: Bayesian", "Dynamic allocation index", "Production/scheduling: sequencing", "Stochastic", "Single machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.935", "e:abstract": "This paper investigates the application of a new class of neighborhood search algorithms—cyclic transfers—to multivehicle routing and scheduling problems. These algorithms exploit the two-faceted decision structure inherent to this problem class: First, assigning demands to vehicles and, second, routing each vehicle through its assigned demand stops. We describe the application of cyclic transfers to vehicle routing and scheduling problems. Then we determine the worst-case performance of these algorithms for several classes of vehicle routing and scheduling problems. Next, we develop computationally efficient methods for finding negative cost cyclic transfers. Finally, we present computational results for three diverse vehicle routing and scheduling problems, which collectively incorporate a variety of constraint and objective function structures. Our results show that cyclic transfer methods are either comparable to or better than the best published heuristic algorithms for several complex and important vehicle routing and scheduling problems. Most importantly, they represent a novel approach to solution improvement which holds promise in many vehicle routing and scheduling problem domains.", "e:keyword": ["Programming", "Integer", "Heuristic: cyclic transfer algorithms for fleet planning problems", "Transportation", "Vehicle routing: cyclic transfer algorithms for multivehicle problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.947", "e:abstract": "A cyclic schedule is a sequence of tasks on a set of machines that is repeated indefinitely. We model cyclic schedules as Markov chains and use ergodic theory to analyze and improve the performance of cyclic schedules in environments with machine breakdowns, yield losses, and other sources of variability. The concept of cyclic task criticality is developed as a natural extension of task criticalities in PERT networks. We show that cyclic task criticalities can and should be used to guide the management of cyclic schedules.", "e:keyword": ["Production/scheduling: cyclic", "Project management", "PERT: Markovian networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.959", "e:abstract": "Resource allocation problems focus on the allocation of limited resources among competing activities. We examine such problems when certain substitutions among resources are possible. The substitutional relations can be represented by a graph comprised of multiple components. In each component, the nodes correspond to resources and the arcs correspond to feasible substitutions. The objective is of the minimax form, where each term is a continuous, strictly decreasing function of a single activity level. The objective is to minimize the largest term, subject to a limited supply of multiple resources. Potential applications to such problems are found, for example, in the manufacture of high tech products. We develop an efficient algorithm to solve such problems. At each iteration, a relaxed minimax problem is solved. A max-flow algorithm is then applied to determine whether the solution of the relaxed problem is feasible for the original problem. If the solution is infeasible, a tighter relaxed problem is formulated and resolved. The algorithm is also extended to find the lexicographic minimax solution. Computational results are presented.", "e:keyword": ["Production/scheduling", "Applications: allocation of manufacturing resources", "Programming", "Linear and nonlinear", "Algorithms: minimax algorithm for substitutable resources"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.972", "e:abstract": "Exact and heuristic procedures are often developed to obtain optimal and near-optimal solutions to decision problems modeled as activity networks. Testing the accuracy and efficiency of these procedures requires the use of activity networks with various sizes, structures, and parameters. The size of the network is determined by its number of nodes and arcs, where the structure is chosen from the set of all structures for the specified network size. The network parameters depend on the nature of the decision problem. Often, it is desirable for test problems to be generated at random from the space of all feasible networks. This paper deals with the problem of generating the size and structure of the network at random from the space of all feasible networks. It develops a theory which guarantees the randomness of the network structure. The theory is the basis for two methods. One can be used to generate dense networks, where the other is used to generate nondense networks. The methods, which are practical and easy to use, have been programmed for use on mainframe or personal computers. CPU time requirements are negligible. Copies of the computer program can be obtained from the authors.", "e:keyword": ["Networks/graphs", "Stochastic theory", "Project management", "PERT", "Resource constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.981", "e:abstract": "The problem of preemptively scheduling <i>N</i> jobs on <i>M</i> identical parallel machines to minimize the maximum completion time is considered. Jobs are divided into <i>B</i> batches and a setup time on a machine is necessary whenever there is a switch from processing a job in one batch to a job in another batch. Setup times are assumed to depend only on the batch of the job to be scheduled next. Two types of heuristics are proposed and analyzed. The first type uses list scheduling for complete batches and then splits batches between selected pairs of machines. It has a time requirement of <i>O</i>(<i>N</i> + (<i>M</i> + <i>B</i>) log(<i>M</i> + <i>B</i>)). Furthermore, for a certain class of problems which includes the case that each batch contains a single job, it has a worst-case performance ratio of 3/2 − 1/(4<i>M</i> − 4) when <i>M</i> ≤ 4 and of 5/3 − 1/<i>M</i> when <i>M</i> is a multiple of 3 and <i>M</i> > 3. The second type of heuristic uses a procedure which resembles McNaughton's preemptive scheduling algorithm. It requires <i>O</i>(<i>N</i>) time and has a worst-case performance ratio of <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$2-1/(\\lfloor M/2\\rfloor + 1)$\\end{document}</tex-math></inline-formula>.", "e:keyword": ["Production/scheduling: identical parallel machines", "Preemption", "Batch setup times", "Heuristics", "Worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.5.994", "e:abstract": "The repair kit problem is concerned with finding an optimal kit of parts and tools to carry for on-site repairs. The choice of a kit involves evaluating two attributes—an annual holding cost and a penalty for failing to complete repairs. We present a unifying approach for the repair kit problem which demonstrates that a monotone sequence of optimal kits exists for several parameterizations of the objective function combining the two attributes. We analyze the structure of the Pareto set of the convex hull of the kits in the attribute space and show the relationship between the extreme points of these Pareto sets and the optimal kits. Decomposition and various monotonicity properties of the repair kit problem yield some computational simplifications in generating optimal kits.", "e:keyword": ["Inventory/production", "Sensitivity analysis: repair kits", "Programming: minimizing a submodular function", "Programming", "Multiple criteria: Pareto optimality"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1013", "e:abstract": "A standard formulation of a real-world distribution problem could not be solved, even for a good solution, by a commercial mixed integer programming code. However, after reformulating it by reducing the number of 0-1 variables and tightening the linear programming relaxation, an optimal solution could be found efficiently. The purpose of this paper is to demonstrate, with a real application, the practical importance of the need for good formulations in solving mixed integer programming problems.", "e:keyword": ["Programming", "Integer", "Algorithms: solving a distribution problem", "Transportation", "Models: solving a distribution problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1020", "e:abstract": "The Project Appraisal Methodology (PAM) was originally developed in 1975 and first run at the Gas Research Institute (GRI) in 1978 to help upper management select portfolios of research and development (R&D) projects to fund. Since that time, GRI has had 132 successful project commercializations and has achieved a success rate of 30% or over twice the documented industry average. GRI attributes its success to several decision making principles founded in management science. PAM is the cornerstone of GRI's R&D decision making process. In particular, GRI's projects are generating benefits for natural gas consumers and suppliers (savings plus sales increases) of $7–15 billion per year on an annual investment of about $1.6 billion and netting almost a 7 to 1 annual rate of return on their investment. Just in 1985–1989 these new technologies contributed to a savings of $70 billion ($78 billion, NPV, 1990$s) in the wellhead price of natural gas in the USA. Additionally, the application of management science at GRI has led to increases in available natural gas supplies (valued at $10–35 billion), a corresponding reduction in dependence on imports, major environmental and safety benefits, and vital changes in the position of the gas industry in the home heating, commercial cooking, and numerous other markets.", "e:keyword": ["Decision analysis: multiple criteria", "Information systems: decision support system", "Research and development: project selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1033", "e:abstract": "P. Korhonen, H. Moskowitz, and J. Wallenius (1986) developed a progressive algorithm and the supporting theory for modeling and solving multiple criteria decision problems with discrete alternatives. A special feature of the algorithm is that it relaxes the usual assumption of a fixed set of available decision alternatives and complete knowledge of a decision maker's (DM's) preference structure or value function. The algorithm is based on progressively sampling the decision space, obtaining preference information from the DM, determining the likelihood of finding possibly/surely better alternatives, and based on this information, continuing the search or terminating it by making the final choice. We describe a computerized implementation and extensive computational tests of the algorithm, as well as some of our experiences in applying and field testing it in practice. We also consider several improvements and developments of the algorithm to further facilitate its use in practice.", "e:keyword": ["Decision analysis: sequential", "Programming: multiple criteria", "Utility/preference: applications", "Multiattribute theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1046", "e:abstract": "All previous modeling research involving optimization of performance associated with work-rest cycles has focused upon individual employees working independently. We extend this earlier research by considering the choices of placement for and duration of a single rest period that must be taken simultaneously by all employees in a work group. Assuming linear work-rate decay and recovery functions for individual employees within the group, we show that an appropriate model can be formulated as a mixed-binary, cubic programming problem. We develop an efficient optimal solution procedure with a computational time that appears to be a linear function of the number of employees considered. We report on a preliminary simulation experiment that evaluates the productivity loss that occurs when the rest break policy is determined based on the work characteristics of a randomly selected subset of employees in the work group, and provides an initial exploration of the nature of optimal policy variables. Finally, we offer suggestions for future research.", "e:keyword": ["Labor: finding the optimal timing and length of a common rest break", "Programming", "Nonlinear algorithms an efficient algorithm for a mixed-binary cubic problem", "Simulation", "Applications: measuring employee work-group performance"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1055", "e:abstract": "Given a complete directed graph <i>G</i> = (<i>V</i>, <i>A</i>), the delivery man problem (DMP) consists of determining a Hamiltonian circuit minimizing the sum of distances (along the circuit) from a given vertex <i>v</i><sub>1</sub>, to every vertex of <i>V</i>, including <i>v</i><sub>1</sub> itself. There exists a number of applications of the DMP in the fields of distribution and machine scheduling. The DMP is NP-hard. The objective of this paper is to develop new theoretical results and an exact algorithm for the problem. A new, integer linear programming formulation is provided, and results on the matroidal structure of a class of combinatorial problems are developed. These are used to derive lower bounds for the DMP. These bounds are embedded into an enumerative algorithm. The largest problems solved to optimality with the proposed algorithm involve 60 vertices. This compares favorably with previously published methods.", "e:keyword": ["Mathematics", "Combinatorics: cumulative matroid problems", "Transportation", "Scheduling: single machine sequencing with change-over costs", "Transportation", "Vehicle routing: routing problems with cumulative distances"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1065", "e:abstract": "We present an overview of basic models and solution algorithms for the lot streaming problem. We include models with continuous and discrete sublot sizes, models with and without intermittent idling of machines, and models with consistent and variable sublots. We also introduce a model with limited transporter capacity. First we present solutions for two machines, then generalize to three machines and, where possible, to several machines. We synthesize previous research and introduce several new results.", "e:keyword": ["Inventory/production", "Multistage: overlapping operations on consecutive resources", "Production/scheduling: flow shop scheduling for batch production", "Production/scheduling", "Multiple machine: transfer lots scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1077", "e:abstract": "Automated guided vehicles (AGVs) are a highly sophisticated and increasingly popular type of material handling device in flexible manufacturing systems. This paper details solution methodologies for the static routing problem in which demand assignment of the AGVs are known; the focus is to obtain an implementable solution within a reasonable amount of computer time. The objective is to minimize the makespan, while routing AGVs on a bidirectional network in a conflict-free manner. This problem is solved via column generation. The master problem in this column generation procedure has the makespan and vehicle interference constraints. Columns in the master problem are routes iteratively generated for each AGV. The subproblem is a constrained shortest path problem with time-dependent costs on the edges. An improvement procedure is developed to better the solution obtained at the end of the master-subproblem interactions. Several methods of iterating between the master and subproblem are experimented with in-depth computational experiments. Our empirical results indicate that the procedure as a whole usually generates solutions that are within a few percent of a proposed bound, within reasonable computer time.", "e:keyword": ["Production/scheduling: planning", "Transportation: freight/materials handling", "Traffic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1091", "e:abstract": "Many flexible manufacturing systems (FMS) involve complex queueing networks in the processing of lots (jobs) in a production facility. In particular, the feedback of lots to a common, multipurpose workstation, or hub, processing different tasks for the same lot at various points in the production process, can create serious scheduling problems. This paper models the hub structure and presents some results for dual measures of flow time in the facility, namely, total flow time required for a fixed number of lots (or makespan), and average flow time for all lots (or average completion time). The flow time performance is a function of different batch sizes and priority scheduling rules for lots arriving at the hub. The results can be used to establish strategies for batching and scheduling lots according to adjustments in the importance of the cycle time versus the work-in-process (WIP) tradeoff. Extensions of the results to more general production facilities are also discussed.", "e:keyword": ["Inventory/production", "Heuristics: algorithms for FMS hub scheduling", "Production/scheduling", "Flexible manufacturing: scheduling deterministic hub networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1104", "e:abstract": "A fluid network is a deterministic network model in which dynamic continuous flows are circulated and processed among a set of stations. A fluid network often describes the asymptotic behavior of a stochastic queueing network via functional strong law of large numbers. We study the dynamic scheduling of multiple classes of fluid traffic in such a network. An algorithm is developed that systematically solves the dynamic scheduling problem by solving a sequence of linear programs. It generates a policy, in the form of dynamic capacity allocation at each station (among all fluid classes), that consists of a finite set of linear “pieces” over the entire time horizon. In a single-station, or equivalently, single-server, network, this solution procedure recovers the priority index set that is optimal for the corresponding discrete queueing model, generally known as Klimov's problem.", "e:keyword": ["Production/scheduling: dynamic control", "Resource allocation", "Programming: linear", "Parametric", "Queues: fluid network", "Multiclass"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1116", "e:abstract": "We propose a class of nonstationary policies called policy time sharing (PTS), which possesses several desirable properties for problems where the criteria are of the average-cost type; an optimal policy exists within this class, the computation of optimal policies is straightforward, and the implementation of this policy is easy. While in the finite state case stationary policies are also known to share these properties, the new policies are much more flexible, in the sense that they can be applied to solve adaptive problems, and they suggest new ways to incorporate the particular structure of the problem at-hand into the derivation of optimal policies. In addition, they provide insight into the pathwise-structure of controlled Markov chains. To use PTS policies one alternates between the use of several stationary deterministic policies, switching when reaching some predetermined state. In some (countable state) cases optimal solutions of the PTS type are available and easy to compute, whereas optimal stationary policies are not available. Examples that illustrate the last point and the usefulness of the new approach are discussed, involving constrained optimization problems with countable state space or compact action space.", "e:keyword": ["Decision analysis", "Multiple criteria: Markov decision processes with several constraints", "Dynamic programming/optimal control", "Markov: nonstationary policies", "Sample path properties"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1125", "e:abstract": "Optimal group maintenance policies for a set of <i>M</i> identical machines subject to stochastic failures are considered. The control of the system is not based on the complete age configuration of all components, nor on the number of failed components only. We compromise between these two extreme cases by introducing four possible states for each component: good, doubtful, preventive maintenance is due, and failed. Two types of control policies are considered, both based on the number of doubtful components at component failure epochs. Starting from a general model with general (but identical) lifetime distributions for the individual components, we introduce an approximate model in which the four possible states are identified with certain age intervals for each individual component. The sojourn times in the good and the doubtful state are supposed to be exponentially distributed. For this resulting approximate model, explicit expressions are derived for various performance measures, like the time to system replacement and the average costs per unit time. By making use of the results obtained for the approximate model several approximations for the performance measures of the original model are presented. Validation of these approximations is performed by simulation.", "e:keyword": ["Reliability", "Multistate systems: simple replacement policies", "Reliability", "Replacement/renewal: systems with many identical components"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1137", "e:abstract": "In this paper we obtain the optimal admission control policy for a first-come, first-served (FCFS) <i>M</i>/<i>M</i>/<i>m</i> ordered-entry queueing system to maximize the expected discounted (and the long-run average) profit (reward minus cost). We introduce a new approach for determining the optimal admission control policy. The underlying idea of this approach is to construct a dual system: a preemptive last-come, first-served (LCFS-P) <i>M</i>/<i>M</i>/<i>m</i> ordered-entry queueing system that is subject to <i>expulsion control</i>. We show that an LCFS-P system with expulsion control is isomorphic to an FCFS system with admission control, and the two systems share the same optimal control policy. Eluding the conventional dynamic programming formulation, we approach the solution from behaviors of individual customers and their impact on the social outcome. This makes our analysis simple and intuitive and reveals a better insight into the structural properties of the optimal control policy. Besides providing formulas to compute the optimal threshold, we use the operational characteristics of the dual system to obtain an easily computable approximation for the optimal threshold. The applicability of the approach transcends well beyond the problem addressed in this paper.", "e:keyword": ["Queues", "Optimization: admission control policy", "Bulking and reneging ordered-entry queues", "FCFS", "LCFS"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1153", "e:abstract": "This paper considers a general form of the single facility minisum location problem (also referred to as the Fermat-Weber problem), where distances are measured by an <i>l<sub>p</sub></i> norm. An iterative solution algorithm is given which generalizes the well-known Weiszfeld procedure for Euclidean distances. Global convergence of the algorithm is proven for any value of the parameter <i>p</i> in the closed interval [1, 2], provided an iterate does not coincide with a singular point of the iteration functions. However, for <i>p</i> > 2, the descent property of the algorithm and as a result, global convergence, are no longer guaranteed. These results generalize the work of Kuhn for Euclidean (<i>p</i> = 2) distances.", "e:keyword": ["Facilities/equipment planning", "Location", "Continuous: convergence of a procedure for location with lp distances"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1164", "e:abstract": "For the Euclidean single facility location problem, E. Weiszfeld proposed a simple closed-form iterative algorithm in 1937. Later, numerous authors proved that it is a convergent descent algorithm. In 1973, J. Eyster, J. White and W. Wierwille extended Weiszfeld's idea and proposed a Hyperboloid Approximation Procedure (HAP) for solving the Euclidean multifacility location problem. They believed, based on considerable computational experience, that the HAP always converges. In 1977, Ostresh proved that the HAP is a descent algorithm under certain conditions. In 1981, Morris proved that a variant of the HAP always converges. However, no convergence proof for the original HAP has ever been given. In this paper, we prove that the HAP is a descent algorithm and that it always converges to the minimizer of the objective function from any initial point.", "e:keyword": ["Facilities/equipment planning: location", "Transportation: location model"]}, {"@id": "http://dx.doi.org/10.1287/opre.41.6.1172", "e:abstract": "We present an implicit enumeration procedure for solving pure integer 0/1 minimax problems which arise in the context of Benders decomposition for mixed integer 0/1 linear programming problems, or in various practical settings such as the location of facilities and assembly line balancing. The procedure is an extension of the additive algorithm of E. Balas for pure integer 0/1 programming problems. We solve minimax problems directly (i.e., as minimax problems, not as mixed integer programming problems). A numerical example is used to illustrate the procedure. Extensions of the basic algorithm are discussed.", "e:keyword": ["Production/scheduling: production line balancing", "Programming", "Integer", "Algorithms: Benders decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.102", "e:abstract": "We consider the single-machine problem of scheduling <i>n</i> jobs to minimize the sum of the deviations of the job completion times from a given small common due date. For this NP-hard problem, we develop a branch-and-bound algorithm based on Lagrangian lower and upper bounds that are found in <i>O</i>(<i>n</i> log <i>n</i>) time. We identify conditions under which the bounds concur; these conditions can be expected to be satisfied by many instances with <i>n</i> not too small. In our experiments with processing times drawn from a uniform distribution, the bounds concur for <i>n</i> ≥ 40. For the case where the bounds do not concur, we present a refined lower bound that is obtained by solving a subset-sum problem of small dimension to optimality. We further develop a 4/3-approximation algorithm based upon the Lagrangian upper bound.", "e:keyword": ["Production/scheduling: one-machine scheduling problem", "Programming: Lagrangian relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.111", "e:abstract": "A broad class of production-inventory systems is studied in which a number of producing machines are susceptible to failure following which they must be repaired to make them operative again. The machines' production can also be stopped deliberately due to stocking capacity limitations or any other relevant considerations. The interplay between the processes involved, namely, production, demand, and failure/repair or reliability, in conjunction with the shutdown policy used, determine the inventory accumulation process and possible shortages. We first obtain the stationary distribution of the inventory process for different assumptions on the random behavior of the production, demand, and reliability processes. By employing level-crossing techniques, a mathematical analysis is carried out for a “core” model, which then serves the role of the nucleus for the study of a wide range of models. We compute performance measures that characterize the operation of the production-inventory system with respect to its service-level to customers, expected inventory stocked, machines' utilization, repairmen utilization, and so on. A numerical illustration is provided which shows the effect of machine breakdowns on service and inventory levels.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic: several machines", "Reliability: machine failures"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.119", "e:abstract": "In heavy traffic analysis of open queueing networks, processes of interest such as queue lengths and workload levels are generally approximated by a multidimensional reflected Brownian motion (RBM). Decomposition approximations, on the other hand, typically analyze stations in the network separately, treating each as a single queue with adjusted interarrival time distribution. We present a hybrid method for analyzing generalized Jackson networks that employs both decomposition approximation and heavy traffic theory: Stations in the network are partitioned into groups of “bottleneck subnetworks” that may have more than one station; the subnetworks then are analyzed “sequentially” with heavy traffic theory. Using the numerical method of J. G. Dai and J. M. Harrison for computing the stationary distribution of multidimensional RBMs, we compare the performance of this technique to other methods of approximation via some simulation studies. Our results suggest that this hybrid method generally performs better than other approximation techniques, including W. Whitt's QNA and J. M. Harrison and V. Nguyen's QNET.", "e:keyword": ["Queues", "Networks: performance analysis of generalized Jackson networks", "Probability", "Stochastic model applications: heavy traffic and Brownian system model"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.137", "e:abstract": "This paper discusses the application of the likelihood ratio gradient estimator to simulations of large Markovian models of highly dependable systems. Extensive empirical work, as well as some mathematical analysis of small dependability models, suggests that (in this model setting) the gradient estimators are not significantly more noisy than the estimates of the performance measures themselves. The paper also discusses implementation issues associated with likelihood ratio gradient estimation, as well as some theoretical complements associated with application of the technique to continuous-time Markov chains.", "e:keyword": ["Probability", "Stochastic model applications: highly dependable systems", "Simulation: statistical analysis of derivative estimates", "Simulation", "Efficiency: importance sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.14", "e:abstract": "Emergence of a new discount pricing schedule called Business Volume Discount becomes a major obstacle for procurement managers in finding the best purchasing strategy. In the context of business volume discount, a supplier offers discounts on total dollar amount of sales volume, not on the quantity or variety of the products purchased from the supplier. This paper describes a Procurement Decision Support System (PDSS) that has been successfully implemented to improve the purchasing activities of regional Bell telephone companies. In most purchasing operations, a flexible procurement plan is considered a necessity because of the uncertainties associated with product demand and procurement budget. PDSS provides this flexibility by combining two different purchasing strategies—purchasing on an <i>annual commitment</i> basis, and on an <i>as-ordered</i> basis. Although the development of the system is motivated by a particular application (procurement of transmission plug-in) in one of the regional telephone companies, the system is being used for a variety of procurement decisions involving business volume discount in the telecommunications industry. The results of the implementation indicate that cost savings of up to 15% are achieved. PDSS use is not limited to the companies in the telecommunications industry. The PDSS model is applicable to any organization having a centralized procurement operation with business volume discount.", "e:keyword": ["Decision support systems: procurement analysis", "Inventory/production", "Policies", "Pricing: business volume discounts", "Programming", "Linear", "Integer", "Applications: mixed integer programming model for procurement problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.158", "e:abstract": "In this paper, we analyze a queue to which the arrival process is the superposition of separate arrival streams, each of whose interarrival time distributions is of phase type, and the service time distribution is also of phase type. The performance measures derived for this queue include: the distribution of the number in the system as seen by each customer class upon arrival, Laplace-Stieltjes transform (LST) of the waiting-time distribution for each customer class, stationary interdeparture time distribution and the lag correlation coefficients of the departure process, and characteristics of the tails of the waiting time and queue length distributions.", "e:keyword": ["Queues", "Algorithms: algorithms for analyzing the ΣPhi/Ph/1 queue", "Queues", "Approximations: for departure processes", "Queues", "Birth-death: quasibirth-death processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.175", "e:abstract": "A stationary policy and an initial state in an MDP (Markov decision process) induce a stationary probability distribution of the reward. The problem analyzed here is generating the Pareto optima in the sense of high mean and low variance of the stationary distribution. In the unichain case, Pareto optima can be computed either with policy improvement or with a linear program having the same number of variables and one more constraint than the formulation for gain-rate optimization. The same linear program suffices in the multichain case if the ergodic class is an element of choice.", "e:keyword": ["Dynamic programming", "Markov: mean-variance tradeoff", "Programming", "Multiple criteria: mean-variance tradeoff"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.184", "e:abstract": "The problem analyzed here is the computation of Pareto optima in the sense of high mean and low variance of the stationary distribution in the unichain, undiscounted Markov decision process (MDP, for short).", "e:keyword": ["Decision analysis", "Risk: mean-variance tradeoffs in MDP", "Dynamic programming", "Markov", "Finite state: mean-variance tradeoffs"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.189", "e:abstract": "We consider open shop problems with unit processing times and due dates, where <i>n</i> jobs have to be processed on <i>m</i> machines. The order in which a given job is processed on the machines is not fixed. Such problems occur in testing components of an electronic system or doing repair work on automobiles. In an earlier paper, C. Y. Liu and R. L. Bulfin gave an <i>O</i>(<i>n</i><sup>2</sup><i>m</i>) algorithm to minimize total tardiness and the number of tardy jobs. We will give a polynomial algorithm to minimize the completion time of all jobs where a deadline is imposed for each job. The complexity of this problem is still open. Then we apply this solution to give improved algorithms to minimize the number of tardy jobs and the maximum lateness.", "e:keyword": ["Production/scheduling: open shop"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.192", "e:abstract": "The joint distribution of the length of a busy period and the number of customers served during that busy period in an <i>M</i>/<i>G</i>/1 queue with a finite capacity is expressed as coefficients of a power-series expansion of an explicit function. This new result is related to the previous result by T. J. Harris (1971). An error in his result is found and corrected.", "e:keyword": ["Queues", "Busy period analysis: M/G/1/K queue"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.24", "e:abstract": "We obtain an optimal operation policy for a general <i>n</i>-server (channel, machine) stand-by system involving preventive maintenance and operating in <i>real time</i>. We show that such a policy is achieved by successive total use of each server capacity, and <i>does not depend on the arrival pattern</i> of processed data and task duration. We show also that this policy is optimal for any finite, initial period of time, and not only in the sense of long-run availability. The analysis is performed in two stages: a worst-case analysis is performed, and then, a general case is treated. Such a consecutive approach also seems to be useful in the analysis of more complicated systems. Rigorous mathematical proofs are provided. Actual and possible applications of results to military and production control systems are discussed.", "e:keyword": ["Military", "Defense systems: intelligence", "Production/scheduling: real-time data processing", "Reliability", "Availability: multiserver stand-by system"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.31", "e:abstract": "The OR Practice Section of <i>Operations Research</i> is approaching its tenth anniversary. This note examines and interprets the nature of the papers that it has attracted and argues that they suggest vigor in the practice of operations research.", "e:keyword": ["Professional", "Comments on: ten years of OR practice", "Professional", "Journal policies: OR practice section"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.34", "e:abstract": "This paper reviews the record of OR/MS applications to library management. Although libraries constitute a major factor in both the public and the private (not-for-profit) sectors, OR/MS has not fulfilled its potential in enhancing the cost effectiveness of library systems.", "e:keyword": ["Libraries: survey of OR in libraries", "Taxonomy of library applications of OR/MS"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.41", "e:abstract": "We give a new formulation to the multiple-depot vehicle scheduling problem as a set partitioning problem with side constraints, whose continuous relaxation is amenable to be solved by column generation. We show that the continuous relaxation of the set partitioning formulation provides a much tighter lower bound than the additive bound procedure previously applied to this problem. We also establish that the additive bound technique cannot provide tighter bounds than those obtained by Lagrangian decomposition, in the framework in which it has been used so far. Computational results that illustrate the robustness of the combined set partitioning-column generation approach are reported for problems four to five times larger than the largest problems that have been exactly solved in the literature. Finally, we show that the gap associated with the additive bound based on the assignment and shortest path relaxations can be arbitrarily bad in the general case, and as bad as 50% in the symmetric case.", "e:keyword": ["Transportation", "Mass transit: multiple-depot vehicle scheduling", "Transportation", "Scheduling", "Vehicles: urban bus"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.5", "e:abstract": "In the last decade, new advances in algorithms have been as important as the impressive advances in computer technology. Using the new interior-point algorithms and advanced implementations of simplex methods, we can now solve linear programs with more than one million variables and thousands of constraints. Preprocessing and polyhedral theory have yielded at least an order of magnitude improvement in branch-and-bound algorithms for solving mixed integer programs. Moreover, these algorithmic advances have been incorporated in commercially inexpensive software that is readily available, easily portable, and supported by a variety of systems that make it possible for unsophisticated users to input and check their models and obtain understandable outputs. This paper, based on the Morse Lecture given in May 1993 at the TIMS/ORSA meeting in Chicago, begins with some of the modem history of optimization, then surveys some recent developments (illustrating them with an application in the airline industry), and closes with some remarks about the future.", "e:keyword": ["Professional", "Addresses: May 1993 Morse lecture on optimization", "Programming advances in algorithms for large-scale", "Real-world problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.53", "e:abstract": "A technique for generating cutting planes for integer programs is introduced that is based on the ability to optimize a linear function on a polyhedron rather than explicit knowledge of the underlying polyhedral structure of the integer program. The theoretical properties of the cuts and their relationship to Lagrangian relaxation are discussed, the cut generation procedure is described, and computational results are presented.", "e:keyword": ["Programming", "Integer", "Algorithms", "Relaxation/subgradient: Fenchel cutting planes"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.65", "e:abstract": "This paper reports on a new parallel implementation of the primal simplex method for minimum cost network flow problems that decomposes both the pivoting and pricing operations. The self-scheduling approach is flexible and efficient; its implementation is close in speed to the best serial code when using one processor, and is capable of substantial speedups as parallel computing units are added. An in-depth computational study of randomly generated transportation and transshipment problems verified the effectiveness of this approach, with results on a 20-processor 80386-based system that are competitive with, and occasionally superior to, massively parallel implementations using tens of thousands of processors. A microanalysis of the code's behavior identified unexpected sources of (the occasionally superlinear) speedup, including the evolutionary topology of the network basis.", "e:keyword": ["Computers/computer science", "Software: shared-memory parallel processing software", "Networks", "Flow algorithms: primal simplex algorithm for pure networks", "Programming", "Large-scale systems: optimization of large time-critical networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.81", "e:abstract": "This paper addresses a material staging problem arising from a dual delivery pick-and-place machine that is used for the assembly of printed circuit boards where components are attached using “surface mount technology.” The staging problem of such machines involves allocation of component feeders to its two-feeder carriers along with the corresponding sets of vacuum nozzles to dispense the components. The usual managerial objective for the staging problem is minimization of the total time required to populate the circuit boards, i.e., the pick-and-place machine's cycle time. For the dual delivery pick-and-place machines this implies minimization of the workload imbalance between the two carriers and reduction of the number of vacuum nozzle changes required. In this paper, we develop algorithms to optimally solve the single and multiple product staging problem for the above managerial objectives.", "e:keyword": ["Production/scheduling: pick-and-place machine", "Electronic assembly", "Programming", "Integer: Lagrangian relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.1.92", "e:abstract": "We extend the traditional model of the economic lot scheduling problem by considering various issues associated with the management of the raw materials for production. Several items are produced on a single machine at finite production rates with setup times, and setup and inventory holding costs. Raw materials are used for the production of these items. In the presence of setup and holding costs for the raw materials, we first formulate a planning model which provides a sharp lower bound on the cost of any policy for the problem. The solution is used to obtain policies which are guaranteed to be very close to optimal in the worst case. Finally, we consider the problem of obtaining good feasible schedules for both the machine and the raw materials.", "e:keyword": ["Inventory/production: multi-item", "Multi-echelon production schedules", "Production/scheduling: economic lot scheduling problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.201", "e:abstract": "Deductive algorithmic science has reached a high level of sophistication, but its worst-case and average-case results seldom tell us how well an algorithm is actually going to work in practice. I argue that an empirical science of algorithms is a viable alternative. I respond to misgivings about an empirical approach, including the prevalent notion that only a deductive treatment can be “theoretical” or sophisticated. NP-completeness theory, for instance, is interesting partly because it has significant, if unacknowledged, empirical content. An empirical approach requires not only rigorous experimental design and analysis, but also the invention of empirically-based explanatory theories. I give some examples of recent work that partially achieves this aim.", "e:keyword": ["Analysis of algorithms: empirical methods for analysis of algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.213", "e:abstract": "Twelve selected expert modelers described themselves as modelers, the models they make, the problems they model, and the way they model. They also expressed opinions about the qualities of effective models, modelers, modeling processes, and desirable modeling clients. Finally, they provided stories about their modeling experiences. Their responses provide benchmarks for developing a modeling science and validate the importance of the craft aspects of OR/MS practice. The paper suggests changes in instructors' approaches to OR/MS content and describes a new course that responds to the experts' insights.", "e:keyword": ["Philosophy of modeling: experts' opinions on model craft and quality"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.223", "e:abstract": "Increasing performance pressures on fixed-income managers have led to a search for new and creative ways to add to portfolio returns. The largest pension plan sponsors, insurance companies, foundations, and money management firms are using indexed portfolios as their fixed-income assets management strategies since the late 1970s. Tracking a fixed-income index is a difficult task due to transaction costs, portfolio size and diversification restrictions, liquidity requirements, bid/ask spreads, etc. This paper develops an integrated simulation and optimization approach for tracking fixed-income indices. The model was implemented at Metropolitan Life Insurance Company. We introduce a simulation model for generating scenarios of <i>holding period returns</i> of the securities in the index. Then we develop optimization models to select a portfolio that tracks the index. The models penalize downside deviations of the portfolio return from the index. The developed framework is used to track the <i>Salomon Brothers Mortgage Index</i>. In backtesting over the period 1989–1991, the models outperformed the index by 50bp. Underperformance never exceeded more than −5bp in any single month. A small tracking error was also observed during the recent months that the model has been in use.", "e:keyword": ["Finance: portfolio management", "Indexation", "Financial institutions: insurance companies", "Programming: utility maximization"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.234", "e:abstract": "We examine scheduling problems where we control not only the assignment of jobs to machines, but also the time used by the job on the machine. For instance, many tooling machines allow control of the speed at which a job is run. Increasing the speed incurs costs due to machine wear, but also increases throughput. We discuss some fundamental scheduling problems in this environment and give algorithms for some interesting cases. Some cases are inherently difficult so for these we give heuristics. Our approach illustrates the exploitation of underlying network structure in combinatorial optimization problems. We create heuristics that optimally schedule a large portion of the jobs and then attempt to fit in the remainder. This also gives a method for quickly finding valid inequalities violated by the linear relaxation solution. For the problem of minimizing the sum of makespan and production costs, a rounding heuristic is within a constant factor of optimal. Our heuristics are compared to other classical heuristics.", "e:keyword": ["Networks/graphs", "Heuristics: heuristics for scheduling machines", "Production/scheduling", "Open shop", "Single stage: variable speed machines"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.249", "e:abstract": "Motivated by the important problem of congestion costs (they were estimated to be $2 billion in 1991) in air transportation and observing that ground delays are more preferable than airborne delays, we have formulated and studied several integer programming models to assign ground-holding delays optimally in a general network of airports, so that the total (ground plus airborne) delay cost of all flights is minimized. All previous research on this problem has been restricted to the single-airport case, which neglects “down-the-road” effects due to transmission of delays between successive flights performed by the same aircraft. We formulate several models, and then propose a heuristic algorithm which finds a feasible solution to the integer program by rounding the optimal solution of the LP relaxation. Finally, we present extensive computational results with the goal of obtaining qualitative insights on the behavior of the problem under various combinations of the input parameters. We demonstrate that the problem can be solved in reasonable computation times for networks with at least as many as 6 airports and 3,000 flights.", "e:keyword": ["Programming: integer", "Applications", "Transportation: air traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.262", "e:abstract": "This paper explores the application of stochastic process ideas to answer some fundamental questions about a state system which arises in the Mode Select Beacon System (Mode-S), an air-space management system. A series of models is developed, based on semi-Markov processes, population models, and Monte Carlo simulation. Computer programs in the APL programming language to implement the models and to process the resulting data are developed. The output of the models illuminate the behavior of the state system by providing predictions with confidence bounds about the movements of aircraft among the states over time.", "e:keyword": ["Probability", "Markov processes: semi-Markov processes", "Probability", "Stochastic model applications: air-space management"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.274", "e:abstract": "To remain competitive, manufacturers must seek transportation strategies that both reduce costs and maintain high levels of service. One approach is to consolidate inbound freight at transshipment points. This provides economies of scale and promotes capacity efficient mixes of high and low density items. When service level considerations are included via inventory holding costs, the approach yields a nonlinear network model with multiattribute multicommodity flows. The model is difficult to solve for global optimality in that the objective function is neither convex nor concave; therefore, a composite algorithm is proposed. The algorithm alternates between a linearization technique to find local optima and a heuristic search based on “adjacent concave flows” to provide local improvements. Computational results demonstrate the ability of the multiattribute approach to identify savings in overall transportation and inventory costs when compared to a single attribute approach.", "e:keyword": ["Manufacturing", "Strategy: transportation cost reductions under JIT", "Transportation", "Freight/materials handling: freight consolidation at transshipment terminals", "Transportation", "Models", "Network: nonlinear costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.287", "e:abstract": "We consider the famous bin packing problem where a set of items must be stored in bins of equal capacity. In the classical version, the objective is to minimize the number of bins used. Motivated by several optimization problems that occur in the context of the storage of items, we study a more general cost structure where the cost of a bin is a concave function of the number of items in the bin. The objective is to store the items in such a way that total cost is minimized. Such cost functions can greatly alter the way the items should be assigned to the bins. We show that some of the best heuristics developed for the classical bin packing problem can perform poorly under the general cost structure. On the other hand, the so-called next-fit increasing heuristic has an absolute worst-case bound of no more than 1.75 and an asymptotic worst-case bound of 1.691 for any concave and monotone cost function. Our analysis also provides a new worst-case bound for the well studied next-tit decreasing heuristic when the objective is to minimize the number of bins used.", "e:keyword": ["Analysis of algorithms: heuristics and worst-case analysis", "Mathematics: bin packing and combinatorial optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.299", "e:abstract": "The dispersion problem arises in selecting facilities to <i>maximize</i> some function of the distances between the facilities. The problem also arises in selecting nondominated solutions for multiobjective decision making. It is known to be NP-hard under two objectives: maximizing the minimum distance (<b>MAX-MIN</b>) between any pair of facilities and maximizing the average distance (<b>MAX-AVG</b>). We consider the question of obtaining near-optimal solutions. for <b>MAX-MIN</b>, we show that if the distances do not satisfy the triangle inequality, there is no polynomial-time relative approximation algorithm unless <b>P</b> = <b>NP</b>. When the distances satisfy the triangle inequality, we analyze an efficient heuristic and show that it provides a performance guarantee of two. We also prove that obtaining a performance guarantee of less than two is NP-hard. for <b>MAX-AVG</b>, we analyze an efficient heuristic and show that it provides a performance guarantee of four when the distances satisfy the triangle inequality. We also present a polynomial-time algorithm for the 1-dimensional <b>MAX-AVG</b> dispersion problem. Using that algorithm, we obtain a heuristic which provides an asymptotic performance guarantee of π/2 for the 2-dimensional <b>MAX-AVG</b> dispersion problem.", "e:keyword": ["Analysis of algorithms: computational complexity", "Facilities/equipment planning: discrete location", "Programming: heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.311", "e:abstract": "Simulation and optimization are among the most commonly used elements in the OR toolkit. Often times, some of the data elements used to define an optimization problem are best described by random variables, yielding a stochastic program. If the distributions of the random variables cannot be specified precisely, one may have to resort to simulation to obtain observations of these random variables. In this paper, we present conditional stochastic decomposition (<b>CSD</b>), a method that may be construed as providing an algorithmic interface between simulation and optimization for the solution of stochastic linear programs with recourse. Derived from the concept of the stochastic decomposition of such problems, <b>CSD</b> uses randomly generated observations with a Renders decomposition of the problem. In this paper, our method is analytically verified and graphically illustrated. In addition, <b>CSD</b> is used to solve several test problems that have appeared in the literature. Our computational experience suggests that <b>CSD</b> may be particularly well suited for situations in which randomly generated observations are difficult to obtain.", "e:keyword": ["Programming: stochastic programming", "Simulation: large-scale optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.323", "e:abstract": "We study the control of a production process which moves at a random time from an in-control state to an out-of-control state where an increased number of defective units is produced. After each unit is produced, a decision maker has three choices: continue production, invest in routine maintenance that restores the process to control, and invest in a more expensive learn maintenance that, in addition, may decrease the tendency of the process to go out of control. The optimal policy structure is shown to be of the control-limit type with the property that learning is not optimal if the tendency of the process to go out of control is small enough. If there is no opportunity to inspect the process's output, an optimal policy can be interpreted as a fixed production run. For this case an exact algorithm is developed. When the process's output is inspected, an optimal policy can be interpreted as a random production run. Approximation techniques are presented for this case. The fixed production run model is an alternative technique for determining production lot sixes. The random production run model is an alternative to traditional Shewhart process control.", "e:keyword": ["Dynamic programming/optimal control: models and policy structure for optimal process control", "Manufacturing", "Performance/productivity: tradeoffs between routine and preventive maintenance"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.337", "e:abstract": "A manufacturing facility consisting of two stations in tandem operates in a make-to-stock mode: After production, items are placed in a finished goods inventory that services an exogenous Poisson demand. Demand that cannot be met from inventory is backordered. Each station is modeled as a queue with controllable production rate and exponential service times. The problem is to control these rates to minimize inventory holding and backordering costs. Optimal controls are computed using dynamic programming and compared with the kanban, basestock and buffer control mechanisms that have been proposed for manufacturing facilities. Conditions are found under which certain simple controls are optimal using stochastic coupling arguments. Insights are gained into when to hold work-in-process and finished goods inventory, comparable to previous studies of production lines in make-to-order and unlimited demand environments.", "e:keyword": ["Dynamic programming/optimal control", "Models: control of tandem make-to-stock queues", "Inventory/production", "Multistage: optimal control"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.351", "e:abstract": "for (<i>s</i>, <i>S</i>) inventory systems, we derive sample path derivatives of performance measures with respect to the two parameters <i>s</i> and <i>S</i>. These derivatives yield derivative estimators which can be estimated from a single sample path or simulation of the inventory system, in some cases not even requiring actual knowledge of the underlying demand distribution. Such derivative estimates would be useful in sensitivity analysis or in gradient-based optimization techniques. We consider the nondiscounted periodic review system with general independent and identically distributed (i.i.d.) continuous demands, full backlogging, and general holding and shortage costs. For the infinite horizon model, consistency proofs are given for some special cases, although we argue why the estimators should be correct for the more general case.", "e:keyword": ["Inventory/production: sample path derivatives", "Performance evaluation", "Simulation: sensitivity analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.365", "e:abstract": "We consider a queueing system with two servers, each with its own queue. The interarrival times are generally distributed. The service time for each server is exponentially distributed but the rates may be different. No jockeying between the two queues is allowed. We consider situations in which a smart customer can delay joining a queue until some arrivals or service departures have been observed. All other customers join the shortest queue. We find conditions under which the smart customer can lower its expected sojourn time in the system by waiting and observing rather than immediately joining the shortest queue.", "e:keyword": ["Queues", "Multichannel: two parallel queues", "Delay in joining"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.372", "e:abstract": "We consider a multiclass <i>GI</i>/<i>G</i>/1 queueing system, operating under an arbitrary work-conserving scheduling policy π. We derive an invariance relation for the Cesaro sums of waiting times under π, which does not require the existence of limits of the Cesaro sums. This allows us to include important classes in the set of admissible policies such as time-dependent and adaptive policies. For these classes of policies, ergodicity is not known a priori and may not even exist. Therefore, the classical invariance relations that involve statistical averages do not hold. For an <i>M</i>/<i>G</i>/1 system, we derive inequalities involving the Cesaro sums of waiting times that further characterize the achievable performance region of the system.", "e:keyword": ["Probability", "Statistic model applications: nonergodic rules", "Queues", "Limit theorems: nonergodic priority rules in steady state", "Queues", "Priority: derivation of conservation laws"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.2.380", "e:abstract": "We consider an <i>M</i>/<i>G</i>/1 queue with feedback, in which customers, after receiving service, either return to the tail of the queue or depart the system, according to some feedback policy. We derive simple expressions for the expected response time for feedback policies that include Bernoulli feedback and feeding back a fixed number of times. Our results reveal some interesting and nonintuitive properties of the behavior of such feedback policies when the coefficient of variation of service time is varied. One result shows that for the Bernoulli feedback and fixed feedback policies with an equal mean number of visits to the queue, the expected response time for the Bernoulli policy is smaller than for the fixed policy if the coefficient of variation of service time is greater than 1. The relationship reverses if the coefficient of variation is less than 1.", "e:keyword": ["Queues: feedback"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.393", "e:abstract": "There are many tools and much literature that combine the expert systems and mathematical modeling paradigms. This survey focuses on a subset consisting of: decision making and unification, and not mere co-existence, of the two approaches. The unification effort is new and presents many research challenges at the theoretical, methodological, and tool levels. At the theoretical level, accepted prescriptions now exist that stipulate in which situations it is valid to use various forms of mathematical and qualitative reasoning. This is leading to a unified theory of the decision sciences for problems spanning choice, forecasting, risk assessment, design, operations, and many others. At the tool level three forms of synthesis of expert systems and mathematical models are particularly noteworthy: knowledge-based decision aids, intelligent decision modeling systems, and decision analytic expert systems. This survey gives definitions, surveys, and examples of each of these ways of unifying expert systems and modeling. Following this are lessons learned and further research needs. A great deal of synthesis work remains to be done, and a goal of this survey is to highlight some of the issues and invite discussion.", "e:keyword": ["Computers/computer science: artificial intelligence", "Decision analysts: applications", "Information systems: decision support systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.414", "e:abstract": "State-dependent queues and finite capacity queueing network models of facilities are important tools for the topological design of facilities, the routing of customers, and the allocation of resources to accommodate customer traffic. Key properties of these <i>M</i>/<i>G</i>/<i>C</i>/<i>C</i> queueing models and their applications in facility planning are described in detail. The incorporation of these concepts, tools, and techniques is important to the OR profession because they provide a unifying, system-wide planning methodology for tackling the many complex issues of designing, analyzing, and synthesizing pedestrian traffic flows in large-scale facilities and their environments. The scope and limitations of the methodology are demonstrated in the design of a pedestrian/vehicular circulation system of a large regional hospital campus.", "e:keyword": ["Facilities/equipment planning: layout", "Routing and resource allocation", "Queues", "Networks: state-dependent queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.428", "e:abstract": "This work gives a methodology for analyzing a class of discrete minimization problems with random element weights. The minimum weight solution is shown to be an absorbing state in a Markov chain, while the distribution of weight of the minimum weight element is shown to be of phase type. We then present two-sided bounds for matroids with NBUE distributed weights, as well as for weights with bounded positive hazard rates. We illustrate our method using a realistic military communications problem.", "e:keyword": ["Mathematics: combinatorics", "Networks/graphs: stochastic", "Probability: Markov processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.439", "e:abstract": "We develop bounds on the value function and a suboptimal design for the partially observed Markov decision process. These bounds and suboptimal design are based on the <i>M</i> most recent observations and actions. An a priori measure of the quality of these bounds is given. We show that larger <i>M</i> implies tighter bounds. An operations count analysis indicates that (<sup>#</sup><i>A</i><sup>#</sup><i>Z</i>)<i><sup>M</sup></i><sup>+1</sup>(<sup>#</sup><i>S</i>) multiplications and additions are required per successive approximations iteration of the suboptimal design algorithm, where <i>A</i>, <i>Z</i>, and <i>S</i> are the action, observation, and state spaces, respectively, suggesting the algorithm is of potential use for problems with large state spaces. A preliminary numerical study indicates that the quality of the suboptimal design can be excellent.", "e:keyword": ["Dynamic programming: Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.456", "e:abstract": "We show for the general dynamic lot sizing model how minimal forecast horizons may be detected by a slight adaptation of an earlier <i>O</i>(<i>n</i> log <i>n</i>) or <i>O</i>(<i>n</i>) forward solution method for the model. A detailed numerical study indicates that minimal forecast horizons tend to be small, that is, include a small number of orders. We describe a new planning approach to ensure stability of the lot sizing decisions over an initial interval of time or <i>stability horizon</i> in those (relatively rare) cases where no planning horizon is detected or where the stability horizon extends beyond the planning horizon. To this end, we develop a heuristic, but <i>full horizon-based</i> adaptation of the optimal lot sizing schedule, designed to minimize an upper bound for the worst-case optimality gap under the desired stability conditions. We also show how the basic horizon length <i>n</i> may be chosen to guarantee any prespecified positive optimality gap.", "e:keyword": ["Inventory/production: policies", "Planning horizons", "And sensitivity analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.469", "e:abstract": "A system with a functional life of <i>T</i> units of time requires a certain functional part for its operation. If this part fails before the failure of the system, it has to be replaced immediately to keep the system in operation. Two brands are available for replacements. These two brands differ in unit costs and life distributions. The objective is to determine a time-dependent replacement policy that minimizes the expected operational cost. We show that if certain conditions are satisfied, then the optimal policy exists and has a simple intuitive structure.", "e:keyword": ["Reliability", "Replacement/renewal: policies with simple structure non-Markovian life distributions"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.476", "e:abstract": "A wide variety of manufacturing operations can be characterized as co-production with substitutable demand. That is, there are many situations in which the availability of two or more items are related, and because of randomness in either supply or demand, it can be advantageous to substitute one of these items for another. Our research was motivated by the semiconductor industry, where chips are produced in large batches. Because of the presence of randomness in the process, individual chips in a given batch can perform differently. Because some customers have stricter specifications than others, chips within the same batch can be classified and sold as different products according to their measurable performance. We model the production and inventory problem as a stochastic dynamic program in which the objective is to minimize the costs of meeting contractual obligations. After developing heuristic methods of solving the problem in practice, we validate them against a lower bound on the cost of an optimal solution to the dynamic program.", "e:keyword": ["Inventoly/production", "Approximations", "Heuristics: co-production processes with random yields", "Inventory/production", "Stochastic: co-production and allocation of substitutable products"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.492", "e:abstract": "Capacity-oriented production scheduling can be described as the assignment of competing products to several single level, capacitated production lines over a given planning horizon. This study was initially motivated by the production planning of various types of tiles by a tile manufacturing company. We considered different integer programming formulations and found that a disaggregated model, while increasing the size of the model, lends itself best to Lagrangian techniques and produces the strongest bounds. Additionally, from every Lagrangian solution, we can generate a feasible production schedule by systematically reassigning lines from products whose production exceeded demand to products with unsatisfied demands. The technique is not specific to tile companies, but can be used by any firm where product setups on production lines can be scheduled between consecutive periods with changeover cost, but without production loss. Our computational experience with real data from the tile company and randomly generated problem instances gave excellent lower and upper bounds.", "e:keyword": ["Inventory/production: production planning with changeover", "Productlon/scheduling: integer programming for product changeover", "Programming", "Integer algorithms: Lagrangian relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.504", "e:abstract": "This paper explores the improvements in manufacturing efficiency that can be achieved by broadening the scope of production scheduling to include both the sequencing of work and the coordination of the resource inputs required to perform work. Recognizing that some resources are inherently flexible and thus can be reassigned dynamically to processing centers as needed, and that job processing times are often a function of the amount of resource dedicated to specific operations, we formulate the flexible-resource scheduling problem with the objective of simultaneously determining the permutation job sequence, resource allocation policy, and operation start times that optimize system performance. Focusing on flexible-resource scheduling in flow shop production systems, we discuss problem complexity, identify properties of and establish lower bounds for optimal schedules, develop optimal and heuristic solution approaches, and report the results of extensive computational experimentation designed to explore the operational benefits of resource flexibility. The computational results demonstrate that the performance improvements associated with flexible-resource scheduling are substantial, and suggest that the heuristic provides an effective means for solving larger problems.", "e:keyword": ["Productloni/schedullng", "Sequencing: flexible resource allocation in flow shops"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.523", "e:abstract": "In this paper we present two algorithms for a machine allocation problem occurring in manufacturing systems. For the two algorithms presented we prove worst-case performance ratios of 2 and 3/2, respectively. The machine allocation problem we consider is a general convex resource allocation problem, which makes the algorithms applicable to a variety of resource allocation problems. Numerical results are presented for two real-life manufacturing systems.", "e:keyword": ["Manufacturing: allocation of machines in an FMS", "Manufacturing", "Performance/productivlty: optimization of steady-state performance", "Queues", "Networks: optimization of queueing networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.531", "e:abstract": "An important parameter of the regenerative method of simulation output analysis is the choice of return state used for blocking observations. Computational experience has shown that the statistical properties of estimators based on different regeneration points can vary widely. In this paper we study the limiting joint distribution of the normalized regenerative point and standard-deviation estimators for general state-space Markov chains. The asymptotic covariance between the point and standard-deviation estimators is shown to be the same for all return states, and the quantity is related to the normalized skewness of the partial sums of the chain. Since the covariance is constant, it follows that the choice of return state that minimizes the asymptotic variance of the standard-deviation estimator will maximize the correlation between the point and standard-deviation estimator. Consideration of asymptotic variance and covariance alone suggests that confidence intervals have the best coverage probabilities if the asymptotic variance of the standard-deviation estimator is minimized (thereby maximizing the correlation).", "e:keyword": ["Simulation", "Efficiency: low variance estimators", "Simulation", "Statistical analysis: covariance of estimators"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.543", "e:abstract": "The probabilistic traveling salesman problem (PTSP) is defined on a graph <i>G</i> = (<i>V</i>, <i>E</i>), where <i>V</i> is the vertex set and <i>E</i> is the edge set. Each vertex <i>v<sub>i</sub></i> has a probability <i>p<sub>i</sub></i> of being present. With each edge (<i>v<sub>i</sub></i>, <i>v<sub>j</sub></i>) is associated a distance or cost <i>c<sub>ij</sub></i>. In a first stage, an a priori Hamiltonian tour on <i>G</i> is designed. The list of present vertices is then revealed. In a second stage, the a priori tour is followed by skipping the absent vertices. The PTSP consists of determining a first-stage solution that minimizes the expected cost of the second-stage tour. The problem is formulated as an integer linear stochastic program, and solved by means of a branch-and-cut approach which relaxes some of the constraints and uses lower bounding functionals on the objective function. Problems involving up to 50 vertices are solved to optimality.", "e:keyword": ["Networks/graphs", "Stochastic stochastic traveling salesman problem", "Networks/graphs", "Traveling salesman: stochastic traveling salesman problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.550", "e:abstract": "In recent years various structures have been proposed for estimating intensity of consumer preference in consumer surveys. Empirical studies have shown intensity measures to be reliable and cost effective by providing more information per question. These structures use consumers' responses to determine whether the relevant preference function to be estimated is interval, ratio or ordinal in nature. This paper shows that the statistical tests to determine the nature of the preference function in one of the more popular structures often introduce bias in subsequent analysis. An alternative approach is indicated for use in some situations, which avoids some of the complexities involved in the more commonly used tests.", "e:keyword": ["Decision analysis: testing in consumer surveys", "Marketing: consumer preference surveys", "Utility/preference: multiattribute"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.556", "e:abstract": "We devise a new simplex pivot rule which has interesting theoretical properties. Beginning with a basic feasible solution, and any nonbasic variable having a negative reduced cost the pivot rule produces a sequence of pivots such that ultimately the originally chosen nonbasic variable enters the basis, and all reduced costs which were originally nonnegative remain nonnegative. The pivot rule thus monotonically builds up to a dual feasible, and hence optimal, basis. A surprising property is that the pivot sequence results in intermediate bases which are neither primal nor dual feasible. We prove the correctness of the procedure, and relate it to other pivoting rules for linear programming.", "e:keyword": ["Programming", "Linear", "Algorithms: monotonic buildup", "Programming", "Linear", "Parametric: shadow vertex algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.562", "e:abstract": "We show that an <i>O</i>(<i>n</i><sup>3</sup> log <i>n</i>) algorithm can find optimal power-of-two lot size policies for finite production rate assembly systems. This Improves an <i>O</i>(<i>n</i><sup>5</sup>) algorithm proposed in D. Atkins, M. Queyranne and D. Sun's 1992 paper.", "e:keyword": ["Inventory/production: heuristics", "Multistage", "Lot sizing"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.3.566", "e:abstract": "In machine scheduling the first problem is to find a timetable that is optimal with respect to some efficiency criterion. If the jobs come from different clients the solution of the optimization problem is not the end of the story. In addition, we have to decide how the minimal total cost must be distributed among the parties involved. In this note, cost allocation problems will be considered to arise from one-machine scheduling problems with an additive and weakly increasing cost function. We will show that the cooperative games related to these cost allocation problems have a nonempty core. Furthermore, we give a rule that assigns a core element of the associated cost saving game to each scheduling problem of this kind and an initial order of the jobs.", "e:keyword": ["Games/group decisions", "Cooperative: Methods used to find far cost allocations", "Production/scheduling", "Sequencing", "Deterministic", "Single machine: cost distribution under additive regular criteria"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.577", "e:abstract": "Ackoff has decried the “devolution” of OR/MS, Corbett and Van Wassenhove have spoken of its “natural drift,” and a sociologist has described its “regression” as typifying that of other learned professions. To shed light on these views, we undertook a detailed survey of a segment of the OR/MS literature, with particular focus on the space in flagship journals devoted to theory on the one hand and applications on the other. While the literature of OR/MS contains many articles and texts with the word application in the title and the word data in the text, the definitions and uses of these terms are not precise. The claimed applications differ in degree and the actual data differ in kind. To encompass these different meanings we used a five-point scale to classify articles in the 1962 and 1992 volumes of <i>Operations Research</i> and <i>Management Science</i> and the 1972 and 1992 volumes of <i>Interfaces</i>. The resulting statistical analyses shed considerable light on the direction that OR/MS is taking and raise questions about its appropriateness. Finally, the paper raises fundamental questions about the direction that OR/MS should take and the roles that its journals should play in pursuing this direction.", "e:keyword": ["Professional", "OR/MS education: meta research", "Professional", "OR/MS philosophy: research on research", "Epistemology of management science"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.589", "e:abstract": "Many static and dynamic models have been used to assist decision making in the area of consumer and commercial credit. The decisions of interest include whether to extend credit, how much credit to extend, when collections on delinquent accounts should be initiated, and what action should be taken. We survey the use of discriminant analysis, decision trees, and expert systems for static decisions, and dynamic programming, linear programming, and Markov chains for dynamic decision models. Since these models do not operate in a vacuum, we discuss some important aspects of credit management in practice, e.g., legal considerations, sources of data, and statistical validation of the methodology. We provide our perspective on the state-of-the-art in theory and in practice.", "e:keyword": ["Finance", "Corporate finance: bankruptcy prediction", "Financial institutions", "Banks: credit analysis", "Statistics", "Data analysis: discriminant analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.614", "e:abstract": "In this paper, we discuss the problem of devising a cost effective schedule for a baseball league. Sports scheduling is a notoriously difficult problem. A schedule must satisfy constraints on timing such as the number of games to be played between every pair of teams, the bounds on the number of consecutive home (or away) games for each team, that every pair of teams must have played each other in the first half of the season, and so on. Often, there are additional factors to be considered for a particular league, for example, the availability of venues on specific dates, home-game preferences of teams on specific dates, and balancing of schedules so that games between two teams are evenly-spaced throughout the season. In addition to finding a feasible schedule that meets all the timing restrictions, the problem addressed in this paper has the additional complexity of having the objective of minimizing travel costs. We discuss some structural properties of a schedule that meets the timing constraints and present two heuristics for finding a low-cost schedule. The methodology is used to develop an improved schedule for the Texas Baseball League.", "e:keyword": ["Production/scheduling", "Applications: heuristics for cost effective baseball timetables", "Recreation and sports: heuristics for cost effective baseball schedules", "Transportation", "Costs: minimizing baseball league travel costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.626", "e:abstract": "We consider the problem of optimally scheduling a fleet of K vehicles to make deliveries to <i>n</i> customers subject to vehicle capacity constraints. Given a graph with <i>n</i> + 1 nodes, a K-tree is defined to be a set of <i>n</i> + <i>K</i> edges that span the graph. We show that the vehicle routing problem can be modeled as the problem of finding a minimum cost K-tree with two K edges incident on the depot and subject to some side constraints that impose vehicle capacity and the requirement that each customer be visited exactly once. The side constraints are dualized to obtain a Lagrangian problem that provides lower bounds in a branch-and-bound algorithm. This algorithm has produced proven optimal solutions for a number of difficult problems, including a well-known problem with 100 customers and several real problems with 25–71 customers.", "e:keyword": ["Programming", "Integer", "Algorithms", "Relaxation/subgradient: k-tree based", "Transportation", "Vehicle routing", "Optimization algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.643", "e:abstract": "We show that under the (sufficient) conditions usually given for infinitesimal perturbation analysis (IPA) to apply for derivative estimation, a finite-difference scheme with common random numbers (FDC) has the same order of convergence, namely <i>O</i>(<i>n</i><sup>−1/2</sup>), provided that the size of the finite-difference interval converges to zero fast enough. This holds for both one- and two-sided FDC. This also holds for different variants of IPA, such as some versions of smoothed perturbation analysis (SPA), which is based on conditional expectation. Finally, this also holds for the estimation of steady-state performance measures by truncated-horizon estimators, under some ergodicity assumptions. Our developments do not involve monotonicity, but are based on continuity and smoothness. We give examples and numerical illustrations which show that the actual difference in mean square error (MSE) between IPA and FDC is typically negligible. We also obtain the order of convergence of that difference, which is faster than the convergence of the MSE to zero.", "e:keyword": ["Simulation: derivative estimation", "Sensitivity analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.657", "e:abstract": "We study the lower bounds on the optimal objective function value of linear pure integer programming problems obtainable by the convexification in parts that results from using Benders' or cross decomposition, and the best lower bounds obtainable by the convexification resulting from Lagrangian relaxation together with subgradient optimization or Dantzig-Wolfe decomposition. A comparison shows that generalized Benders' and generalized cross decomposition yield the best of these bounds, while ordinary Benders' decomposition yields bounds that are sometimes better and sometimes worse than those of Lagrangian relaxation. However, cross decomposition can be used to automatically get the best of the two bounds. The conclusion of this paper is that cross decomposition can be useful for getting good lower bounds for pure integer programming problems, bounds that can be made better than those of the frequently used Lagrangian relaxation.", "e:keyword": ["Integer programming", "Algorithms: duality gaps in decomposition", "Mathematics", "Convexity: duality gaps for integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.669", "e:abstract": "Sequencing rules that rely on priority indices are particularly attractive because they are simple to calculate and easy to implement. It has been established that there are exactly two classes of delay cost functions for which policies that are determined by time-invariant priority indices are capable of producing optimal sequences: linear delay costs and discounted linear delay costs. We consider index-based policies that allow dynamic recalculation of priority indices and show that this class does not enlarge the set of delay cost functions for which index-based rules can produce optimal policies. Our analysis relies on the argument that index-based rules must induce a transitive ranking of the tasks. We show that the classes of linear and discounted linear functions are the only ones that can be associated with such rankings.", "e:keyword": ["Production/scheduhng", "Sequencing: dynamic recomputation of priority indices", "Mathematics: sequencing and transitivity"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.677", "e:abstract": "We present a greedy randomized adaptive search procedure (<b>GRASP</b>) for the network 2-partition problem. The heuristic is empirically compared with the Kernighan-Lin (K&L) method on a wide range of instances. The <b>GRASP</b> approach dominates K&L with respect to solution value on a large percentage of the instances tested. The ability of <b>GRASP</b> to find optimal solutions is assessed by comparing its performance with a general purpose mixed integer programming package.", "e:keyword": ["Networks/graphs", "Applications: two-partition problem", "Networks/graphs", "Heuristics: greedy randomized adaptive search procedures"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.688", "e:abstract": "Maximum average weight ideal problems in ordered sets arise from modeling variants of the investment problem and, in particular, learning problems in the context of concepts with tree-structured attributes in artificial intelligence. Similarly, trying to construct tests with high reliability leads to a nontrivial maximum average weight ideal problem. This paper investigates the computational complexity and shows that the general problem is NP-complete. Important special cases (e.g., finding rooted subtrees of maximal average weight), however, can be handled with efficient algorithms.", "e:keyword": ["Analysis of algorithms: computational complexity", "Networks/graphs: networks arising from precedence constraints", "Programming: integer programming and dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.694", "e:abstract": "In the manufacture of mechanical heart valves, very high precision manufacturing and quality control is an absolute necessity. Since a mechanical heart valve consists of a number of components, it is imperative that the components match precisely. Motivated by this need in heart valve manufacturing, we study a new production control model for a product comprised of matching components. The model is also applicable to other production systems for manufacturing high precision products requiring careful matching of components. For a two-component case, we prove that a “greedy” production sequencing rule is optimal. Based on this result, we then develop an effective greedy heuristic sequencing rule for more general cases. An extensive empirical study is used to evaluate the performance of this and one other heuristic rule.", "e:keyword": ["Production/scheduling: product with matching components"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.709", "e:abstract": "We consider a system with <i>m</i> modules as components. These modules are composed of parts of finitely many types, and the number of parts of each type that is needed in each of the modules is given, e.g., module <i>i</i> requires <i>n<sub>ui</sub></i> parts of type <i>u</i>. Parts of the same type may have different reliabilities, but they are functionally interchangeable. A module works if and only if all of its parts work, i.e., the internal composition of the modules has series structure. An <i>assembly</i> of the modules consists of an assignment of each of the ∑<i><sub>u</sub></i> ∑<i><sub>i</sub></i> <i>n<sub>ui</sub></i> parts to the modules such that each module meets its specification by getting the required number of parts of each type. Such an assembly is called <i>monotone</i> if the best parts of each type go to one module, the next best parts of each type go to a second module, and so on, until finally the last module gets the worst parts of each type. We prove that for coherent systems, there always exists a monotone assembly which maximizes the reliability of the system. Furthermore, we obtain sufficient conditions under which every optimal assembly is monotone.", "e:keyword": ["Mathematics: combinatorics", "Optimal partitions", "And convexity", "Reliability: coherent structure", "Optimal assembly", "And monotone assembly"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.721", "e:abstract": "In this paper, we study a single-item continuous-review inventory system with Poisson demand. In addition to the standard cost structure of a fixed setup cost and a quasiconvex expected inventory holding and shortage cost, special opportunities for placing orders at a discounted setup cost occur according to a Poisson process that is independent of the demand process. This model has been studied as a subproblem of multi-item/location inventory systems where there are economies-of-scale in joint replenishment. For the single-item model, the literature proposes the (<i>s</i>, <i>c</i>, <i>S</i>) policy, under which an order is placed to increase the inventory position to <i>S</i> either when the inventory position drops to <i>s</i>, or when the inventory position is at or below <i>c</i> and a discount opportunity occurs. We prove that the (<i>s</i>, <i>c</i>, <i>S</i>) policy is optimal for the model, develop an efficient algorithm for computing optimal control parameters <i>s</i>*, <i>c</i>*, S*, and carry out a parametric analysis showing the effects of changes in problem parameters on the optimal control parameters and the minimum cost.", "e:keyword": ["Inventory/production: lot sizing policies", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.739", "e:abstract": "We present new numerical algorithms and bounds for the infinite horizon, discrete stage, finite state and action Markov decision process with imprecise transition probabilities. We assume that the transition probability mass vector for each state and action is described by a finite number of linear inequalities. This model of imprecision appears to be well suited for describing statistically determined confidence limits and/or natural language statements of likelihood. The numerical procedures for calculating an optimal max-min strategy are based on successive approximations, reward revision, and modified policy iteration. The bounds that are determined are at least as tight as currently available bounds for the case where the transition probabilities are precise.", "e:keyword": ["Dynamic programming/optimal control: Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.750", "e:abstract": "In this paper we describe the time-dependent moments of the workload process in the <i>M</i>/<i>G</i>/1 queue. The <i>k</i>th moment as a function of time can be characterized in terms of a differential equation involving lower moment functions and the time-dependent server-occupation probability. For general initial conditions, we show that the first two moment functions can be represented as the difference of two nondecreasing functions, one of which is the moment function starting at zero. The two nondecreasing components can be regarded as probability cumulative distribution function (cdf's) after appropriate normalization. The normalized moment functions starting empty are called moment cdf's; the other normalized components are called moment-difference cdf's. We establish relations among these cdf's using stationary-excess relations. We apply these relations to calculate moments and derivatives at the origin of these cdf's. We also obtain results for the covariance function of the stationary workload process. It is interesting that these various time-dependent characteristics can be described directly in terms of the steady-state workload distribution.", "e:keyword": ["Queues", "Transient results: M/G/1 workload process", "Queues", "Busy period analysis: M/G/l queue"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.765", "e:abstract": "This paper concerns the optimal control of input to a FIFO jobshop with a single workstation. The input is jobs for which the processing and delivery times are observable upon arrival. The control is exercised by charging a price for each completed job. The objective is either profit maximization or welfare maximization. The semi-Markov decision processes that maximize the two objectives are studied simultaneously. Optimal prices are specified in terms of opportunity costs. The opportunity cost of a job is the expected future loss of earnings caused by having the job submitted to the shop. Results for the cases with and without discounting are established simultaneously by a new approach. It is based on the idea of studying the infinite-horizon model directly and it allows the state space and the decision set to be denumerable. Mild assumptions ensure that the opportunity cost is increasing as a function of the work backlog, and increasing and convex as a function of the processing time.", "e:keyword": ["Cost analysis: opportunity costs of a jobshop", "Transfer pricing", "Dynamic programming/optimal control", "Semi-Markov: infinite state space", "Infinite decision set", "α-optimal", "Queues", "Optimization: input control by charging prices"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.4.775", "e:abstract": "Given a graph with <i>n</i> + 1 nodes, a <i>K</i>-tree is defined to be a set of <i>n</i> + <i>K</i> edges that span the graph. This paper presents an algorithm for finding a minimum cost <i>K</i>-tree with a specified degree at a designated node. The algorithm runs in <i>O</i>(<i>n</i><sup>3</sup>) time and is useful in the optimal solution of certain Lagrangian relaxations arising in vehicle routing.", "e:keyword": ["Networks/graphs", "Tree algorithms: minimum k-tree algorithms", "Transportation", "Vehicle routing: optimization algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.793", "e:abstract": "Values pervade the field of operations research. Expressed as objectives, goals, criteria, performance measures, and/or objective functions, they are necessary in theoretical operations research models and in applications. Because of their critical role, it is useful to develop these expressions of values from basic principles. This paper outlines how to identify values for a specific decision problem, how to structure these values to facilitate thinking and analysis, and how to quantify values. Since values provide the basis for interest in a problem, these same values should guide all of our effort on that problem. Two important uses of values are to create better alternatives for decision problems and to define decision problems that are more appealing than those that confront us. On another level, the operations researcher's values are crucial in selecting the research and applications that he or she pursues. I illustrate this with a brief summary of a few projects concerning both life-threatening risks and the storage of nuclear waste. The presentation concludes with a challenge to operations researchers to consider devoting some effort and talent to what I think of as the mega-risks facing our country.", "e:keyword": ["Decision analysis", "Multiple criteria: structuring and quantifying objectives", "Government", "Regulations: examination of indirect consequences", "Professional", "OR/MS implementation: using values to select problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.814", "e:abstract": "During its operational test and evaluation, despite top management support and significant technical achievements, a personnel assignment model implemented for the United States Navy to support assignment decisions experienced overwhelming resistance from the users, the 200 or so enlisted detailers, located at the Bureau of Naval Personnel in Washington, D.C. Our MS/OR research team had neglected to assess the negative impact of the personnel assignment model on an important detailing function: assignment negotiations or bargaining between the detailers and their customers, the service members. By involving the detailers in revising the model and making the failings of the old model the strengths of the new model, we turned certain failure into a successful program. By managing the behavioral aspects of the implementation with special emphasis on problem identification and requirements structuring, we overcame the difficulties of introducing change to a largely manual and highly decentralized decision process and we compare lessons learned with the experiences of other implementers.", "e:keyword": ["Military", "Personnel: assigning rotating sailors to vacant jobs", "Professional", "Comments on: assigning rotating sailors to vacant jobs", "Professional", "OR/MS implementation: assigning rotating sailors to vacant jobs"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.823", "e:abstract": "This paper describes a four-month study performed for the Dutch Minister of Transport, Public Works, and Water Management that examined the consequences of alternative policies for providing flood protection to the nontidal branches of the Rijn and Maas Rivers in The Netherlands. The paper focuses on estimating the flood damage that would occur under alternative safety standards, estimating the financial costs of alternative dike improvement strategies, and estimating the damage that would be inflicted on the landscape, natural, and cultural values along the rivers under each of these strategies. The primary objective of the study was to identify policies that would provide a high level of safety, would not cost too much, and would preserve as much as possible of the existing landscape, natural, and cultural values along the rivers. Less than six months after the completion of the study, the Dutch Parliament approved a new river dike policy that was based on the study's results.", "e:keyword": ["Cost analysis: estimating the costs of dike improvements and flood losses", "Environment", "Protecting the environment", "Government", "Services: providing security from flooding"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.837", "e:abstract": "A new approach to evaluate lower bounds for a class of quadratic assignment problems (<b>QAP</b>) is presented. An instance of a <b>QAP</b> of size <i>n</i> is specified by two <i>n</i> × <i>n</i> matrices <i>D</i> and <i>F</i>, and is denoted by <b>QAP</b>(<i>D</i>, <i>F</i>). This approach is presented for <b>QAPs</b> where <i>D</i> is the matrix of rectilinear distances between points on a regular grid. However, it can easily be generalized to a wider class of problems. Two matrices <i>F<sub>opt</sub></i> and <i>F<sub>res</sub></i> are constructed such that <i>F</i> = <i>F<sub>opt</sub></i> + <i>F<sub>res</sub></i>, and the optimal solution to <b>QAP</b>(<i>D</i>, <i>F<sub>opt</sub></i>) is known. Any existing lower bound can then be applied to <b>QAP</b>(<i>D</i>, <i>F<sub>res</sub></i>), which in sum with the optimal value for <b>QAP</b>(<i>D</i>, <i>F<sub>opt</sub></i>), provides a lower bound to <b>QAP</b>(<i>D</i>, <i>F</i>). Thus, this method could serve as a reduction process to possibly improve the results from a variety of bounding techniques. This approach is tested on two bounds from the literature, the Gilmore-Lawler bound (GLB) and an eigenvalue bound (PB), for various problems of size ranging from 6–49. Computational results show a good improvement in bounds for all the test problems. An extension of our method to a more general class of <b>QAPs</b> is also presented.", "e:keyword": ["Facilities/equipment planning", "Location: quadratic assignment program", "Programming: heuristic combinatorial optimization", "Lower bounds"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.846", "e:abstract": "We consider the asymmetric capacitated vehicle routing problem (<b>CVRP</b>), a particular case of the standard asymmetric vehicle routing problem in which only the vehicle capacity constraints are imposed. <b>CVRP</b> is known to be NP-hard and finds practical applications in distribution and scheduling. We describe two new bounding procedures for <b>CVRP</b>, based on the so-called <i>additive approach</i>. Each procedure computes a sequence of nondecreasing lower bounds, obtained by solving different relaxations of <b>CVRP</b>. Effective implementations of the procedures are also outlined which considerably reduce the computational effort. The two procedures are combined into an overall bounding algorithm. A branch-and-bound exact algorithm is then proposed, whose performance is enhanced by means of reduction procedures, dominance criteria, and feasibility checks. Extensive computational results on both real-world and random test problems are presented, showing that the proposed approach favorably compares with previous algorithms from the literature.", "e:keyword": ["Programming", "Algorithms: bounding procedures", "Branch-and-bound algorithms", "Transportation: asymmetric capacitated vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.860", "e:abstract": "An efficient randomized heuristic for a maximum independent set is presented. The procedure is tested on randomly generated graphs having from 400 to 3,500 vertices and edge probabilities from 0.2 to 0.9. The heuristic can be implemented trivially in parallel and is tested on an MIMD computer with 1, 2, 4 and 8 processors. Computational results indicate that the heuristic frequently finds the optimal or expected optimal solution in a fraction of the time required by implementations of simulated annealing, tabu search, and an exact partial enumeration method.", "e:keyword": ["Mathematics: combinatorics", "Networks/graphs: heuristics", "Programming: integer algorithms/heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.879", "e:abstract": "This paper deals with the problem of determining the frequencies at which several products have to be shipped on a common link to minimize the sum of transportation and inventory costs. A set of feasible shipping frequencies is given. Transportation costs are supposed to be proportional to the number of journeys performed by vehicles of a given capacity. Vehicles may or may not be supposed to carry out completely all materials available, and products assigned to different frequencies may or may not share the same truck. Integer and mixed integer linear programming models are formulated for each of the resulting four situations, and their properties are investigated. In particular, we show that allowing products to be split among several shipping frequencies makes trucks traveling at high frequencies to be filled up completely. In this situation, trucks may always be loaded with products shipped at the same frequency.", "e:keyword": ["Transportation", "Costs: cost effectiveness in freight transportation"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.895", "e:abstract": "Customers move through a series of <i>M</i> service stations. Each customer, independent of all others, requires service from only one of the stations, for a duration of 1 time unit, this being station <i>i</i> with probability <i>p<sub>i</sub></i>. The customer has zero service at all the other stations, but there is no overtaking between the customers, and so queueing occurs. In the case where there is unlimited waiting room between the servers, we show that the system is interchangeable—permuting the order of the stations has no effect on the distribution of the output stream. When there is no waiting room between the stations we investigate optimal loads of the servers in terms of optimal <i>p<sub>i</sub></i>'s for up to 10 stations, and observe that optimal loads exhibit the <i>bowl phenomenon</i>. We also obtain some bounds on the throughput for equal loads as a function of <i>M</i>.", "e:keyword": ["Production/scheduling", "Flexible manufacturing/line balancing: bowl shape phenomenon", "Queues", "Optimization: cafeteria process", "Queues", "Tandem: interchangeability", "0-1 dependent service times"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.913", "e:abstract": "Most models of multilevel production and distribution systems assume unlimited production capacity at each site. When capacity limits are introduced, an ineffective policy may lead to increasingly large order backlogs: The <i>stability</i> of the system becomes an issue. In this paper, we examine the stability of a multi-echelon system in which each node has limited production capacity and operates under a <i>base-stock</i> policy. We show that if the mean demand per period is smaller than the capacity at every node, then inventories and backlogs are stable, having a unique stationary distribution to which they converge from all initial states. Under i.i.d. demands we show that the system is a Harris ergodic Markov chain and is thus wide-sense regenerative. Under a slightly stronger condition, inventories return to their target levels infinitely often, with probability one. We discuss cost implications of these results, and give extensions to systems with random lead times and periodic demands.", "e:keyword": ["Inventory/production", "Multistage: stability under base-stock policy", "Probability", "Regenerative processes: Harris recurrent inventory processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.926", "e:abstract": "Queues with a finite population of customers and occasional periods (called vacations) when the server is unavailable appear in many engineering systems, but have not been studied. In this paper, we specifically consider an <i>M</i>/<i>G</i>/1//<i>N</i> queueing system in which the server takes repeated vacations each time it has emptied the queue, until it finds a customer waiting. For the steady state, we show that performance measures such as the system throughput and mean response time can be obtained from the known analysis of a regenerative cycle of busy and vacation periods. We then study the joint distribution of the server state, the queue size, and the remaining service or vacation time at an arbitrary point in time for certain initial conditions. In the steady state, we explicitly obtain the distributions of the unfinished work, the virtual waiting time, and the real waiting time. The mean response times in several other vacation models are also provided.", "e:keyword": ["Queues: finite-population queues with server vacations"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.940", "e:abstract": "Accelerating procedures for solving <i>discounted</i> Markov decision processes problems are developed based on a one-step lookahead analysis of the value iteration algorithm. We apply the criteria of minimum difference and minimum variance to obtain good adaptive relaxation factors that speed up the convergence of the algorithm. Several problems (including Howard's automobile replacement) are tested and a preliminary numerical evaluation reveals considerable reductions in computation time when compared to existing value iteration schemes.", "e:keyword": ["Dynamic programming/optimal control: discounted Markov decision processes", "Value iteration algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.947", "e:abstract": "We consider a flexible manufacturing facility that can be operated in any of <i>m</i> different modes. While running in mode <i>k</i> certain intermediate products are consumed and other intermediate or finished products are created. There may be variability in the manufacturing process, as well as random arrivals of raw materials and orders for finished products. We establish conditions that ensure demands can be satisfied while maintaining bounded levels of inventories. These results that may be viewed as generalizing to the flexible manufacturing context the notion of stability for a queueing system.", "e:keyword": ["Probability", "Stochastic model applications: stabilizing a flexible manufacturing system", "Production/scheduling", "Flexible manufacturing: stabilizing control of a manufacturing model"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.958", "e:abstract": "This paper examines a multiproduct dynamic investment model for making technology choices and expansion decisions over a finite planning horizon. The motivation for our problem comes from recent developments in the field of flexible technology such as CAD, CAM, and CIM that permit firms to invest in these more expensive, flexible technologies to provide a competitive edge in the form of an ability to respond rapidly to changing product mix. On the other hand, more specialized (dedicated) equipment may be less costly. The decisions on appropriate mixes of dedicated and flexible capacity involve many complex considerations such as economies of scale, demand patterns, and mix flexibility. We formulate the problem as a mathematical program with the objective of minimizing total investment cost. Since the problem is difficult to solve optimally, we develop a two-phased approach and present heuristics to obtain good expansion schedules. These procedures are based on an easily solvable sequence of subproblems derived from the planning problem. Our computational results suggest that these methods work well and provide acceptable solutions with reasonable effort.", "e:keyword": ["Facilities/equipment planning: dynamic capacity expansion", "Production/scheduling: heuristics and approximations", "Technology: technology selection", "Flexible manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.5.977", "e:abstract": "In this note we prove that the relaxation approach in designing the subproblem of pricing out only the feasible routes for the set partition formulation of the <b>VRPTW</b> is justified on complexity grounds. That is, the first dynamic programming model presented in M. Desrochers, J. Desrosiers and M. Solomon (1992), that is able to price out all feasible routes, is NP-hard in the strong sense.", "e:keyword": ["Dynamic programming/optimal control: complexity of shortest path time windows", "Transportation: complexity of shortest path time windows"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1010", "e:abstract": "Implementing forest plans involves decisions regarding the geographic arrangement and timing of management activities, and the design of efficient transportation networks to provide access for these activities. These planning problems have been modeled successfully as mixed integer linear programs. However, these models are difficult to solve with commercial software due to their relatively large number of binary variables. We present a heuristic approach that has solved these problems well. The heuristic rules and a linear programming package interact within a closed system requiring no special user intervention. We describe several applications of this system for decision making in the USDA Forest Service.", "e:keyword": ["Production/scheduling", "Planning: forest management activities", "Programming", "Integer", "Heuristics: heuristic solution algorithm for mixed-integer formulation", "Transportation", "Models", "Network: schedules road construction and routes traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1025", "e:abstract": "We provide a unified framework for the total tardiness problem by surveying the related literature in the single-machine, parallel machine, flowshop and jobshop settings. We focus on critically evaluating the heuristic algorithms; we also propose new heuristics for both the single-machine and the parallel-machine tardiness problems. Finally, we identify the areas where further research is needed and we give directions for future research.", "e:keyword": ["Production/scheduling: sequencing", "Deterministic", "Single/multiple machines", "Production/scheduling: approximations/heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1042", "e:abstract": "We present a detailed and up-to-date survey of the literature on parallel branch-and-bound algorithms. We synthesize previous work in this area and propose a new classification of parallel branch-and-bound algorithms. This classification is used to analyze the methods proposed in the literature. To facilitate our analysis, we give a new characterization of branch-and-bound algorithms, which consists of isolating the performed operations without specifying any particular order for their execution.", "e:keyword": ["Programming", "Implementation on parallel architectures", "Programming", "Integer", "Algorithms: survey"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1067", "e:abstract": "We address the Joint Replenishment Problem (<b>JRP</b>) where, in the presence of joint setup costs, dynamic lot sizing schedules need to be determined for <i>m</i> items over a planning horizon of <i>N</i> periods, with general time-varying cost and demand parameters. We develop a new, so-called, partitioning heuristic for this problem, which partitions the complete horizon of <i>N</i> periods into several relatively small intervals, specifies an associated joint replenishment problem for each of these, and solves them via a new, efficient branch-and-bound method. The efficiency of the branch-and-bound method is due to the use of a new, tight lower bound to evaluate the nodes of the tree, a new branching rule, and a new upper bound for the cost of the entire problem. The partitioning heuristic can be implemented with complexity <i>O</i>(<i>mN</i><sup>2</sup>log log <i>N</i>). It can be designed to guarantee an ε-optimal solution for any ε > 0, provided that some of the model parameters are uniformly bounded from above or below. In particular, the heuristic is asymptotically optimal as <i>N</i> → ∞ for any fixed number of items <i>m</i>, and it remains asymptotically optimal when both <i>m</i> and <i>N</i> are simultaneously increased to infinity. Most importantly, a numerical study shows that the partitioning heuristic performs exceptionally well. Even for small problems, the average optimality gap is only 0.38% and in none of the problem categories is it larger than 0.78%.", "e:keyword": ["Inventory/production: joint replenishment problems", "Inventory/production", "Approximations: asymptotically-optimal heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1087", "e:abstract": "Conventional analyses of stochastic resource allocation problems based on Gittins' indices frequently yield policies which involve an unacceptable amount of switching of the resource from one option to another. This paper discusses a variety of methodologies aimed at solving this problem. They principally involve the development and analysis of stochastic resource allocation models incorporating switching costs together with a consideration of a new class of single-visit policies for which each option has a single (random) period during which it is in receipt of the key resource.", "e:keyword": ["Dynamic programming/optimal control", "Markov: good policies of simple structure", "Probability", "Stochastic model applications: models for stochastic resource allocation", "Programming/scheduling", "Stochastic: development and evaluation of heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1100", "e:abstract": "Two standard results of previous bargaining models state that delay is generally not expected to occur under complete information, and bargaining outcome depends crucially on the bargainers' intrinsic characteristics that determine their reservation prices, and, in general, bargainers with “better” reservation prices tend to get better outcomes. This paper considers a bilateral bargaining problem with complete information in which one of the bargainers, the buyer, is allowed to search while bargaining. More importantly, we assume that the buyer is allowed to recall past outside offers. We find that, quite contrary to the standard results of previous complete information bargaining models, complete information renders no guarantee for immediate resolution of bargaining, and the effect of changing the buyer's search cost on each player's bargaining outcome is unpredictable. The major driving force of these two results is the assumption that the buyer can recall past outside offers.", "e:keyword": ["Games/group decisions", "Bargaining: bargaining and search with recall"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1110", "e:abstract": "In the Method of Abstract Forces for the direction finding (or tradeoff cut) subproblems of interactive multicriterion optimization, it is necessary to scale criterion gradients. Previously, an ad hoc, analyst assisted, but nevertheless effective scaling method was used. This paper introduces a concept of abstract mass. It is shown that the previous ad hoc scaling method may be considered an approximation to the present direction-specific approach. Also, the abstract mass approach provides a strengthening of the original Newton's Second Law of Motion analogy motivation for the Method of Abstract Forces. A method for automatic and direction-specific scaling is proposed which depends on the solution of a continuous piecewise polynomial system of equations. The method is illustrated on an example with three criterion functions. In this example, Newton's method for solving simultaneous nonlinear systems converges to a solution. More generally, restart homotopy methods may be required.", "e:keyword": ["Mathematics", "Fixed points: Newton's method", "Programming", "Multiple criteria: method of abstract forces", "Utility/preference", "Estimation: direction elicitation"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1120", "e:abstract": "In this paper we present tatonnement models for calculating static Wardropian user equilibria on congested networks with fully general demand and cost structures. We present both a qualitative analysis of stability and numerical studies which show that such an approach provides a reliable means for determining static user equilibria. We also describe circumstances for which these models depict day-to-day adjustments from one realizable disequilibrium state to another and how these adjustment processes differ depending on the “quality” of the information being provided by (abstract) traveler information systems. Specifically, we demonstrate that such dynamic adjustment processes settle down to equilibria both when information is complete and when it is incomplete.", "e:keyword": ["Games/group decisions: noncooperative", "Programming: infinite-dimensional", "Transportation: assignment models"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1137", "e:abstract": "Several recent papers have suggested using a <i>product estimator</i> in Monte Carlo Markov chain sampling for estimating the volume of a convex body, the permanent of a matrix and the distribution of first-passage time for a positive recurrent Markov chain. The present paper analyzes the properties of this estimator when each replication starts in an arbitrarily selected state. In particular, it describes a procedure for determining optimal warm-up intervals and optimal sample sizes to achieve a specified level of statistical accuracy at minimal cost. Also, it examines the variation in the optimal solution in response to changes in the parameters of the problem.", "e:keyword": ["Simulation: statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1146", "e:abstract": "Scheduling strategies for real-time systems often employ semipreemptive priorities, allowing for a deadline enforcement by preemptive priorities while avoiding the overhead of unnecessary interrupts. A variety of these strategies can be described by preemption-distance priorities in a straightforward and flexible fashion. A preemption-distance is a globally assigned positive integer number. An arriving task must exceed the priority of the task being served by at least the preemption-distance to cause a preemption. We derive the Laplace-Stieltjes transforms of the marginal waiting and sojourn time distributions for each task class in <i>M</i>/<i>GI</i>/1 single-server queues with preemption-distance priorities. The solutions generalize results for the classical <i>M</i>/<i>GI</i>/1 preemptive and nonpreemptive priority queues and cover a variety of priority systems working under different service policies. The basic derivations are straightforward and lead to solutions which are easy to interpret with respect to the influence of the different system properties. Since the basic equations are modular, solutions for different service policies can be obtained by rederiving auxiliary measures using different mathematical techniques. The modeling power of preemption-distance priorities is illustrated by an example taken from the scheduling of real-time process control systems. This example also validates the derived solutions.", "e:keyword": ["Probability: distributions", "Queues: priority"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1162", "e:abstract": "In a recent paper, L. M. Wein (1992) addressed the problem of scheduling a network of queues. Given a multistation, multiclass queueing network, the problem involves deciding when to release a job to the network as well as how to sequence jobs at each machine in the network to meet a desired throughput level. By approximating this problem by a control problem involving Brownian motion, Wein derived effective heuristics, which easily outperformed traditional work release and sequencing rules. However, Wein's work release rules are complex and his sequencing rules are dynamic. In this paper, we test the performance of a simpler work release policy based on CONWIP (constant work-in-process) in conjunction with static sequencing rules. The results of our simulation study indicate that this simple work release rule can be effective.", "e:keyword": ["Production/inventory: release and sequencing in a job shop", "Queues: multiclass closed queueing networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.1171", "e:abstract": "We consider a problem of scheduling several batches of jobs on two identical parallel machines to minimize the total completion time of jobs. A setup time is incurred whenever there is a switch from processing a job in one batch to a job in another batch. When the number of jobs is arbitrary, the computational complexity of the problem is posed as an open problem in the literature. We show in this note that the problem is binary NP-hard even when the setup times are sequence independent and all processing times are equal.", "e:keyword": ["Analysis of algorithms", "Computational complexity: NP-hardness", "Production/scheduling: parallel machine scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.987", "e:abstract": "In 1953 <i>Operations Research</i> (then called the <i>Journal of the Operations Research Society of America</i>) published a paper that represented a model of practical operations research in the civilian field of agriculture. A recent reexamination, however, showed it to have gaps in exposition and discrepancies in reported times. Research to resolve these difficulties revealed that the original paper omitted some earlier steps in the work, and concentrated on the end-product of the research. This involved developing a climatic calendar based on rates of growth in pea plants, and applying it to scheduling sowing and harvesting vegetable crops for a firm producing frozen food. Behind this development, but unmentioned in the original paper, lay an analysis of water use by plants and the translation of growth measurements into temperature-related growth units applicable to many types of vegetable and fruit crops. This translation allowed plant maturing to be tracked easily and conveniently with a view toward harvest scheduling, and made the development of the climatic calendar possible. With it a farmer gains great economy in the costs and orderliness of planting and harvesting. This paper completes the picture of the work described in Thornthwaite's original paper. The management methods derived originally have remained in use unchanged for over forty years. Thornthwaite's approach and original attitude toward field research may still serve as guides for analysts working on practical problems today.", "e:keyword": ["Agriculture: harvesting and sowing pea plants"]}, {"@id": "http://dx.doi.org/10.1287/opre.42.6.998", "e:abstract": "Discussion of alternative proposals to control emission precursors of acid precipitation focused on their effects on the electric power and coal industries. The Bureau of Mines was concerned about possible indirect effects on electricity-intensive mineral processing facilities, and sponsored research to help it estimate these effects. The research developed a modeling system to estimate compliance strategies and associated changes in electricity costs under different legislation, and dissemination of these results helped inform and broaden discussion of alternative proposals. Early recognition that design of the modeling system would have to allow for flexibility and change enabled development of a system that the Bureau of Mines can use to analyze a broad range of issues involving the electric utility industry, and that utilities or states can use for developing and screening plans to comply with the 1990 Clean Air Act Amendments.", "e:keyword": ["Environment: acid rain title of Clean Air Act", "Industries", "Mining/metals: electricity costs", "Programming", "Linear", "Applications: electric utility industry"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.102", "e:abstract": "A reconfigurable network is a circuit-switched network where the effective topology and capacities can be dynamically adapted to changes in the traffic requirements or to changes in the structure of the network due to failures. The application of reconfigurable networks considered in this paper is data communications. We formulated the joint topology, capacity and routing problem in a reconfigurable network as a nonlinear, mixed integer programming problem. To solve this problem, we present a partial branch-and-bound algorithm. The reduced gradient method is used to solve the problem with the integrality constraints relaxed. Lagrangian relaxation is applied to obtain lower bounds. In the computational experiments, the algorithm found good solutions in a few minutes of CPU time. In addition, the reduction in the minimal delay due to the reconfiguration capability can be as large as 62%.", "e:keyword": ["Communications", "Topology", "Capacity assignment routing", "Networks/graphs: multicommodity flow algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.117", "e:abstract": "Unlike the leaky-bucket scheme which regulates the input rate, the filters we study in this paper reduce the variability of interarrival times (subject to a maximum delay constraint for each customer). These filters are called smoothing filters for point processes. By considering the output processes of various queueing systems, we show that an infinite-server queue acts as a smoothing filter for a doubly stochastic Poisson process, and a single-server queue with deterministic service times acts as a smoothing filter for a stationary and ergodic point process. Based on the second point, we provide a smoothing algorithm that satisfies a maximum delay constraint. The algorithm is shown to be robust and consistent. We then consider two examples where these filters can be applied to increase throughput: One is the single-server loss system with exponential service times, the other is the leaky-bucket scheme with token buffer size 1. Numerical examples and simulations are also given for comparing the tradeoff between delay and throughput. Our approach is based on the variability ordering and a cut criterion for the majorization ordering. The criterion appears to be new and of independent interest.", "e:keyword": ["Communications: leaky bucket scheme", "Probability: distribution comparison", "Queues: output processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.130", "e:abstract": "In this paper we present a new formulation for the Capacitated Minimal Spanning Tree (<b>CMST</b>) problem. One advantage of the new formulation is that it is more compact (in the number of constraints) than a well-known formulation. Additionally, we show that the linear programming relaxation of both formulations produces optimal solutions with the same cost. We present a brief discussion concerning valid inequalities for the (<b>CMST</b>) which are directly derived from the new formulation. We show that some of the new inequalities are not dominated by some sets of facet-inducing inequalities for the (<b>CMST</b>). We derive some Lagrangian relaxation-based methods from the new formulation and present computational evidence showing that reasonable improvements on the original linear programming bounds can be obtained if these methods are strengthened by the use of cutting planes. The reported computational results indicate that one of the methods presented in this paper dominates, in most of the cases, the previous best methods reported in the literature. The most significant improvements are obtained in the instances with the root in the corner.", "e:keyword": ["Networks/graphs: capacitated tree algorithms", "Programming", "Integer: Lagrangian relaxation/cutting planes"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.142", "e:abstract": "This paper studies a topical and economically significant capacitated network design problem that arises in the telecommunications industry. In this problem, given point-to-point communication demand in a network must be met by installing (loading) capacitated facilities on the arcs: Loading a facility incurs an arc specific and facility dependent cost. This paper develops modeling and solution approaches for loading facilities to satisfy the given demand at minimum cost. We consider two approaches for solving the underlying mixed integer program: a Lagrangian relaxation strategy, and a cutting plane approach that uses three classes of valid inequalities that we identify for the problem. We show that a linear programming formulation that includes these inequalities always approximates the value of the mixed integer program at least as well as the Lagrangian relaxation bound. Our computational results on a set of prototypical telecommunication data show that including these inequalities considerably improves the gap between the integer programming formulation and its linear programming relaxation: from an average of 25% to an average of 8%. These results show that strong cutting planes can be an effective modeling and algorithmic tool for solving problems of the size that arise in the telecommunications industry.", "e:keyword": ["Networks/graphs", "Applications: design of telecommunications networks", "Programming", "Integer", "Cutting plane/facet generation: facets for capacitated network design problems", "Programming", "Integer", "Relaxation/subgradient: comparison of Lagrangian and polyhedral approaches"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.158", "e:abstract": "A relatively robust method for the approximate analysis of the mean queue length of an <i>M/G/c</i> queueing system is proposed. The approximation method is developed based on the following assumptions: the residual service time of one busy server is independent of those of the other busy servers, and the system in which all the servers are busy is treated in the same way as a single-server system with <i>c</i> times the service rate of one of the servers. The application of these two assumptions is coupled through the introduction of a parameter <i>n<sub>p</sub></i>. If the number of customers in the system is larger than <i>n<sub>p</sub></i>, assumption 2 is used; otherwise assumption 1 is used. We found that certain properties of <i>n<sub>p</sub></i> allow an estimation of the mean queue length of a large <i>M/G/c</i> queueing system through the approximate analysis of the mean queue length of a much smaller <i>M/G/c</i> queueing system. Numerical results show that the approximation is accurate even when the coefficient of variation of the service time and the number of channels of the system are as large as 20 and 200, respectively.", "e:keyword": ["Queues", "Approximate analysis: robust approximation of mean queue length for large systems", "Queues", "Multiserver: M/G/c for modeling multiple access systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.166", "e:abstract": "We study a multiserver queueing system in which delays influence service times. The system arises in a proposed application of speech compression technology to telephone directory assistance. Customers who must wait for service record their service requests and these are compressed and played to a server as soon as one becomes available (even if the customer is still talking). The longer the delay, the larger the portion of the request that may be compressed, and hence the larger the service time reduction. We develop a model that predicts various performance measures of the system relative to the corresponding system without speech compression.", "e:keyword": ["Communications: speech compression to shorten service times", "Queues", "Multichannel: delay-dependent service times"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.177", "e:abstract": "Service in a loop-based polling system consists of a single server moving around a closed tour, stopping to perform services wherever requests are encountered. There are <i>N</i> stations (unit buffer queues) spaced one unit of distance apart, and the server moves at a unit speed. All queues are identical, and the service time is deterministic. We compare the two well known cyclic polling and greedy servers with a new control policy called the horizon server. The cyclic polling server moves in one direction, even if no requests are waiting, and stops whenever it encounters a request. The greedy server selects the nearest request for its next service. At any station the greedy server can reverse its direction if a new request arrives nearby, and if no requests are waiting the greedy server does not move. The horizon server, with parameter <i>d</i>, ignores all requests for service from a distance farther than <i>d,</i>. Within its horizon (≤ d) it acts like the greedy server. Analytical solutions for <i>N = 2</i> and 3 and numerical results for <i>N</i>≤ 6 show that the horizon server, with the optimum value of <i>d</i>, outperforms the polling and the greedy servers.", "e:keyword": ["Queues", "Cyclic: dynamic scheduling of a server in a polling system"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.187", "e:abstract": "The minimum cost multiperiod capacity expansion of one location in telecommunications network planning can be formulated as a time-dependent knapsack problem. The problem consists of meeting integral demands at distinct time periods at minimum total discounted cost through a selection of items (with integral costs and capacities) from a collection of <i>N</i> distinct types of objects. This note presents an efficient pseudopolynomial time solution to this time-dependent knapsack problem. The technique involves an initial dynamic programming run with time complexity <i>O(N(D + C))</i> followed by a shortest path algorithm with worst-case time complexity <i>O(C<sup>2T</sup>)</i> through a suitably defined network, where <i>D</i> and <i>C</i> are the maxima of the values of the demands and capacities, and <i>T</i> is the number of time periods to be considered. The application of this technique to the problem of optimal capacity expansion of one location in telecommunications network planning is described and computational results are reported.", "e:keyword": ["Dynamic programming", "Deterministic: solution to time-dependent knapsack problems", "Facilities/equipment planning", "Capacity expansion: exact solution via shortest paths"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.19", "e:abstract": "This paper presents an application of a formal process for encoding experts’ probabilistic judgments. The objective is to characterize scientific judgment regarding the risk of chronic lung injury to children aged 8 through 16 and to adult outdoor workers due to long-term ozone exposure in areas with patterns of exposure similar to those found in Southern California and the Northeast. Our measure of injury is the incidence of mild or moderate lesions in the centriacinar region of the lung. Probabilities over population response rates were elicited from six health experts actively researching ozone-induced lung injury. We describe our approach, present some judgmental probability distributions over the population response rates for formation of lesions induced by exposure to ozone, summarize some qualitative results, and offer some concluding comments.", "e:keyword": ["Decision analysis", "Risk: probabilistic risk assessment", "Environment: chronic health risks associated with ozone exposure", "Probability: elicitation of experts’ probabilistic judgments"]}, {"@id": "http://dx.doi.org/10.1287/opre.1050.0249", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.29", "e:abstract": "Putting together a special issue of the ORSA journal that concentrates on the application of operations research in telecommunications was a pleasant task of assembling some of the best papers on different modeling and analysis techniques. At the same time, it was disappointing to realize how little operation researchers know about recent and emerging developments in the field and the opportunities they present for innovative research applied to telecommunication problems. In an attempt to remedy this situation, I decided to accomplish two tasks in writing the preface: To present and highlight some of the important research issues in telecommunications vying for contributions by operation researchers, applied mathematicians, and economists, and to link them, in the appropriate places, to the papers published in the special issue.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.33", "e:abstract": "A model for a cellular mobile network is given which uses the dynamic channel allocation strategy proposed by Everitt and Macfadyen (Everitt, D. E., N. W. Macfadyen. 1983. Analysis of multicellular mobile radiotelephone systems with loss. <i>British Telecom. Tech. J.</i> <b>1</b>37–45.). In this model, handovers of calls from one cell to another are explicitly included. The equilibrium distribution of this network is shown to have product form under certain reversibility conditions. The conditions are restrictive and are, perhaps, unlikely to hold in practical models. However, their study does provide insight into the behavior of cellular systems with dynamic channel allocation. Using the product-form equilibrium distribution we are able to express the blocking probability for new calls, and the blocking probability for handover calls in terms of appropriate normalizing constants. The model is also extended to allow directed retry for handovers in which blocked handover attempts may try to gain access via another cell. This provides differential service for handovers. Again, some reversibility conditions ensure product form, and the blocking probabilities for both new and handover calls are given in terms of normalizing constants.", "e:keyword": ["Communications: telecommunications", "Queues", "Networks: telecommunications"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.43", "e:abstract": "This paper suggests an improved formulation for the multiperiod network topology and capacity expansion problem and proposes new lower bounding schemes based on it. It differs from earlier formulations and solution methods in that entirely new and different subproblems are solved and a number of lower bound tightening schemes are added within the framework of a Lagrangian relaxation. Dual ascent and multiplier adjustment procedures are suggested for the Lagrange multiplier updating procedure. Computational results are reported to demonstrate the tightness of the bounds generated by the suggested procedures. Heuristics based on converting the dual information obtained from the Lagrangian procedure into primal feasible solutions are tested. The tests show that the Lagrangian-based heuristics generate solutions superior to solutions generated by other heuristics proposed in the literature.", "e:keyword": ["Facilities/equipment planning: capacity expansion of telecommunications networks", "Networks/graphs", "Theory: lower bound for multiperiod network expansion problems", "Programming", "Integer", "Heuristic: dual-based algorithms", "Lower bounding procedures"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.58", "e:abstract": "Growing demand, increasing diversity of services, and advances in transmission and switching technologies are prompting telecommunication companies to rapidly expand and modernize their networks. This paper develops and tests a decomposition methodology to generate cost-effective expansion plans, with performance guarantees, for one major component of the network hierarchy—the local access network. The model captures economies of scale in facility costs and tradeoffs between installing concentrators and expanding cables to accommodate demand growth. Our solution method exploits the special tree and routing structure of the expansion planning problem to incorporate valid inequalities, obtained by studying the problem’s polyhedral structure, in a dynamic program which solves an uncapacitated version of the problem. Computational results for three realistic test networks demonstrate that our enhanced dynamic programming algorithm, when embedded in a Lagrangian relaxation scheme (with problem preprocessing and local improvement), is very effective in generating good upper and lower bounds: Implemented on a personal computer, the method generates solutions within 1.2–7.0% of optimality. In addition to developing a successful solution methodology for a practical problem, this paper illustrates the possibility of effectively combining decomposition methods and polyhedral approaches.", "e:keyword": ["Communications: integer programming for local access expansion", "Networks/graphs", "Applications: capacity expansion of local access telephone systems", "Programming", "Integer", "Algorithms: expanding local access telephone networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.6", "e:abstract": "The costs, benefits and strategic role of intermediate echelons in distribution networks are not well understood in many industries. This paper describes a study of such multilevel systems in the industrial paper and plastics industry. We quantify the impact of redistributors, who buy products from manufacturers and sell them exclusively to other distributors. The methodology was applied to an industry-wide study. We derived statistics based on optimal distributor policies for channel choice and stock control. Based on our analysis, we found that effective use of redistribution, in conjunction with these policies, has the potential to generate savings of $446 million for the industry.", "e:keyword": ["Industries: paper and plastics", "Inventory/production: policies for distribution", "Channel choice", "Marketing: retailing and wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.77", "e:abstract": "This paper considers a stochastic fluid model of a buffer content process {<i>X</i>(<i>t</i>), <i>t</i> ≥ 0} that depends on a finite-state, continuous-time Markov process {<i>Z</i>(<i>t</i>), <i>t</i> ≥ 0} as follows: During the time-intervals when <i>Z</i>(<i>t</i>) is in state <i>i</i>, <i>X</i>(<i>t</i>) is a Brownian motion with drift μ<sub><i>i</i></sub>, variance parameter σ<sub><i>i</i></sub><sup>2</sup>and a reflecting boundary at zero. This paper studies the steady-state analysis of the bivariate process {(<i>X</i>(<i>t</i>), <i>Z</i>(<i>t</i>)), <i>t</i> ≥ 0} in terms of the eigenvalues and eigenvectors of a nonlinear matrix system. Algorithms are developed to compute the steady-state distributions as well as moments. Numerical work is reported to show that the variance parameter has a dramatic effect on the buffer content process.", "e:keyword": ["Communication: fluid models using diffusion", "Probability", "Diffusion: reflected Brownian motion", "Random environment"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.1.89", "e:abstract": "The optimal control of arrivals to a two-station token ring network is analyzed in this paper. By adopting a maximum system throughput under a system time-delay constraint optimality criterion, we study a network optimality problem with the assumption that both stations have global information (i.e., the number of packets at each station). The controlled arrivals are assumed to be state-dependent Poisson streams and have exponentially distributed service time. The optimality problem will be formulated using dynamic programming with a convex cost function. Combining with duality theory, we then show that the optimal control is <i>almost bang-bang</i> and in the special case when both queues have the same service rate and sufficiently large buffers, the optimal control is further shown to be switchover. A nonlinear program is used to numerically determine the optimal local control for the purpose of comparison. The results obtained under global and local information can be used to provide a measure of the tradeoff between maximum throughput efficiency and protocol complexity. Numerical examples illustrating the theoretical results are also provided.", "e:keyword": ["Computers: systems design", "Dynamic programming: semi-Markov decision process", "Queues: optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.199", "e:abstract": "Management (at least as reflected by popular management culture) is now more interested in redesigning systems completely than in marginal or continuing improvements. Some practical MS/OR work has recognized this and produced new systems that have changed their sponsoring organizations and provided them with competitive advantages. This paper considers the importance for MS/OR workers of focusing on system design rather than analysis aimed at supporting improvements in existing systems. It notes that MS/OR has traditionally perceived itself predominantly as analysis in this latter sense, and discusses the difference between design and this form of analysis. It then uses some Edelman prize finalist papers to illustrate the cycle of design-oriented MS/OR projects. Finally, this paper considers the opportunities and challenging problems afforded by system design, with particular reference to techniques, education, and the future success of MS/OR.", "e:keyword": ["Information systems", "Analysis and design", "Relationship with OR/MS", "OR/MS philosophy", "Design of systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.208", "e:abstract": "This paper develops a nonlinear bilevel programming model of an aluminium smelter that is capable of representing all the major production processes. The model encompasses all the areas of the smelter which operates in a multilevel way. However, as shown, it can be reduced quite simply to a bilevel programming problem. The problem specification involves nonlinearities with respect to the variables and the presence of ratios among the constraints. The problem is also characterized by a two-way material flow in the production process. A relatively simple solution algorithm that facilitates the attainment of the global optimum is developed. While the problem specification appears daunting, a number of simplifications can be made, due to nature of the problem, allowing its quick solution. While the model developed in this paper together with the solution algorithm appear simple, considerable effort was required to identify the fundamental relationships in the aluminium smelting process, quantify them, and then develop appropriate expressions to represent them. It is believed that the solution of this class of nonlinear bilevel programming problems will have implications for other applications.", "e:keyword": ["Programming", "Nonlinear", "Bilevel", "Production planning", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.219", "e:abstract": "In the last decade, the U.S. electronics industry has experienced a 10% annual growth rate in the assembly of printed wiring boards (PWBs). For many companies, this activity represents the most profitable component of their business with revenues in the billions for the industry as a whole. The basic functions associated with PWB assembly include inventory management, materials handling, production scheduling, and quality control, all of which are subject to a host of system-wide constraints. For the Texas Instruments facility in Austin, Texas, planning and scheduling is further complicated by the need to deal with over 10,000 different components and up to 400 different board types, each with their individual routing and bill of materials. This paper describes a decision support system known as I<sc>nsites</sc> designed to assist Texas Instruments in the day-to-day assembly operations of their PWB facilities. The emphasis is on the heuristic techniques used to solve the underlying multiple machine scheduling problem and the efforts undertaken to validate and deploy the system. The identified benefits, coupled with the acceptance of I<sc>nsites</sc> by both shop floor personnel and management, led the way to full system integration in early 1992.", "e:keyword": ["Production/scheduling", "Decision support system and heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.231", "e:abstract": "Arc routing problems arise in several areas of distribution management and have long been the object of study by mathematicians and operations researchers. In the first of a two-part survey, the Chinese postman problem (<b>CPP</b>) is considered. The main algorithmic results for the <b>CPP</b> are reviewed in five main sections: the undirected <b>CPP</b>, the directed <b>CPP</b>, the windy postman problem, the mixed <b>CPP</b>, and the hierarchical <b>CPP</b>.", "e:keyword": ["Transportation", "Vehicle routing", "Chinese postman problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.243", "e:abstract": "A single evader attempts to traverse a path between two nodes in a network while a single interdictor attempts to detect the evader by setting up an inspection point along one of the network arcs. For each arc there is a known probability of detection if the evader traverses the arc that the interdictor is inspecting. The evader must determine a probabilistic “path-selection” strategy which minimizes the probability of detection while the interdictor must determine a probabilistic “arc-inspection” strategy which maximizes the probability of detection. The interdictor represents, in a simplified form, U.S. and allied forces attempting to interdict drugs and precursor chemicals as they are moved through river, road, and air routes in Latin America and the Caribbean. We show that the basic scenario is a two-person zero-sum game that might require the enumeration of an exponential number of paths, but then show that optimal strategies can be found using network flow techniques of polynomial complexity. To enhance realism, we also solve problems with unknown origins and destinations, multiple interdictors or evaders, and other generalizations.", "e:keyword": ["Games/group decisions", "Two-person zero sum", "Military", "Search and surveillance", "Smuggling", "Networks/graphs", "Applications", "Drug interdiction"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.252", "e:abstract": "This paper addresses the question of when to refuse discount bookings from airline passengers to reserve seats for potential future passengers who are willing to pay a higher fare. When passengers arrive in sequential fare classes, the optimal policy will be to accept reservation requests as long as the cumulative seats booked does not exceed a given booking limit. This paper relates the probability of filling the plane, under the optimal policy, with the ratios of the current to the highest remaining fare classes. In addition, it extends the solution from monotonically increasing fares to fares occurring in arbitrary order. Finally, it demonstrates how Monte Carlo integration is easy to use to get arbitrarily close approximations to the optimal policy.", "e:keyword": ["Probability", "Stochastic model applications", "Monte Carlo integration for airline yield management", "Transportation", "Yield management for airlines"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.264", "e:abstract": "Mathematical programming models with noisy, erroneous, or incomplete data are common in operations research applications. Difficulties with such data are typically dealt with <i>reactively</i>—through sensitivity analysis—or <i>proactively</i>—through stochastic programming formulations. In this paper, we characterize the desirable properties of a solution to models, when the problem data are described by a set of <i>scenarios</i> for their value, instead of using point estimates. A solution to an optimization model is defined as: <i>solution robust</i> if it remains “close” to optimal for all scenarios of the input data, and <i>model robust</i> if it remains “almost” feasible for all data scenarios. We then develop a general model formulation, called <i>robust optimization</i> (RO), that explicitly incorporates the conflicting objectives of solution and model robustness. Robust optimization is compared with the traditional approaches of sensitivity analysis and stochastic linear programming. The classical diet problem illustrates the issues. Robust optimization models are then developed for several real-world applications: power capacity expansion; matrix balancing and image reconstruction; air-force airline scheduling; scenario immunization for financial planning; and minimum weight structural design. We also comment on the suitability of parallel and distributed computer architectures for the solution of robust optimization models.", "e:keyword": ["Finance", "Portfolio optimization", "Programming", "Stochastic programming", "Simulation", "Large-scale optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.282", "e:abstract": "In steady-state simulation output analysis, mean-square consistency of the process-variance estimator is important for a number of reasons. One way to construct an asymptotically valid confidence interval around a sample mean is via construction of a consistent estimator of the process variance and a central limit theorem. Also, if an estimator is consistent in the mean-square sense, a mean-square error analysis is theoretically justified. Finally, batch-size selection is an open research problem in steady-state output analysis, and a mean-square error analysis approach has been proposed in the literature; to be valid, the process-variance estimators constructed must be consistent in the mean-square sense. In this paper, we prove mean-square consistency of the process-variance estimator for the methods of batch means, overlapping batch means, standardized time series (area), and spaced batch means, by rigorously computing the rate of decay of the variance of the process-variance estimators. Asymptotic results for third and higher centered moments of the batch means and area variance estimators are also given, along with central limit theorems.", "e:keyword": ["Simulation", "Statistical analysis", "Process-variance estimation"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.292", "e:abstract": "This paper studies a model of the situation in which two players are bargaining face-to-face over the partition of a unit size cake and, moreover, one of the players can choose to temporarily leave the negotiating table to search for an outside option. A main conclusion is that the equilibrium outcome does not depend on whether a bargainer is allowed (within the game form) to choose to return to the negotiating table to resume bargaining after having searched for some finite time. Moreover, it is shown that our strategic bargaining-search game approximately implements an appropriately defined Nash bargaining solution.", "e:keyword": ["Games/group decisions", "Bargaining", "Noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.298", "e:abstract": "This paper discusses the distributional Little's law and examines its applications in a variety of queueing systems. The distributional law relates the steady-state distributions of the number in the system (or in the queue) and the time spent in the system (or in the queue) in a queueing system under FIFO. We provide a new proof of the distributional law and in the process we generalize a well known theorem of Burke on the equality of pre-arrival and postdeparture probabilities. More importantly, we demonstrate that the distributional law has important algorithmic and structural applications and can be used to derive various performance characteristics of several queueing systems which admit distributional laws. As a result, we believe that the distributional law is a powerful tool for the derivation of performance measures in queueing systems and can lead to a certain unification of queueing theory.", "e:keyword": ["Queues", "Approximations", "Algorithms", "Multichannel"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.311", "e:abstract": "This paper reviews the literature on quantitatively-oriented approaches for determining lot sizes when production or procurement yields are random. We discuss issues related to the modeling of costs, yield uncertainty, and performance in the context of systems with random yields. We provide a review of the existing literature, concentrating on descriptions of the types of problems that have been solved and important structural results. We identify a variety of shortcomings of the literature in addressing problems encountered in practice, and suggest directions for future research.", "e:keyword": ["Inventory/production", "Policies", "Random yields", "Inventory/production", "Scale-diseconomies/lot sizing", "Lot sizing", "Inventory/production", "Uncertainty", "Stochastic", "Lot sizing with random yields"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.335", "e:abstract": "For deterministic series inventory systems with backlogging, we show that the continuous relaxation of the best integer frequency policies is a lower bound on all feasible policies. The problem with backlogging can be reduced, in <i>O</i>(<i>n</i>) time, to an equivalent problem without backlogging. Therefore, based on the result of R. O. Roundy (Roundy, R. O. 1983. 94%-Effective lot-sizing in multistage assembly systems. Technical Report 674, School of Operations Research and Industrial Engineering, Cornell University, Ithaca, New York.), the best integer frequency policy provides a solution which is within 2% of the optimal.", "e:keyword": ["Inventory/production", "Multistage inventory/production", "Backlogging heuristics", "Worse-case performance"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.346", "e:abstract": "This paper introduces a new two-stage assembly scheduling problem. There are <i>m</i> machines at the first stage, each of which produces a component of a job. When all <i>m</i> components are available, a single assembly machine at the second stage completes the job. The objective is to schedule jobs on the machines so that the makespan is minimized. We show that the search for an optimal solution may be restricted to permutation schedules. The problem is proved to be NP-hard in the strong sense even when <i>m</i> = 2. A schedule associated with an arbitrary permutation of jobs is shown to provide a worst-case ratio bound of two, and a heuristic with a worst-case ratio bound of 2 − 1/<i>m</i> is presented. The compact vector summation technique is applied for finding approximation solutions with worst-case absolute performance guarantees.", "e:keyword": ["Production/scheduling", "Assembly problem and complexity", "Production/scheduling", "Approximations/heuristic", "Worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.356", "e:abstract": "A two-dimensional warranty policy is characterized by a region in a two-dimensional plane with the horizontal axis representing time and the vertical axis the usage. In this paper, we study two-dimensional failure-free warranty policies for nonrepairable items that require the manufacturer to replace all items which fail under warranty by new ones at no cost to the buyer. We derive expressions for the expected warranty cost per item sold and for the expected life cycle cost for four different warranty regions. We illustrate these by numerical examples and discuss some resulting implications for the consumer and the manufacturer.", "e:keyword": ["Cost analysis", "Free replacement warranties", "Probability", "Renewal processes", "Two-dimensional", "Reliability", "Failures and replacement"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.2.367", "e:abstract": "This paper presents the development of new elimination tests which greatly enhance the performance of a relatively well established dynamic programming approach and its application to the minimization of the total traveling cost for the traveling salesman problem with time windows. The tests take advantage of the time window constraints to significantly reduce the state space and the number of state transitions. These reductions are performed both a priori and during the execution of the algorithm. The approach does not experience problems stemming from increasing problem size, wider or overlapping time windows, or an increasing number of states nearly as rapidly as other methods. Our computational results indicate that the algorithm was successful in solving problems with up to 200 nodes and fairly wide time windows. When the density of the nodes in the geographical region was kept constant as the problem size was increased, the algorithm was capable of solving problems with up to 800 nodes. For these problems, the CPU time increased linearly with problem size. These problem sizes are much larger than those of problems previously reported in the literature.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Transportation", "Vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.379", "e:abstract": "Petroleum products are distributed worldwide from refineries and lube plants to retail outlets and industrial customers. Proper dispatching of shipments of such products, packaged and in bulk, may result in significant transportation and inventory cost savings. This work examines the variety of operational environments which exist in dispatching petroleum products, and the operations research tools used by oil companies to dispatch such products. In addition, it identifies gaps where additional research is needed.", "e:keyword": ["Industries", "Petroleum", "Products distribution", "Transportation", "Scheduling", "Distribution of petroleum products"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.388", "e:abstract": "This paper is a case study on the role of complex computer simulation models in the regulation of the electricity industry. The analysis focuses on electricity production cost simulation models as they are used to set prices for certain nonutility generators in California. This represents an early part of the trend toward “markets” for electric power, and away from pure monopoly supply. The introduction of even limited competition creates conditions both favoring and hindering the reliability of cost estimates, and demonstrates the sensitivity of results to problem specification and model implementation. Attention is focused on the representation of power system operational constraints, particularly the unit commitment problem, in the modeling process and their effect on the prices which result. Examples from the litigation history illustrate the problem of managing the strategic use and abuse of modeling techniques for competitive advantage. Highly structured procedures for using models in such situations offer some constraints on manipulation by competing parties. Otherwise, experts are apt to overwhelm regulators.", "e:keyword": ["Cost analysis", "For pricing nonutility generation", "Industries", "Electric", "Regulation of nonutility generation", "Simulation", "Applications", "Production cost models"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.399", "e:abstract": "This is the second half of a two-part survey on arc routing problems. The first part appeared in the March–April 1995 issue of this journal. Here, the rural postman problem (<b>RPP</b>) is reviewed. The paper is organized as follows: applications, the undirected <b>RPP</b>, the directed <b>RPP</b>, the stacker crane problem, and the capacitated arc routing problem.", "e:keyword": ["Transportation", "Vehicle routing", "Rural postman problem", "Survey"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.415", "e:abstract": "This paper uses multiple objective decision analysis to conduct an <i>ex post</i> analysis of a specific fisheries management decision involving conflicting long-term objectives for mixed stocks. The paper illustrates the potential role of subjective judgment in fisheries and other resource management contexts and the relevance of decision analysis for in-season salmon management. The decision context is first defined in terms of objectives and alternatives, then subjective probability distributions are elicited from experts regarding uncertain biological parameters. A simulation is then used to estimate the consequences of alternative openings, given the biological uncertainties. A utility function is elicited from a fisheries manager and used to select among alternative commercial fishery openings. The results show that objectives other than those typically assumed in fisheries modeling, and subjective judgments by technical experts, can be important for in-season salmon management. The results also show that, in this application, the equivalent of nearly $8 million in potential benefits are available from delaying the opening of the commercial fishery by a single day.", "e:keyword": ["Decision analysis", "Applications", "Judgmental approaches", "Natural resources", "Fisheries management"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.427", "e:abstract": "In this paper we study optimal strategies for renting hotel rooms when there is a stochastic and dynamic arrival of customers from different market segments. We formulate the problem as a stochastic and dynamic programming model and characterize the optimal policies as functions of the capacity and the time left until the end of the planning horizon. We consider three features that enrich the problem: we make no assumptions concerning the particular order between the arrivals of different classes of customers; we allow for multiple types of rooms and downgrading; and we consider requests for multiple nights. We also consider implementations of the optimal policy. The properties we derive for the optimal solution significantly reduce the computational effort needed to solve the problem, yet in the multiple product and/or multiple night case this is often not enough. Therefore, heuristics, based on the properties of the optimal solutions, are developed to find “good” solutions for the general problem. We also derive upper bounds which are useful when evaluating the performance of the heuristics. Computational experiments show a satisfactory performance of the heuristics in a variety of scenarios using real data from a medium size hotel.", "e:keyword": ["Dynamic programming", "Applications", "Renting hotel rooms in stochastic dynamic environments", "Industries", "Hotel/motel", "Optimal sales strategies", "Inventory/production", "Perishable/aging items", "Yield management with downgrading"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.444", "e:abstract": "This paper considers the problem of positioning component fixtures on the carriers of computer, numerically controlled dual delivery machines used for populating printed circuit boards with surface mounted technology. This reel positioning problem (<b>RPP</b>) is one of a series of optimization problems that are critical for improving system productivity and realizing the full potential of concurrent operations. We formulate the <b>RPP</b> as a mathematical program and establish its complexity. Since the problem is NP-complete we focus on the development of heuristics. Our solution procedure was prompted by engineering considerations that included concerns for minimizing the changes in the carrier direction and total movement. We also present encouraging results with test problems. The method has been implemented and achieved 7 to 8% reductions in cycle time.", "e:keyword": ["Manufacturing", "Automated systems", "Populating printed circuit boards", "Production/scheduling", "Approximation/heuristics", "Sequencing", "Concurrent operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.458", "e:abstract": "We consider a production/distribution system represented by a general directed acyclic network. Each node is associated with a specific “product” at a given location and/or production stage. An arc (<i>i</i>, <i>j</i>) indicates that item <i>i</i> is used to “produce” item <i>j</i>. External demands may occur at any of the network's nodes. These demands occur continuously at item-specific constant rates. Components may be assembled in any given proportions. The cost structure consists of inventory carrying, variable, and fixed production/distribution costs. The latter depend, at any given replenishment epoch, on the specific set of items being replenished, according to an arbitrary set function merely assumed to be monotone and submodular. It has been shown that a simply structured, so-called power-of-two policy is guaranteed to come within 2% of a lower bound for the minimum cost. In this paper, we derive efficient algorithms for the computation of an optimal power-of-two policy, possibly in combination with this lower bound. These consist of a limited number of polymatroidal maximum flow calculations in networks closely associated with the original production/distribution network.", "e:keyword": ["Inventory/production", "Multiater echelon", "Stage", "Inventory/production", "Approximations and heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.471", "e:abstract": "Much research has been performed in finding optimal group replacement policies for production systems consisting of parallel components, where the failure times of the components are independent identically distributed exponential random variables with a common parameter λ. This paper introduces a class of decision rules that utilizes the statistical information obtained during operation of the components. Two forms of statistical input are allowed. We assume that a prior distribution over the possible values of λ is available. It is not required that this prior distribution be in conjugate form. Statistical information that is provided by the actual failure times of the components is incorporated into the decision rule via the sufficient statistics for the problem. This results in group replacement policies that are intuitively attractive, easy to implement, and mathematically tractable.", "e:keyword": ["Inventory/production", "Group replacement of parallel machines", "Production/scheduling", "Learning about failure distributions of production systems", "Statistics", "Bayesian learning applied to production models"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.477", "e:abstract": "A novel parallel decomposition algorithm is developed for large, multistage stochastic optimization problems. The method decomposes the problem into subproblems that correspond to scenarios. The subproblems are modified by separable quadratic terms to coordinate the scenario solutions. Convergence of the coordination procedure is proven for linear programs. Subproblems are solved using a nonlinear interior point algorithm. The approach adjusts the degree of decomposition to fit the available hardware environment. Initial testing on a distributed network of workstations shows that an optimal number of computers depends upon the work per subproblem and its relation to the communication capacities. The algorithm has promise for solving stochastic programs that lie outside current capabilities.", "e:keyword": ["Programming", "Stochastic", "Scenario decomposition", "Parallel computation"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.491", "e:abstract": "The long-haul crew pairing problem involves the assignment of crews to scheduled flights such that overall costs are minimized and crew availability and work rule restrictions are satisfied. These problems are characterized by international flights that typically do not operate on a daily schedule, resulting in a sparsity of flights and extended periods of inactivity for crews at some stations. To eliminate these extended rest periods and reduce overall costs, it is advantageous in some cases to deadhead crews, that is, to assign crews to flights as passengers for repositioning and better utilization. In this paper, a heuristic methodology is developed to improve crew pairing solutions through the efficient selection and utilization of deadhead flights. The methodology uses the dual solutions determined in solving the linear programming relaxation of the crew pairing problem to build arrival and departure profiles at each station. These profiles provide a mechanism to <i>price-out</i> potential deadhead flights. Flights that price-out favorably may be used to build improved solutions to the crew pairing problem. The Deadhead Selection Procedure is tested using data provided by a long-haul airline and is shown to achieve significant improvement in crew costs by reducing the total number of deadhead hours flown and by reducing the total duration of rest periods.", "e:keyword": ["Networks/graphs", "Multicommodity", "Programming", "Applications", "Large-scale systems", "Transportation", "Network models", "Personnel scheduling", "Airline"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.500", "e:abstract": "We consider an optimal partitioning problem that occurs in the assignment of computer jobs to a multiple cache and in other combinatorial optimization problems: For a given set of <i>n</i> elements, where each element <i>i</i> has a given frequency <i>p</i><sub><i>i</i></sub> and a specific weight <i>w</i><sub><i>i</i></sub>, we would like to divide the elements into <i>m</i> mutually exclusive groups such that the sum over all the groups of the average group weight is maximal. We characterize the optimal solution and present an algorithm which is polynomial in <i>n</i> for obtaining the optimal partitioning. Optimal partitioning policies for two groups has an especially simple characterization: There exist two numbers α and β, with min<i>w</i><sub><i>i</i></sub> < α < β ≤ max<i>w</i><sub><i>i</i></sub>, such that all the elements with weight <i>w</i><sub><i>i</i></sub> satisfying α ≤ <i>w</i><sub><i>i</i></sub> ≤ β belong to one group and all other elements belong to the other group. A modification of this policy gives the optimal partitioning for an arbitrary number of groups.", "e:keyword": ["Analysis of algorithms", "Computational complexity", "Polynormal algorithms", "Computers", "Systems design", "Partitioning of program items", "Mathematics", "Combinatorics", "Optimal partitioning"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.509", "e:abstract": "In the simulation of Markov chains, importance sampling involves replacing the original transition matrix, say <i>P</i>, with a suitably chosen transition matrix <i>Q</i> that tends to visit the states of interest more frequently. The likelihood ratio of <i>P</i> relative to <i>Q</i> is an important random variable in the importance sampling method. It always has expectation one, and for any interesting pair of transition matrices <i>P</i> and <i>Q</i>, there is a sample path length that causes the likelihood ratio to be close to zero with probability close to one. This may cause the variance of the importance sampling estimator to be larger than the variance of the traditional estimator. We develop sufficient conditions for ensuring the tightness of the distribution of the logarithm of the likelihood ratio for all sample path lengths, and we show that when these conditions are satisfied, the likelihood ratio is approximately lognormally distributed with expected value one. These conditions can be used to eliminate some choices of the alternative transition matrix <i>Q</i> that are likely to result in a variance increase. We also show that if the likelihood ratio is to remain well behaved for all sample path lengths, the alternative transition matrix <i>Q</i> has to approach the original transition matrix <i>P</i> as the sample path length increases. The practical significance of this result is that importance sampling can be difficult to apply successfully in simulations that involve long sample paths.", "e:keyword": ["Probability", "Markov processes", "Simulation", "Statistical analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.520", "e:abstract": "In this paper, we solve a type of shortest queue problem, which is related to multibeam satellite systems. We assume that the packet interarrival times are independently distributed according to an arbitrary distribution function, that the service times are Markovian with possibly different service rates, that each server has its own buffer for packet waiting, and that jockeying among buffers is permitted. Packets always join the shortest buffer(s). Jockeying takes place as soon as the difference between the longest and shortest buffers exceeds a preset number (not necessarily 1). In this case, the last packet in a longest buffer jockeys instantaneously to the shortest buffer(s). We prove that the equilibrium distribution of packets in the system is modified vector geometric. Expressions of main performance measures, including the average number of packets in the system, the average packet waiting time in the system, and the average number of jockeying, are given. Based on these solutions, numerical results are computed. By comparing the results for jockeying and nonjockeying models, we show that a significant improvement of the system performance is achieved for the jockeying model.", "e:keyword": ["Queues", "Multichannel", "Jockeying"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.530", "e:abstract": "We often try to draw inferences from partial observations of queueing systems in real-life situations. For example, if we observe many customer arrivals, we may presume that the system is crowded and many customers are served. Unfortunately, such an intuitive statement is not necessarily valid. We provide sufficient conditions under which the intuition can be justified, and investigate related properties of queueing systems. We also study a way to exploit the partial information in a quantitative manner for simple queueing systems. One numerical result is rather counterintuitive. Specifically, the number of customers in the system at time <i>t</i> given that the cumulative number of departures is a certain constant is not necessarily stochastically increasing in <i>t</i> for a simple <i>M</i>/<i>M</i>/1 system with finite capacity.", "e:keyword": ["Queues", "Inference", "Transient results"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.3.537", "e:abstract": "We consider a network of queues with multiple classes of customers, <i>signals</i>, and arbitrary service time distributions. The signals bring commands to the service nodes and may trigger customers to move instantly within the network. We consider symmetric service disciplines (e.g., processor sharing, LIFO preemptive, infinite server) similar to those in F. Kelly (Kelly, F. 1979. <i>Reversibility and Stochastic Networks</i>. John Wiley, New York.), and show that when each node has a single server operating under a symmetric service discipline, the stationary distribution has a product-form solution. The product form, however, depends on the service time distributions of customers at each node beyond their means. This is in contrast to the insensitivity result of the conventional Jackson networks and can be attributed to the effect of signals. When the nodes have multiple servers in parallel, the usual definition of signals does not lead to a product-form solution. We propose a new definition of signals that depends on the service effort and provides a product-form solution.", "e:keyword": ["Probability", "Stochastic model applications", "Arbitrary service time distribution", "Queues", "Networks of queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.551", "e:abstract": "This paper applies the concept of a <i>life cycle</i> to the subject of goal programming. The results suggest that journal article productivity in goal programming exhibits life cycle behavior, and that its life cycle is now in decline. The implications are a portent not only for goal programming research but also for other OR fields.", "e:keyword": ["Professional", "OR/MS education", "Innovation", "Research and development", "Programming", "Multiple criteria", "Research on goal programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.558", "e:abstract": "Needle exchange is an intervention for slowing HIV transmission among drug injectors. Most studies of needle exchange rely on changes in self-reported risky behaviors among program participants. This paper reports two models based on objectively observed operational data, such as needle distribution rates, client visit dates, needle circulation times, and the fraction of returned needles testing HIV positive. Application of the models to data from the New Haven needle exchange suggests that HIV transmission has been slowed significantly among program participants.", "e:keyword": ["Health care", "Epidemiology", "Needle exchange evaluation/HIV/AIDS", "Probability", "Applications", "Poisson infection models/change points/Markov models", "Statistics", "Estimation", "Maximum likelihood estimation of change point/Markov models"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.570", "e:abstract": "Two medical applications of linear programming are described in this paper. Specifically, linear programming-based machine learning techniques are used to increase the accuracy and objectivity of breast cancer diagnosis and prognosis. The first application to breast cancer diagnosis utilizes characteristics of individual cells, obtained from a minimally invasive fine needle aspirate, to discriminate benign from malignant breast lumps. This allows an accurate diagnosis without the need for a surgical biopsy. The diagnostic system in current operation at University of Wisconsin Hospitals was trained on samples from 569 patients and has had 100% chronological correctness in diagnosing 131 subsequent patients. The second application, recently put into clinical practice, is a method that constructs a surface that predicts when breast cancer is likely to recur in patients that have had their cancers excised. This gives the physician and the patient better information with which to plan treatment, and may eliminate the need for a prognostic surgical procedure. The novel feature of the predictive approach is the ability to handle cases for which cancer has not recurred (censored data) as well as cases for which cancer has recurred at a specific time. The prognostic system has an expected error of 13.9 to 18.3 months, which is better than prognosis correctness by other available techniques.", "e:keyword": ["Health care", "Diagnosis", "Breast cancer diagnosis", "Programming", "Linear", "Applications", "Cancer diagnosis and prognosis via linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.578", "e:abstract": "This paper surveys the use of mathematical programming models for controlling environmental quality. The scope includes air, water, and land quality, stemming from the first works in the 1960s. It also includes integrated models, generally that are economic equilibrium models which have an equivalent mathematical program or use mathematical programming to compute a fixed point. A primary goal of this survey is to identify interesting research avenues for people in mathematical programming with an interest in applying it to help control our environment with as little economic sacrifice as possible.", "e:keyword": ["Environment", "Environmental control and economics", "Natural resources", "Air and water quality", "Land contamination", "Waste disposal", "Programming", "Mathematical programming models"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.623", "e:abstract": "Discretionary service facilities are providers of products and/or services that are purchased by customers who are traveling on otherwise preplanned trips such as the daily commute. Optimum location of such facilities requires them to be at or near points in the transportation network having sizable flows of different potential customers. N. Fouska (Fouska, N. 1988. Optimal Location of Discretionary Service Facilities. MS Thesis, Operations Research Center, MIT, Cambridge, Mass.) and O. Berman, R. Larson and N. Fouska (BLF [Berman, O., R. C. Larson, N. Fouska. 1992. Optimal location of discretionary service facilities. <i>Trans. Sci.</i> <b>26</b>(3) 201–211.]) formulate a first version of this problem, assuming that customers would make no deviations, no matter how small, from the preplanned route to visit a discretionary service facility. Here the model is generalized in a number of directions, all sharing the property that the customer may deviate from the preplanned route to visit a discretionary service facility. Three different generalizations are offered, two of which can be solved approximately by greedy heuristics and the third by any approximate or exact method used to solve the <i>p</i>-median problem. We show for those formulations yielding to a greedy heuristic approximate solution, including the formulation in BLF, that the problems are examples of optimizing submodular functions for which the G. Nemhauser, L. Wolsey and M. Fisher (Nemhauser, G. L., L. A. Wolsey, M. L. Fisher. 1978. An analysis of approximations for maximizing sub-modular set functions, I. <i>Math. Prog.</i> <b>14</b> 265–294.) bound on the performance of a greedy algorithm holds. In particular, the greedy solution is always within 37% of optimal, and for one of the formulations we prove that the bound is tight.", "e:keyword": ["Financial institutions", "Banks", "Locating automatic teller machines", "Networks/graphs", "Heuristics", "Locating n points to intercept max flow", "Transportation", "Models", "Location", "Maximizing flow of potential customers"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.633", "e:abstract": "We consider the problem of comparing a small number of stochastic systems via computer simulation when the basis for comparison is the expected value of some system performance measure. To solve this problem we develop two-stage sampling procedures that provide confidence intervals for the difference between the expected performance of each system and the best of the others. These confidence intervals are valid under mild conditions, and the procedures allow the experimenter to specify the desired precision in advance. Special cases of our results include standard indifference-zone selection procedures. The paper includes guidelines for experiment design and an illustrative example.", "e:keyword": ["Simulation", "Statistical analysis", "Design of experiments"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.641", "e:abstract": "In this paper we study the parameter space investigation method of I. M. Sobol and R. B. Statnikov. The method is a “random hitting” procedure for solving the kinds of small, but difficult, multiple objective nonlinear programming problems often encountered in engineering design and optimal control. Sobol and Statnikov have argued the effectiveness of their approach and its application possibilities. In this paper, we computationally test the parameter space investigation method to examine the extent to which claims about their method can be substantiated.", "e:keyword": ["Engineering", "Design problems", "Programming", "Multiple criteria", "Interactive procedures", "Programming", "Nonlinear", "Multiple objectives"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.649", "e:abstract": "We present a general framework for modeling routing problems based on formulating them as a traditional location problem called the capacitated concentrator location problem. We apply this framework to two classical routing problems: the capacitated vehicle routing problem and the inventory routing problem. In the former case, the heuristic is proven to be asymptotically optimal for any distribution of customer demands and locations. Computational experiments show that the heuristic performs well for both problems and, in most cases, outperforms all published heuristics on a set of standard test problems.", "e:keyword": ["Analysis of algorithms", "Suboptimal algorithms", "Programming", "Integer", "Relaxation", "Subgradient", "Transportation", "Vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.661", "e:abstract": "This paper considers a specially structured uncapacitated facility location problem. We show that several problems, including certain tool selection problems, substitutable inventory problems, supplier sourcing problems, discrete lot sizing problems, and capacity expansion problems, can be formulated as instances of the problem. We also show that the problem with <i>m</i> facilities and <i>n</i> customers can be solved in <i>O</i>(<i>mn</i>), as a shortest path problem on a directed graph.", "e:keyword": ["Facilities/equipment planning", "Discrete location", "Networks/graphs", "Distance algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.670", "e:abstract": "In this paper we study the nonlinear resource allocation problem, defined as the minimization of a convex function over one convex constraint and bounded integer variables. This problem is encountered in a variety of applications, including capacity planning in manufacturing and computer networks, production planning, capital budgeting, and stratified sampling. Despite its importance to these and other applications, the nonlinear resource allocation problem has received little attention in the literature. Therefore, we develop a branch-and-bound algorithm to solve this class of problems. First we present a general framework for solving the continuous-variable problem. Then we use this framework as the basis for our branch-and-bound method. We also develop reoptimization procedures and a heuristic that significantly improve the performance of the branch-and-bound algorithm. In addition, we show how the algorithm can be modified to solve nonconvex problems so that a concave objective function can be handled. The general algorithm is specialized for the applications mentioned above and computational results are reported for problems with up to 200 integer variables. A computational comparison with a 0, 1 linearization approach is also provided.", "e:keyword": ["Programming", "Nonlinear", "Algorithms", "Nonlinear resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.684", "e:abstract": "We study the experimental design problem of selecting a most informative subset, having prespecified size, from a set of correlated random variables. The problem arises in many applied domains, such as meteorology, environmental statistics, and statistical geology. In these applications, observations can be collected at different locations, and possibly, at different times. Information is measured by “entropy.” In the Gaussian case, the problem is recast as that of maximizing the determinant of the covariance matrix of the chosen subset. We demonstrate that this problem is NP-hard. We establish an upper bound for the entropy, based on the eigenvalue interlacing property, and we incorporate this bound in a branch-and-bound algorithm for the exact solution of the problem. We present computational results for estimated covariance matrices that correspond to sets of environmental monitoring stations in the United States.", "e:keyword": ["Facilities/equipment planning", "Location of environmental monitoring stations", "Programming", "Nonlinear combinatorial optimization", "Statistics", "Maximum entropy sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.692", "e:abstract": "Economies of scale are fundamental to manufacturing operations. With respect to scheduling, this phenomenon manifests itself in efficiencies gained from grouping similar jobs together. This paper reviews the rapidly growing literature on single-machine scheduling models that incorporate benefits from job grouping. We focus on three basic models known as <i>family scheduling with item availability</i>, <i>family scheduling with batch availability</i>, and <i>batch processing</i>. We present known results and introduce new results, and we pay special attention to key theoretical properties and the use of these properties in optimization procedures.", "e:keyword": ["Production/scheduling", "Deterministic sequencing and grouping"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.704", "e:abstract": "Analytical expressions (formulas) for normalizing constants in closed queueing networks with different classes of customers are presented and discussed in this article.", "e:keyword": ["Queues/networks", "Calculation of normalizing constants"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.712", "e:abstract": "This paper concerns the throughput rate of multistation reliable production lines with no intermediate buffers. Processing times at the service stations are independent, exponential random variables, possibly with different means. We extend the work started in E. J. Muth (Muth, E. J. 1984. Stochastic processes and their network representations associated with production line queueing model. <i>Eur. J. Opnl. Res.</i> <b>15</b> 63–83.) to provide an algorithm that allows for the efficient computation of longer lines and to provide results for the nonidentical server case. A result is presented which provides the distribution function of the holding time at the stations.", "e:keyword": ["Manufacturing", "Performance", "Project management", "GERT", "Queues", "Tandem", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.4.716", "e:abstract": "In this note, an algorithm previously introduced by B. Lev and H. Weiss to optimally solve the finite horizon EOQ model with price changes is modified to avoid infeasible solutions. An example is provided.", "e:keyword": ["Inventory/production", "Finite horizon ordering strategy for single price increase"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.731", "e:abstract": "It is possible to distinguish seven process categories among OR/MS research strategies: ripple, embedding, bridging, transfer of technology, creative application, structuring, and statistical modeling. To examine how often OR/MS workers use these processes, this paper analyzes as a sample the contents of the papers in the 1992 Issues of <i>Operations Research, Management Science</i>, and <i>Interfaces</i>. It finds that the ripple process is dominantly used in theoretical research and the transfer-of-technology process is the one most frequently used in true applications. The paper also sets forth the full spectrum of results for all seven processes in the 1992 sample, and reaches some conclusions with possible implications for the future of OR/MS.", "e:keyword": ["Philosophy of modeling: distinguishing process categories", "Professional", "ORIMS education: philosophy"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.741", "e:abstract": "Many service organizations encounter personnel tour-scheduling problems which focus on the efficient assignment of daily shift schedules and work days to employees across a weekly planning horizon. The airline Industry, in particular, faces a highly complex and difficult tour-scheduling environment for their ground station personnel, During the past twenty years, the airlines have worked to improve continuously their abilities to effectively schedule personnel working at planesides, counters, and gates in airline stations. Substantial reductions in labor costs have resulted from the use of efficient tour-scheduling methods for such workers. We report on the development and implementation of two modules designed to enhance the tour-scheduling process associated with United Airlines’ <i>Pegasys</i> Manpower Planning System. The first module uses column generation to improve the selection of employee shifts. The second module, a local search heuristic based on simulated annealing, enables initial feasible tour-scheduling solutions to rapidly improve. Using data collected from all 119 United Airlines stations across the U.S., we find that the incorporation of the modules results in a potential annual cost savings of more than $8 million. We conclude with a discussion of notable implementation issues and extensions.", "e:keyword": ["Programming", "Integer", "Heuristics: modified set covering problem", "Transportation", "Scheduling", "Personnel: ground-crew scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.752", "e:abstract": "“Real-world” adaptation of a linear programming model for cutting stock problems has led to significant financial and operational improvements at Kendall Corporation. Originally developed for controlling waste when slitting rolls of cloth, the project also reduced work-in-process inventory, which helped the firm reach just-in-time production goals. Another dramatic, but largely unplanned, benefit from the project was the company’s ability to investigate operations redesign. Direct and indirect benefits derived from making scheduling decisions more visible, systematic, and tractable have been estimated by Kendall to be more than $2 million per year.", "e:keyword": ["Production/scheduling: applications", "Cutting stock/trim"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.758", "e:abstract": "Since it is safer and less expensive to absorb delays on the ground, air traffic control management tries to limit the duration of airborne delays by holding aircraft previous to departure when congestion at the airport of destination is anticipated. The problem of assigning appropriate ground-holds to aircraft is known as the ground-holding problem. Ground-holding decisions must be implemented in real time and for multiple airports; therefore, the speed of solution for algorithms is critical. This paper tests static and dynamic optimal solutions, and a very fast heuristic for the assignment of ground-holds in air traffic control. The optimal solutions are based on stochastic linear programming. The heurtstic incorporates elements of stochastic modeling by utilizing information conveyed by a probabilistic forecast of airport landing capacity, while taking into consideration the dynamic nature of the problem. In extensive computational experiments based on data for Logan airport, the heuristic performed significantly better than the optimal static solution, a deterministic solution, and the passive strategy of no-ground-holds; and within 5% of the optimal dynamic solution at a fraction of the computational time. Due to its remarkable efficiency, the stochastic-dynamic heuristic appears to be a promising building block in the development of fast ground-holding algorithms for the complete network of airports.", "e:keyword": ["Programming", "Stochastic: optimal algorithms and heuristics for ground-holding assignments", "Transportation", "Models: ground-holding problem in air traffic control"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.771", "e:abstract": "We formulate an optimization model of a multiple reservoir water resource system that encompasses interbasin transfers among two or more river basins. Autocorrelated inflows are modeled with a linear autoregressive stochastic process. Benefits for each period are assumed to depend separably on storage levels and discharges with the dependence on discharges being linear. For the important special case of a single river basin, a myopic policy (hence, computed easily) is optimal. When the model includes the possibility of interbasin transfers, a myopic policy is optimal if the deterministic and stochastic portions of the inflow process are always nonnegative. Even if this assumption is not valid, myopic policies yield useful bounds.", "e:keyword": ["Dynamic programming", "Applications: river basin reservoirs", "Natural resources", "Water resources: reservoir discharges"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.781", "e:abstract": "An example of the quadratic assignment problem (QAP) is the facility location problem, in which <i>n</i> facilities are assigned, at minimum cost, to <i>n</i> sites. Between each pair of facilities, there is a given amount of flow, contributing a cost equal to the product of the flow and the distance between sites to which the facilities are assigned. Proving optimality of QAPs has been limited to instances having fewer than 20 facilities, largely because known lower bounds are weak. We compute lower bounds for a wide range of QAPs using a linear programming-based lower bound studied by Z. Drezner (1995). On the majority of QAPs tested, a new best known lower bound is computed. On 87% of the instances, we produced the best known lower bound. On several instances, including some having more the 20 facilities, the lower bound is tight. The linear programs, which can be large even for small QAPs, are solved with an interior point code that uses a preconditioned conjugate gradient algorithm to compute the interior point directions. Attempts to solve these instances using the CPLEX primal simplex algorithm as well as the CPLEX barrier (primal-dual interior point) method were successful only for the smallest instances.", "e:keyword": ["Facilities/equipment planning: quadratic assignment", "Programming", "Linear", "Algorithms", "Large-scale systems: interior point method", "Programming", "Quadratic: interior point method"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.792", "e:abstract": "In this paper, we develop a weighted least squares (WLS) approach to computer simulation factor screening. Like frequency domain methodology (FDM), the regression analysis is performed in the frequency domain. However, unlike FDM, where unknown nuisance parameters are eliminated by cancellation in a ratio statistic, the WLS procedure is designed to estimate these parameters (the WLS weights) and incorporate this information into the regression analysis. We propose and compare one- and two-run approaches for estimating the weights in the WLS approach.", "e:keyword": ["Simulation", "Design of experiments: computer simulation factor screening"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.807", "e:abstract": "In many decision analysis problems, we have only limited information about the relevant probability distributions. In problems like these, it is natural to ask what conclusions can be drawn on the basis of this limited information. For example, in the early stages of analysis of a complex problem, we may have only limited fractile information for the distributions in the problem; what can we say about the optimal strategy or certainty equivalents given these few fractiles? This paper describes a very general framework for analyzing these kinds of problems where, given certain “moments” of a distribution, we can compute bounds on the expected value of an arbitrary “objective” function. By suitable choice of moment and objective functions we can formulate and solve many practical decision analysis problems. We describe the general framework and theoretical results, discuss computational strategies, and provide specific results for examples in dynamic programming, decision analysis with incomplete information, Bayesian statistics, and option pricing.", "e:keyword": ["Decision analysis", "Inference: bounds given by incomplete information", "Probability: generalized Chebychev inequalities", "Programming", "Linear: applications in probability and decision analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.826", "e:abstract": "We study a discrete time, infinite-horizon, dynamic programming model for the replacement of components in a binary coherent system with <i>n</i> components. Costs are incurred when the system fails and when failed components are replaced. The objective is to minimize the expected discounted infinite-horizon cost or the long-run expected average undiscounted cost per period. An earlier paper found general conditions under which it is optimal to follow a critical component policy (CCP), i.e., a policy specified by a critical component set and the rule: Replace a component if and only if it is failed and in the critical component set. Computing an optimal CCP is a binary nonlinear programming problem in <i>n</i> variables. This paper specializes to <i>k</i>-out-of-<i>n</i> systems and develops a branch-and-bound algorithm for finding an optimal decision. Its memory storage requirement is <i>O((n+1)(n-k+1))</i>, and the number of nodes examined is under <i>O(n<sup>k</sup>)</i>. Extensive computational experiments with <i>n</i> ranging from 10 to 100 find it to be effective when <i>k</i> is small or near <i>n</i>. In our 120,000 test problems with <i>k=n</i> (parallel systems), the average computation time on a 20Mhz 386 microcomputer is 0.106 seconds.", "e:keyword": ["Dynamic programming/optimal control: Markov decision model", "Programming branch and bound", "Reliability", "Replacement/renewal: multicomponent system"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.838", "e:abstract": "A frequent goal in scheduling projects and production operations is determining the financial impact of project duration or production lead time. This paper considers net project value effects that increase proportionally to time in excess of a deadline. New bounds on the expected value of these tardiness costs are given. The bounds provide robust measurements without requiring extensive assumptions about activity distributions.", "e:keyword": ["Probability: moment problem", "Production/scheduling: tardiness bounds", "Project management: objective bounds"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.851", "e:abstract": "We consider a single-product firm in which arriving orders are processed by an information processing system before being downloaded to the production system. The design problem is to choose the appropriate processing and control technologies for the information (production) systems, the decision variables being the speed with which these systems can process orders (products). Rapid systems are assumed to be more costly to operate than slower systems, and it is assumed that products can be inventoried, but information processing cannot. Longer information processing times incur greater delay costs, but provide benefits in the form of more advanced warning of upcoming demand to the production function. This paper is a first attempt to model this tradeoff. We describe a plausible set of conditions under which the information processing time will never exceed the production lead time in an optimal design. Conditions are also described that guarantee that the greatest marginal benefit, starting from a suboptimal system design, will be to invest in time reductions in the information processing, rather than the production, system. Performance measures are suggested that provide the appropriate incentives to integrate the information processing and production functions.", "e:keyword": ["Information systems: order processing time", "Inventory/production: order processing and production lead times"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.862", "e:abstract": "This paper deals with the problem of finding an estimate of the maximal loss of optimality which can arise when terminal payoffs are uncertain and policies are restricted in some way. The original non-convex optimization problem is converted to a sequence of sub-problems involving the maximization of a bilinear function over a convex region. The paper deals solely with the theoretical issues.", "e:keyword": ["Dynamic programming/optimal control", "Markov", "Finite state: optimality of first period action for uncertain terminal payoffs"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.870", "e:abstract": "We study a system with unlimited service potential where all service requests are served in parallel. The entire system itself becomes unavailable for a random period of time at the first instance that the system becomes idle. A queue builds up while the system is unavailable, and then all waiting customers enter the system simultaneously—each to its own processor—when the system becomes available again. All customers who arrive to find the system in operation proceed directly into service. The analysis of this system entails finding the distribution of the <i>delayed</i> busy period of an <i>M/G/∞</i> queue. The steady-state distribution of the number of customers in the system is obtained for the special cases of exponential and deterministic service times. Among other applications, our results enable us to analyze and solve for the optimal <i>N</i>-policy for the systems with unlimited service potential. We also study a multiclass model of a polling system with exhaustive service.", "e:keyword": ["Probability: stochastic model applications", "Queues: busy period analysis", "Multichannel", "Optimization transient results"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.879", "e:abstract": "We consider a sequential matching problem where <i>M</i> offers arrive in a random stream and are to be sequentially assigned to <i>N</i> waiting candidates. Each candidate, as well as each offer, is characterized by a random attribute drawn from a known discrete-valued probability distribution function. An assignment of an offer to a candidate yields a (nominal) reward <i>R</i> > 0 if they match, and a smaller reward <i>r</i> ≤ <i>R</i> if they do not. Future rewards are discounted at a rate 0 ≤ α ≤ 1. We study several cases with various assumptions on the problem parameters and on the assignment regime and derive optimal policies that maximize the total (discounted) reward. The model is related to the problem of donor-recipient assignment in live organ transplants, studied in an earlier work.", "e:keyword": ["Dynamic programming/optimal control: sequential assignment", "Match processes", "Health care: live-organ transplants"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.885", "e:abstract": "This paper focuses on simple exponential approximations for tail probabilities of the steady-state waiting time in infinite-capacity multiserver queues based on small-tail asymptotics. For the <i>GI/GI/s</i> model, we develop a heavy-traffic asymptotic expansion in powers of one minus the traffic intensity for the waiting-time asymptotic decay rate. We propose a two-term approximation for the asymptotic decay rate based on the first three moments of the interarrival-time and service-time distributions. We also suggest approximating the asymptotic constant by the product of the mean and the asymptotic decay rate. We evaluate the exponential approximations based on the exact asymptotic parameters and their approximations by making comparisons with exact results obtained numerically for the <i>BMAP/GI/1</i> queue, which has a batch Markovian arrival process, and the <i>GI/GI/s</i> queue. Numerical examples show that the exponential approximations are remarkably accurate, especially for higher percentiles, such as the 90th percentile and beyond.", "e:keyword": ["Queues", "Approximations: exponential approximations for tail probabilities", "Queues", "Limit theorems: small tail asymptotics and heavy traffic asymptotic expansions"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.5.902", "e:abstract": "This note introduces a model where inventory is repeatedly wiped out due to exogenous failure processes. The difference between this model and perishable inventory models is pinpointed by two sample examples. Relevant applications are also mentioned.", "e:keyword": ["Inventory/production", "Applications: obsolescent inventory", "Technological change", "Inventory/production", "Perishable/aging items: exogenous versus endogenous aging"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.1012", "e:abstract": "We consider the important practical and theoretical problem of designing a low-cost communications network which can survive failures of certain network components. Our initial interest in this area was motivated by the need to design certain “two-connected” survivable topologies for fiber optic communication networks of interest to the regional telephone companies. In this paper, we describe some polyhedral results for network design problems with higher connectivity requirements. We also report on some preliminary computational results for a cutting plane algorithm for various real-world and random problems with high connectivity requirements, which shows promise for providing good solutions to these difficult problems.", "e:keyword": ["Forthcoming"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.1025", "e:abstract": "This paper investigates a model representing a make-to-stock production system, where intermediate and finished goods are produced and stored in advance of demand. In other respects the system operates much like a network of queues. There are several nodes and items. Each item is produced at one of the nodes using another item as input. Customer demands for the items and the unit-processing times at the nodes obey simple Markovian laws. We develop a tractable approximation scheme for estimating performance and test it through computer simulation. Numerical results show that the approximation is quite accurate. The model and the approximation express the effects of different degrees of component commonality.", "e:keyword": ["Inventory/production", "Multi-item", "Processing networks", "Queues", "Networks", "Planned inventories"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.1037", "e:abstract": "Many optimization problems that are intractable with conventional approaches will yield to stochastic approximation algorithms. This is because these algorithms can be used to optimize functions that cannot be evaluated analytically, but have to be estimated (for instance, through simulation) or measured. Thus, stochastic approximation algorithms can be used for optimization in simulation. Unfortunately, the classical stochastic approximation algorithm sometimes diverges because of unboundedness problems. We study the convergence of a variant of stochastic approximation defined over a growing sequence of compact sets. We show that this variant converges under more general conditions on the objective function than the classical algorithm, while maintaining the same asymptotic convergence rate. We also present empirical evidence that shows that this algorithm sometimes converges much faster than the classical algorithm.", "e:keyword": ["Programming", "Stochastic", "Simulation", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.1049", "e:abstract": "This paper presents an effective neighborhood structure for the traveling salesman problem. The neighbors of a tour are defined as the tours that can be produced by breaking the initial tour into two closed subtours, rejoining the subtours in a new configuration, and finally performing local optimization around all the changed edges. This process of generating a neighbor is termed <i>divide and merge</i>. Neighbor lists are used to develop variants of divide and merge that require linear and constant time per iteration, as well as an <i>O</i>(<i>Nln</i>(<i>N</i>)) tour construction algorithm based on insertion into the convex hull.", "e:keyword": ["Networks/graphs", "Traveling salesman", "Heuristics", "Tour construction", "And tour improvement"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.1058", "e:abstract": "This paper exploits the interactions between the machine scheduling and the scheduling of the material handling system in an FMS by addressing them simultaneously. The material transfer between machines is done by a number of identical automated guided vehicles (AGVs) which are not allowed to return to the load/unload station after each delivery. This operating policy introduces an additional complexity to the problem because it results in sequence-dependent travel times for the deadheading trips between successive loaded trips of the AGVs. The problem is formulated as a nonlinear mixed integer programming model. Its objective is makespan minimization. The formulation consists of constraint sets of a machine scheduling subproblem and a vehicle scheduling subproblem which interact through a set of time window constraints for the material handling trip starting times. An iterative procedure is developed where, at each iteration, a new machine schedule is generated by a heuristic procedure, the operation completion times obtained from this schedule are used to construct time windows for the trips, and a feasible solution is searched for the second subproblem, which is handled as a sliding time window problem. The procedure is numerically tested on 90 example problems.", "e:keyword": ["Production/scheduling", "Flexible manufacturing", "Simultaneous scheduling of machines and AGVs", "Transportation", "Vehicle scheduling", "AGV scheduling using sliding time windows"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.911", "e:abstract": "Based on an address given at the April 1995 INFORMS meeting in Los Angeles, this paper's purpose is to suggest the opportunities that the Pacific Rim will bring to OR/MS and INFORMS. It first looks at the two important needs in the profession: to improve the scope of its communications with the public and to expand the context in which OR/MS work is done. After describing some of the radically unfamiliar and different conditions affecting enterprises on the Pacific Rim and suggesting their important challenges, the paper describes some influences acting to inhibit the free growth of the profession and its contributions to society. It closes by suggesting how meeting the challenges of the Pacific Rim cannot only help solve the problems this new context poses, but also help prepare the profession for its larger international future.", "e:keyword": ["Professional", "Address", "OR/MS implementation"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.916", "e:abstract": "Model formulation is a critical craft skill that deserves more attention from OR practitioners and educators. As a step toward understanding how experts formulate OR models, I conducted an experiment in which twelve skilled analysts generated think-aloud protocols while working model formulation exercises 60 minutes long. The coded transcripts tracked the modelers' attention to five topics corresponding to stages in the usual OR paradigm: problem context and model structure, realization, assessment, and implementation. Analysis of the transcripts yielded these conclusions: topics other than structure attracted a significant proportion of modelers' attention; modelers frequently switched their attention among the topics; the switching was usually an alternation between structure and the other topics, especially assessment; on average, the modelers progressed through the topics in the given order; there was limited support for the notion of modeler-specific and problem-specific effects influencing the attention given to each topic; and there was support for the idea of individual modeling styles.", "e:keyword": ["Philosophy of modeling", "Model formulation", "Professional", "OR/MS education and implementation"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.933", "e:abstract": "This paper employs multiattribute value assessment and risk analysis to evaluate the benefits of four alternatives to improve electrical system reliability in British Columbia. A multiattribute value model is constructed as a cost-equivalent function, based on value judgments provided by a group of senior system planners and two senior vice presidents from BC Hydro. Using data drawn from the utility's service records and probabilities elicited from the utility's technical specialists, a risk analysis is developed to estimate the magnitude and duration of outages associated with the various alternatives. The value model and probabilities are combined to estimate the expected equivalent costs of outages over time, for the four alternatives. The influence of this analysis on BC Hydro's transmission system planning is also discussed.", "e:keyword": ["Decision analysis", "Applications", "How to improve electric reliability", "Facilities/equipment planning", "Constructing new electric transmission lines", "Utility/preference", "Evaluating reliability improvements"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.948", "e:abstract": "In applications of operations research models, decision makers must assess the sensitivity of outputs to imprecise values for some of the model's parameters. Existing analytic approaches for classic optimization models rely heavily on duality properties for assessing the impact of local parameter variations, parametric programming for examining systematic variations in model coefficients, or stochastic programming for ascertaining a robust solution. This paper accommodates extensive simultaneous variations in any of an operations research model's parameters. For constrained optimization models, the paper demonstrates practical approaches for determining relative parameter sensitivity with respect to a model's optimal objective function value, decision variables, and other analytic functions of a solution. Relative sensitivity is assessed by assigning a portion of variation in an output value to each parameter that is imprecisely specified. The computing steps encompass optimization, Monte Carlo sampling, and statistical analyses, in addition to model specification. The required computations can be achieved with commercially available off-the-shelf software available for microcomputers and other platforms. The paper uses a broad set of test models to demonstrate the merit of the approaches. The results are easily put to use by a practitioner. The paper also outlines further research developments to extend the applicability of the approaches.", "e:keyword": ["Programming", "Linear", "Sensitivity analysis", "Statistics", "Estimation", "Fitting value of optimal objective function"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.970", "e:abstract": "Financial intermediaries—banks, thrifts, and life insurance companies—have experienced low productivity over the last decade or two. Low productivity has manifested itself as a declining market share of their products relative to capital market assets. In some cases, low productivity caused a failure to meet contractual obligations embodied in their financial products. These failures resulted in customer losses, and/or taxpayer losses when failed intermediaries were guaranteed by government agencies. This productivity problem has been analyzed mostly from an economic science perspective, by R. C. Merton (Merton, R. C. 1990. The financial system and economic performance. <i>J. Fin. Serv. Res.</i> 263–300.) and Z. Bodie (Bodie, Z. 1990. Pension funds and financial innovation. <i>Finan. Mgmt.</i> (Autumn).). The focus of the economic analysis is the improvement of regulatory measures for intermediaries whose financial products are guaranteed by government agencies. In this paper we take a management science perspective by focusing on the technology of financial product management. An assessment of current technologies finds that their use can leave financial intermediaries exposed to substantial risks. An improved technology—<i>integrated product management</i> (IPM)—is suggested that enables intermediaries to increase productivity. Therefore, they can respond more effectively to market pressures from competing capital market assets and to regulatory pressures from government agencies. Technical and organizational aspects of integrated product management are described, and its application to three examples is discussed. The problem outlined here presents a major challenge to management scientists. It is an example of the service-sector applications that A. Geoffrion (Geoffrion, A. M. 1992. Forces, trends and opportunities in MS/OR. <i>Opns. Res.</i> <b>40</b> 423–445.) addressed in his 1991 Omega Rho lecture.", "e:keyword": ["Finance", "Financial product management", "Financial institutions", "Insurance companies", "Professional", "OR/MS implementation"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.983", "e:abstract": "When bidders have a common value or strongly affiliated values for an object or contract being auctioned by sealed bids, it is possible that the maker of a rational and apparently winning bid would, upon learning the competing bids, prefer losing the auction to honoring his bid. The ability to withdraw a bid, perhaps at a cost, in such circumstances provides a form of “winner's curse insurance.” Bidding with such insurance is analyzed, obtaining the general condition for rational bid withdrawal and sufficient conditions for the existence of an equilibrium with more aggressive bidding. Next, we define a “compensation penalties” bid withdrawal penalty scheme and show that with it the bid-taker is better off on the average than if bid withdrawal is impossible. Finally, we find equilibrium bidding and withdrawal strategies in a multiplicative model as a function of the magnitude of the bid withdrawal penalty and of the bid-taker's likelihood, after a withdrawal, of honoring the second-best bid. This model has cases in which allowing withdrawal at a cost is in the bid-taker's interest and ones in which it is not.", "e:keyword": ["Games/group decisions", "Bidding/auctions", "Withdrawable bids"]}, {"@id": "http://dx.doi.org/10.1287/opre.43.6.995", "e:abstract": "A previous paper (1992) by the same authors studied the phenomenon of transient congestion in landings at an airport and developed a recursive approach for computing moments of queue lengths and waiting times. This paper extends our approach to a network, developing two approximations based on the prior method. Both approaches work by using delay information estimated at one location to update arrival schedules at other points in the network. We present computational results for a simple 2-node network, comparing the performance of the approximations with an alternative simulation approach. The methods give similar results in light to moderate traffic but show a growing disparity under heavier traffic, where the algorithms underestimate the true magnitude of delay propagation relative to simulation. Finally, to illustrate the usefulness of the modeling, we show how the results may be used to explore the issue of interaction between airports. Although this particular application motivated development of the model, the method is, in principle, applicable to other multiclass queueing networks where service capacity at a station may be modeled as a Markov or semi-Markov process. The model represents a new approach for analyzing transient congestion phenomena in such networks.", "e:keyword": ["Probability", "Stochastic model applications", "Semi-Markov and Markov models of airport capacity", "Queues", "Transient results", "Approximations to compute performance measures in networks", "Transportation", "Network models", "Study of congestion's effects in airline hub-and-spoke networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.100", "e:abstract": "In this paper, we develop a stochastic dynamic programming formulation for the valuation of global manufacturing strategy options with switching costs. Overall, we adopt a hierarchical approach. First, exchange rates are modeled as stochastic diffusion processes that exhibit intercountry correlation. Second, the firm's global manufacturing strategy determines options for alternative product designs as well as supply chain network designs. Product options introduce international supply flexibility. Supply chain network options determine the firm's manufacturing flexibility through production capacity and supply chain network linkages. Third, switching costs determine the cost of operational hedging, i.e., the costs associated with reducing downside risks. Overall, the firm maximizes its expected, discounted, global, after-tax value through the exercise of product and supply chain network options and/or through exploitation of operational flexibility contingent on exchange rate realizations. In this environment, the firm must trade off fixed operating costs, switching costs, and the economic benefits derived from exploiting differentials in factor costs and corporate tax rates. A multinomial approximation of correlated exchange rate processes is proposed that leads to a consistent and tractable lattice model for this compound option valuation problem. We then demonstrate how the global manufacturing strategy planning model framework can be utilized to analyze financial and operational hedging strategies.", "e:keyword": ["Manufacturing: operational and financial global strategy", "Networks/graphs: flexibility", "And optimal value of network", "Production/scheduling: supply chain production", "Distribution network"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.114", "e:abstract": "This study examines the time series behavior of quality costs reported by 49 manufacturing units of 21 companies. Analyses using both pooled annual data and plant-specific quarterly data indicate that the majority of units in the sample achieved ongoing reductions in nonconformance costs while maintaining or reducing reported prevention and appraisal costs. These findings are consistent with recent quality-based learning and continuous improvement models which suggest that, once an effective quality program is established, companies can reduce nonconformance costs over time with little or no subsequent increase in conformance expenditures.", "e:keyword": ["Cost analysis: quality-cost tradeoffs", "Reliability", "Quality control: quality control economics"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.131", "e:abstract": "In this paper, we provide an integrated framework for forecasting and inventory management of short life-cycle products. The literature on forecasting and inventory management does not adequately address issues relating to short life-cycle products. We first propose a growth model that can be used to obtain accurate monthly forecasts for the entire life cycle of the product. The model avoids limiting data requirements of traditional methods. Instead, it extracts relevant information from past product histories and utilizes the information on total life-cycle sales and the peak sales timing. Using disguised demand data from a personal computer (PC) manufacturer, we validate the model. Next, we model the inventory management problem for the short life-cycle environment. The uncertainty in demand is modeled through the uncertainty in the realized values of the parameters of the forecasting model. The high cost of terminal inventory, shortages, and rapidly changing procurement costs are all included in the model. Extensions to the basic model are also developed. Using optimal control theory, we derive a solution that provides valuable information on procurement cutoff time and terminal service levels. A detailed example explains the characteristics of the policy and its relevance in decision making. Many of the issues covered in the models were brought to our attention while implementing a forecasting model at a PC manufacturer. The benchmark monthly forecasts and the associated inventory levels provide information that can be very helpful in planning and controlling marketing, sales, and production.", "e:keyword": ["Dynamic programming/optimal control: applications", "Forecasting", "Application: product life-cycle model", "Inventory/production", "Application: inventory management of short life-cycle products"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.151", "e:abstract": "One of the major challenges to operational managers is product proliferation. Product proliferation makes it difficult to forecast demands accurately, and consequently, leads to high inventory investment and poor customer service. Such proliferation is often a result of the global nature of the market place. Different markets may have different requirements for the product, due to differences in taste, language, geographical environment, or government regulations. Another reason for product proliferation is the expansion of the customer base. Different product versions are often developed for different market segments (e.g., education, personal, business, or government users may have different needs of a product). To gain control of inventory and service, significant benefits can be obtained by properly exploring the opportunities in the design of the product or the process by which the product is made. Logistic issues like inventory and service are thus important dimensions that design engineers should consider, in addition to measures like functionality, performance, and manufacturability. This paper describes how some simple inventory models can be used to support the logistic dimensions of product/process design. Actual examples are used for illustration.", "e:keyword": ["Inventory/production", "Applications: product/process design", "Multi-echelon", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.160", "e:abstract": "This paper provides formal modeling for and analytical aid to the startup and capacity problem facing Receiving Plant #1 of National Cranberry Cooperative. The analysis is based on a finite-horizon, single-facility queueing system with general variability. Using marginal analysis, we derive simple formulas that the optimal starting time and processing capacity should satisfy. These formulas lead to the comparative statics results showing that the starting time should be optimally delayed or the processing capacity optimally decreased when the operating cost increases, when the waiting cost decreases, when the cumulative arrival process decreases, or when the capacity of storage increases. We then demonstrate that three important performance measures, the expected waiting, tardiness, and operating costs, are jointly convex with respect to two control variables, starting time and processing capacity, in a generalized version of the model. Therefore, in the applications that involve minimizing these measures, the simple formulas derived from first-order conditions are necessary and sufficient for optimality. The applications of the model in service and manufacturing operations are numerous.", "e:keyword": ["Queues: finite horizon", "Single facility", "General arrival processes", "Queues", "Optimization: optimal starting time and processing capacity", "Queues", "Simulation: waiting and operating costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.173", "e:abstract": "This paper addresses a problem of simultaneous quality and quantity control motivated by semiconductor manufacturing. After wafers are fabricated, the individual chips on the wafers are probed, or electrically tested, and, in some cases, the probing facility is the bottleneck for the entire IC manufacturing process. Under this assumption, we consider the problem of choosing the optimal start rate of lots of wafers into the fabrication facility and the optimal chip screening policy in front of the probing facility to maximize the expected profit, which is the revenue from good chips minus the variable fabrication and probing costs. These decisions are subject to capacity constraints at both the wafer fabrication and probing facilities. Over 300 wafers from two industrial facilities are analyzed in this paper, and a Markov random field model is employed to capture the spatial clustering of bad chips. Chip screening strategies are proposed that exploit the various types of yield nonuniformities that are detected in the data, such as radial effects, spatial clustering of bad chips, and yield variation by chip location. The numerical results suggest that screening at the chip level can be highly profitable in a semiconductor manufacturing process constrained by its probe capacity.", "e:keyword": ["Inventory/production: probabilistic yield models", "Probability: Markov random fields"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.196", "e:abstract": "This paper addresses the same quality management problem as Longtin, Wein and Welsch (Longtin, M., L. M. Wein, R. E. Welsch. 1996. Sequential screening in semiconductor manufacturing, I: Exploiting spatial dependence. <i>Opns. Res.</i> <b>44</b> 173–195.), except that here screening is performed at the wafer level, rather than at the chip level. An empirical Bayes approach is employed: The number of bad chips on a wafer is assumed to be a gamma random variable, where the scale parameter is unknown and varies from lot to lot according to another gamma distribution. We fit the yield model to industrial data and test the optimal policy on these data. The numerical results suggest that screening at the chip level, as in Longtin, Wein and Welsch, is significantly more profitable than screening at the wafer level.", "e:keyword": ["Inventory/production: probabilistic yield models", "Probability: Markov random fields"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.2", "e:abstract": "In the summer of 1992, we announced a special issue on “New Directions for Operations Management Research.” The objective of the special issue was to broaden the range of research articles published in <i>Operations Research</i> in the operations management field. We were particularly interested in identifying new problem contexts and new research paradigms for this rapidly evolving field. Traditional, model-based research associated with operations management is being augmented by a wide range of interdisciplinary and empirical approaches. For example, economic models (which are typically more parsimonious than engineering models) have been applied to areas such as global manufacturing strategy and new product development. Similarly, survey research and benchmarking studies, focusing on operations performance metrics concerned with quality, time, and flexibility, have emerged in studies with significant managerial impact.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.206", "e:abstract": "In this paper we seek to reconcile two competing frameworks for inventory management, namely the classical “optimization” framework and JIT. While the first is predicated on the premise that the optimal level of inventory is obtained by balancing the costs and benefits of inventory, the latter claims that inventory is a form of waste and thus should be eliminated completely (but gradually). We show that some of the operational characteristics of JIT, as opposed to its rhetoric, can be obtained within the optimization framework provided that the relevant factors are taken into account. In particular, we show that in an environment which is subject to “learning from mistakes,” it is better to carry inventory levels which are lower than what is “operationally optimal.” We also study the (complex) relation between variability reduction, inventory levels, and overall cost. In general, reducing the variability of a system (in the sense of first or second-order stochastic dominance) will not necessarily result in a lower level of inventory, although optimal cost will be reduced. We study conditions on variability improvement that guarantee that variability reductions will always result in reduced levels of optimal inventory.", "e:keyword": ["Inventory/production: JIT systems", "Continue improvement"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.21", "e:abstract": "In “bucket brigade” manufacturing, such as recently introduced to the apparel industry, a production line has <i>n</i> workers moving among <i>m</i> stations, where each worker independently follows a simple rule that determines what to do next. Our analysis suggests and experiments confirm that if the workers are sequenced from slowest to fastest then, independently of the stations at which they begin, a stable partition of work will spontaneously emerge. Furthermore, the production rate will converge to a value that, for typical production lines, is the maximum possible among all ways of organizing the workers and stations.", "e:keyword": ["Industries", "Textile/apparel: multifunctional workers", "Mathematics", "Fixed points: convergence of nonlinear dynamical systems to a fixed point", "Production/scheduling", "Flexible manufacturing/line balancing: the allocation of work to each worker is self-balancing"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.215", "e:abstract": "How should inventory management respond when there is a possibility of imminent obsolescence (or, more generally, deteriorating demand)? We use an inventory-control model to address this question. The model incorporates a Markovian submodel to describe the uncertain events leading to obsolescence. These events and their uncertainties come in a variety of patterns. We devote considerable attention to specifying the submodel, and we compare a few alternatives numerically. Also, we compare optimal policies to simpler alternatives, and we investigate the response of the model to parameter changes. Generally, we find that obsolescence does (or should) have a substantial impact in the way inventories are managed. The nature of these effects, moreover, is fairly intricate. It appears that obsolescence <i>cannot</i> be captured in a simpler model through parameter adjustments. These conclusions presume that we cannot dispose of excess stock, either directly or through price promotions; we show also that the disposal option can make the problems of obsolescence more manageable.", "e:keyword": ["Inventory/production: stocking to account for obsolescence"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.223", "e:abstract": "This paper addresses the empirical verification of hypotheses that relate to the strategic use and implementation of manufacturing flexibility. We begin with a literature review and framework for analyzing different types of flexibility in manufacturing. Next, we examine some of the propositions in the framework using data from 31 printed circuit-board plants in Europe, Japan, and the United States. Based on our analysis and findings, we then suggest several new strategic insights related to the management of flexibility and some potentially fruitful areas for further theoretical and empirical research. Our findings include: more automation is associated empirically with less flexibility, as found in other studies; nontechnology factors, such as high involvement of workers in problem-solving activities, close relationships with suppliers, and flexible wage schemes, are associated with greater mix, volume, and new-product flexibility; component reusability is significantly correlated with mix and new-product flexibility; achieving high-mix or new-product flexibility does not seem to involve a cost or quality penalty; mix and new-product flexibility are mutually reinforcing and tend to be supported by similar factors; and mix flexibility may reduce volume fluctuations, which could theoretically reduce the need for volume flexibility.", "e:keyword": ["Manufacturing: flexible"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.35", "e:abstract": "Based on our interactions with managers at two large hotels, we present a realistic model of the hotel reservation problem. Unlike traditional models, ours does not assume that all customers arrive simultaneously on the targeted booking date. We explain why this assumption may not be appropriate for the hotel industry and develop a model of reservation booking which explicitly includes the room allocation decisions which are made on the targeted booking date. Based on observations of how the problem is solved in practice as well as the insights gained from this analysis, we develop simple heuristic procedures for accepting reservations. Computational results demonstrate that these heuristics perform well relative to an upper bound that is based on perfect information about reservations requests and customer arrivals.", "e:keyword": ["Industries", "Hotel/motel: reservations planning", "Inventory/production", "Perishable/aging items: booking limits for hotel reservations"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.50", "e:abstract": "The aim of this paper is to develop a general purpose analytical method for performance evaluation of multistage kanban controlled production systems. We consider a single-part type production system decomposed into stages in series. Each stage consists of a manufacturing cell and an output buffer. With each stage is associated a given number of kanbans. The kanban controlled production system is modeled as a queueing network with synchronization mechanisms. The basic principle of the proposed approximation method is to decompose the original kanban system into a set of subsystems, each subsystem being associated with a particular stage. Each subsystem is analyzed in isolation using a product-form approximation technique. An iterative procedure is then used to determine the unknown parameters of each subsystem. Numerical results show that the method is fairly accurate.", "e:keyword": ["Production/scheduling: kanban controlled production systems", "Queues: decomposition and product-form approximations"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.65", "e:abstract": "In this paper, a new application of the methodology of set partitioning formulation augmented with heuristic column generation is presented. An efficient method for the partitioning of large-scale electronic circuits is developed based on this methodology. Circuit partitioning constitutes a major step of the physical design phase of electronic circuits, the fundamental components of electronic products. The major advantage of the scheme presented here is to provide a framework for an effective integration of most existing circuit partitioning methods. Another attractive feature of the current approach is the incorporation of interactive optimization: The circuit designer controls the operation of the procedure and enhances its performance by suggesting and/or requiring specific partitions. Following the development of the model, the solution approach is presented and computational results are reported for several benchmark circuits.", "e:keyword": ["Industries", "Electronic: electronic circuits product design", "Programming", "Integer", "Algorithms: set partitioning with column generation", "Programming", "Integer", "Applications: application to electronic circuit partitioning"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.7", "e:abstract": "Metal-forming operations such as extrusion, drawing, and rolling offer many opportunities for operations improvement through better process understanding and improved planning practices. This paper addresses medium-term planning issues in aluminum tube manufacturing operations. First, we identify certain distinctive characteristics—the inherent process flexibility, close inter-dependence between successive stages, and economies of scale—of metal-forming operations, and identify performance trade-offs across stages. To exploit the strategic potential of process planning, it must be closely coupled with process engineering efforts, and must simultaneously consider the facility's entire product mix. In contrast, current process engineering efforts are mainly reactive, focusing on fixing problems at individual operations and with less emphasis on the interactions between successive stages. Similarly, planning activities are incremental, considering only individual products or orders one at a time rather than the entire range of product sizes to be manufactured. By working together, planners and engineers can develop effective process plans that exploit process capability, and adopt proactive process improvement strategies that focus on critical constraints. We describe a medium-term planning model to select standard extrusion sizes, illustrate the close linkages between planning and engineering activities, and identify research opportunities spanning management science, materials science, and mechanical engineering.", "e:keyword": ["Engineering: process engineering", "Planning linkages", "Industries", "Mining/metals: process planning for metal forming operations", "Production/scheduling", "Planning: medium- and short-term planning models"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.77", "e:abstract": "Does knowledge acquired through learning by doing on one shift transfer to a second shift when it is introduced at a manufacturing plant? The answer to this question has important theoretical implications about where knowledge is embedded in organizations and about sources of productivity growth. The answer also has important practical implications for managers planning to introduce additional facilities. This paper analyzes the amount of transfer across shifts at a manufacturing facility. Specifically, we analyze the amount of knowledge that is carried forward when the plant makes the transition from one to two shifts. We also investigate whether the rate of knowledge acquisition differs by shift, and we estimate the amount of transfer that occurs across shifts once both are in operation. In addition, we study transfer over time by analyzing whether knowledge acquired through learning by doing is cumulative and persists through time or whether it depreciates.", "e:keyword": ["Manufacturing", "Performance/productivity: learning by doing", "Organizational studies", "Productivity: organizational learning", "Technology: technology transfer"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.1.87", "e:abstract": "Traditionally, fashion products have incurred high losses due to stockouts and inventory obsolence because long lead times coupled with a concentrated selling season force all or at least most production to be committed before demand information is available. Under a Quick Response system, lead times are shortened sufficiently to allow a greater portion of production to be scheduled in response to initial demand. We model and analyze the decisions required under Quick Response and give a method for estimating the demand probability distributions needed in our model. We applied these procedures with a major fashion skiwear firm and found that cost relative to the current informal response system was reduced by enough to increase profits by 60%. Relative to the cost that would have been incurred if no response were used, optimized response reduces cost by enough to roughly quadruple profits.", "e:keyword": ["Cost analysis: accurate response", "Forecasting: applications", "Industries: textiles/apparel"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.257", "e:abstract": "In this paper, which the late Charles J. Hitch gave as a Phi Beta Kappa Lecture in 1978, he surveys his experiences as Comptroller of the U.S. Defense Department and President of the University of California system. He found that, in spite of the widely differing contexts, the two settings exhibited similar structural problems. His planning and strategic decision-making experiences with them support some principles common to both, and contain interesting lessons for the operations research profession.", "e:keyword": ["Education: planning", "Military: cost effectiveness", "Professional: address"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.265", "e:abstract": "The central Chicago Public School warehouse was responsible for the distribution of supplies to 600 schools, including over $10 million annually of engineering and educational supplies. The system was fraught with problems—deliveries were not made according to schedule, schools were hoarding inventories, and some schools were paying a premium for reliable service from third party suppliers. This paper reports how we improved this logistics system. We built a mathematical model of the system, validated our model using historical data, and used the model to evaluate the impact of potential changes to the system. Our recommended changes were implemented throughout the system. We report the impact on system performance. The redesigned system shows a dramatic reduction in lead times, a reduction in capacity requirements, and an overall reduction in system costs.", "e:keyword": ["Inventory/production: policies review/lead times", "Programming: integer applications", "Transportation: vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.274", "e:abstract": "Bayesian acceptance sampling was used to monitor illegal drug use in a population of probationers. The study utilizes an economic model of drug testing based on single-sample, single-attribute acceptance sampling. This approach reduces from 100% the amount of testing which must be done to monitor the use of illegal drugs in the population and provides a decision rule, vis-à-vis a sampling plan, that specifies under what sampling outcome the entire population must be tested. The objective is to minimize the expected total cost of a drug testing program while ensuring that the proportion of users in the population is not increasing over time. A field study of the acceptance sampling approach was conducted using probationers assigned to Intensive Drug Supervision Programs in six Illinois counties. The degree to which drug testing results were reported to probation officers was controlled during the experiment. Counties were assigned to receive: no feedback of drug-test results; random proportion of feedback using Bayesian acceptance sampling plans; and 100% feedback—the <i>status quo</i> situation. Results show that those counties using acceptance sampling could have reduced the number of drug tests performed without increasing drug use. In those counties with no feedback, there was an upward trend over time in the proportion testing positive for drug use. Acceptance sampling-based drug testing programs are now being implemented by Illinois probation offices.", "e:keyword": ["Crime: drug testing", "Quality control: application of Bayesian sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.286", "e:abstract": "In recent years new insights and algorithms have been obtained for the classical, deterministic vehicle routing problem as well as for natural stochastic and dynamic variations of it. These new developments are based on theoretical analysis, combine probabilistic and combinatorial modeling, and lead to new algorithms that produce near-optimal solutions, and a deeper understanding of uncertainty issues in vehicle routing. In this paper, we survey these new developments with an emphasis on the insights gained and on the algorithms proposed.", "e:keyword": ["Analysis of algorithms: worst case", "Probabilistic", "Transportation: vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.305", "e:abstract": "Suppose that an aircraft carrier is in transit to an assigned position within strike range of a designated target, and is required to be there at a specified time. The carrier may use aircraft assets for defense against threats that may be encountered en route, but doing so will encumber the carrier's progress toward the required objective. We present a highly detailed integer programming model for scheduling aircraft launches and recoveries, to achieve an optimal balance between the conflicting needs of self-protection and on-time arrival.", "e:keyword": ["Military", "Logistics: aircraft carrier flight operations", "Programming", "Integer: applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.313", "e:abstract": "Methods for determining the form of utilities are needed for the implementation of utility theory in specific decisions. An important step forward was achieved when utility theorists characterized useful parametric families of utilities and simplifying decompositions of multiattribute utilities. The standard development of these results is based on expected utility theory which is now known to be descriptively invalid. The empirical violations of expected utility impair the credibility of utility assessments. This paper shows, however, that parametric and multiattribute utility results are robust against the major violations of expected utility. They retain their validity under nonexpected utility theories that have been developed to account for actual choice behavior. To be precise, characterizations of parametric and multiattribute representations are extended to rank-dependent utility, state-dependent utility, Choquet-expected utility, and prospect theory.", "e:keyword": ["Decision analysis: extending utility analysis to nonexpected utility"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.327", "e:abstract": "We develop strategies for integrated use of certain well-known variance reduction techniques to estimate a mean response in a finite-horizon simulation experiment. The building blocks for these integrated variance reduction strategies are the techniques of conditional expectation, correlation induction (including antithetic variates and Latin hypercube sampling), and control variates; all pairings of these techniques are examined. For each integrated strategy, we establish sufficient conditions under which that strategy will yield a smaller response variance than its constituent variance reduction techniques will yield individually. We also provide asymptotic variance comparisons between many of the methods discussed, with emphasis on integrated strategies that incorporate Latin hypercube sampling. An experimental performance evaluation reveals that in the simulation of stochastic activity networks, substantial variance reductions can be achieved with these integrated strategies. Both the theoretical and experimental results indicate that superior performance is obtained via joint application of the techniques of conditional expectation and Latin hypercube sampling.", "e:keyword": ["Simulation", "Design of experiments: antithetic variates", "Latin hypercube sampling", "Simulation", "Efficiency: conditioning", "Control variates", "Correlation induction", "Simulation", "Statistical analysis: combined Monte Carlo methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.347", "e:abstract": "Because of its specificity, it is usually difficult to reuse computer code developed for a given combinatorial problem to deal with another one. We use the Object Oriented Programming methodology to derive general purpose software including four different neighborhood search techniques (descent method, tabu search, exchange procedure, simulated annealing) to deal with any assignment-type problem with a bare minimum of coding effort. The structure of the software even allows a more advanced user to play around with several parameters of these techniques and to modify the techniques to create specific variants.", "e:keyword": ["Computers/computer science", "Software: object-oriented methodology", "Programming", "Integer", "Heuristic: neighborhood search techniques", "Assignment-type problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.360", "e:abstract": "This paper gives the exact optimal solution to the problem of designing inspection schedules with fallible and time-consuming test procedures. The optimality criterion is designed to achieve a balance between the cost of inspections and the cost of undetected failure. The solution is derived in continuous time, with arbitrary failure distribution, and is based on infinite-horizon dynamic programming with time-dependent utilities, and with an additional optimization with respect to initial conditions. Results include uniqueness and monotonicity properties of the optimal solution, as well as computational algorithms.", "e:keyword": ["Decision analysis: applications", "Sequential", "Programming: infinite dimensional", "Reliability: failure models", "Inspections"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.368", "e:abstract": "We present a simple, fast, and robust algorithm for numerically computing the first <i>N</i> moments (arbitrary <i>N</i>) of a nonnegative probability distribution from its Laplace-Stieltjes transform (continuous/mixed case) or <i>z</i>-transform (discrete case). The algorithm is based on numerically inverting an adaptively modified moment generating function. It only requires computation of the transform at several complex values of its argument. We also show that the high-order moments may be used in detecting the presence of exponential or geometric tails in distributions, and in case they are present the two parameters characterizing such a tail may be accurately computed. Several numerical examples of interest in the queueing literature illustrate the use of the algorithm. They include commonly used distributions as well as the waiting time, queue length, and busy period in queues with Poisson or more general Markovian arrival processes. Priority queues are also considered.", "e:keyword": ["Probability", "Stochastic model applications: moment computation from transforms", "Queues", "Algorithms: moments and tail probability computations"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.382", "e:abstract": "This paper studies a serial production line that is uncertain. We introduce a linear model of the uncertainty that can exist in the demand for the product and in each stage's processing time, yield, rework probability, and reliability. We construct a linear discrete-time rule for controlling production, and we show that repeated application of this rule leads the system to steady-state conditions, which include closed-form formulas for the mean and variance of each buffer's stock and the mean and variance of the workload in each stage. We optimize these operating characteristics by a convex nonlinear program. For the case of identical stages with no scrap, we show that this optimal solution tends to place larger buffer stocks toward the end of the line. We adapt these discrete-time results to the short-period surrogate of the analogous continuous-time control problem.", "e:keyword": ["Production/scheduling: linear decision rules", "Stochastic control", "Optimizing operating characteristics by convex nonlinear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.393", "e:abstract": "In this paper we show how the theory of variational inequalities can be applied to the formulation, qualitative analysis, and computation of a new competitive spatial market model in the presence of goal targets. The model allows the decision maker to impose supply goals at production locations, demand goals at consumption locations, and transportation goals between supply and demand locations, along with associated penalties for failure to comply. This work may be viewed as a contribution to the growing literature on the development of mathematical methodologies for policy modeling.", "e:keyword": ["Economics: policy modeling computation of equilibria", "Programming: generalized spatial market model", "Programming: variational inequality formulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.2.407", "e:abstract": "Given a set of items, a set of scenarios, and a knapsack of fixed capacity, a nonnegative weight is associated with each item; and a value is associated with each item under each scenario. The max-min Knapsack (<b>MNK</b>) problem is defined as filling the knapsack with a selected set of items so that the minimum total value gained under all scenarios is maximized. The <b>MNK</b> problem is a generalization of the conventional knapsack problem to situations with multiple scenarios. This extension significantly enlarges its scope of applications, especially in the application of recent robust optimization developments. In this paper, the <b>MNK</b> problem is shown to be strongly NP-hard for an unbounded number of scenarios and pseudopolynomially solvable for a bounded number of scenarios. Effective lower and upper bounds are generated by surrogate relaxation. The ratio of these two bounds is shown to be bounded by a constant for situations where the data range is limited to be within a fixed percentage from its mean. This result leads to an approximation algorithm for <b>MNK</b> in the special case. A branch-and-bound algorithm has been implemented to efficiently solve the <b>MNK</b> problem to optimality. Extensive computational results are presented.", "e:keyword": ["Analysis of algorithms: strong/weak NP-hardness of a max-min problem", "Programming", "Integer: surrogate relaxation upper/lower bounds", "Programming", "Integer: heuristics and worst-case performance"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.425", "e:abstract": "This study investigated the feasibility and impacts of various U.S. and USSR time-phased strategic force structure reduction alternatives (commonly referred to as <i>drawdowns</i>) under the Strategic Arms Reduction Treaty (START). The study resulted from the Soviet Union's request for a U.S. position on the proposed Soviet drawdown limits. Treaty drawdown limits are time-phased numerical ceilings specified in the treaty, e.g., total weapons must be less than or equal to 8000 by January 1, 1996 and 6000 by January 1, 1998. Various modernized START force structures were evaluated under four drawdown limit alternatives. Two linear programming models (U.S. and USSR) were developed to rapidly assess each drawdown limit alternative. The models determined drawdown feasibility and identified the systems to dismantle each year to maximize force capability. For the U.S., the preferred drawdown limit alternatives were independent of the force structures considered, primarily because constraints on U.S. destruction rates drove the drawdown. For the USSR, significant differences occurred between each drawdown limit alternative, especially concerning multiple warhead systems like the SS-18. The results of this study were used to determine the U.S. START negotiation positions and assess the final START agreement.", "e:keyword": ["Arms control", "Linear programming analysis of treaty limit provisions", "Linear programming", "Nuclear arms control agreement"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.435", "e:abstract": "The allocation of servers and the allocation of work are two important decision variables in designing production line systems. Previous studies of each variable in isolation have found that throughput is maximized by using an allocation that gives preferential treatment to interior stations (especially center stations) over the two end stations. In this paper we study the simultaneous optimization of server and work allocations and obtain some surprising results of a different nature. One key finding is the <i>L-phenomenon</i>, whereby the throughput is maximized by assigning all extra servers over one per station to just one of the end stations and then adjusting the work allocation so that this station has by far the greatest amount of work per server. Another key finding is the <i>multiple-server phenomenon</i>, whereby extra servers add far more throughput per server than the initial one-per-station servers. Both findings have important implications for the design of some production line systems in ways that will greatly improve their efficiency. Similar conclusions are drawn when lower and/or upper bounds are imposed on the number of servers per station and the number of stations is included as a decision variable.", "e:keyword": ["Manufacturing", "Performance/productivity", "Optimal server and work allocation", "Queues", "Tandem", "Optimal server and work allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.444", "e:abstract": "This paper discusses the use of multivariate distributions that are functions of their marginals for aggregating information from various sources. The function that links the marginals is called a <i>copula</i>. The information to be aggregated can be point estimates of an unknown quantity θ or, with suitable modeling assumptions, probability distributions for θ. This approach allows the Bayesian decision maker performing the aggregation to separate two difficult aspects of the model-construction procedure. Qualities of the individual sources, such as bias and precision, are incorporated into the marginal distributions. Dependence among sources is encoded into the copula, which serves as a dependence function and joins the marginal distributions into a single multivariate distribution. The procedure is designed to be suitable for situations in which the decision maker must use subjective judgments as a basis for constructing the aggregation model.We review properties of copulas pertinent to the information-aggregation problem. A subjectively assessable measure of dependence is developed that allows the decision maker to choose from a one-parameter family of copulas a specific member that is appropriate for the level of dependence among the information sources. The discussion then focuses on the class of Archimedean copulas and Frank's family of copulas in particular, showing the specific relationship between the family and our measure of dependence. A realistic example demonstrates the approach.", "e:keyword": ["Decision analysis", "Inference", "Aggregating expert information", "Forecastings", "Combining forecasts", "Probability", "Distributions", "Copulas"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.458", "e:abstract": "A controller observes a production system periodically, over time. If the system is in the GOOD state during one period, there is a constant probability that it will deteriorate and be in the BAD state during the next period (and remains there). The true state of the system is unobservable and can only be inferred from observations (quality of output). Two actions are available: CONTINUE or REPLACE (for a fixed cost). The objective is to maximize the expected discounted value of the total future income. For both the finite- and infinite-horizon problems, the optimal policy is of a CONTROL LIMIT (CLT) type: continue if the good state probability exceeds the CLT, and replace otherwise. The computation of the CLT involves a functional equation. An analytical solution for this equation is as yet unknown.For uniformly distributed observations we obtain the infinite-horizon CLT <i>analytically</i>. We also show that the finite horizon CLTs, as a function of the time remaining, are not necessarily monotone, which is counterintuitive.", "e:keyword": ["Dynamic programming/optimal control", "Markov", "Finite state", "Inventory/production", "Maintenance/replacement"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.464", "e:abstract": "This paper deals with a serial production line where each of the stages can be either a processing or an assembly stage. A processing stage processes outputs from an upstream stage and transforms them into inputs for the downstream stage. Each processing stage is subject to yield losses. At an assembly stage, a batch of identical input components is processed and the nondefective components are then combined with the output from the upstream stage. The processing operation for the input component is imperfect. For such a system, we characterize the form of the optimal policy for input quantity at each stage in the production line, when stochastically proportional yield is assumed at each stage, i.e., the distribution of the yield fraction is assumed to be independent of the lot size. The optimal policy exhibits some form of a critical number policy. For production lines consisting of assembly stages only, the critical numbers can be computed easily. When yields are of the “exponential” type, the critical number policy is still optimal, although there are interesting differences in the policies for the different types of yield characterizations.", "e:keyword": ["Inventory/production", "Multistage", "Issuing policies", "Yield uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.469", "e:abstract": "This paper considers a version of the stochastic vehicle routing problem where customers are present at locations with some probabilities and have random demands. A tabu search heuristic is developed for this problem. Comparisons with known optimal solutions on problems whose sizes vary from 6 to 46 customers indicate that the heuristic produces an optimal solution in 89.45% of cases, with an average deviation of 0.38% from optimality.", "e:keyword": ["Vehicle routing", "Tabu search algorithm", "Stochastic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.478", "e:abstract": "We study a class of models, known as overlay optimization problems, composed of “base” and “overlay” subproblems, linked by the requirement that the overlay solution be contained in the base solution. In some telecommunication settings, a feasible base solution is a spanning tree, and the overlay solution is an embedded Steiner tree or path. For the general overlay optimization problem, we describe a composite heuristic solution procedure that selects the better of two feasible solutions obtained by independently solving the base and the overlay subproblems, and establish worst-case performance guarantees (as a function of the worst-case bounds for the subproblems) for the composite heuristic as well as an LP relaxation of the model. For certain special cases, both the heuristic and the LP relaxation have a worst-case bound of 4/3. We extend this analysis to multiple overlays on the same base solution, producing the first known worst-case bounds (approximately proportional to the square root of the number of commodities) for the uncapacitated multicommodity network design problem. In a companion paper, we develop heuristic performance guarantees for various new multi-tier, survivable network design models that incorporate both multiple facility types and differential node connectivity levels.", "e:keyword": ["Communications", "Heuristics—network design with multiple grade facilities", "Programming", "Integer", "Heuristic", "Worst case analysis of composite heuristics", "Networks/graphs", "Steiner trees and paths on trees"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.497", "e:abstract": "Burn-in procedure is used to improve the quality of products. In field operation only those components which survive the burn-in process will be used. Various additive cost functions are considered in this paper. One part of the cost function is the expense incurred until the first component surviving burn-in is obtained. The other part of cost function is either (i) the gain proportional to the mean life in field operation or (ii) the expenditure due to replacement at failure during field operation. We assume that the component before undergoing the burn-in procedure has a bathtub-shaped failure rate function with change points <i>t</i><sub>1</sub> and <i>t</i><sub>2</sub>. It is shown that the optimal burn-in time <i>b</i>* minimizing the cost function is always before <i>t</i><sub>1</sub>. It is also shown that a large initial failure rate justifies burn-in, i.e., <i>b</i>* > 0.", "e:keyword": ["Cost analysis", "Cost incurred in born-in", "Reliability", "Mean residual life", "Renewal reward process"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.501", "e:abstract": "In the Vehicle Routing Problem with Time Windows, a set of customers are served by a fleet of vehicles of limited capacity, initially located at a central depot. Each customer provides a period of time in which they require service, which may consist of repair work or loading/unloading the vehicle. The objective is to find tours for the vehicles, such that each customer is served in its time window, the total load on any vehicle is no more than the vehicle capacity, and the total distance traveled is as small as possible. In this paper, we present a characterization of the asymptotic optimal solution value for general distributions of service times, time windows, customer loads and locations. This characterization leads to the development of a new algorithm based on formulating the problem as a stylized location problem. Computational results show that the algorithm is very effective on a set of standard test problems.", "e:keyword": ["Transportation", "Vehicle routing", "Probability", "Stochastic models and applications", "Analysis of algorithms", "Suboptima algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.3.510", "e:abstract": "An important class of machine scheduling problems is characterized by a no-wait or blocking production environment, where there is no intermediate buffer between machines. In a no-wait environment, a job must be processed from start to completion, without any interruption either on or between machines. Blocking occurs when a job, having completed processing on a machine, remains on the machine until a downstream machine becomes available for processing. A no-wait or blocking production environment typically arises from characteristics of the processing technology itself, or from the absence of storage capacity between operations of a job. In this review paper, we describe several well-documented applications of no-wait and blocking scheduling models and illustrate some ways in which the increasing use of modern manufacturing methods gives rise to other applications. We review the computational complexity of a wide variety of no-wait and blocking scheduling problems and describe several problems which remain open as to complexity. We study several deterministic flowshop, jobshop, and openshop problems and describe efficient and enumerative algorithms, as well as heuristics and results about their performance. The literature on stochastic no-wait and blocking scheduling problems is also reviewed. Finally, we provide some suggestions for future research directions.", "e:keyword": ["Production/scheduling", "No-wait production environment", "Manufacturing", "Automated systems", "Review of complexity", "Algorithms and heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.533", "e:abstract": "The University of Houston (UH) Small Business Development Centers (SBDCs) were analyzed by Data Envelopment Analysis (DEA) and Assurance Region (AR) methods. The DEA/AR analyses were made for 11 DMUs in 1990 and 1991 and for 13 DMUs in 1992. The DEA ratio (CCR) method was applied to Model I in all three years; unique optimal solutions to the DEA-inefficient DMUs were verified. Also, ARs were applied to eliminate some DEA-efficient DMUs requiring unreasonable multipliers to maximize efficiency. In addition, measures of returns-to-scale (RTS) intervals from application of the DEA convex (BCC) method in 1992 showed several additional DEA-efficient DMUs needed some additional funds to work toward the most productive scale size.", "e:keyword": ["Organizational studies", "Productivity", "Leadership", "Planning", "Government", "Finance", "Management"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.543", "e:abstract": "We study pooled (or group) testing as a cost-effective alternative for screening donated blood products (sera) for HIV; rather than test each sample individually, this method combines various samples into a pool, and then tests the pool. A group testing policy specifies an initial pool size, and based on the HIV test result, either releases all samples in the pool for transfusion, discards all samples in the pool, or divides the pool into subpools for further testing. We develop a hierarchical statistical model that relates the HIV test output to the antibody concentration in the pool, thereby capturing the effect of pooling together different samples. The model is validated using data from a variety of field studies. The model is embedded into a dynamic programming algorithm that derives a group testing policy to minimize the expected cost due to false negatives, false positives, and testing. Because the implementation of the dynamic programming algorithm is cumbersome, a simplified version of the model is used to develop near optimal heuristic policies. A simulation study shows that significant cost savings can be achieved without compromising the accuracy of the test. However, the efficacy of group testing depends upon the use of a classification rule (that is, discard the samples in the pool, transfuse them or test them further) that is dependent on pool size, a characteristic that is lacking in currently implemented pooled testing procedures.", "e:keyword": ["Dynamic programming", "Applications", "Health care", "Screening for HIV", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.570", "e:abstract": "The problem we treat is defined on a graph where each node is associated with a variable and there are loss functions defined on the arcs, depending on the difference between the corresponding node variables. The objective is to compute values for the node variables so as to minimize the sum of losses. We exploit the relation between this problem and network flows optimization and use it in developing an approximation algorithm for the problem. A main application of the problem is the synchronization of fixed cycle traffic signals.", "e:keyword": ["Network flow algorithms", "Application to synchronization of node variables", "Production scheduling", "Synchronization of cyclic work stations", "Transportation traffic models", "Traffic signal syncronization"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.580", "e:abstract": "We consider two economic order quantity models where multiple items use a common resource. In the <i>tactical model</i>, the objective is to establish and coordinate order quantities to minimize total inventory ordering and carrying costs without ever exceeding a capacity constraint on the resource. In the <i>strategic model</i>, the objective is to establish and coordinate order quantities to minimize the average cost which includes in addition to the ordering and inventory costs, a cost proportional to the peak usage of the resource. The common resource may be space in an automated warehouse or total capital invested in inventory. We show that a lower bound on the peak resource usage, known for certain subsets of policies, is valid for <i>any</i> feasible policy. We use this to derive lower bounds on the optimum average cost for both models and show that simple heuristics for either model have bounded worst-case performance ratios. More sophisticated heuristics require an effort to solve the embedded <i>staggering problem</i> of time-phasing the arrival of the orders to minimize the maximum use of the resource. We develop a heuristic for the staggering problem and use it to obtain efficient heuristics for the strategic and tactical models.", "e:keyword": ["Inventory", "Heuristic", "Approximation", "Multi-item"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.596", "e:abstract": "We develop an algorithm to solve the problem of the optimal dimensioning of a fluid (gas or water) transmission network when the topology of the network is known. The pipe diameters must be chosen to minimize the sum of the investment and operating costs. This two stage problem is solved by application of the bundle method for nonsmooth optimization. The validity of the approach is tested on a problem corresponding to a real situation: the optimal design of a reinforcement of the Belgian gas network.", "e:keyword": ["Equipment planning", "Capacity expansion", "Expansion of a gas transmission network", "Industries", "Natural gas", "Optimal dimensioning of gas transmission networks", "Programming", "Nondifferentiable", "Application of the bundle method"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.609", "e:abstract": "The standard decomposition theorem for additive and multiplicative utility functions (Pollak [Pollak, R. O. 1967. Additive von Neumann-Morgenstern utility functions. <i>Econometrica</i> <b>35</b> 485–494.], Keeney [Keeney, R. L. 1974. Multiplicative utility functions. <i>Opns. Res.</i> <b>22</b> 22–34.]) assumes that the outcome set is a whole product set. In this paper this assumption is relaxed, and the question of whether or not a natural revision of this theorem necessarily holds is investigated. This paper proves that two additional conditions are needed for the decomposition theorem to hold in the context where the outcome set is a subset of a Cartesian product. It is argued that these two new conditions are satisfied by a large family of subsets corresponding to significant real-world problems. Further research avenues are suggested including a generalization of this new decomposition result to nonexpected utility theories.", "e:keyword": ["Utility/preference", "Multiattribute", "Utility theory on subsets of product sets", "Utility/preference", "Theory", "Utility theory on subsets of product sets"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.617", "e:abstract": "This scheduling model is derived from the real problem of scheduling looms in a textile industry. Jobs may be independently split over several specified machines, and preemption is allowed. Deadlines are specified for each job, and jobs are assumed to be available. It is shown that minimizing maximum weighted tardiness can be done in polynomial time. The case of uniform machines (as in the considered application) can be modeled as a network flow, and minimization of maximum tardiness can be done in strongly polynomial time. The case of unrelated machines can be solved either by generalized flow techniques or by Linear Programming. Attention is also focused on the problem of finding so-called Unordered Lexico Optima, in order to schedule nonbinding jobs as early as possible.", "e:keyword": ["Industries", "Textiles", "Model for loom scheduling", "Network flow algorithms", "Application to scheduling", "Scheduling", "Sequencing multiple machines", "Application in textile industry"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.629", "e:abstract": "We consider the classical polling model: queues served in cyclic order with either exhaustive or gated service, each with its own distinct Poisson arrival stream, service-time distribution, and switchover-time (the server's travel time from that queue to the next) distribution. Traditionally, models with zero switchover times (the server travels at infinite speed) and nonzero switchover times have been considered separately because of technical difficulties reflecting the fact that in the latter case the mean cycle time approaches zero as the travel speed approaches infinity. We argue that the zero-switchover-times model is the more fundamental model: the mean waiting times in the nonzero-switchover-times model <i>decompose</i> (reminiscent of vacation models) into a sum of two terms, one being a simple function of the sum of the mean switchover times, and the other the mean waiting time in a “corresponding” model obtained from the original by setting the switchover times to zero and modifying the service-time variances. This generalizes a recent result of S. W. Fuhrmann for the case of constant switchover times, where no variance modification is necessary. The effect of these studies is to reduce computation and to improve theoretical understanding of polling models.", "e:keyword": ["Queues", "Polling models", "Cyclic-service queues", "Decomposition", "Waiting times", "Switchover times", "Server vacations"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.634", "e:abstract": "A single machine produces several different classes of items in a make-to-stock mode. We consider the problem of scheduling the machine to regulate finished goods inventory, minimizing holding and backorder, or holding and lost sales costs. Demands are Poisson, service times are exponentially distributed, and there are no delays or costs associated with switching products. A scheduling policy dictates whether the machine is idle or busy and specifies the job class to serve in the latter case. Since the optimal solution can be numerically computed only for problems with several products, our goal is to develop effective policies that are computationally tractable for a large number of products. We develop index policies to decide which class to produce, including Whittle's “restless bandit” index, which possesses a certain asymptotic optimality. Several idleness policies are derived, and the best policy is obtained from a heavy traffic diffusion approximation. Nine sample problems are considered in a numerical study, and the average suboptimality of the best policy is less than 3%.", "e:keyword": ["Production/scheduling", "Approximations", "Dynamic stochastic queues", "Diffusion models", "Optimal control"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.648", "e:abstract": "We present an <i>O</i>(<i>mn</i>) two-group (TG) heuristic for the <i>m</i>-machine, <i>n</i>-job permutation flow-shop scheduling problem. We show that heuristic TG has a worst-case performance ratio of (<i>m</i> + 1)/2. We also establish worst-case bounds for several heuristics proposed in the past.", "e:keyword": ["Heuristic", "Worst-case analysis", "Scheduling", "Flow-shop sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.4.653", "e:abstract": "In this paper, we study a multiple class discriminatory processor-sharing queue. The queue is assumed to have Poisson input and exponentially distributed service times. In this discipline there are <i>K</i> classes of customers. When there are <i>n</i><sub><i>i</i></sub> customers present in the system of class <i>i</i>(<i>i</i> = 1, …, <i>K</i>), each member of class <i>j</i> receives a fraction of the server's capacity given by α<sub><i>j</i></sub>/∑<sub><i>i</i>=1</sub><sup><i>K</i></sup><i>n</i><sub><i>i</i></sub>α<sub><i>i</i></sub>. Thus, associated with class <i>i</i> customers is a weight α<sub><i>i</i></sub> which determines the level of service discrimination. For this problem, we find the moments of the queue-length distribution as a solution of linear simultaneous equations. We also prove a heavy traffic limit theorem for the joint queue-length distribution for this queue.", "e:keyword": ["Probability", "Markov processes", "Queue-length distribution", "Queues", "Limit theorems", "Heavy traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.665", "e:abstract": "A highly speculative parallel stock market, called Souk al-Manakh, emerged in Kuwait in 1980–81. Due to the absence of checks and balances, the default of one trader led to the collapse of al-Manakh in 1982. This triggered a major financial national crisis as debts arising from the crash reached a staggering US $94 billion, or more than four times Kuwait's GDP. The crash precipitated an economic recession, business failures, and more bankruptcies among traders. It was impossible for the government or the courts to untangle traders, <i>one at a time</i>, due to then simultaneous dependence on, and entanglement with, each other. We designed a linear programming model to disentangle the bankruptcies of al-Manakh's insolvents. It identified solvent from insolvent traders, determined their debt settlement ratios, and set a fair and equitable distribution of debtors' mixed assets to their respective creditors. We demonstrate the special properties of the LP model and its relationship to a system of linear equations through a numerical example. We also present the legal, financial, and economic controversies that arose during the course of resolving the stock market crash, which lasted about four years.", "e:keyword": ["Programming", "Linear application", "Financial institutions", "Trading"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.677", "e:abstract": "The Ontario Ministry of Natural Resources (OMNR) operates a fleet of nine CL-215 airtankers for forest fire control. The aircraft are based at a small number of airports that serve as home bases, from which they are deployed to a larger set of airports that serve as initial attack bases for fire fighting operations each day. We helped regional fire managers derive subjective airtanker deployment rules that specified how many airtankers were to be deployed at each initial attack base each day. We then developed a mathematical programming model that was used to help identify a home-basing strategy that would minimize the average annual cost of satisfying daily airtanker deployment demands. Our analysis provided OMNR decision makers with valuable insight into airtanker management and was used to help decide where to base the OMNR airtankers for the 1993 and subsequent fire seasons.", "e:keyword": ["Transportation", "Models", "Location", "Basing of aircraft", "Government", "Services", "Forest fire management", "Programming", "Linear", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.687", "e:abstract": "This paper presents a model that estimates the relative cost-effectiveness of four cocaine-control programs: three “supply control” programs (source-country control, interdiction, and domestic enforcement) and a “demand control” program (treating heavy users). Treatment emerges as by far the most cost-effective, and sensitivity analyses show that this result is very robust.", "e:keyword": ["Government", "Cost-effectiveness", "Judicial/legal", "Illicit drug policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.696", "e:abstract": "In drug therapy, efficient dosage policies are needed to maintain drug concentrations at target. The relationship between the concentration of a drug and the dosages is often described by compartment models in which the parameters are unknown, although prior knowledge may be available and can be updated after blood samples are taken during the therapy. In this paper we define some tractable policies for adaptive control of drug concentrations in compartment models and compare their performances using computer simulation in a one-compartment model. We also discuss the effects of assuming normal priors, discrete approximation of a continuous prior, using nonquadratic costs, and information probing. From the simulation we derive intuition as to what types of policies perform well and address the topic of actively versus passively learning.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Probability", "Applications", "Healthcare", "Treatment"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.710", "e:abstract": "This paper considers two new decision-making problems in the context of transporting hazardous materials (HM). These problems are motivated by the fact that many practical situations only have a finite number of shipments of HM to be made and that shipments are likely to be suspended after a threshold number of accidents. Both problems are nonlinear, constrained shortest path problems. Exact solution methods are proposed and computational results are reported for the case where the threshold number of accidents is one. Extension of the methodology for higher values of the threshold number of accidents is also outlined.", "e:keyword": ["Transportation", "Network models", "Probability", "Stochastic model applications", "Networks/graphs", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.724", "e:abstract": "We consider the problem of allocating production capacity among multiple items, assuming that a fixed proportion of overall capacity can be dedicated exclusively to the production of each item. Given a capacity allocation, production of each item follows a base-stock policy, i.e., each demand triggers a replenishment order to restore safety stocks to target levels. We present procedures for choosing base-stock levels and capacity allocations that are asymptotically optimal. Our objective is to minimize holding and backorder costs, or to minimize holding costs subject to a service-level constraint. <i>Asymptotic</i> optimality refers to large backorder penalties or stringent service-level constraints. Numerical results indicate that our rules perform very well even far from the asymptotic regime. A further approximation step results in allocation rules based on heavy-traffic limits; these, too, perform well.", "e:keyword": ["Inventory/production approximation", "Asymptotically optimal allocations", "Probability", "Stochastic model applications", "Production allocation", "Programming", "Resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.735", "e:abstract": "Group technology plays an important role in the design of an automated manufacturing system. The ideal situation is to partition the machines into machine cells and the parts into part families so that each machine cell together with the part family for the cell is independent of the rest of the system. We will give a characterization of the independent cells for an arbitrary manufacturing system. An efficient algorithm to identify these independent cells will also be discussed. An effective policy for subcell formation (not necessarily independent) from larger cells will also be discussed. Computational results show that our algorithms always obtain solutions with higher grouping efficiency compared with some of the existing algorithms. We also discuss the effectiveness of grouping efficiency as a measure for group technology.", "e:keyword": ["Manufacturing", "Automated system", "Algorithm for group technology", "Facilities", "Design", "Design of cellular manufacturing", "Programming", "Integer", "Minimum spanning tree algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.745", "e:abstract": "The <i>Delta-Wye Approximation Procedure</i> (DWAP) is a procedure for estimating the two-terminal reliability of an undirected planar network <i>G</i> = (<i>V</i>, <i>E</i>) by reducing the network to a single edge via a sequence of local graph transformations. It combines the probability equations of Lehman—whose solutions provide bounds and approximations of two-terminal reliability for the individual transformations—with the <i>Delta-Wye Reduction Algorithm</i> of the second two authors—which performs the corresponding graph reduction in <i>O</i>(|<i>V</i>|<sup>2</sup>) time. A computational study is made comparing the DWAP to one of the best currently known methods for approximating two-terminal reliability, and it is shown that the DWAP produces approximations that are between 10 and 80 times as accurate.", "e:keyword": ["Reliability", "(s", "t)-connectedness reliability", "Analysis of algorithms network/graphs", "Application of the Delta-Wye reduction procedure for graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.758", "e:abstract": "We continue to focus on simple exponential approximations for steady-state tail probabilities in queues based on asymptotics. For the <i>G</i>/<i>GI</i>/1 model with i.i.d. service times that are independent of an arbitrary stationary arrival process, we relate the asymptotics for the steady-state waiting time, sojourn time, and workload. We show that the three asymptotic decay rates coincide and that the three asymptotic constants are simply related. We evaluate the exponential approximations based on the exact asymptotic parameters and their approximations by making comparisons with exact numerical results for <i>BMAP</i>/<i>G</i>/1 queues, which have batch Markovian arrival processes. Numerical examples show that the exponential approximations for the tail probabilities are remarkably accurate at the 90th percentile and beyond. Thus, these exponential approximations appear very promising for applications.", "e:keyword": ["Queues", "Approximations", "Exponential approximations for tail probabilites", "Queues", "Limit theorems", "Asymptotics for delay and workload tail probabilites"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.764", "e:abstract": "This paper considers a reentrant job shop with one <i>hub</i> machine which a job enters <i>K</i> times. Between any two consecutive entries into the hub, the job is processed on other machines. The objective is to minimize the total flow time. Under two key assumptions, the <i>bottleneck</i> assumption and the <i>hereditary order</i> (HO) assumption on the processing times of the entries, it is proved that there is an optimal schedule with the shortest processing time (SPT) job order and a dynamic programming algorithm is derived to find such a schedule. An approximation algorithm based on the shortest path algorithm and the SPT job order is also proposed to solve the problem. The approximation algorithm finds an optimal clustered schedule. In clustered schedules, jobs are scheduled in disjoint clusters; they resemble batch processing and seem to be of practical importance. Worst-case bounds for clustered schedules are proved with the HO assumption relaxed. Two special cases with the restriction that there are only two entries to the hub machine are further analyzed to offer more insight into the structure of optimal solutions.", "e:keyword": ["Production/scheduling", "Hub reentrant job shops and SPT rule", "Dynamic programming", "Deterministic", "Analysis of algorithms suboptimal algorithms and worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.777", "e:abstract": "We consider Job-Shop and Flow-Shop scheduling problems with two machines, no more than two operations per job, and Time Lags, i.e., a minimum time interval between the completion time of the first operation and the starting time of the second one. We give complexity results for the preemptive and nonpreemptive cases and study the relationship between the two problems. For the Flow-Shop problem we give lower bounds and upper bounds and analyze their worst-case performances. Finally we define a Tabu Search algorithm and prove the effectiveness of the proposed bounds through extensive computational results.", "e:keyword": ["Analysis of algorithms", "Computational complexity", "Production/scheduling", "Flow shop", "Job shop", "Programming", "Algorithms worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.788", "e:abstract": "<i>Stochastic trees</i> are semi-Markov processes represented using tree diagrams. Such trees have been found useful for prescriptive modeling of temporal medical treatment choice. We consider utility functions over stochastic trees which permit recursive evaluation in a graphically intuitive manner analogous to decision tree rollback. Such rollback is computationally intractable unless a low-dimensional preference summary exists. We present the <i>most general</i> classes of utility functions having specific tractable preference summaries. We examine three preference summaries—<i>memoryless, Markovian</i>, and <i>semi-Markovian</i>—which promise both computational feasibility and convenience in assessment. Their use is illustrated by application to a previous medical decision analysis of whether to perform carotid endarterectomy.", "e:keyword": ["Utility/preference", "Theory", "Utility over time streams", "Utility preference/applications", "Medical treatment decisions", "Decision analysis", "Theory", "Stochastic trees"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.810", "e:abstract": "In this paper, we study the departure process of the <i>GI</i>/<i>G</i>/1 queue. We develop a simple recursive procedure to calculate the MacLaurin series of its moments and covariances with respect to a parameter in the service time. Based on this recursive procedure the explicit formulas of the coefficients of these MacLaurin series can be obtained in terms of derivatives of the probability density function of the interarrival time evaluated at zero and the moments of the interarrival time and the service time. One important application of these MacLaurin series is that they can be used to obtain the entire response curves of the moments and variances of the departure process, for example, via interpolation by polynomials or rational functions.", "e:keyword": ["Queues", "Output process", "Moments and autocorrelations"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.816", "e:abstract": "We analyze the random number generators obtained by combining two or more multiple recursive generators. We study the lattice structure of such combined generators and argue that combination is a good way of obtaining robust generators, based on a recurrence with many nonzero coefficients, and which also possess a fast implementation.", "e:keyword": ["Simulation", "Random number generation", "Linear congruential", "Lattice structure", "Multiple recursive", "Combined generators"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.5.823", "e:abstract": "Demands for service arrive at random times, in random locations, in a region of the plane. The service time of each demand is random. A server that travels with constant speed moves from demand to demand providing service. The server spends its time either in providing service or in traveling. The objective is to route the server, based on the location of the current demands on the plane and the anticipated demand arrivals, such that the time spent in traveling is minimal and the service is provided efficiently. A routing policy is provided that achieves maximum throughput and is independent of the statistical parameters of the system, under the assumption that the arrival process is Poisson. For a renewal arrival process, a class of policies is specified that achieve maximum throughput based on some knowledge of the system parameters. Finally, an adaptive version of the partitioning policies is given, which makes them independent of the system statistics.", "e:keyword": ["Queues", "Algorithms", "Markovian network/graphs routing", "Stochastic traveling salesman", "Transportation", "Vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.1002", "e:abstract": "This paper presents a new equilibrium-seeking algorithm, called the <i>decoupling algorithm</i>, for calculation of multiperiod equilibrium of supplies and demands when demand has a geometric distributed lag (GDL) structure and supply is represented by a linear process submodel. The new algorithm is required because it may be difficult to obtain the equilibrium by a diagonalization algorithm such as PIES. In each step of the decoupling algorithm, a modified GDL equilibrium model, the “decoupled submodel,” is solved by the PIES algorithm; successive approximations move closer to the true equilibrium. Two versions of a large-scale realistic model of North American energy supplies and demands are solved with the decoupling algorithm to aid in understanding the behavior of the decoupling algorithm.", "e:keyword": ["Economics", "Equilibrium models with geometric distributed lag demand", "Programming", "Nonlinear", "Sequences of NLPs to calculate economic equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.1013", "e:abstract": "This paper considers the stochastic, single-item, periodic review inventory problem. Most importantly we assume a finite production capacity per period and a production cost function containing a fixed (as well as a variable) component. With stationary data, a convex expected holding and shortage cost function, we show that generally the modified (<i>s</i>, <i>S</i>) policy is not optimal to the finite horizon problems. The optimal policy does, however, show a systematic pattern which we call the <b>X-Y band</b> structure. This <b>X-Y</b> band policy is interpreted as follows: whenever the inventory level drops below <b>X</b>, order up to capacity; when the inventory level is above <b>Y</b>, do nothing; if the inventory level is between <b>X</b> and <b>Y</b>, however, the ordering pattern is different from problem to problem. Although the <b>X</b> and <b>Y</b> bounds may change from period to period, we prove the existence of a pair of finite <b>X</b> and <b>Y</b> values that can apply for all the periods (i.e., bounds on individual bounds). One calculation for such <b>X</b> and <b>Y</b> bounds that are tight in some cases is also provided. By exploring the <b>X-Y</b> band structure, we can drastically reduce the computation effort for finding the optimal policies.", "e:keyword": ["Inventory/production", "Policies", "Review/lead times", "Production/scheduling", "Planning", "Dynamic programming/optimal control", "Markov infinite slate"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.843", "e:abstract": "We develop a model and solution algorithm for sequencing production on a flexible assembly line. The model incorporates the practical considerations of printed circuit board (PCB) assembly where the goal is to reduce the time spent in setup. We discuss the practical implementation problems of integrating optimization software on the shop floor—both from an operations research and systems perspective. We describe Hewlett-Packard's successful use of the software and the limitations of production sequencing in PCB assembly.", "e:keyword": ["Industries", "Computers", "Electronics", "Production scheduling", "Applications", "Flexible manufacturing", "Sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.852", "e:abstract": "Express shipment service requires that shipments be picked up and delivered within specified time intervals (e.g., 24 hours, 48 hours or 3–5 days). In this paper, we describe the express shipment service design problem faced by a carrier and present a model and column generation approach for its solution. Our approach can find near optimal air service designs for a fixed aircraft fleet or for a fleet of unspecified size and make-up. In the latter case, the service design, fleet size and fleet composition are determined simultaneously. We have implemented our solution procedure, and a large carrier is using it to plan their air service operations and to evaluate various operating scenarios. The results indicate that service designs are generated that allow for improved service with a reduction in annual operating costs measuring in the millions of dollars.", "e:keyword": ["Transportation", "Freight", "Express intermodal (air", "Truck) service", "Transportation", "Scheduling vehicles", "Aircraft routing and scheduling", "Programming", "Linear", "Integer", "Column generation", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.864", "e:abstract": "The plant location problem has been studied for many years. Yet, a number of important real world issues and variants have not been investigated or resolved and merit further attention and research. This paper describes new statements of the problem (1) with new and different objectives, (2) with multiple products and multiple machines in which new models of production are considered, and (3) with spatial interactions.", "e:keyword": ["Facility location", "Plant and machine location"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.875", "e:abstract": "We discuss a branch and bound algorithm for set covering, whose centerpiece is a new integrated upper bounding/lower bounding procedure called dynamic subgradient optimization (DYNSGRAD). This new procedure, applied to a Lagrangean dual at every node of the search tree, combines the standard subgradient method with primal and dual heuristics that interact to change the Lagrange multipliers and tighten the upper and lower bounds, fix variables, and periodically restate the Lagrangean itself. Extensive computational testing is reported. As a stand-alone heuristic, DYNSGRAD performs significantly better than other procedures in terms of the quality of solutions obtainable with a certain computational effort. When incorporated into a branch-and-bound algorithm, DYNSGRAD considerably advances the state of the art in solving set covering problems.", "e:keyword": ["Integer programming", "Set covering algorithms", "Subgradient optimization", "Dynamic version"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.891", "e:abstract": "This paper considers the problem of sequencing <i>n</i> jobs in a three-machine flow shop with the objective of minimizing the makespan, which is the completion time of the last job. An <i>O</i>(<i>n</i> log <i>n</i>) time heuristic that is based on Johnson's algorithm is presented. It is shown to generate a schedule with length at most 5/3 times that of an optimal schedule, thereby reducing the previous best available worst-case performance ratio of 2. An application to the general flow shop is also discussed.", "e:keyword": ["Production/scheduling", "Flow shop", "Approximations/heuristic", "Worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.899", "e:abstract": "We study a two-machine flowshop in which all processing times are independently and identically distributed, with values known to the scheduler. We are able to describe in detail the expected behavior of the flowshop under optimal and heuristic schedules. Our results suggest that minimizing makespan might be a superfluous objective: random schedules are easier to construct and require significantly less intermediate storage between the machines; moreover, they are known to be asymptotically optimal.", "e:keyword": ["Sequencing", "Stochastic scheduling", "Approximations", "Heuristics for two-machine flowshop"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.909", "e:abstract": "This paper develops new bounds on the expectation of a convex-concave saddle function of a random vector with compact domains. The bounds are determined by replacing the underlying distribution by unique discrete distributions, constructed using second-order moment information. The results extend directly to new second moment lower bounds in closed-form for the expectation of a convex function. These lower bounds are better than Jensen's bound, the only previously known lower bound for the convex case, under limited moment information. Application of the second moment bounds to two-stage stochastic linear programming is reported. Computational experiments, using randomly generated stochastic programs, indicate that the new bounds may easily outperform the usual first-order bounds.", "e:keyword": ["Probability", "Stochastic model", "Approximations in stochastic programming", "Programming", "Stochastic", "Lower and upper bounds for stochastic linear programs"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.923", "e:abstract": "Hub facilities serve as switching and transshipment points in transportation and communication networks. Hub networks concentrate flows on the hub-to-hub links and benefit from economies of scale in interhub transportation. Most hub location research has focused on problems where each origin/destination is allocated to a single hub. However, multiple allocation to more than one hub is necessary to minimize total transportation costs. This paper defines a <i>p</i>-hub median, analogous to a <i>p</i>-median, and presents integer programming formulations for the multiple and single allocation <i>p</i>-hub median problems. Two new heuristics for the single allocation <i>p</i>-hub median problem are evaluated. These heuristics derive a solution to the single allocation <i>p</i>-hub median problem from the solution to the multiple allocation <i>p</i>-hub median problem. Computational results are presented for problems with 10–40 origins/destinations and up to eight hubs. The new heuristics generally perform well in comparison with other heuristics.", "e:keyword": ["Facilities/equipment planning", "Location", "Hub location", "Transportation", "Models", "Location", "Single and multiple allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.936", "e:abstract": "The QNET method for two-moment analysis of multiclass open networks is extended to allow complex workstations of various types. For example, the extension described here allows one to treat stations where several unreliable machines are tended by a small number of repair technicians, or stations where several machines that require setups are tended by a small number of operators. To illustrate the general concepts, a four-station manufacturing example is discussed in detail. In the QNET method, one first replaces the original queueing network by an approximating Brownian system model. The Brownian approximation is motivated by heavy traffic theory, and to achieve a unified treatment of complex workstations within the QNET framework we apply the following principle: For purposes of heavy traffic analysis, a workstation can be characterized by just two parameters, the asymptotic mean and asymptotic variance of its cumulative potential output process. This heavy traffic principle has long been known to researchers in the field, but we show that it has power and utility even in circumstances where the mean and variance parameters cannot be determined analytically. We explain how the heavy traffic principle can be applied successfully under certain conditions, and show by example that those conditions are not always met.", "e:keyword": ["Queues", "Approximations", "Heavy traffic analysis of multiclass open networks", "Queues", "Diffusion models", "Brownian models of networks with complex service stations", "Queues", "Networks", "Heavy traffic approximations with complex service stations"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.951", "e:abstract": "We consider the class of multistage dynamic networks with random arc capacities a framework that is well suited to model dynamic fleet management problems. We propose a successive convex approximation approach that produces an approximation to the expected recourse function which captures the future effects of current decisions under uncertainty. This method decomposes the network in each stage into tree subproblems, whose expected recourse functions are easy to obtain. We also compare this method with two alternative methods on a set of dynamic fleet management problems. The numerical results show that this method is superior to the two alternative methods.", "e:keyword": ["Stochastic programming", "Stochastic networks", "Transportation network models"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.964", "e:abstract": "Management of process improvement activities is an essential part of the manufacturing strategy of a firm to remain globally competitive in the long run. This paper considers a manufacturing environment where process improvement activities require use of the productive capacity of the firm in addition to other investments. Thus the firm must allocate its productive capacity between production activities and improvement activities. The output of production activities is used to meet customer demand. Process improvement activities improve the quality of the output, which in turn leads to lower quality related costs (both internal and external) and possibly lower per-unit production cost. It is assumed that the demand function is downward sloping and that revenue is a concave function of output. A continuous-time, finite-horizon, profit maximization, resource allocation model is developed to find an optimal time path for process improvement activities and production activities. Computational results are provided to study the effect of various problem parameters on the optimal decisions.", "e:keyword": ["Dynamic programming/optimal control", "Applications", "Optimal quality path", "Manufacturing", "Strategy resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.976", "e:abstract": "In this paper we consider the <i>M</i><sub><i>t</i></sub>/<i>G</i>/<i>s</i>/0 model, which has <i>s</i> servers in parallel, no extra waiting space, and i.i.d. service times that are independent of a nonhomogeneous Poisson arrival process. Arrivals finding all servers busy are blocked (lost). We consider approximations for the average blocking probabilities over subintervals (e.g., an hour when the expected service time is five minutes) obtained by replacing the nonstationary arrival process over that subinterval by a stationary arrival process. The stationary-Poisson approximation, using a Poisson (<i>M</i>) process with the average rate, tends to significantly underestimate the blocking probability. We obtain much better approximations by using a non-Poisson stationary (<i>G</i>) arrival process with higher stochastic variability to capture the effect of the time-varying deterministic arrival rate. In particular, we propose a specific approximation based on the heavy-traffic peakedness formula, which is easy to apply with either known arrival-rate functions or data from system measurements. We compare these approximations to exact numerical results for the <i>M</i><sub><i>t</i></sub>/<i>M</i>/<i>s</i>/0 model with linear arrival rate.", "e:keyword": ["Queues", "Nonstationary", "Stationary-process approximations", "Queues", "Approximations", "The nonstationary Erlang loss model"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.984", "e:abstract": "This paper develops a transform-free approximation for the steady-state queue-length distribution in an <i>M</i>/<i>G</i>/<i>s</i> queue with finite waiting spaces. The approximation is obtained by using a conservation law and some heuristics. It is shown that the approximation is exact for the cases with either no extra waiting space, exponential service-time distribution, or a certain two-parameter family of service-time distributions. It is also shown that the approximation has the same light-traffic properties as the known light-traffic limit theorem for the infinite capacity case when the number of waiting spaces is not less than one.", "e:keyword": ["Queues", "Multichannel", "M/G/s queue with finite waiting spaces", "Queues", "Approximations", "Steady-state queue-length distribution", "Queues", "Applications", "Buffer capacity design"]}, {"@id": "http://dx.doi.org/10.1287/opre.44.6.989", "e:abstract": "We characterize the impact of the means and distributions of setup times in polling systems. Starting with cyclic systems with exhaustive service, we show that the vector of average setup times affects the complete distribution of waiting times and queue sizes only via its sum. In addition, all moments of all waiting times and queue sizes are reduced if the higher order moments of one or more of the setup times is reduced while maintaining their means. We also obtain a fundamental decomposition result of the queue sizes at polling instants which is reminiscent of those obtained in vacation models. We show that the above results continue to apply under more general and more efficient protocols, e.g., general polling tables, systems with gated service, and systems with mixed service.", "e:keyword": ["Manufacturing", "Performance/productivity", "Queues", "Cyclic", "Inventory/production", "Multi-item/echelon/state"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.1", "e:abstract": "The growth of service industries and their impact on the U.S. economy have attracted considerable attention in recent years. While some service sectors, most notably airline and telecommunication industries, have been in the forefront of model development, the industry is rather fragmented, and similar rigor is lacking in most other sectors.This paper describes an application of a model-based approach to some of the short-term ride capacity and visitor flow issues faced by the Six Flags Magic Mountain (SFMM), a major national theme park. Specifically, we consider daily operations at the theme park and focus on the generation and evaluation of alternative strategies for managing ride capacities and visitor flow. Management of demand involves two aspects: (a) understanding customer preferences as revealed by routing behavior, and (b) using the model to evaluate the implications of changes in transition-behavior.A crucial component of the study relates to the empirical data collected. Besides verifying the validity of the models, these data provide several insights for developing schemes to manage the day-to-day operations of the park. The SFMM management was actively involved in various phases of this study and as a result has been introducing the proposed models in a phased manner.", "e:keyword": ["Service", "Design", "Capacity"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.102", "e:abstract": "We investigate the one warehouse multiretailer distribution problem with traveling salesman tour vehicle routing costs. We model the system in the framework of the more general production/distribution system with arbitrary non-negative monotone joint order costs. We develop polynomial time heuristics whose policy costs are provably close to the cost of an optimal policy. In particular, we show that given a submodular function which is close to the true order cost then we can find a power-of-two policy whose cost is only moderately greater than the cost of an optimal policy. Since such submodular approximations exist for traveling salesman tour vehicle routing costs we present a detailed description of heuristics for the one warehouse multiretailer distribution problem.We formulate a nonpolynomial dynamic program that computes optimal power-of-two policies for the one warehouse multiretailer system assuming only that the order costs are non-negative monotone.Finally, we perform computational tests which compare our heuristics to optimal power of two policies for problems of up to sixteen retailers. We also perform computational tests on larger problems; these tests give us insight into what policies one should employ.", "e:keyword": ["Transportation", "Costs", "TSP tour lengths and delivery costs with economies of scale", "Inventory/production", "Multi-item/echelon/stage", "Bounds hold for general production/distribution network", "Dynamic programming/optimal control", "Applications", "Optimal power of two schedule for one warehouse problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.116", "e:abstract": "We consider the problem of minimizing the makespan when scheduling tasks on two uniform parallel machines, where one machine is <i>q</i> times as efficient on each task as is the other. We compute the maximum relative error of the LPT (largest processing time first) heuristic as an explicit function of <i>q</i>. In the special case that the two machines are identical (<i>q</i> = 1), our problem and heuristic reduce to the problem and heuristic analyzed by Graham (Graham, R. 1969. Bounds on multiprocessing timing anomalies. <i>SIAM J. Appl. Math.</i> <b>17</b> 416–429.).", "e:keyword": ["Scheduling", "Approximations/heuristic", "Multiple machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.126", "e:abstract": "In this paper we deal with a stochastic project network and consider the impact of activity delay to maximize the expected present value of a project. It is shown that in certain situations, delaying the onset of an activity from its earliest start time can indeed increase the present value of a project due to the postponing of associated negative cash flows. Furthermore, a project that could otherwise be rejected (negative expected present value) may become profitable (positive expected present value) due to delay. We demonstrate that even activities on the critical path, as determined by each activity's expected duration, may be profitably delayed. Optimal and approximate procedures are developed to determine the amount of delay of the various activities.", "e:keyword": ["Project management"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.14", "e:abstract": "Most earlier mathematical studies of baseball required particular models for advancing runners based on a small set of offensive possibilities. Other efforts considered only teams with players of identical ability. We introduce a Markov chain method that considers teams made up of players with different abilities and which is not restricted to a given model for runner advancement. Our method is limited only by the available data and can use any reasonable deterministic model for runner advancement when sufficiently detailed data are not available. Furthermore, our approach may be adapted to include the effects of pitching and defensive ability in a straightforward way. We apply our method to find optimal batting orders, run distributions per half inning and per game, and the expected number of games a team should win. We also describe the application of our method to test whether a particular trade would benefit a team.", "e:keyword": ["Probability Markov processes", "Markov process to model baseball", "Recreation and sports", "Finding optimal batting orders in baseball"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.140", "e:abstract": "We study in this paper an approximation method for the calculation of various performance measures of a <i>GI</i>/<i>G</i>/1 queue. Instead of solving the waiting time directly, we analyze the idle-period distribution as the starting point. The result is then taken as input to many known results to get other performance measures. We show that the distribution of the <i>GI</i>/<i>G</i>/1 idle period satisfies a nonlinear integral equation. This equation directly leads to an accurate approximate solution of the idle-period distribution of the <i>GI</i>/<i>G</i>/1 queue where the interarrival times have a generalized hyperexponential distribution (<i>GH</i>). Since all distribution functions can be approximated by a <i>GH</i> distribution at any given accuracy (Botta and Harris [Botta, R. F., C. M. Harris. 1986. Approximation with generalized hyperexponential distributions: Weak convergence results. <i>Queueing Systems</i> <b>2</b> 169–190.]), the solution method developed in this paper serves as a unified basis for the analysis of <i>GI</i>/<i>G</i>/1 queues.", "e:keyword": ["Queues", "Approximations", "Analysis of GI/G/1 queues", "Probability", "Distributions", "Analysis of the idle-period distribution of GI/G/1 queues", "Queues", "Algorithms", "Solving GI/G/1 queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.145", "e:abstract": "This paper addresses a scheduling problem with interprocessor communication delays: the jobs and the communication delays are of unit length. The number of processors is unbounded. The aim is to minimize the makespan.We develop a new list scheduling heuristic and we prove that its worst-case relative performance is 4/3.", "e:keyword": ["Production/scheduling", "Precedence constraints", "Interprocessor communication delays", "Approximations/heuristics", "Continuous relaxation of an integer linear program"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.148", "e:abstract": "In this paper we consider the Ring Loading Problem, which arises in the design of SONET bidirectional rings. The issue of demand splitting divides the ring loading problem into the two kinds. One allows a demand to be split and routed in two different directions and the other does not. The former kind becomes a relaxation of the latter. We present an efficient exact solution procedure for the case with demand splitting, and a two-approximation algorithm for the case without demand splitting. Computational results are also shown to prove the efficiency of the proposed procedures.", "e:keyword": ["Communications", "Design of self healing rings", "Networks/graphs", "Applications", "Load balancing of rings", "Programming", "Linear", "Algorithms", "Exact solution to a ring loading problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.153", "e:keyword": ["Dynamic programming", "Production/scheduling", "Semiconductor manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.24", "e:abstract": "A firm has inventories of a set of components that are used to produce a set of products. There is a finite horizon over which the firm can sell its products. Demand for each product is a stochastic point process with an intensity that is a function of the vector of prices for the products and the time at which these prices are offered. The problem is to price the finished products so as to maximize total expected revenue over the finite sales horizon. An upper bound on the optimal expected revenue is established by analyzing a deterministic version of the problem. The solution to the deterministic problem suggests two heuristics for the stochastic problem that are shown to be asymptotically optimal as the expected sales volume tends to infinity. Several applications of the model to network yield management are given. Numerical examples illustrate both the range of problems that can be modeled under this framework and the effectiveness of the proposed heuristics. The results provide several fundamental insights into the performance of yield management systems.", "e:keyword": ["Transportation", "Yield management", "Seat allocation", "Pricing", "Dynamic programming", "Intensity control"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.42", "e:abstract": "This paper considers the dynamic scheduling problem of a single-server, make-to-stock queue with two products. For the case of Poisson demands and exponential production times, we show that it is optimal to produce the product with the larger <i>b</i>μ index when it is backordered. If the production times are identically distributed, the optimal policy can further be characterized by monotone switching curves. We also prove that a base stock policy coupled with a switching curve is optimal for some initial inventory levels. A simple linear switching rule for determining production priority is proposed and compared with other heuristic policies in a computational experiment.", "e:keyword": ["Inventory/production", "Optimal control policies", "Production/scheduling", "Sequencing in a stochastic system", "Dynamic programming/optimal control", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.54", "e:abstract": "We consider a competitive version of the classical newsboy problem—in which a firm must choose an inventory or production level for a perishable good with random demand, and the optimal solution is a fractile of the demand distribution—and investigate the impact of competition upon industry inventory. A splitting rule specifies how initial industry demand is allocated among competing firms and how any excess demand is allocated among firms with remaining inventory. We examine the relation between equilibrium inventory levels and the splitting rule and provide conditions under which there is a unique equilibrium. Our most general result is that if all excess demand is reallocated, i.e., there is perfect substitutability, then competition never leads to a decrease in industry inventory.", "e:keyword": ["Inventory", "Perishable", "One-period newsboy problem", "Games", "Noncooperative", "Strategic", "Competitive", "Inventory levels"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.66", "e:abstract": "In this paper we consider single machine scheduling problems with a common due-date for all jobs, <i>arbitrary</i> monotone earliness and tardiness costs and <i>arbitrary</i> breakdown and repair processes. We show that the problem is equivalent to a deterministic one <i>without</i> breakdowns and repairs and with an <i>equivalent cost function</i> of a job's completion time. A <i>V</i>-shaped schedule without idle times is shown to be optimal, if this equivalent cost function is <i>quasi-convex</i>.Conversely, we show that a <i>V</i>-shaped schedule may fail to be optimal if the property does not apply. We derive general conditions for the earliness and tardiness cost structure and repair and breakdown processes under which the equivalent cost function is <i>quasi-convex</i>. When a <i>V</i>-shaped schedule is optimal, an efficient (though pseudo-polynomial) algorithm can be used to compute an optimal schedule.", "e:keyword": ["Production/scheduling", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.72", "e:abstract": "This paper discusses the periodic job shop scheduling problem, a problem where an identical mixture of items, called a minimal part set (MPS), is repetitively produced. The performance and behavior of schedules are discussed. Two basic performance measures, cycle time and makespan, are shown to be closely related. The minimum cycle time is identified as a circuit measure in a directed graph. We establish that there exists a class of schedules that minimizes cycle time and repeats an identical timing pattern every MPS. An algorithm is developed to construct such schedules. We show that minimizing the makespan as a secondary criterion, minimizes several other performance measures.For makespan minimization, we examine earliest starting schedules where each operation starts as soon as possible. We characterize the cases where after a finite number of MPSs, the earliest starting schedule repeats an identical timing pattern every fixed number of MPSs. We also develop a modification to an earliest starting schedule that repeats an identical timing pattern every MPS when the beginning operations on the machines are delayed.", "e:keyword": ["Sequencing", "Deterministic", "Periodic scheduling", "Job shops", "Performance criteria", "Constraints", "Networks/graphs", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.1.92", "e:abstract": "We present a genetic algorithm for the multiple-choice integer program that finds an optimal solution with probability one (though it is typically used as a heuristic). General constraints are relaxed by a nonlinear penalty function for which the corresponding dual problem has weak and strong duality. The relaxed problem is attacked by a genetic algorithm with solution representation special to the multiple-choice structure. Nontraditional reproduction, crossover and mutation operations are employed. Extensive computational tests for dual degenerate problem instances show that suboptimal solutions can be obtained with the genetic algorithm within running times that are shorter than those of the OSL optimization routine.", "e:keyword": ["Programming-integer-heuristic", "Convergent genetic algorithm", "Programming-integer-theory", "Strong duality", "Computers/cs-artificial intelligence", "Genetic algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.157", "e:abstract": "The Waste Isolation Pilot Plant (WIPP) is being developed by the U.S. Department of Energy as a disposal facility for transuranic waste and must comply with several environmental regulations, including the U.S. Environmental Protection Agency's standard for geologic disposal of radioactive waste (40 CFR 191). Procedures used in recent performance assessments for the WIPP to translate regulations into a structure that facilitates quantitative analysis are described. Topics considered include (1) development of a clear conceptual representation for problems that are initially ill-defined, (2) conversion of qualitative guidance into numerical calculations. (3) explicit incorporation of different types of uncertainty (i.e., aleatory and epistemic) into analysis outcomes, (4) requirements for efficient computation and models at different levels of complexity, (5) use of sensitivity analysis to guide additional data collection and future calculations, (6) requirements for results to be presented and explained to audiences with different interests and levels of sophistication, and (7) ambiguity with respect to final uses of the analysis and its outcomes. The need to address similar concepts and problems arises in many analyses.", "e:keyword": ["Decision analysis", "Risk", "Performance assessment for radioactive waste disposal", "Government", "Regulations", "Application of 40 CFR 191.13 to waste isolation pilot plant", "Simulation", "Statistical analysis", "Incorporation of aleatory and epistemic uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.178", "e:abstract": "Reisman and Kirschnick (Reisman, A., F. Kirschnick. 1994. The devolution of OR/MS: Implications from a statistical content analysis of papers in flagship journals. <i>Opns. Res.</i> <b>42</b> 577–588.) have analysed United States Flagship OR/MS Journals over a thirty-year period using statistical content analysis. We have applied their method of analysis, which seeks to distinguish between the space devoted to theory on the one hand and that devoted to applications on the other, to journals based in the United Kingdom. Using their definitions we have classified articles published in the <i>Journal of the Operational Research Society</i> (previously <i>OR Quarterly</i>), <i>OMEGA</i>, and <i>OR Insight</i> over the period from 1962 to 1994 to parallel their study. As well as shedding some light on the direction that OR/MS is taking in the United Kingdom, comparison is made with trends in the United States. Further, the <i>European Journal of Operational Research</i> has also been included in the analysis, adding a European dimension. The country of affiliation of the authors has been analysed, providing data on the publishing habits of U.S. authors in the United Kingdom and European-based journals. This casts new light on the findings of Reisman and Kirschnick (Reisman, A., F. Kirschnick. 1994. The devolution of OR/MS: Implications from a statistical content analysis of papers in flagship journals. <i>Opns. Res.</i> <b>42</b> 577–588.). Finally, the paper contributes to the debate about the current state and future direction of OR/MS.", "e:keyword": ["Professional", "OR/MS education", "Meta research", "Professional", "OR/MS philosophy", "Research on research", "Epistemology of management science"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.188", "e:abstract": "Airline crew scheduling is concerned with finding a minimum cost assignment of flight crews to a given flight schedule while satisfying restrictions dictated by collective bargaining agreements and the Federal Aviation Administration. Traditionally, the problem has been modeled as a set partitioning problem. In this paper, we present a new model based on breaking the decision process into two stages. In the first stage we select a set of duty periods that cover the flights in the schedule. Then, in the second stage, we attempt to build pairings using those duty periods. We suggest a decomposition approach for solving the model and present computational results for test problems provided by a major carrier. Our formulation provides a tighter linear programming bound than that of the conventional set partitioning formulation but is more difficult to solve.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.201", "e:abstract": "In this paper a branch-and-bound procedure is described for scheduling project activities subject to precedence diagramming type of precedence relations, ready times, due dates, and variable multiple resource availability constraints, where the objective is to minimize project duration. The procedure is based on a depth-first solution strategy in which nodes in the solution tree represent resource and precedence feasible partial schedules. Branches emanating from a parent node correspond to exhaustive and minimal combinations of activities, the delay of which resolves resource conflicts at each parent node. A precedence based lower bound and several dominance rules are introduced in order to restrict the growth of the solutions tree. The procedure has been programmed in the <i>C</i> language. Extensive computational experience is reported.", "e:keyword": ["Project management", "Generalized resource-constrained project scheduling", "Programming", "Branch-and-bound", "Networks/graphs", "Precedence diagramming"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.213", "e:abstract": "In this paper we introduce a nonparametric linear programming formulation for the general multigroup classification problem. Previous research using linear programming formulations has either been limited to the two-group case, or required complicated constraints and many zero-one variables. We develop general properties of our multigroup formulation and illustrate its use with several small example problems and previously published real data sets. A comparative analysis on the real data sets shows that our formulation may offer an interesting robust alternative to parametric statistical formulations for the multigroup discriminant problem.", "e:keyword": ["Programming", "Linear applications", "Statistics", "Nonparametric", "Discriminant analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.226", "e:abstract": "We propose a knowledge-based crossover mechanism for genetic algorithms that exploits the structure of the solution rather than its coding. More generally, we suggest broad guidelines for constructing the knowledge-based crossover mechanisms. This technique uses an optimized crossover mechanism, in which the one of the two children is constructed in such a way as to have the best objective function value from the feasible set of children, while the other is constructed so as to maintain the diversity of the search space. We implement our approach on a classical combinatorial problem, called the independent set problem. The resulting genetic algorithm dominates all other genetic algorithms for the problem and yields one of the best heuristics for the independent set problem in terms of robustness and time performance. The primary purpose of this paper is to demonstrate the power of knowledge based mechanisms in genetic algorithms.", "e:keyword": ["Programming", "Integer", "Heuristic", "Optimized crossover for independent set problem", "Networks/graphs", "Heuristic", "Optimized crossover for independent set problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.235", "e:abstract": "We discuss how long-range dependence can influence the characteristics of a single server queue. We take the analogue of the <i>G</i>/<i>M</i>/1 queue except that the input stream is altered to exhibit long-range dependence. The equilibrium queue size and equilibrium waiting time distributions have heavy tails. By suitably selecting the parameters of the inputs, the queue size or waiting time can be made to possess infinite variance and even infinite mean. Some simulations dramatically illustrate the potential for undetected long-range dependence to significantly alter the queueing behavior compared to what is anticipated with traditional inputs.", "e:keyword": ["Queues", "With serially correlated input"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.244", "e:abstract": "We develop bounds and approximations for setting base-stock levels in production-inventory systems with limited production capacity. Our approximations become exact as inventories become <i>critical</i>, meaning either that the target service level is very high or the backorder penalty is very large. Our bounds apply even without this requirement. We consider both single-stage and multi-stage systems. For single-stage systems, we find tight bounds and asymptotically exact approximations for optimal base-stock levels; for multistage systems, our results give partial characterizations of the optimal levels. Part of our analysis is a precise connection, in the critical regime, between a multistage system and an associated single-stage system consisting solely of the bottleneck facility.", "e:keyword": ["Inventory/production", "Service level approximations", "Probability applications", "Rare events"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.258", "e:abstract": "This paper is concerned with near-optimal control of manufacturing systems consisting of two unreliable machines in tandem and having the objective of minimizing the total discounted cost of inventories/shortages over an infinite horizon. Asymptotic optimal feedback controls are constructed with respect to the rate of machine breakdown/repair as compared to the given discount rate. Performance of these controls, known as <i>hierarchical controls</i>, is compared with the optimal cost (when possible) and the costs obtained with two well-known heuristics, known as <i>Kanban controls</i> and <i>two boundary controls</i>. It is shown that hierarchical controls perform better than Kanban controls in some cases and no worse in others. Costs of hierarchical and two boundary controls are not significantly different, although the former is a simpler policy than the latter. Also examined computationally is the asymptotic nature of hierarchical controls.", "e:keyword": ["Production/scheduling", "Approximations/heuristics", "Hierarchical decision making", "Asymptotic optimality", "Dynamic programming/optimal control", "Finite state markov", "Manufacturing with unreliable machines", "Computation of MDP", "Simulation", "Applications", "Performance comparison of different policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.275", "e:abstract": "We consider a distribution system with a central warehouse and multiple retailers. The warehouse orders from an outside supplier and replenishes the retailers which in turn satisfy customer demand. The retailers are nonidentical, and their demand processes are independent compound Poisson. There are economies of scale in inventory replenishment, which is controlled by an echelon-stock, batch-transfer policy. For the special case with simple Poisson demand, we develop an exact method for computing the long-run average holding and backorder costs of the system. Based on this exact method, we provide approximations for compound Poisson demand. Numerical examples are used to illustrate the accuracy of the approximations. We also present a numerical comparison between the average costs of a heuristic, echelon-stock policy and an existing lower bound on the average costs of all feasible policies.", "e:keyword": ["Inventory/production", "Echelon-stock", "Hatch-transfer policies", "Multiechelon", "Nonidentical retailers", "Inventory/production", "Stochastic", "Compound Poisson demand", "Continuous review"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.288", "e:abstract": "We consider the open shop, job shop, and flow shop scheduling problems with integral processing times. We give polynomial-time algorithms to determine if an instance has a schedule of length at most 3, and show that deciding if there is a schedule of length at most 4 is (N-script)(P-script)-complete. The latter result implies that, unless (P-script) = (N-script)(P-script), there does not exist a polynomial-time approximation algorithm for any of these problems that constructs a schedule with length guaranteed to be strictly less than 5/4 times the optimal length. This work constitutes the first nontrivial theoretical evidence that shop scheduling problems are hard to solve even approximately.", "e:keyword": ["Analysis of algorithms", "Computational complexity", "NP-completeness results", "Production/scheduling", "Approximations", "Impossibility results", "Production/scheduling", "Multiple machine deterministic scheduling", "Nonpreemptive shop scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.295", "e:abstract": "The Vehicle Routing Problem with Time Windows (VRPTW) is one of the most important problems in distribution and transportation. A classical and recently popular technique that has proven effective for solving these problems is based on formulating them as a <i>set covering problem</i>. The method starts by solving its <i>linear programming relaxation</i>, via column generation, and then uses a branch and bound strategy to find an integer solution to the set covering problem: a solution to the VRPTW. An empirically observed property is that the optimal solution value of the set covering problem is very close to its linear programming relaxation which makes the branch and bound step extremely efficient. In this paper we explain this behavior by demonstrating that for any distribution of service times, time windows, customer loads, and locations, the relative gap between fractional and integer solutions of the set covering problem becomes arbitrarily small as the number of customers increases.", "e:keyword": ["Transportation", "Vehicle routing", "Vehicle routing with time windows", "Programming", "Integer", "Applications", "Linear relaxations"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.302", "e:abstract": "This note addresses the discrete version of the well-known time-cost tradeoff problem for project networks, which has been studied previously in the standard project management literature as well as in the related literature on Decision-CPM. All the algorithms proposed thus far for the solution of the general problem exhibit exponential worst-case complexity, with the notable exception of the pseudo-polynomial dynamic program due to Hindelang and Muth. We first demonstrate that this algorithm is flawed, and that when we correct it, it no longer remains pseudo-polynomial. Continuing on in the main result of the note, we show that this is not at all surprising, since the problem is strongly NP-hard. Finally, we discuss the complexities of various network structures and validate an old conjecture that certain structures are necessarily more difficult to solve.", "e:keyword": ["Project management", "Time-cost tradeoff", "Analysis of algorithms", "Computational complexity", "Strong NP-completeness"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.307", "e:abstract": "Dyer and Sarin (Dyer, J. S., R. K. Sarin. 1979. Measurable multiattribute value functions. <i>Opns. Res.</i> <b>27</b> 810–822.) proposed an important generalization of the additive-multiplicative decomposition theorem for von Neumann-Morgenstern (vNM) utility functions to measurable value functions based on the cardinality property of measurable value functions. However, because of a difference between the cardinality of measurable value functions and the cardinality of vNM utility functions, an additional technical condition is needed, and presented here, for the decomposition theorem to hold.", "e:keyword": ["Utility/preference", "Multiattribute", "Multiattribute measurable value functions", "Utility/preference", "Theory", "Multiattribute measurable value functions"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.309", "e:abstract": "We demonstrate the instability of the “join-the-shortest-queue” routing policy and the “first-come-first-served” dispatching policy for a multiclass single-station queuing system with multiple nonidentical servers. Although instability is demonstrated in a deterministic setting, we have found strong evidence that it is not limited to this setting. The phenomenon that leads to instability is different from the one reported recently in the literature for nonacyclic systems, namely, servers starvation. The systems considered here are acyclic, and instability is caused by the failure of the policies to assign jobs to servers in an efficient manner. A modification to the investigated policies is proposed to make them stable. The modified policies (called <i>guided policies</i>) provide an oversight control that ensure efficient utilization of the servers and hence stability.", "e:keyword": ["Production/scheduling", "Instability of join-the-shortest-queue and FCFS policies", "Queues", "Instability of join-the-shortest-queue and FCFS policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.2.315", "e:abstract": "The problem of maximizing the production of <i>good sets</i> of semiconductor chips under random yield is reexamined in this paper. (A set of semiconductor chips is called a semiconductor kit.) This problem has been considered by Avram and Wein (Avram, F., L. Wein. 1992. A product design problem in semiconductor manufacturing. <i>Opns. Res.</i> <b>40</b>(5) 986–998.) and Singh et al. (Singh, M. R., C. T. Abraham, R. Akella. 1988. Planning for production of a set of components when yield is random. <i>Fifth IEEE CHMT Internat. Electronic Manufacturing Technology Proc.</i>). To solve this problem we show that under certain combinations of assumptions the production process can be replaced by a black box. The use of the black box model considerably simplifies the analysis and reduces the simulation effort required for carrying out parametric analysis of the proposed solution procedure. The model includes that of Avram and Wein, and we extend their results to more general settings and strengthen their conclusions. Using the black box model, it is shown that the strategy of placing different types of chips on a single wafer gives larger yield of kits in a stochastic sense than the traditional method of placing single types of chips on a wafer. We compare the production of kits under different chip design and lot release policies and also carry out a parametric analysis with respect to factors such as set proportions and yield.", "e:keyword": ["Inventory/production", "Random yields", "Product design", "Industries", "Semiconductor", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.327", "e:abstract": "This work was initiated and supported by a manufacturer of mail processing equipment, which stocks 30,000 distinct parts in a distribution center to support field maintenance of their equipment. To find an effective stocking policy for this system we formulate a constrained optimization model with the objective of minimizing overall inventory investment at the distribution center subject to constraints on customer service and order frequency. Because size, integrality, and nonconvexity make this problem intractable to exact analysis, we develop three heuristic algorithms based on simplified representations of the inventory and service expressions. These lead to what we call <i>easily implementable</i> inventory policies, in which the control parameters for a newly introduced part can be computed in closed form without reoptimizing the rest of the system. Numerical comparisons against a lower bound on the cost function show that even our simplest heuristic works well when a high service level is required. However, we show that a more sophisticated heuristic is more robustly accurate. We also compare our heuristics to methods previously in use by the firm whose system motivated this research and show that they are more efficient in the sense of attaining the same customer service level with a 20–25% smaller inventory investment. Finally, we discuss implementation issues related to the specific needs of the client firm, such as how to handle parts with no or low recent usage and dynamically changing demand for parts.", "e:keyword": ["Inventory/production", "Approximations/heuristics", "Multi-item"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.341", "e:abstract": "We use the models of cognitive psychology and the early literature on linear programming models to understand how experts organize their thinking about models. We show that several different patterns appear in the way models are related to each other. This paper also is a kind of cognitive history of the formulation of linear programming models in the first decade after the invention of the field.", "e:keyword": ["Programming", "Linear applications", "Early linear programming models"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.357", "e:abstract": "Suppose <i>n</i> blind, speed one, players are placed by a random permutation onto the integers 1 to <i>n</i>, and each is pointed randomly to the right or left. What is the least expected time required form <i>m</i> ≤ <i>n</i> of them to meet together at a single point? If they must all use the same strategy we call this time the symmetric rendezvous value <i>R</i><sub><i>n</i>,<i>m</i></sub><sup><i>s</i></sup>; otherwise the asymmetric value <i>R</i><sub><i>n</i>,<i>m</i></sub><sup><i>a</i></sup>. We show that <i>R</i><sub>3,2</sub><sup><i>a</i></sup> = 47/48, and that <i>R</i><sub><i>n</i>,<i>n</i></sub><sup><i>s</i></sup> is asymptotic to <i>n</i>/2. These results respectively extend those for two players given by Alpern and Gal (Alpern, S., S. Gal. 1995. Rendezvous search on the line with distinguishable players. <i>SIAM J. Control Optim.</i> <b>33</b> 1270–1276.) and Anderson and Essegaier (Anderson, E. J., S. Essegaier. 1995. Rendezvous search on the line with indistinguishable players. <i>SIAM J. Control Optim.</i> <b>33</b> 1637–1642.).", "e:keyword": ["Search and surveillance", "Rendezvous search with many players"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.365", "e:abstract": "The Traveling Salesman Problem with Time Window and Precedence Constraints (<b>TSP-TWPC</b>) is to find an Hamiltonian tour of minimum cost in a graph <i>G</i> = (<i>X</i>, <i>A</i>) of <i>n</i> vertices, starting at vertex 1, visiting each vertex <i>i</i> ∈ <i>X</i> during its time window and after having visited every vertex that must precede <i>i</i>, and returning to vertex 1. The <b>TSP-TWPC</b> is known to be NP-hard and has applications in many sequencing and distribution problems. In this paper we describe an exact algorithm to solve the problem that is based on dynamic programming and makes use of bounding functions to reduce the state space graph. These functions are obtained by means of a new technique that is a generalization of the “State Space Relaxation” for dynamic programming introduced by Christofides et al. (Christofides, N., A. Mingozzi, P. Toth. 1981b. State space relaxation for the computation of bounds to routing problems. <i>Networks</i> <b>11</b> 145–164.). Computational results are given for randomly generated test problems.", "e:keyword": ["Networks/graphs", "Traveling salesman with additional constraints", "Dynamic programming/optimal control", "State space relaxation strategies"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.378", "e:abstract": "We consider a variant of the classical symmetric Traveling Salesman Problem in which the nodes are partitioned into clusters and the salesman has to visit at least one node for each cluster. This <i>NP</i>-hard problem is known in the literature as the symmetric Generalized Traveling Salesman Problem (GTSP), and finds practical applications in routing, scheduling and location-routing. In a companion paper (Fischetti et al. [Fischetti, M., J. J. Salazar, P. Toth. 1995. The symmetric generalized traveling salesman polytope. <i>Networks</i> <b>26</b> 113–123.]) we modeled GTSP as an integer linear program, and studied the facial structure of two polytopes associated with the problem. Here we propose exact and heuristic separation procedures for some classes of facet-defining inequalities, which are used within a branch-and-cut algorithm for the exact solution of GTSP. Heuristic procedures are also described. Extensive computational results for instances taken from the literature and involving up to 442 nodes are reported.", "e:keyword": ["Networks/graphs", "Generalized traveling salesman", "Programming", "Integer", "Cutting plane", "Branch-and-cut algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.395", "e:abstract": "Our paper presents a new optimization method for the Vehicle Routing Problem with Time Windows (VRPTW). The VRPTW is a generalization of the Vehicle Routing Problem, where the service of a customer must start within a given time interval—a so-called time window. Our method is based on a Lagrangian relaxation of the constraint set requiring that each customer must be serviced. The master problem consists of finding the optimal Lagrangian multipliers and the subproblem is a Shortest Path Problem with Time Windows and Capacity Constraints. The optimal multipliers are found using a method exploiting the benefits of subgradient methods as well as a bundle method. The method has been implemented and tested on a series of well-known benchmark problems of size up to 100 customers. Our algorithm turns out to be very competitive compared to algorithms considered in the literature, and we have succeeded in solving several previously unsolved problems.", "e:keyword": ["Programming", "Relaxation/subgradient", "Programming", "Nondifferentiable", "Transportation", "Vehicle routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.407", "e:abstract": "This paper studies competition between firms that produce goods or services for customers sensitive to delay time. Firms compete by setting prices and production rates for each type of customer and by choosing scheduling policies. The existence of a competitive equilibrium is proved. The competitive equilibrium is well defined whether or not a firm can differentiate between customers based upon physical characteristics because each customer has incentive to truthfully reveal its delay cost. Further insights are derived in two special cases. A unique equilibrium exists for each of the cases. In the first case, firms are differentiated by cost, mean processing time, and processing time variability, but customers are homogeneous. The conclusions include that a faster, lower variability and lower cost firm always has a larger market share, higher capacity utilization, and higher profits. However, this firm may have higher prices and faster delivery time, or lower prices and longer delivery time. In the second case, firms are differentiated by cost and mean processing time, but customers are differentiated by demand function and delay sensitivity. The results include that customers with higher waiting costs pay higher full prices, and that each firm charges a higher price and delivers faster to more Impatient customers. Competing firms that jointly serve several types of customers tend to match prices and delivery times.", "e:keyword": ["Production/scheduling", "Time based competition"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.421", "e:abstract": "This paper considers the scheduling of operations in a manufacturing cell that repetitively produces a family of similar parts on two or three machines served by a robot. We provide a classification scheme for scheduling problems in robotic cells. We discuss finding the robot move cycle and the part sequence that jointly minimize the production cycle time, or equivalently maximize the throughput rate. For multiple part-type problems in a two-machine cell, we provide an efficient algorithm that simultaneously optimizes the robot move and part sequencing problems. This algorithm is tested computationally. For a three-machine cell with general data and identical parts, we address an important conjecture about the optimality of repeating one-unit cycles, and show that such a procedure dominates more complicated cycles producing two units. For a three-machine cell producing multiple part-types, we prove that four out of the six potentially optimal robot move cycles for producing one unit allow efficient identification of the optimal part sequence. Several efficiently solvable special cases with practical relevance are identified, since the general problem of minimizing cycle time is intractable. Finally, we discuss ways in which a robotic cell converges to a steady state.", "e:keyword": ["Production/scheduling", "Minimizing cycle time in robot-served machine cells", "Deterministic", "Multiple machine sequencing", "Optimal sequencing of robot moves and of parts"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.440", "e:abstract": "Theoretical analyses incorporating production learning are typically deterministic: costs are posited to decrease in a known, deterministic fashion as cumulative production increases. This paper introduces a stochastic 1earning:curve model that incorporates random variation in the decreasing cost function. We first consider a discrete-time, infinite-horizon, dynamic programming formulation of monopolistic production planning when costs follow a learning curve. This basic formulation is then extended to allow for random variation in the learning process. We also explore properties of the resulting optimal policies. For example, in some of the stochastic models we analyze optimal production is shown to exceed myopic production, echoing a key result from the deterministic learning-curve literature. In other of the stochastic models, however, this result does not hold, underscoring the need for extended analysis in the stochastic setting. We also provide new insights in the deterministic setting: for example, while an increase in the learning rate leads to an increase in the firm's expected profits in the deterministic case, there is not necessarily an increase in the optimal policy—faster learners do not necessarily produce more.", "e:keyword": ["Production", "Learning", "Optimal policies with stochastic learning curve", "Production planning", "Dynamic programming", "Model"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.451", "e:abstract": "We develop an algorithm for computing the (exact) cumulative distribution function of the time-dependent workload in a piecewise-stationary <i>M</i><sub><i>t</i></sub>/<i>G</i><sub><i>t</i></sub>/1 queue with a work-conserving service discipline and general service-time distributions, where service times are determined at arrival instants. The <i>t</i> subscripts indicate that the arrival rate and the general service-time distribution may change with time, but we allow changes only at finitely many time points. The algorithm is based on numerical transform inversion, using the classical Takács double-transform of the transient workload in an <i>M</i>/<i>G</i>/1 queue recursively over the successive stationary intervals. In particular, we apply our recently developed Fourier-series-based inversion algorithms for two-dimensional transforms and nested one-dimensional transforms. We also do additional work to greatly speed up the computation while tightly controlling the error. As a consequence, the computation time grows only quadratically with the number of intervals. The algorithm is effective for ten or fewer intervals, where the intervals may have unlimited and possibly unequal lengths, typically running in at most a few minutes and maintaining high accuracy. We have also demonstrated that the algorithm can solve a 21-interval example with 7-to-10-digit accuracy in about half an hour. Models with only a few intervals are useful to study overload control strategies.", "e:keyword": ["Queues", "Nonstationary", "Numerical solution of single-server models", "Queues", "Algorithms", "Numerical transform inversion for nonstationary queues", "Queues", "Transient results", "Two-dimensional numerical transform inversion"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.464", "e:abstract": "We consider a single stage queueing system with <i>c</i> heterogeneous servers. Customers arrive at this system according to a renewal process with mean 1/λ and squared coefficient of variation (scv) <i>C</i><sub><i>a</i></sub><sup>2</sup>. An incoming customer is routed to server <i>i</i> with probability θ<sub><i>i</i></sub>, ∑<sub><i>i</i>=1</sub><sup><i>c</i></sup>θ<sub><i>i</i></sub> = 1. The service times at server <i>i</i> are i.i.d random variables with mean 1/μ<sub><i>i</i></sub>, and scv <i>C</i><sub><i>s</i><sub><i>i</i></sub></sub><sup>2</sup>. The holding cost rate of queue <i>i</i> is <i>h</i><sub><i>i</i></sub> per customer, <i>i</i> = 1, 2, …, <i>c</i>. The problems of interest are twofold: (a) for a fixed service rate allocation μ<sub><i>i</i></sub>, ∑<sub><i>i</i>=1</sub><sup><i>c</i></sup> μ<sub><i>i</i></sub> = μ, find the routing probabilities, θ<sub><i>i</i></sub>*, ∑<sub><i>i</i>=1</sub><sup><i>c</i></sup> θ<sub><i>i</i></sub>* = 1, that minimize the average total holding cost; and (b) for fixed routing probabilities θ<sub><i>i</i></sub>, ∑<sub><i>i</i>=1</sub><sup><i>c</i></sup> θ<sub><i>i</i></sub>, and total service rate μ, find the service rate allocation μ<sub><i>i</i></sub>* = μδ<sub><i>i</i></sub>*, ∑<sub><i>i</i>=1</sub><sup><i>c</i></sup> δ<sub><i>i</i></sub>* = 1, that minimizes the average total holding cost of the system. For each problem, we characterize the optimal policy under heavy traffic conditions. We also derive the routing probabilities, θ̂<sub><i>i</i></sub> (proportions δ̂<sub><i>i</i></sub>), <i>i</i> = 1, …, <i>c</i>, that are strongly asymptotically optimal. That is, the difference between the average total holding costs under θ̂<sub><i>i</i></sub>, <i>i</i> = 1, …, <i>c</i>, and θ<sub><i>i</i></sub>*, <i>i</i> = 1, …, <i>c</i> (δ̂<sub><i>i</i></sub>, <i>i</i> = 1, …, <i>c</i>, and δ<sub><i>i</i></sub>*, <i>i</i> = 1, …, <i>c</i>) is bounded by a fixed constant independent of the routing probabilities (proportions) and the arrival rate. In addition, we discuss the necessity and sufficiency of the accurate knowledge of the means and scvs of the interarrival and service times m obtaining asymptotically optimal policies.", "e:keyword": ["Queues", "Approximations", "Limit theorems", "Optimization", "Probability", "Stochastic model", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.470", "e:abstract": "We propose a new approach to analyze multiclass queueing systems in heavy traffic based on what we consider as fundamental laws in queueing systems, namely distributional and conservation laws. Methodologically, we extend the distributional laws from single class queueing systems to multiple classes and combine them with conservation laws to find the heavy traffic behavior of the following systems: (a) ∑<i>GI</i>/<i>G</i>/1 queue under FIFO, (b) ∑<i>GI</i>/<i>G</i>/1 queue with priorities, (c) Polling systems with general arrival distributions. Compared with traditional heavy traffic analysis via Brownian processes, our approach gives new insight to the asymptotics used, solves systems that traditional heavy traffic theory has not fully addressed, and, more importantly, leads to closed form answers, which compared to simulation are very accurate even for moderate traffic.", "e:keyword": ["Queues/limit theorems", "Multiclass queueing systems in heavy traffic", "Queues/cyclic", "Priority", "Multiclass priority and polling systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.3.488", "e:abstract": "We describe two optimization methods for vehicle routing problems with time windows. These are a K-Tree relaxation with time windows added as side constraints and a Lagrangian decomposition in which variable splitting is used to divide the problem into two subproblems—a semi-assignment problem and a series of shortest path problems with time windows and capacity constraints. We present optimal solutions to problems with up to 100 customers.", "e:keyword": ["Transportation", "Vehicle routing", "Networks/graphs", "Tree algorithms", "Programming", "Relaxation/subgradient"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.495", "e:abstract": "A discrete-time reaction-diffusion model for black-footed ferret release, population growth, and dispersal is combined with ferret carrying capacity constraints based on prairie dog population management decisions to form a spatial optimization model. Spatial arrangement of active prairie dog colonies within a ferret reintroduction area is optimized over time for maximum expected adult ferret population, This modeling approach is applied in an exploratory case study to a black-footed ferret reintroduction program in Badlands National Park and Buffalo Gap National Grassland, South Dakota. The model is currently being used to evaluate prairie dog population management alternatives and captive-bred ferret release locations for the Buffalo Gap National Grassland. This approach is also being adapted for use on other grasslands and with other species in the northern Great Plains.", "e:keyword": ["Natural resources", "Habitat allocation for endangered species recovery", "Probability", "Diffusion", "Dynamic spatial population optimization", "Programming", "Linear", "Applications", "a black-footed ferret case study from South Dakota"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.508", "e:abstract": "This paper is both an extension and an expansion of two earlier studies concerned with the direction of OR/MS research as reported in its archival literature (Reisman and Kirschnick [Reisman, A., F. Kirshnick. 1994. The devolution of OR/MS: Implications from a statistical content analysis of papers in flagship journals. <i>Opns. Res.</i> <b>42</b>(4) 577–588], [Reisman, A., F. Kirshnick. 1995. Research strategies used by OR/MS workers as shown by an analysis of papers in flagship journals. <i>Opns. Res.</i> <b>43</b>(5) 731–739.]). All of these papers provide a content analysis of the OR/MS archival journals. However, this paper focuses on the entire life-cycle literature of Cellular Manufacturing as a module of OR/MS. It addresses the research strategy employed by, and the theory-vs-applied orientation exhibited by, the authors. In all, 235 articles, starting in 1969, were reviewed and classified on a five-point scale, ranging from pure theory to bonafide applications. Secondly, the articles were classified in terms of seven types of research processes used by authors. Next, statistical correlations were performed relating data from the above classifications. The findings show that the literature is dominated (94 papers, or 40%) by articles classified as pure <i>theory</i> using <i>synthetic</i> “data” and the <i>ripple</i> or <i>incremental</i> process as a basic research strategy.During the first decade of cellular manufacturing research (1969 through 1978) this subset accounted for but 4 out of 42 (or 9.5%) of the papers published, whereas during the most recent decade it accounted for 81 papers (or 47.4%) of the literature. Lastly, the article raises questions about the future of cellular manufacturing research vis-à-vis other related subject matter.", "e:keyword": ["Professional", "OR/MS philosophy", "Meta research on cellular manufacturing", "Production scheduling", "Research on research"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.521", "e:abstract": "The proportional equity flow problem extends a class of problems referred to as equity flow problems whose objective is to equitably distribute flow among the arcs in a flow circulation network. The proportionally bounded flow circulation problem places lower and upper bounds on each arc flow that are nondecreasing continuous functions of the flow through one special arc, and the objective is to maximize the flow through the special arc. The proportional equity flow problem for terminal arcs (Problem TA) is then defined as a special case where all the proportional arcs enter a sink vertex. Applications of both the general problem and Problem TA are given. Two optimality conditions for Problem TA are developed, as well as an algorithm that is polynomially bounded for many types of nondecreasing, continuous, proportional bounding functions. Specifically, the algorithm is shown to be polynomially bounded if the largest root of an equation can be found in polynomial time.", "e:keyword": ["Networks/graphs", "Flow algorithms", "Proportional equity flow problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.536", "e:abstract": "We consider two different single-server cyclic polling models: (i) a model with zero switchover times, and (ii) a model with nonzero switchover times, in which the server keeps cycling when the system is empty. For both models we relate the steady-state queue length distribution at a queue to the queue length distributions at server visit beginning and visit completion instants at that queue; as a by-product we obtain a short proof of the Fuhrmann-Cooper decomposition. For the large class of polling systems that allow a multitype branching process interpretation, we expose a strong relation between the queue length, as well as waiting-time, distributions in the two models. The results enable a very efficient numerical computation of the waiting-time moments under different switchover time scenarios.", "e:keyword": ["Queues", "Cyclic", "Influence of switchover times on queue lengths and waiting times", "Probability", "Stochastic model applications", "Queue length and waiting time distribution in polling models"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.544", "e:abstract": "In open shops with renewable resources, an operation may require additional resources, besides a machine, for its execution. All resources required by the operation are allocated to it all the time during its execution. At no time may total resource requirements exceed resource capacities. We consider the problem of minimizing makespan in a two-machine open shop with a single renewable resource. We show that optimal nonpreemptive schedules are not longer than optimal preemptive schedules, which we then translate into a polynomial time algorithm for the makespan minimization problem. This is an important generalization of a well-known result obtained by Gonzalez and Sahni (Gonzalez, G., S. Sahni. 1976. Open shop scheduling to minimize finish time. <i>J. ACM</i> <b>23</b> 665–679.) for the two-machine open shop without additional resources. We also study the problem of minimizing makespan in a two-machine open shop with at least two different resources. In this case, optimal nonpreemptive schedules may be longer than optimal preemptive ones. We show that this makes the makespan minimization problem NP-hard.", "e:keyword": ["Production/scheduling", "Open shop", "Resource constraints", "Analysis of algorithms", "Computational complexity"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.553", "e:abstract": "Serial assembly systems are formed by arranging several production cells or stations in series. We study a popular class of serial assembly lines where all stations have the same production cycle. We address a workforce planning problem for such lines which finds applications in labor-intensive operations in automobile, fire engine, aircraft, and PC board assembly. The problem presented can be applied to lines that produce several variations of a basic stable design; i.e., mixed model transfer lines. Given a set of <i>n</i> jobs, we want to find a sequence that minimizes the maximum workforce requirements over all production cycles. An optimal polynomial algorithm for the two-station line is presented, and the three-station case is proved to be strongly (N-script)(P-script)-complete. Several heuristic algorithms that produce upper and lower bounds are developed for the general problem. Worst case behavior of the upper bounds, as well as average performance of lower and upper bounds, are reported. Computational results show that some of the heuristics produce near optimal solutions. As an extension of the basic model we exploit the tradeoff between cycle time and workforce level.", "e:keyword": ["Production/scheduling", "Approximations/heuristic", "Sequencing/deterministic applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.568", "e:abstract": "The <i>Covering Tour Problem</i> (CTP) is defined on a graph <i>G</i> = (<i>V</i> ∪ <i>W</i>, <i>E</i>), where <i>W</i> is a set of vertices that must be covered. The CTP consists of determining a minimum length Hamiltonian cycle on a subset of <i>V</i> such that every vertex of <i>W</i> is within a prespecified distance from the cycle. The problem is first formulated as an integer linear program, polyhedral properties of several classes of constraints are investigated, and an exact branch-and-cut algorithm is developed. A heuristic is also described. Extensive computational results are presented.", "e:keyword": ["Transportation", "Route selection", "Programming", "Integer applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.577", "e:abstract": "A set of identical machines are deployed to meet a known and constant demand. If a machine fails, a replacement part must be available before repair of the machine may be initiated. If the part is currently out of stock, it must be ordered. Once repaired at one of a finite number of repair stations, the machine serves as an “operational ready” standby if demand is currently being met; otherwise, the machine is immediately deployed. Machine time-to-failure, ordering leadtimes, and repair times are all assumed to be exponentially distributed. The objective is to determine the number of machines and repair channels that minimize a cost function subject to the service constraint; i.e., on average, the number of machines operating should be at least some fraction of the demand. We present an algorithm that efficiently generates all the boundary points of the feasible region, from which the optimal solution is readily identified, for the special cases in which there is either zero or infinite stock. Use of the algorithm, which is based upon Little's result and first-passage time analysis, requires negligible storage and computational effort. In fact, for problems of moderate size, computation of optimal solutions via a nonprogrammable pocket calculator is feasible.", "e:keyword": ["Reliability/maintenance", "Optimal number of repair stations", "Manufacturing performance", "Optimal number of machines", "Queues optimization optimal number of servers"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.584", "e:abstract": "The issue in Lot Streaming is how to split lots into sublots in order to improve the makespan (or some other criterion). We present a model and an iterative procedure for a general job-shop environment. The procedure alternates between solving a lot-sizing problem with a given sequence of sublots on the machines, and a standard job-shop scheduling problem with fixed sublot sizes. We report the computational results on a significant sample of 120 job-shop and flow-shop scheduling problems (including the famous 10–10). In case of no setup, in a few iterations, the makespan approaches a lower bound using very few sublots, suggesting that the procedure yields a global optimum. As a by-product, this result somehow validates the capacitated lot-sizing models in which the detailed capacity constraints, induced by the sequencing of operations, are ignored.", "e:keyword": ["Production/scheduling", "Sequencing", "Lot streaming"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.596", "e:abstract": "We give a unified probabilistic analysis for a general class of bin packing problems by directly analyzing corresponding mathematical programs. In this general class of packing problems, objects are described by a given number of attribute values. (Some attributes may be discrete; others may be continuous.) Bins are sets of objects, and the collection of feasible bins is merely required to satisfy some general consistency properties. We characterize the asymptotic optimal value as the value of an easily specified linear program whose size is independent of the number of objects to be packed, or as the limit of a sequence of such linear program values. We also provide bounds for the rate of convergence of the average cost to its asymptotic value. The analysis suggests an (a.s.) asymptotically ϵ-optimal heuristic that runs in linear time. The heuristics can be designed to be asymptotically optimal while still running in polynomial time. We also show that in several important cases, the algorithm has both polynomially fast convergence and polynomial running time. This heuristic consists of solving a linear program and rounding its solution up to the nearest integer vector. We show how our results can be used to analyze a general vehicle routing model with capacity and time window constraints.", "e:keyword": ["Analysis of algorithms", "Probabilistic", "Transportation", "Vehicle routing", "Production/scheduling", "Bin packing/cutting stock"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.610", "e:abstract": "This paper is concerned with the estimation of performance measures of two priority disciplines in a <i>d</i>-station re-entrant queueing network. Such networks arise from complex manufacturing systems such as wafer fabrication facilities. The priority disciplines considered are First-Buffer-First-Served (FBFS) and Last-Buffer-First-Served (LBFS). An analytical method is developed to estimate the long-run average workload at each station and the mean sojourn time in the network. When the first-buffer-first-served discipline is used, a refined estimate of the mean sojourn time is also developed. The workload estimation has two steps. In the first step, following Harrison and Williams (Harrison, J. M., R. J. Williams. 1992. Brownian models of feedforward queueing networks: Quasireversibility and product form solutions. <i>Anns. Appl. Prob.</i> <b>2</b> 263–293.), we use a <i>d</i>-dimensional reflecting Brownian motion (RBM) to model the workload process. We prove that the RBM exists and is unique in distribution and that it has a unique stationary distribution. We then use an algorithm of Dai and Harrison (Dai, J. G., J. M. Harrison. 1992. Reflected Brownian motion in an orthant: Numerical methods for steady-state analysis. <i>Anns. Appl. Prob.</i> <b>2</b> 65–86.) to compute the stationary distribution of the RBM. Our method uses both the first and second moment information, and it is rooted in heavy traffic theory. It is closely related to the QNET method of Harrison and Nguyen (Harrison, J. M., V. Nguyen. 1993. Brownian models of multiclass queueing networks: Current status and open problems. <i>Queueing Systems: Theory and Appl.</i> <b>13</b> 5–40.) for two-moment analysis of First-In-First-Out (FIFO) discipline. Our performance estimates of several example problems are compared to the simulation estimates to illustrate the effectiveness of our method.", "e:keyword": ["Queues", "Networks", "Diffusion models", "Brownian models", "Heavy traffic", "Performance measures", "Re-entrant lines", "Priority disciplines"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.624", "e:abstract": "The Tactical Fixed Interval Scheduling Problem (TFISP) is the problem of determining the <i>minimum number</i> of parallel nonidentical machines, such that a feasible schedule exists for a given set of jobs. In TFISP, each job must belong to a specific job class and must be carried out in a prespecified time interval. The problem is complicated by the restrictions that (1) each machine can handle only one job at a time, (2) each machine can handle only jobs from a subset of the job classes, and (3) preemption is not allowed. In this paper we discuss the occurrence of TFISP in practice, we analyze the computational complexity of TFISP, and we present exact and approximation algorithms for solving TFISP. The paper concludes with a computational study.", "e:keyword": ["Programming", "Integer", "Production/scheduling", "Approximations", "Heuristics", "Analysis of algorithms", "Computational complexity"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.4.639", "e:abstract": "The Traveling Salesman Problem with Backhauls (TSPB) is defined on a graph <i>G</i> = (<i>V</i>, <i>E</i>). The vertex set is partitioned into <i>V</i> = ({<i>v</i><sub>1</sub>}, <i>L</i>, <i>B</i>), where <i>v</i><sub>1</sub> is a depot, <i>L</i> is a set of linehaul customers, and <i>B</i> is a set of backhaul customers. A cost matrix satisfying the triangle inequality is defined on the edge set <i>E</i>. The TSPB consists of determining a least-cost Hamiltonian cycle on <i>G</i> such that all vertices of <i>L</i> are visited contiguously after <i>v</i><sub>1</sub>, followed by all vertices of <i>B</i>. Following a result by Christofides for the Traveling Salesman Problem, we propose an approximation algorithm with worst-case performance ratio of 3/2 for the TSPB.", "e:keyword": ["Analysis of algorithms", "Combinatorial problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.649", "e:abstract": "The construction of flight attendant (cabin crew) rosters for short-haul (domestic) airline flight services that satisfies rostering constraints and employment contract regulations is a combinatorially complex problem. In this paper the problem is described and an effective optimisation-based solution method is introduced. The rostering problem involves the allocation of <i>days-off</i> and various <i>duties</i> to each crew member over a roster period. The days-off and the duty allocation problems are separated into two distinct subproblems. The days-off allocation solution approach involves complete enumeration of all possible <i>days-off lines</i> for each crew member over the roster period, and then the solution of a set partitioning optimisation to determine a best quality feasible <i>days-off roster</i>. The duty allocation solution approach first involves the generation of many <i>lines-of-work</i> consistent with the days-off solution for each crew member over a <i>subroster</i> period and then the solution of a set partitioning optimisation to determine an optimal feasible subroster. These two steps of generation and optimisation are repeated for each subsequent subroster period until a full legal and feasible roster is constructed for the complete roster period. The use of subrosters reduces the combinatorial complexity resulting in problems that can be solved efficiently. After construction of the initial roster, the quality can often be improved using <i>re-rostering</i> techniques. The method leads to efficient construction of good quality legal rosters, and has been used to produce all short-haul flight attendant rosters at Air New Zealand since 1993.", "e:keyword": ["Transportation", "Scheduling", "Personnel", "Flight attendant rostering", "Mathematics", "Combinatorics", "Set partitioning"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.662", "e:abstract": "This paper describes the development of an air mission planning algorithm for the Joint Stochastic Warfare Analysis Research (JSTOCHWAR). The overall problem scope was to develop an algorithm to handle major factors bearing on the combat mission planning problem while providing hook-ups for the JSTOCHWAR architecture. Other aspects of the problem included finding the appropriate level of detail, developing a fast solving technique, and attempting to use existing data. The problem was handled by using some ideas from existing aircraft allocation algorithms and by adding some new techniques.The proposed air mission planning algorithm supplies the optimum degree of force for campaign objectives by using a mixed integer program (MIP) to allocate the optimum number and type of aircraft and munitions against each target. The MIP takes advantage of the force multiplying effects of mass and mutual support through its use of strike packages with air defense suppression and air-to-air escort aircraft. Additionally, a decision tree algorithm determines the best plan in light of the uncertainties of weather and weather forecasts.This air mission planning algorithm omits many of the details in the actual aircraft tasking process, but in its continuous variable version, provides fast, nearly optimal solutions that should approximate real-world tasking results.", "e:keyword": ["Decision analysis", "Applications", "Influence diagrams and decision trees", "Military", "Targeting", "Theater level air mission planning", "Programming", "Integer", "Assignment of aircraft and weapons to targets"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.677", "e:abstract": "We consider a general class of queueing systems with multiple job types and a flexible service facility. The arrival times and sizes of incoming jobs are random, and correlations among the sizes of arriving job types are allowed. By choosing among a finite set of configurations, the facility can dynamically control the rates at which it serves the various job types. We define system <i>work</i> at any given time as the minimum time required to process all jobs currently in the backlog. This quantity is determined by solving a linear program defined by the set of processing configurations.The problem we study is how to dynamically choose configurations to minimize the time average system work. Using bounds and heuristics, we analyze a class of service policies that is provably asymptotically optimal as system utilization approaches one, as well as a policy that in numerical studies performs near-optimally in moderate traffic. Our analysis also yields a closed-form expression for the optimal, average work in heavy traffic.This general problem has a number of applications in job shop and flexible manufacturing, in service organizations, and in the management of parallel processing and distributed database systems.", "e:keyword": ["Queues", "Multiclass", "Flexible", "Queues", "Limit theorems", "Heavy traffic", "Optimal expected system work", "Production/scheduling", "Approximations/heuristics", "Job shops", "Flexible manufacturing/service systems", "Dynamic resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.694", "e:abstract": "In this paper we study a (S-1, S) type multiechelon inventory system where all the stocking locations have the option to replenish their inventory through either a normal or a more expensive emergency resupply channel. When ordering a unit, each stocking location decides which channel to use based on its inventory level and the remaining leadtimes of the its outstanding orders. We consider the implications of this policy by developing expressions for the operating characteristics of the system and describing a procedure for finding the optimal policy parameters. A numerical study presented in the paper indicates that when alternative resupply modes are available our policy, which incorporates the information on the remaining leadtimes of the outstanding orders in selecting the resupply mode, can result in considerable cost savings when compared to policies which allow a single resupply mode.", "e:keyword": ["Inventory", "Multiechelon", "Emergency replenishments", "Inventory", "Policies", "Order-for-order policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.702", "e:abstract": "We study the problem of scheduling a chain-reentrant shop, in which each job goes for its processing first to a machine called the primary machine, then to a number of other machines in a fixed sequence, and finally back to the primary machine for its last operation. The problem is to schedule the jobs so as to minimize the makespan. This problem is unary <i>NP</i>-hard for a general number of machines. We focus in particular on the two-machine case that is also at least binary <i>NP</i>-hard. We prove some properties that identify a specific class of optimal schedules, and then use these properties in designing an approximation algorithm and a branch-and-bound type optimization algorithm. The approximation algorithm, of which we present three versions, has a worst-case performance guarantee of 3/2 along with an excellent empirical performance. The optimization algorithm solves large instances quickly. Finally, we identify a few well solvable special cases and present a pseudo-polynomial algorithm for the case in which the first and the last operations of any job (on the primary machine) are identical.", "e:keyword": ["Production/scheduling", "Deterministic", "Reentrant shop scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.713", "e:abstract": "We consider a single stage production system with Poisson demand and exponential processing times. After producing a good item, the production process can shift to an “out-of-control” state with a given probability and start producing bad items. The state of the process is known only when the next stage (or customer) receives the item. Once an out-of-control process is detected, process correction is instantaneous. Customers arriving to an empty system get backlogged. In this framework, we examine FIFO (First In First Out) and LIFO (Last In First Out) issuing policies. The objective is to minimize the total expected discounted or average costs over an infinite time horizon. We characterize the structure of the optimal production policy for FIFO and LIFO, show that LIFO is better than FIFO and, in general, better than a large class of issuing policies. A numerical example illustrates that savings up to 20 percent can be obtained from using LIFO over FIFO. We also derive conditions under which maintaining <i>zero inventory</i> is optimal, and show that zero inventory is more likely to be optimal when either the backlogging cost or arrival rate of customers is small, and when the inventory carrying cost or the processing rate or the probability of getting a good item is large.", "e:keyword": ["Inventory/production", "Optimal inventory", "Zero inventory", "Quality", "Process shifts", "Dynamic programming/control", "Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.725", "e:abstract": "This paper presents new results for the interdeparture time distributions in single-server priority queues with an arbitrary number of high-priority Poisson-arrival classes and one lowest-priority general-arrival class. Little-known results for the waiting time distributions in the same queue are presented, including a new proof for the powerful result that the waiting time distribution in the non-preemptive priority queue is insensitive to the interarrival time distributions of lower-priority classes, beyond their mean.", "e:keyword": ["Queues", "Priority", "Waiting times and interdeparture times"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.736", "e:abstract": "Consider a single-item, periodic review, infinite-horizon, undiscounted, inventory model with stochastic demands, proportional holding and shortage costs, and full backlogging. Orders can arrive in every period, and the cost of receiving them is negligible (as in a JIT setting). Every <i>T</i> periods, one observes the current stock level and orders deliveries for the next <i>T</i> periods, thus incurring a fixed setup cost. The goal is to find a review period <i>T</i> and an ordering policy that minimize the long run expected average cost per period. Flynn and Garstka (Flynn, J., S. Garstka.", "e:keyword": ["Inventory/production", "Optimal review period", "Inventory/production", "Policies", "Periodic review", "Inventory/production", "Stochastic inventory model"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.751", "e:abstract": "In this paper we further investigate the problem of finding nadir criterion values (minimum criterion values over the nondominated set) in multiple objective linear programming. Although easy to obtain, the minimum values present in a payoff table are unreliable and should only be used with caution, especially in problems that have more than a small number of extreme points. To obtain better estimates of the nadir criterion values without adding great complexity to the task, we present an approach based upon the use of reference directions. At each iteration of this approach, a reference direction is chosen that maximally minimizes the objective under consideration. We proceed with reference directions that accomplish this until the objective under consideration reaches a local minimum over the nondominated set. Then a cutting plane is inserted into the problem and another direction, if one can be found, that maximally minimizes the objective under consideration is employed. Although the method is heuristic, computational experience shows that much better estimates of the nadir criterion values can be obtained than from payoff tables.", "e:keyword": ["Programming", "Multiple criteria", "Nadir solution", "Programming", "Linear", "Nonconvex set"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.758", "e:abstract": "Several types of large-sized 0-1 Knapsack Problems (KP) may be easily solved, but in such cases most of the computational effort is used for sorting and reduction. In order to avoid this problem it has been proposed to solve the so-called <i>core</i> of the problem: a Knapsack Problem defined on a small subset of the variables. The exact core cannot, however, be identified before KP is solved to optimality, thus, previous algorithms had to rely on approximate core sizes.In this paper we present an algorithm for KP where the enumerated core size is minimal, and the computational effort for sorting and reduction also is limited according to a hierarchy. The algorithm is based on a dynamic programming approach, where the core size is extended by need, and the sorting and reduction is performed in a similar “lazy” way.Computational experiments are presented for several commonly occurring types of data instances. Experience from these tests indicate that the presented approach outperforms any known algorithm for KP, having very stable solution times.", "e:keyword": ["Programming", "Integer", "Algorithms", "Dynamic programming algorithm for knapsack problem", "Dynamic programming", "Knapsack problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.768", "e:abstract": "It is well-known that many instances of the 0-1 knapsack problem can be effectively solved to optimality also for very large values of <i>n</i> (the number of binary variables), while other instances cannot be solved for <i>n</i> equal to only a few hundreds. We propose upper bounds obtained from the mathematical model of the problem by adding valid inequalities on the cardinality of an optimal solution, and relaxing it in a Lagrangian fashion. We then introduce a specialized iterative technique for determining the optimal Lagrangian multipliers in polynomial time. A branch-and-bound algorithm is finally developed. Computational experiments prove that several classes of hard instances are effectively solved even for large values of <i>n</i>.", "e:keyword": ["Programming", "Integer", "Algorithms", "Branch-and-bound", "0-1 knapsack problem", "Programming", "Integer", "Algorithms", "Relaxation/subgradient", "Valid inequalities", "Lagrangian relaxations"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.5.779", "e:abstract": "A frequently encountered scheduling problem is to determine simultaneously a material and job ready time and production sequence based on customer-specified due dates. Each job has a stochastic production time and a deterministic due date. The ready time is constrained in that the probability that each job will be complete by its due date must meet some minimum level of confidence. The objective in such an instance is to postpone the ready time as late as possible without violating these constraints. The steps and effort necessary to determine the maximum ready time and optimal production sequence, and cases in which this effort may be significantly reduced are presented. The resulting model is applied directly to single-facility and flow-shop production environments. Methods are shown for scheduling in a dynamic environment.", "e:keyword": ["Production/scheduling", "Stochastic sequencing probabilities", "Scheduling facilities with stochastic service", "Probability", "Stochastic models", "Service time data probabilistically distributed", "Networks", "Stochastic", "Shop modeled as stochastic networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.789", "e:abstract": "We describe a scheduling system for the curing operation at Bridgestone/Firestone Off-The-Road (BFOR), a manufacturer of large tires for heavy off-the-road machines such as trucks, tractors, and earthmoving equipment used in the construction, lumber, and mining industries. The huge tires, having different priorities, are built in molds and put into heaters for the curing process. The problem is to find a feasible assignment of tires to molds and molds to heaters to achieve a maximum total priority. Our system produces about 7% more tires <i>per shift</i> compared to the previous manual way of developing schedules and moves the company toward its goal of quick response manufacturing with low inventories. The core algorithm is a column generation procedure to produce a production schedule for one work shift. This approach is then used in a “rolling horizon” fashion. The size of the problem posed several computational challenges. Computational efficiencies come from (1) a dramatic effort to eliminate alternative solutions up front in an aggregation preprocessor, (2) considering the most “interesting” tires to be scheduled subsequently at each point of the algorithm, and (3) using cutting planes to provide for tight LP formulations for the subproblems.", "e:keyword": ["Production/scheduling", "Applications", "Planning", "Programming", "Integer", "Algorithms", "Decomposition", "Dantzig-Wolfe"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.797", "e:abstract": "The original concept for this paper comes from my 1994 Omega Rho lecture in which I described the concept of a Factory-in-the-Computer<sup>©</sup> as a set of models, primarily simulation-based, that could be embedded in a manufacturing system and be a part of an executive decision support system. In attempting to write a paper based on this Omega Rho presentation, it became evident that it was necessary to include information on basic processes related to the use of modeling and simulation. The end result was the writing of a paper which addresses the process for enhancing performance and issues related to the use of modeling in such processes.", "e:keyword": ["Philosophy of modeling", "Relation to scientific method", "Simulation", "Languages", "Development steps", "Technology", "Process evolution from systems approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.805", "e:abstract": "We focus on the problem of buying fashion goods for the “big book” of a catalogue merchandiser. This company also owns outlet stores and thus has the opportunity, as the season evolves, to divert inventory originally purchased for the big book to the outlet store. The obvious questions are: (1) how much to order originally, and (2) how much to divert to the outlet store as actual demand is observed.We develop a model of demand for an individual item. The model is motivated by data from the women's designer fashion department and uses both historical data and buyer judgement. We build a stochastic dynamic programming (DP) model of the fashion buying problem that incorporates the model of demand. The DP model is used to derive the structure of the optimal inventory control policy. We then develop an updated Newsboy heuristic that is intuitively appealing and easily implemented. When this heuristic is compared to the optimal solution for a wide variety of scenarios, we observe that it performs very well. Similar numerical experiments show that the current company practice does not yield consistently good results when compared to the optimal solution.", "e:keyword": ["Inventory/production", "Stochastic models", "Bayesian updates"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.820", "e:abstract": "This paper examines the optimality of inventory “balancing” in a one-warehouse <i>N</i>-retailer distribution system facing stochastic demand for a single product over <i>T</i> successive time intervals. In particular, we consider the division of predetermined quantities of warehouse stock among retailers in each interval. Balancing attempts to bring the retailer-inventories to the same (normalized with respect to first-interval demand) net inventory level. When the demand distribution over the <i>T</i> periods is symmetric with respect to all pairs of retailers, and retailer cost over the <i>T</i>-intervals is a convex function of the <i>T</i> shipments, we show that balancing divisions are optimal. The required convexity condition is shown to be satisfied for some familiar cost functions when shortages are backordered and lost. However for other cost functions, the convexity condition is satisfied under backordering, but not under lost sales. We also consider the nonidentical-retailer (demands are not symmetric) case and provide examples to show that balancing is generally not optimal, even for relatively common, simple cost functions.", "e:keyword": ["Inventory multiechelon/stage", "Optimality of allocation policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.831", "e:abstract": "The generalized assignment problem examines the maximum profit assignment of jobs to agents such that each job is assigned to precisely one agent subject to capacity restrictions on the agents. A new algorithm for the generalized assignment problem is presented that employs both column generation and branch-and-bound to obtain optimal integer solutions to a set partitioning formulation of the problem.", "e:keyword": ["Integer programming", "Optimization algorithm for the GAP"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.842", "e:abstract": "We present a new approach to automatic test pattern generation for very large scale integrated sequential circuit testing. This approach is more efficient than past test generation methods, since it exploits knowledge of potential circuit defects. Our method motivates a new combinatorial optimization problem, the <i>Tour Covering Problem</i>. We develop heuristics to solve this optimization problem, then apply these heuristics as new test generation procedures. An empirical study comparing our heuristics to existing methods demonstrates the superiority of our approach, since our approach decreases the number of input vectors required for the test, translating into a reduction in the time and money required for testing sequential circuits.", "e:keyword": ["Industry", "Computer electronic", "Semiconductor manufacturing", "Manufacturing performance/productivity", "Improved quality control by effective testing", "Programming integer heuristic", "Heuristic approach for the tour cover problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.857", "e:abstract": "We introduce an analytical model for the design of a multiple-vehicle automated guided vehicle system (AGVS) with multiple-load capacity operating under a “go-when-filled” dispatching rule. The AGVS delivers containers of material from a central depot to workcenters throughout the factory floor. The workcenters are partitioned into delivery zones. They are served by a common pool of automated guided vehicles (AGVs), each of which can carry multiple orders per delivery. The demand of the workcenters and the time until delivery are stochastic. We develop a nonlinear binary integer program to determine the optimal partition of workcenters into zones, the optimal number of AGVs to purchase, and the set of workcenters that warrant AGV delivery, subject to constraints on maximum allowable mean waiting time for material delivery. We develop an analytical expression for the mean waiting time until material delivery and present an efficient branch-and-bound algorithm that solves the AGVS design model optimally. Without the analytical solution method, one would have to simulate the system for all zoning options, all combinations of open and closed workcenters, and all possible numbers of AGVs, in order to determine the optimal AGVS design—an approach likely to be infeasible for most problems.", "e:keyword": ["Manufacturing", "Automated systems", "Queuing systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.874", "e:abstract": "This paper addresses a problem of batch scheduling which arises in the burn-in stage of semiconductor manufacturing. Burn-in ovens are modeled as batch-processing machines which can handle up to <i>B</i> jobs simultaneously. The processing time of a batch is equal to the longest processing time among the jobs in the batch. The scheduling problem involves assigning jobs to batches and determining the batch sequence so as to minimize the total flowtime. In practice, there is a small number <i>m</i> of distinct job types. Previously, the only solution techniques known for the single-machine version of this problem were an <i>O</i>(<i>m</i><sup>2</sup>3<i>B</i><sup><i>m</i>+1</sup>) pseudopolynomial algorithm, and a branch-and-bound procedure. We present an algorithm with a running time of <i>O</i>(<i>m</i><sup>2</sup>3<sup><i>m</i></sup>), which is independent of <i>B</i>, the maximum batch size. We also present a polynomial heuristic for the general problem (when <i>m</i> is not fixed), which is a two-approximation algorithm. For any problem instance, this heuristic provides a solution at least as good as that given by previously known heuristics. Finally, we address the <i>m</i>-type problem on parallel machines, providing an exact pseudopolynomial algorithm and a polynomial approximation algorithm with a performance guarantee of (1 + √2)/2.", "e:keyword": ["Production/scheduling", "Applications", "Semiconductor burn-in operations", "Dynamic programming", "Applications", "Scheduling semiconductor burn-in operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.886", "e:abstract": "In this paper we propose a differential game theoretic model to analyze the design quality and pricing decisions for a product over the product life cycle. The market is represented as a duopoly where our product competes against a similar product marketed by our competition. The duopolistic competition is modeled as a two-player nonzero sum noncooperative game. The resulting control theoretic model is solved under certain functional assumptions to describe the Nash equilibrium strategies of the two competitors. Finally, observations made on an extensive set of numerical experimentations over a variety of model parameters are summarized into a set of relevant managerial guidelines on developing design quality and pricing strategies.", "e:keyword": ["Manufacturing", "Strategy", "Quality management", "Games", "Noncooperative", "Differential", "Optimal control", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.894", "e:abstract": "Cyclic or periodic schedules can be implemented in a job shop where demands for various products have a stable rate and mix. Numerous results on cyclic scheduling in deterministic settings are available, but studies considering uncertainties such as machine failure are rare. This paper examines the behavior of cyclic schedules in a stochastic environment characterized by random machine failures that may delay execution of tasks and thus may cause the actual production to deviate from a specified cyclic schedule. The authors intend to understand the behavior of cyclic schedules under uncertainty and to find those cyclic schedules that are the least disturbed by occurrences of machine failure. The cyclic scheduling problem of one or multiple machines can be formulated into a convex program for which the objective is to minimize a weighted sum of expected ergodic delays experienced by tasks. For one-machine schedules, the ergodic distributions of delays, as well as their expected values, are displayed. For multiple-machine schedules, a necessary condition for ergodicity is presented and stochastic lower bounds on task delays are derived.", "e:keyword": ["Production/scheduling", "Stochastic cyclic schedules"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.904", "e:abstract": "This paper considers a stochastic inventory model in which supply availability is subject to random fluctuations that may arise due to machine breakdowns, strikes, embargoes, etc. It is assumed that the inventory manager deals with two suppliers who may be either individually ON (available) or OFF (unavailable). Each supplier's availability is modeled as a semi-Markov (alternating renewal) process. We assume that the durations of the ON periods for the two suppliers are distributed as Erlang random variables. The OFF periods for each supplier have a general distribution. In analogy with queuing notation, we call this an <i>E</i><sub><i>s</i><sub>1</sub></sub>[<i>E</i><sub><i>s</i><sub>2</sub></sub>]/<i>G</i><sub>1</sub>[<i>G</i><sub>2</sub>] system. Since the resulting stochastic process is non-Markovian, we employ the “method of stages” to transform the process into a Markovian one, albeit at the cost of enlarging the state space. We identify the regenerative cycles of the inventory level process and use the renewal reward theorem to form the long-run average cost objective function. Finite time transition functions for the semi-Markov process are computed numerically using a direct method of solving a system of integral equations representing these functions. A detailed numerical example is presented for the <i>E</i><sub>2</sub>[<i>E</i><sub>2</sub>]/<i>M</i>[<i>M</i>] case. Analytic solutions are obtained for the particular case of “large” (asymptotic) order quantity, in which case the objective function assumes a very simple form that can be used to analyze the optimality conditions. The paper concludes with the discussion of an alternative inventory policy for modeling the random supply availability problem.", "e:keyword": ["Inventory/production", "Supplier uncertainty", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.919", "e:abstract": "This paper develops a simple but effective heuristic policy for a stochastic production-inventory system, where several products share a single processor of limited capacity. Our policy combines the best features, and avoids the weaknesses, of prior approaches in the literature. Limited numerical tests suggest that the method performs well.", "e:keyword": ["Production/scheduling", "Approximations/heuristic", "Queues", "Make-to-stock"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.931", "e:abstract": "This paper is concerned with a generalization of classical inventory models (with fixed ordering costs) that exhibit (<i>s</i>, <i>S</i>) policies. In our model, the distribution of demands in successive periods is dependent on a Markov chain. The model includes the case of cyclic or seasonal demand. The model is further extended to incorporate some other realistic features such as no ordering periods and storage and service level constraints. Both finite and infinite horizon nonstationary problems are considered. We show that (<i>s</i>, <i>S</i>) policies are also optimal for the generalized model as well as its extensions.", "e:keyword": ["Inventory models with Markovian demand", "Optimality of (s", "S) policies", "Models with fixed ordering costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.940", "e:abstract": "In the previous work, we have shown that for a production/inventory system arranged in series with backlogging at its final product, the total cost of the best power-of-two frequency lot-size heuristic is within 6% of the optimal (or 2% if the base period is allowed to vary). In this paper, we extend our results to an assembly production/inventory system with constant external demand at its final product with backlogging allowed. By using a submodular property, we show that the total cost of any feasible policy is bounded below by finding the minimum of a set of series systems. In this way, we can get a best power-of-two frequency policy that is within 2% of the optimal: However, the number of series systems to be considered can be proportional to, in the worst case, the factorial of <i>n</i> (the number of nodes in the assembly system). As a consequence, this reduction cannot give us a polynomial algorithm and we have to use a different approach. By using a series of transformations, we reduce our problem to a special case of a polymatroidal network flow problem. The lower bound and the optimal power-of-two frequency policy for assembly systems with backlogging can then be found in <i>O</i>(<i>n</i><sup>6</sup>) time.", "e:keyword": ["Inventory/production", "Multiechelon lot sizing", "Backlogging"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.952", "e:abstract": "We consider a robotic flowshop in which one type of product is to be repeatedly produced, and where transportation of the parts between the machines is performed by a robot. The identical parts cyclic scheduling problem is then to find a shortest cyclic schedule for the robot; i.e., a sequence of robot moves that can be infinitely repeated and that has minimum cycle time. This problem has been solved by Sethi et al. (Sethi, S. P., C. Sriskandarajah, G. Sorger, J. Blazewicz, W. Kubiak. 1992. Sequencing of parts and robot moves in a robotic cell. <i>Internat. J. Flexible Manufacturing Systems</i> <b>4</b> 331–358.) when <i>m</i> ≤ 3. In this paper, we generalize their results by proving that the identical parts cyclic scheduling problem can be solved in time polynomial in <i>m</i>, where <i>m</i> denotes the number of machines in the shop. In particular, we present a dynamic programming approach that allows us to solve the problem in <i>O</i>(<i>m</i><sup>3</sup>) time. Our analysis relies heavily on the concept of pyramidal permutation, a concept previously investigated in connection with the traveling salesman problem.", "e:keyword": ["Manufacturing", "Automated systems", "Materials handling in robotic cells", "Production/scheduling", "Sequencing", "Flow shop", "Cycle time minimization", "Dynamic programming", "Deterministic", "Traveling salesman", "Pyramidal permutations"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.966", "e:abstract": "Multiplicity of solutions is typical for systems where the individual's tendency to act in a certain way increases when more of the other individuals in the population act in this way. We provide a detailed analysis of a queueing model in which two priority levels can be purchased. In particular, we compute all of the Nash equilibrium strategies (pure and mixed) of the threshold type.", "e:keyword": ["Queues", "Markovian", "Priority", "Balking and reneging"]}, {"@id": "http://dx.doi.org/10.1287/opre.45.6.974", "e:abstract": "We consider a single inventory item that is graded into one of two quality levels after production. The proportion of grade one units produced in a lot is a random variable assumed to follow the lognormal distribution. Demands for grade 2 may be filled with grade 1 product, but not vice versa. Assuming that demands for the two grades are known and constant, we develop a continuous review model in which cycles are defined as times when total inventory equals zero. Since grade 1 inventory may be depleted before total stock and shortages are not permitted, cycles may consist of multiple set-ups. We show that the optimal order-to-point, <i>S</i>, has a form similar to the EOQ. Tables of the CDF of an appropriately defined unit normal multivariate distribution are incorporated into a spreadsheet to facilitate calculations for any parameter setting.", "e:keyword": ["Inventory/production", "Stochastic", "One-way substitutions", "Production/scheduling", "Random grading after production"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.1", "e:abstract": "The nine universities in the Atlantic Coast Conference (ACC) have a basketball competition in which each school plays home and away games against each other over a nine-week period. The creation of a suitable schedule is a very difficult problem with a myriad of conflicting requirements and preferences. We develop an approach to scheduling problems that uses a combination of integer programming and enumerative techniques. Our approach yields reasonable schedules very quickly and gave a schedule that was accepted by the ACC for play in 1997-1998.", "e:keyword": ["Industries", "Recreation/sports", "College basketball", "Production/scheduling", "Applications", "Sports scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.107", "e:abstract": "A batch of products is to be supplied to customers with warranty. The units in the batch are either defective or nondefective, with different lifetime distributions. The defect rate—the proportion of defects in the batch—is itself a random variable, known only in terms of its distribution. We develop a sequential quality control procedure that exploits the knowledge of the defect distribution gained through inspection, and strikes an optimal balance between the inspection repair cost and the warranty cost. We identify a simple threshold policy, and we prove its optimality for a very general class of warranty cost functions without imposing any restrictions on the type of distributions involved. The key to optimality is that the warranty cost, as a function of the number of inspected units and the conditional defect index, satisfies a so-called <i>K</i>-submodularity property, which is a strengthening of the usual notion of submodularity.", "e:keyword": ["Stochastic model applications", "Markov decision program", "Quality control", "Sequential inspection"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.116", "e:abstract": "As the computer, communication, and entertainment industries begin to integrate phone, cable, and video services and to invest in new technologies such as fiber-optic cables, interruptions in service can cause considerable customer dissatisfaction and even be catastrophic. In this environment, network providers want to offer high levels of service—in both serviceability (e.g., high bandwidth) and survivability (failure protection)—and to segment their markets, providing better technology and more robust configurations to certain key customers. We study core models with three types of customers (primary, primary but critical, and secondary) and two types of services/technologies (primary and secondary). The network must connect all primary customers using primary (high bandwidth) services and, additionally, contain a back-up path connecting the critical primary customers. Secondary customers require only single connectivity to other customers and can use either primary or secondary facilities. We propose a general multi-tier survivable network design model to configure cost effective networks for this type of market segmentation. When costs are triangular, we show how to optimally solve single-tier subproblems, with two critical customers, as a matroid intersection problem. We also propose and analyze the worst-case performance of tailored heuristics for several special cases of the two-tier model. Depending upon the particular problem setting, the heuristics have worst-case performance ratios ranging between 1.25 and 2.6. We also provide examples to show that the performance ratios for these heuristics are the best possible.", "e:keyword": ["Networks/graphs", "Heuristics", "Analysis for hierarchical", "Survivable applications", "Networks/graphs", "Tree algorithms", "Hierarchical", "Survivable networks", "Programming", "Integer", "Heuristics", "Multifacility", "Reliable telecommunications networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.137", "e:abstract": "We consider a generalized vacation or polling system, modeled as an input-output process operating over successive “cycles,” in which the service mechanism can be in an “up” mode (processing) or “down” mode (e.g., vacation, walking). Our primary motivation is polling systems, in which there are several queues and the server moves cyclically between them providing some service in each. Our basic assumption is that the amount of work that leaves the system in a “cycle” is no less than the amount present at the beginning of the cycle. This includes the standard gated and exhaustive policies for polling systems in which a cycle begins whenever the server arrives at some prespecified queue. The input and output processes satisfy model-dependent conditions: pathwise bounds on the average rate and the burstiness (Cruz bounds); existence of long-run average rates; a pathwise generalized Law of the Iterated Logarithm; or exponentially or polynomially bounded tail probabilities of burstiness. In each model we show that these properties are inherited by performance measures such as the workload and output processes, and that the system is stable (in a model-dependent sense) if the input rate is smaller than the up-mode processing rate.", "e:keyword": ["Queues", "Polling and vacation systems", "Pathwise analysis", "Performance bounds", "Communications", "Token-ring local area networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.149", "e:abstract": "Discrete-time queueing systems are frequently encountered in telecommunication networks. These systems usually involve finite buffers. The purpose of this paper is to present a generally applicable method to compute the loss probability in discrete-time finite-buffer queues by using only the state probabilities in the corresponding infinite-buffer queue.", "e:keyword": ["Queues", "Approximations", "Loss probability in discrete-time"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.155", "e:abstract": "In this paper we address the problem of locating <i>p</i> mobile service units in an <i>n</i>-dimensional space minimizing the expected response time. It is shown that an optimal solution to the problem is a degenerate distribution for the service, concluding that it is optimal to park the <i>p</i> units. This extends previous results in the literature for location on a segment.", "e:keyword": ["Facilities/equipment planning", "Location", "Continuous", "Multifacility location", "Facilities/equipment planning", "Location", "Stochastic", "Location of mobile units"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.157", "e:abstract": "The paper discusses Max-Min Facility Dispersion (MMFD) and Max-Avg Facility Dispersion (MAFD) problems. These problems are the subject of a paper by Ravi et al. (1994). The results presented in that paper include a simple heuristic for MMFD which provides a performance guarantee of 1/2, and a similar heuristic for MAFD with a performance guarantee of 1/4. It is also proved there that obtaining a performance guarantee of more than 1/2 for MMFD is NP-hard. For the two-dimensional version of MAFD, Ravi et al.'s paper presents a heuristic with an asymptotic performance guarantee of 2/<i>p</i>. Further additional comments in this direction are given.", "e:keyword": ["Analysis of algorithms 011", "Heuristics 485", "Facility location 185"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.17", "e:abstract": "The Dynamic and Stochastic Knapsack Problem (DSKP) is defined as follows. Items arrive according to a Poisson process in time. Each item has a demand (size) for a limited resource (the knapsack) and an associated reward. The resource requirements and rewards are jointly distributed according to a known probability distribution and become known at the time of the item's arrival. Items can be either accepted or rejected. If an item is accepted, the item's reward is received; and if an item is rejected, a penalty is paid. The problem can be stopped at any time, at which time a terminal value is received, which may depend on the amount of resource remaining. Given the waiting cost and the time horizon of the problem, the objective is to determine the optimal policy that maximizes the expected value (rewards minus costs) accumulated. Assuming that all items have equal sizes but random rewards, optimal solutions are derived for a variety of cost structures and time horizons, and recursive algorithms for computing them are developed. Optimal closed-form solutions are obtained for special cases. The DSKP has applications in freight transportation, in scheduling of batch processors, in selling of assets, and in selection of investment projects.", "e:keyword": ["Transportation", "Dynamic and stochastic loading", "Resource allocation", "On-line in a probabilistic environment"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.36", "e:abstract": "The production control problem in unreliable manufacturing systems has generated a great deal of interest recently. The goal is the real-time calculation of production rates to minimize total expected surplus and backlog costs in a production system. A class of feedback control policies, called hedging point policies, has been studied in literature and shown to be optimal for certain single-part-type systems. This paper focuses mainly on the performance of hedging point policies in two-part-type systems. We first generalize known stability results for hedging point policies in single-part-type systems and extend them to two-part-type systems. An average cost analysis of trajectories is then used to partially characterize an optimal policy belonging to this class. A complete characterization is obtained for special parameter values. The results in this paper provide insights for designing simple yet effective heuristics for controlling manufacturing systems producing many part-types.", "e:keyword": ["Manufacturing", "Productivity", "Scheduling unreliable manufacturing", "Dynamic programming", "Application", "Scheduling multipart system", "Production/scheduling", "Stochastic", "Manufacturing systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.46", "e:abstract": "We consider the problem of a service facility that offers <i>m</i> different priority classes to its <i>n</i> customers, and sets a tariff (price-schedule) to maximize profit. Each customer must use the service facility and will select an output rate and the priority classes used to maximize profit. The problem is formulated as a noncooperative game. We show that equilibrium output rates and assignments to priority classes can be determined by solving the integrated system problem (ISP) that maximizes the joint profit of all agents. The service facility can be represented by a network, and ISP is a traffic assignment problem. A tariff schedule that induces each customer to select the integrated system output rate and assignment is said to be vertically efficient. We show that the server can select vertically efficient tariffs that result in an equilibrium. Optimal pricing for the special case where the service facility is modeled as a priority queue is discussed. Examples are included to illustrate the results.", "e:keyword": ["Services", "Pricing of priority services", "Transportation", "Models", "Priority service", "Queues", "Priority", "Pricing of service operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.57", "e:abstract": "Congestion in the air traffic network is becoming an increasingly serious problem that causes inconvenience to passengers, losses to airlines and, last but not least, threats to airspace safety. One way of reducing the amount of congestion is to use Ground Holding policies, i.e., to impose on selected aircraft a ground holding prior to their departure so that congestion during peak periods of time may be smoothed away. In this paper we restrict our attention to the Multiairport Ground Holding problem, where congestion may arise only at the airports due to limited arrival capacity. There are a few algorithms that, under suitable hypotheses, find an “optimal” policy for the Multiairport Ground Holding problem. In this paper we evaluate and compare computationally three of them, namely, the one recently proposed by Vranas, Bertsimas and Odoni, the one suggested by Andreatta and Tidona and that due to Bertsimas and Stock. The computational evaluation is based on two sets of test problems. The first set consists of seven problems taken from the literature. The second set consists of 32 “realistic” test problems. The results indicate the superiority of the Bertsimas and Stock approach among the three models considered.", "e:keyword": ["Programming", "Integer", "Models", "Applications", "Transportation", "Air traffic"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.65", "e:abstract": "The problem of scheduling precedence-constrained task systems characterized by interprocessor communication delays is addressed. It is assumed that task duplication is permitted. The target machine is a homogenous multiprocessor with an unbounded number of processors. The general problem is known to be NP-hard; however, when communication delays are small relative to task execution times, the C.P.M. based approach of Colin and Chrétienne (1991) yields an optimal schedule in polynomial time. Extensions to this method of Colin and Chrétienne are presented here, which allow for polynomial-time optimal schedule generation for certain categories of task systems with arbitrary precedence relations, processing times, and communication delays.", "e:keyword": ["Computers", "Task allocation on distributed memory processors", "Production/scheduling", "Scheduling with interprocessor communication delays"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.73", "e:abstract": "In this paper, we introduce evidence propagation operations on influence diagrams, a concept of the value of evidence to measure the impact/value of new observations/experimentation, and a concept of the value of revelation. Evidence propagation operations are critical for the computation of the value of evidence, general update and inference operations in normative expert systems that are based on the influence diagram (generalized Bayesian network) paradigm. The value of evidence allows us to compute the outcome sensitivity directly defined as the maximum difference among the values of evidence, and the value of perfect information, as the expected value of the values of evidence. We define the value of revelation as the optimal value of the values of evidence. We discuss the relationship between the value of revelation and the value of control. We also discuss implementation issues related to computation of the value of evidence and the value of perfect information.", "e:keyword": ["Decision analysis", "Inference", "Systems", "Theory", "Information systems", "Decision support systems", "Expert systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.84", "e:abstract": "Hit-and-Run algorithms are Monte Carlo procedures for generating points that are asymptotically distributed according to general absolutely continuous target distributions <i>G</i> over open bounded regions <i>S</i>. Applications include nonredundant constraint identification, global optimization, and Monte Carlo integration. These algorithms are reversible random walks that commonly incorporate uniformly distributed step directions. We investigate <i>nonuniform</i> direction choice and show that, under regularity conditions on the region <i>S</i> and target distribution <i>G</i>, there exists a unique direction choice distribution, characterized by necessary and sufficient conditions depending on <i>S</i> and <i>G</i>, which optimizes the Doob bound on rate of convergence. We include computational results demonstrating greatly accelerated convergence for this optimizing direction choice as well as for more easily implemented adaptive heuristic rules.", "e:keyword": ["Simulation", "Random variable generation", "Markov-chain Monte Carlo", "Probability", "Random walk", "Direction choice"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.9", "e:abstract": "This paper is an outgrowth of a talk given at a plenary session of the national meeting of ORSA/TIMS (now INFORMS) in May of 1996. Rather than speculate on what might be the possible domain of OR/MS within a systems framework, the paper gives a sketch of what basic ideas there are today that can help us get to a unified mathematical theory of the subject. The concept of influence occurs in all fields of knowledge—from physics, with its gravitational and electromagnetic influences, to sociology with its societal, political, economic, and technological influences. We argue that influence, a sensed, perceived or inferred stimulus, is the single most central concept for analyzing causal relations in OR/MS problems. Problem solving is contextual and focuses on the distribution of influence in allocation, queuing, inventory, and similar problems by manipulating measurable quantities. Because most influences are abstract and intangible, emphasis on creating structures to represent and measure the flow of influence of intangibles and their propagation is critical for the development of a general scientific theory for OR/MS, more critical than in any other field because our problem domain is very general and interdisciplinary. The paper advocates the need for a systemic integration of the diverse approaches used in OR/MS within a single framework for all areas, including dependencies and feedback among influences to maintain the full integrity of the problems we solve. Examples and illustrations are given together with observations about the use of creativity and intelligence to move the process of creating a theory beyond the traditional process of problem solving.", "e:keyword": ["Philosophy of modeling", "Importance and uses of creativity", "Mathematical modeling", "General approach to systems problems", "Systems solution", "Relating to intangible objectives and criteria"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.1.96", "e:abstract": "We consider a distribution system consisting of a single warehouse and many geographically dispersed retailers. Each retailer faces demands for a single item which arise at a deterministic, retailer specific rate. The retailers' stock is replenished by a fleet of vehicles of limited capacity, departing and returning to the warehouse and combining deliveries into efficient routes. The cost of any given route consists of a fixed component and a component which is proportional with the total distance driven. Inventory costs are proportional with the stock levels. The objective is to identify a combined inventory policy and a routing strategy minimizing system-wide infinite horizon costs. We characterize the asymptotic effectiveness of the class of so-called Fixed Partition policies and those employing Zero Inventory Ordering. We provide worst case as well as probabilistic bounds under a variety of probabilistic assumptions. This insight is used to construct a very effective algorithm resulting in a Fixed Partition policy which is asymptotically optimal within its class. Computational results show that the algorithm is very effective on a set of randomly generated problems.", "e:keyword": ["Transportation", "Vehicle routing", "Inventory/production", "Multiechelon"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.161", "e:abstract": "This paper details a new simulation and optimisation based system for personnel scheduling (rostering) of customs staff at the Auckland International Airport, New Zealand. An integrated approach using simulation, heuristic descent, and integer programming techniques has been developed to determine near-optimal staffing levels. The system begins by using a new simulation system embedded within a heuristic search to determine minimum staffing levels for arrival and departure work areas. These staffing requirements are then used as the input to an integer programming model, which optimally allocates full- and part-time staff to each period of the working day. These shifts are then assigned to daily work schedules having a six-day-on, three-day-off structure. The application of these techniques has resulted in significantly lower staffing levels, while at the same time creating both high-quality rosters and ensuring that all passenger processing targets are met. This paper charts the development of this system, outlines failures where they have occurred, and summarises the ongoing impacts of this work on the organisation.", "e:keyword": ["Organizational studies", "Manpower planning", "Modeling of customs staffing requirements", "Labor", "Efficient scheduling of customs staff"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.176", "e:abstract": "The OR/MS research and publication process provides a particularly effective setting in which to illustrate and emphasize the importance of several general quality improvement principles. From planning the scope and content to final manuscript production, numerous quality checks are beneficial. The academic writer never knows that the quality of an article is sufficient until it has actually been published. Even then, potential defects are possible and should be guarded against throughout the process. Beginning scholars may fail to publish successfully by disregarding some of these principles. This article discusses the connection of general quality principles to the scientific article writing process. Also, some specific techniques used by the author are described.", "e:keyword": ["Professional", "OR/MS policy/standards", "Comments on research and writing", "Scientific publication", "Application of quality improvement principles"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.184", "e:abstract": "Using limited assets, an interdictor attempts to destroy parts of a capacitated network through which an adversary will subsequently maximize flow. We formulate and solve a stochastic version of the interdictor's problem: Minimize the expected maximum flow through the network when interdiction successes are binary random variables. Extensions are made to handle uncertain arc capacities and other realistic variations. These two-stage stochastic integer programs have applications to interdicting illegal drugs and to reducing the effectiveness of a military force moving materiel, troops, information, etc., through a network in wartime. Two equivalent model formulations allow Jensen's inequality to be used to compute both lower and upper bounds on the objective, and these bounds are improved within a sequential approximation algorithm. Successful computational results are reported on networks with over 100 nodes, 80 interdictable arcs, and 180 total arcs.", "e:keyword": ["Military", "Targeting", "Network interdiction", "Programming", "Stochastic", "Sequential approximation", "Networks graphs", "Stochastic", "Maximum flow"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.198", "e:abstract": "There are two major competing procedures for evaluating risky projects where managerial flexibility plays an important role: one is decision analytic, based on stochastic dynamic programming, and the other is option pricing theory (or contingent claims analysis), based on the no-arbitrage theory of financial markets. In this paper, we show how these two approaches can be profitably integrated to evaluate oil properties. We develop and analyze a model of an oil property—either a developed property or a proven but undeveloped reserve—where production rates and oil prices both vary stochastically over time and, at any time, the decision maker may terminate production or accelerate production by drilling additional wells. The decision maker is assumed to be risk averse and can hedge price risks by trading oil futures contracts. We also describe extensions of this model that incorporate additional uncertainties and options, discuss its use in exploration decisions and in evaluating a portfolio of properties rather than a single property, and briefly describe other potential applications of this integrated methodology.", "e:keyword": ["Decision analysis", "Applications", "Evaluating oil properties", "Finance", "Investment", "Evaluating oil properties", "Natural resources", "Energy", "Evaluating oil properties"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.218", "e:abstract": "This paper considers the control of a single-server tandem queueing system with setups. Jobs arrive to the system according to a Poisson process and are produced to order. A single server must perform a number of different operations on each job. There is a setup time for the server to switch between different operations. We assume that there is a holding cost at each operation, which is nondecreasing in operation number (i.e., as value is added to a job, it becomes more expensive to hold). The control problem is to decide which job the server should process at each point in time.We formulate this control problem as a Markov-Decision Process. We partially characterize the optimal policy, develop an exact analysis of exhaustive and gated polling policies, and develop an effective heuristic policy. The results of a simulation study, which tests the performance of the policies considered, are reported. These computational results indicate that our heuristic is effective for a wide variety of cases.", "e:keyword": ["Queues", "Tandem", "Control of tandem queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.231", "e:abstract": "We present a penalty-based algorithm that solves the multicommodity flow problem as a sequence of a finite number of scaling phases. The basis of the algorithm is simple and consists of iteratively detecting and sending flow around negative cost cycles. Two parameters control the algorithm's behavior: the penalty parameter and the scaling parameter. In the ε-scaling phase, where ε is a function of the penalty and scaling parameters, the algorithm determines an ε-optimal solution; a solution in which complementary slackness conditions are satisfied to within ε. We analyze the performance of the algorithm from both the theoretical and practical perspectives. The computational results support the theoretical behavior of the algorithm. They also demonstrate the efficiency of the algorithm for solving problem instances of different structure and size.", "e:keyword": ["Networks/graphs", "Multicommodity", "New algorithm for problem solving", "Analysis of algorithm", "Computational and theoretical analysis of a scaling algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.247", "e:abstract": "The network design problem is a multicommodity minimal cost network flow problem with fixed costs on the arcs, i.e., a structured linear mixed-integer programming problem. It has various applications, such as construction of new links in transportation networks, topological design of computer communication networks, and planning of empty freight car transportation on railways. We present a Lagrangean heuristic within a branch-and-bound framework as a method for finding the exact optimal solution of the uncapacitated network design problem with single origins and destinations for each commodity (the simplest problem in this class, but still NP-hard). The Lagrangean heuristic uses a Lagrangean relaxation as subproblem, solving the Lagrange dual with subgradient optimization, combined with a primal heuristic (the Benders subproblem) yielding primal feasible solutions. Computational tests on problems of various sizes (up to 1000 arcs, 70 nodes and 138 commodities or 40 nodes and 600 commodities) and of several different structures lead to the conclusion that the method is quite powerful, outperforming for example a state-of-the-art mixed-integer code, both with respect to problem size and solution time.", "e:keyword": ["Programming", "Integer", "Algorithms", "Relaxation/subgradient", "Branch-and-bound", "Networks/graphs", "Multicommodity", "Fixed charges"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.260", "e:abstract": "Federal aviation regulations require that all aircraft undergo maintenance after flying a certain number of hours. To ensure high aircraft utilization, maintenance is done at night, and these regulations translate into requiring aircraft to overnight at a maintenance station every three to four days (depending on the fleet type), and to visit a balance-check station periodically. After the schedule is fleeted, the aircraft are routed to satisfy these maintenance requirements. We give fast and simple polynomial-time algorithms for finding a routing of aircraft in a graph whose routings during the day are fixed, that satisfies both the three-day maintenance as well as the balance-check visit requirements under two different models: a static infinite-horizon model and a dynamic finite-horizon model. We discuss an implementation where we embed the static infinite-horizon model into a three-stage procedure for finding a maintenance routing of aircraft.", "e:keyword": ["Transportation", "Routing", "Network models", "Scheduling", "Airline OR"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.272", "e:abstract": "We consider a simple resource allocation problem with a single resource constraint. The objective function is composed of separable, convex performance functions, one for each activity. Likewise, the constraint has separable, convex resource-usage functions, one for each activity. The objective is to minimize the sum of the performance functions, subject to satisfying the resource constraint and nonnegativity constraints. This problem extends the well-studied problem in which the resource constraint is linear. We present several algorithms to solve the problem. These algorithms extend approaches developed for the linearly constrained problem. They can readily solve large problems and find the optimal solution in a number of iterations that does not exceed the number of variables. We provide several examples for illustration purposes, present computational results, and highlight the similarities and differences among the algorithms.", "e:keyword": ["Programming", "Algorithms", "Allocation of one resource", "Programming", "Nonlinear", "Resource allocation algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.2.285", "e:abstract": "The mean time to failure (MTTF) of a Markovian system can be expressed as a ratio of two expectations. For highly reliable Markovian systems, the resulting ratio formula consists of one expectation that cannot be estimated with bounded relative error when using standard simulation, while the other, which we call a nonrare expectation, can be estimated with bounded relative error. We show that some derivatives of the nonrare expectation cannot be estimated with bounded relative error when using standard simulation, which in turn may lead to an estimator of the derivative of the MTTF that has unbounded relative error. However, if particular importance-sampling methods (e.g., balanced failure biasing) are used, then the estimator of the derivative of the nonrare expectation will have bounded relative error, which (under certain conditions) will yield an estimator of the derivative of the MTTF with bounded relative error.", "e:keyword": ["Simulation", "Statistical analysis of derivative estimates", "Simulation", "Efficiency", "Importance sampling", "Probability", "Stochastic model applications", "Highly dependable systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.293", "e:abstract": "The Cartographic Institute of Catalonia (ICC) produces commercial aerial photographic maps of locations in Europe and South America. These maps are often so large that it is necessary to produce one map from two or more photographs, which are combined two at a time in a process called <i>mosaicking</i>. The objective is to make the final map appear to be the product of a single photograph by producing a seam that is invisible even to an expert cartographer. The problem and a variation are modeled via bottleneck shortest paths and cycles. Optimization algorithms are developed for both, and the first has been implemented with demonstrable impact on the company. The second represents a new class of constrained shortest cycle problems.", "e:keyword": ["Cartography", "Mosaicking of photographic maps", "Networks", "Bottleneck shortest path"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.305", "e:abstract": "Managing redundant information is becoming an important issue in today's increasingly large distributed computer networks. As total redundancy is extremely costly to achieve, it has been proposed to keep perfectly updated information only at the servers, while keeping old copies of that information on local computers. For such copies to be useful, a maximum lifetime length is assigned to them. Before the lifetime has elapsed, the local devices must be stashed with a new updated copy. The problem of optimizing the updates so that the maximum lifetime length constraints are respected has been previously formulated as a binary problem and proved to be NP-hard through a reduction to the Steiner tree problem in graphs. In this paper we explore the properties of another formulation, based on a state transition graph approach. We prove that only a subset of states and transitions will be in the optimal solution and that, thanks to those properties, it is possible to greatly reduce the size of the graph. A solution algorithm that is based on an efficient evaluation of similar Steiner tree problems with similar properties is presented. We discuss extensions of this problem to future applications of broadband multicast services.", "e:keyword": ["Computers/computer science", "Distributed systems", "Information", "Management"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.316", "e:abstract": "We discuss formulations of integer programs with a huge number of variables and their solution by column generation methods, i.e., implicit pricing of nonbasic variables to generate new columns or to prove LP optimality at a node of the branch-and-bound tree. We present classes of models for which this approach decomposes the problem, provides tighter LP relaxations, and eliminates symmetry. We then discuss computational issues and implementation of column generation, branch-and-bound algorithms, including special branching rules and efficient ways to solve the LP relaxation. We also discuss the relationship with Lagrangian duality.", "e:keyword": ["Integer programming", "Branch-and-bound", "Decomposition algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.330", "e:abstract": "This article describes a generalized insertion heuristic for the Traveling Salesman Problem with Time Windows in which the objective is the minimization of travel times. The algorithm gradually builds a route by inserting at each step a vertex in its neighbourhood on the current route, and performing a local reoptimization. This is done while checking the feasibility of the remaining part of the route. Backtracking is sometimes necessary. Once a feasible route has been determined, an attempt is made to improve it by applying a post-optimization phase based on the successive removal and reinsertion of all vertices. Tests performed on 375 instances indicate that the proposed heuristic compares very well with alternative methods and very often produces optimal or near-optimal solutions.", "e:keyword": ["Traveling salesman problem with time windows", "Generalized insertion heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.336", "e:abstract": "We consider a range of controlled stochastic systems satisfying conservation laws together with a reducibility property that says that appropriate laws continue to hold when access to the system is restricted to a subset of all possible demand (job, customer) types. We show that for linear objectives, the optimum system-wide performance is a nondecreasing submodular (or supermodular) function of the subset chosen and that these properties are inherited from the geometry of the performance space concerned. These results are of considerable interest in their own right, but they also motivate the use of greedy heuristics for the solution of a range of job selection and scheduling problems which have hitherto seemed intractable. Computational experience suggests that such heuristics perform very well.", "e:keyword": ["Dynamic programming/optimal control", "Characterisation of optimal return functions", "Production/scheduling", "Selection of jobs in advance of scheduling", "Queues", "Optimisation of multiclass queueing systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.347", "e:abstract": "In this paper we describe four axioms that uniquely characterize the class of locations in tree networks that are obtained by minimizing an additively separable, nonnegative, nondecreasing, differentiable, and strictly convex function of distances. This result is analogous to results that have been obtained in the theory of bargaining, social choice, and fair resource allocation.", "e:keyword": ["Facilities", "Location", "Discrete"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.355", "e:abstract": "A branch-and-cut IP solver is developed for a class of structured 0/1 integer programs arising from a truck dispatching scheduling problem. This problem involves a special class of knapsack equality constraints. Families of facets for the polytopes associated with individual knapsack constraints are identified. In addition, a notion of “conflict graph” is utilized to obtain an approximating node-packing polytope for the convex hull of all 0/1 solutions. The branch-and-cut solver generates cuts based on both the knapsack equality constraints and the approximating node-packing polytope, and incorporates these cuts into a tree-search algorithm that uses problem reformulation and linear programming-based heuristics at each node in the search tree to assist in the solution process. Numerical experiments are performed on large-scale real instances supplied by Texaco Trading & Transportation, Inc. The optimal schedules correspond to cost savings for the company and greater job satisfaction for drivers due to more balanced work schedules and income distribution.", "e:keyword": ["Programming", "Integer", "Algorithms", "Cutting plane/facet generation", "Branch-and-cut algorithm for a structured 0/1 problem", "Transportation", "Vehicle routing", "Dispatching of trucks for an oil company"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.368", "e:abstract": "In this paper we present the theoretical foundations for one of the methods used to achieve convergence in the National Energy Modeling System (NEMS). NEMS is a large model with several component models that are built and operated by different branches in the organization and is an example of a system without a hierarchical structure that cannot be solved by traditional equation solving methods. Some of the component models use linear programs to construct supply and demand curves. The discontinuities that result lead to oscillations in the standard relaxation algorithms. We explain where the convergence problems lie and how the convergence theory with step functions links to the convergence theory with continuous functions. To achieve convergence within the entire system, a set of ad hoc techniques were developed to implement a decomposition strategy that allows the individual models to be run separately. We present the theoretical justification for one of them here. The technique presented here has the potential to allow an organization to use operational models for planning without resorting to aggregation. It also facilitates decentralized computing over Internet.", "e:keyword": ["Economics", "Large-scale systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.378", "e:abstract": "We settle a conjecture concerning necessary conditions for finite mean steady-state customer delay at the second node of a tandem queue, using as an example a stable tandem queue with mutually independent i.i.d. interarrival and service times. We assume that the service times have infinite variance at the first node, finite variance at the second node, and smaller mean at the first node than at the second node. We show that this causes infinite mean stationary delay at the second node. Thus, in general, when the mean service time is smaller at the first node than at the second, finite variance of service times at node 1 is necessary for finite mean delay at node 2. This confirms a conjecture made by Wolff. Our result complements sufficiency conditions previously published by Wolfson; together these necessary and sufficient conditions are presented as a theorem at the conclusion of the paper. Our proof uses a known duality between risk processes and queues.", "e:keyword": ["Sample path properties", "Queuing theory", "Finite moments of steady-state distribution", "Applications", "Customer delay"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.381", "e:abstract": "The optimal allocation of indivisible resources is formalized as a stochastic optimization problem involving discrete decision variables. A general stochastic search procedure is proposed, which develops the concept of the branch-and-bound method. The main idea is to process large collections of possible solutions and to devote more attention to the most promising groups. By gathering more information to reduce the uncertainty and by narrowing the search area, the optimal solution can be found with probability one. Special techniques for calculating stochastic lower and upper bounds are discussed. The results are illustrated by a computational experiment.", "e:keyword": ["Programming", "Integer", "Stochastic", "Algorithms", "Branch-and-bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.396", "e:abstract": "A new hierarchy of relaxations is presented that provides a unifying framework for constructing a spectrum of continuous relaxations spanning from the linear programming relaxation to the convex hull representation for linear mixed integer 0-1 problems. This hierarchy is an extension of the Reformulation-Linearization Technique (RLT) of Sherali and Adams (1990, 1994a), and is particularly designed to exploit special structures. Specifically, inherent special structures are exploited by identifying particular classes of multiplicative factors that are applied to the original problem to reformulate it as an equivalent polynomial programming problem, and subsequently, this resulting problem is linearized to produce a tighter relaxation in a higher dimensional space. This general framework permits us to generate an explicit hierarchical sequence of tighter relaxations leading up to the convex hull representation. (A similar hierarchy can be constructed for polynomial mixed integer 0-1 problems.) Additional ideas for further strengthening RLT-based constraints by using conditional logical implications, as well as relationships with sequential lifting, are also explored. Several examples are presented to demonstrate how underlying special structures, including generalized and variable upper bounding, covering, partitioning, and packing constraints, as well as sparsity, can be exploited within this framework. For some types of structures, low level relaxations are exhibited to recover the convex hull of integer feasible solutions.", "e:keyword": ["Programming", "Integer", "Reformulation-linearization technique", "Automatic reformulation procedure", "Hierarchy of relaxations", "Convex hull representations"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.406", "e:abstract": "Throughout the United States and Europe, demand for airport use has been increasing rapidly, while airport capacity has been stagnating. Over the last ten years the number of passengers has increased by more than 50 percent and is expected to continue increasing at this rate. Acute congestion in many major airports has been the unfortunate result. For U.S. airlines, the expected yearly cost of the resulting delays is currently estimated at $3 billion. In order to put this number in perspective, the total reported <i>losses</i> of all U.S. airlines amounted to approximately $2 billion in 1991 and $2.5 billion in 1990. Furthermore, every day 700 to 1100 flights are delayed by 15 minutes or more. European airlines are in a similar plight. Optimally controlling the flow of aircraft either by adjusting their release times into the network (ground-holding) or their speed once they are airborne is a cost effective method to reduce the impact of congestion on the air traffic system. This paper makes the following contributions: (a) we build a model that takes into account the capacities of the National Airspace System (NAS) as well as the capacities at the airports, and we show that the resulting formulation is rather strong as some of the proposed inequalities are facet defining for the convex hull of solutions; (b) we address the complexity of the problem; (c) we extend that model to account for several variations of the basic problem, most notably, how to reroute flights and how to handle banks in the hub and spoke system; (d) we show that by relaxing some of our constraints we obtain a previously addressed problem and that the LP relaxation bound of our formulation is at least as strong when compared to all others proposed in the literature for this problem; and (e) we solve large scale, realistic size problems with several thousand flights.", "e:keyword": ["Transportation", "Air traffic", "Programming", "Integer", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.423", "e:abstract": "Cheng and Chen (1994) use a high-multiplicity encoding scheme to prove binary <i>NP</i>-hardness of a scheduling problem. From this they infer a similar result for a well-known, more general problem. We explain that, although their initial proof is correct, their inference about the more general problem is not.", "e:keyword": ["Analysis of algorithms", "Computational complexity", "NP-hardness", "Production/scheduling", "Parallel machine scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.424", "e:abstract": "This paper considers tandem queues with finite buffers. Throughput values for the cases of independent and dependent service are compared, where in the dependent case a given customer has the same service time at each station. It is shown that for sufficiently large buffer sizes, dependent throughput is greater than independent. Simulation results that demonstrate this phenomenon are given.", "e:keyword": ["Queues", "Tandem", "Dependent and independent service with blocking"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s1", "e:abstract": "Consider a number of jobs that have to be completed within some fixed period of time. Each job consumes or supplies a certain, job-dependent quantity of a resource thus changing the contents of a stock. Natural examples for this scenario arise in the case of trucks delivering goods to a trade house and taking other goods away from it, or in the case where processes change their environment. The goal is to find an ordering of the jobs such that all jobs can be completed and such that the maximum stock size needed over the time period becomes minimum. Since this problem can be shown to be NP-hard, we present three polynomial time approximation algorithms which yield job sequences with maximum stock size at most 2, respectively 8/5 times and 3/2 times the optimum stock size. These approximation algorithms are also compared by a numerical simulation. Moreover, we consider a variation of the problem where consecutive processes are allowed to directly exchange resources without using the stock.", "e:keyword": ["Analysis of algorithms: worst-case analysis of heuristics", "Production/scheduling: minimization of maximum stock size"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s112", "e:abstract": "In inventory systems with stochastic lead times, it is possible that more than a single order will be outstanding. In that case, orders may cross; that is, they may not be received in the same sequence in which they are placed. In this paper we develop a multicycle analysis for the case when lead times are independently and identically distributed. When contrasted with the conventional single cycle analysis, the multicycle approach provides a more accurate assessment of the overall inventory cost and leads to a superior inventory policy.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic lead times", "Order crossover", "Noninterchangeability", "Approximations", "One cycle and multicycle analyses", "Policies", "Reorder point", "Order quantity"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s120", "e:abstract": "We consider the problem of finding an optimal schedule for jobs on a single machine when there are penalties for both tardy and early arrivals. We point out that if attention is paid to how these penalties are measured, then a change of scale of measurement might lead to the anomalous situation where a schedule is optimal if these parameters are measured in one way, but not if they are measured in a different way that seems equally acceptable. In particular, we note that if the penalties measure utilities or disutilities, or loss of goodwill or customer satisfaction, then these kinds of anomalies can occur, for instance if we change both unit and zero point in scales measuring these penalties. We investigate situations where problems of these sorts arise for four specific penalty functions under a variety of different assumptions. The results of the paper have implications far beyond the specific scheduling problems we consider, and suggest that considerations of scale of measurement should enter into analysis of conclusions of optimality both in scheduling problems and throughout combinatorial optimization.", "e:keyword": ["Philosophy of modeling", "Effects of scales", "Transportation", "Scheduling", "Tardiness and earliness", "Production", "Scheduling", "Single machines"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s13", "e:abstract": "This paper presents an analysis of a single-stage hybrid production system that makes multiple types of products, some of which are made to-order while others are made to-stock. The analysis begins with a formal heavy traffic limit theorem of the production system, which is modeled as a mixed queueing network. Taking insights from the limit theorem, the analysis continues with the development of an approximation procedure. Numerical experiments indicate that this procedure provides good estimates for performance measures such as fill rates and average inventory levels.", "e:keyword": ["Production/scheduling", "Approximations of hybrid production systems", "Queues", "Diffusion models", "Heavy traffic unit of mixed queues", "Queues", "Networks", "Queueing networks with open and closed customers"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s135", "e:abstract": "We consider a two-level inventory system with one warehouse and <i>N</i> retailers. Leadtimes (transportation times) are constant, and the retailers face different Poisson demand processes. All facilities apply continuous review installation stock (<i>R</i>, <i>Q</i>)-policies with different reorder points and batch quantities. We show how to evaluate holding and shortage costs exactly in case of two retailers. In case of more than two retailers we use the same methodology as an approximation. When evaluating costs at a certain retailer, the other retailers are then aggregated into a single retailer.", "e:keyword": ["Inventory/production", "Multiechelon", "Batch-ordering policies", "Continuous review", "Inventory/production", "Stochastic", "Poisson demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s146", "e:abstract": "We consider polling systems with random setup times, and show that if setup times are reduced in the convex ordering sense, then waiting times, work loads, and queue sizes will be reduced in the usual stochastic sense. We also show that the joint distributions of various performance measures across time are invariant under shifts in setup times as long as the net shift is zero.", "e:keyword": ["Probability", "Stochastic model applications", "Effect of setups in production", "Production", "Applications", "Stochastic polling models", "Queues", "Applications", "Polling models in production"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s154", "e:abstract": "In many scheduling problems, a newly released job must be stored in an input buffer while it waits to begin processing. The lack of attention given to these buffers in the classical scheduling literature results from the implicit assumption that they have infinite capacity. In modern manufacturing environments, however, there are several important reasons for limiting buffer capacity. We study nonpreemptive single machine dynamic scheduling problems under the assumption that some jobs may be lost, either because of insufficient input buffer capacity, or because due dates cannot be met. The objective is to minimize the weighted or unweighted number of lost jobs. We study problems with zero, fixed or arbitrary buffer capacity, with unit or arbitrary processing times, and with unit or arbitrary buffer storage requirements. We present a complexity classification in which, for each problem, either an efficient algorithm is derived, or a proof is given that such an algorithm is unlikely to exist.", "e:keyword": ["Production/scheduling", "Scheduling with limited input storage capacity", "Deterministic", "Single machine sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s26", "e:abstract": "A stationary policy conducts replenishment activities—the placement and fulfillment of orders—in a stationary fashion. That is, each facility receives a constant batch (facility specific) in equal time intervals (facility specific) under a stationary policy. Although the advantages of stationary policies are clear (i.e., smooth operations), they represent a restriction in policy selection. This paper investigates how costly this restriction can be. For two multiechelon systems (serial and distribution) with deterministic demand and backlogging, we show that stationary policies are 70%-effective. This bound is tight in the sense that an example exists where the bound is reached. On the other hand, the average effectiveness of stationary policies is very high. In a set of 1,000 randomly generated numerical examples, we observed that the average effectiveness was 99%, and the standard deviation was 1.5%. The numerical examples also suggest that the performance of stationary policies deteriorates in systems where the setup cost decreases dramatically from an upstream stage to a downstream stage. Finally, a key building block of the above results is the existing lower bounds on the average costs of all feasible policies in the above systems. We provide a simpler derivation of these bounds.", "e:keyword": ["Inventory/production", "Multiechelon", "Cost-effectiveness of stationary policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s35", "e:abstract": "This paper develops a new model for studying requirements planning in multistage production-inventory systems. We first characterize how most industrial planning systems work, and we then develop a mathematical model to capture some of the key dynamics in the planning process. Our approach is to use a model for a single production stage as a building block for modeling a network of stages. We show how to analyze the single-stage model to determine the production smoothness and stability for a production stage and the inventory requirements. We also show how to optimize the tradeoff between production capacity and inventory for a single stage. We then can model the multistage supply chain using the single stage as a building block. We illustrate the multistage model with an industrial application, and we conclude with some thoughts on a research agenda.", "e:keyword": ["Inventory/production", "Multi-item", "Multi-stage supply chain with uncertain demand and dynamic forecast revisions", "Application to film production", "Dynamic requirements planning and supply chain optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s50", "e:abstract": "We consider the assembly of a product that needs to exceed a certain length. The subassemblies are manufactured with a planned length setting. The resulting lengths of the subassemblies are not known with certainty, and vary about the planned setting. After the subassemblies are manufactured, those that fall short are rerun. This process is repeated until all subassemblies exceed the required length. We build and solve a model, to specify the planned length at each run, that minimizes the expected cost of production.", "e:keyword": ["Production scheduling", "Assembly yields", "Dynamic programming", "Applications", "Production/scheduling", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s65", "e:abstract": "We consider an (<i>S</i> − 1, <i>S</i>) type perishable inventory system in which the maximum shelf life of each item is fixed. An order for an item is placed at each demand time as well as at each time that the maximum shelf life of an item is reached. The order lead times are constant, and the demand process for items is Poisson. Although the resulting process is ostensibly nonregenerative, we adapt level-crossing theory for the case of an <i>S</i>-dimensional Markov process to obtain its stationary law. Within this framework a number of model variants are solved.", "e:keyword": ["Inventory", "Stochastic", "Perishable items", "Lead items", "Probabalistic propensity for customer waiting"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s72", "e:abstract": "This paper presents a hierarchical model framework for the analysis of the stabilizing effect of inventories in multiechelon manufacturing/distribution supply chains. This framework is used to compare the variance of demand to the variance of replenishment orders at different echelons of the system (e.g., retailer, wholesaler). Empirical data and experience with management games suggest that, in most industries, inventory management policies can have a destabilizing effect by increasing the volatility of demand as it passes up through the chain (the “bullwhip” effect). Our model helps to explain these observations and indicates mechanisms that can promote stabilization. The analysis results also define sufficient conditions for the existence of stabilization and relate these conditions to the optimality of myopic control policies.", "e:keyword": ["Multiechelon inventory systems", "Inventory/production smoothing"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s84", "e:abstract": "In many scheduling problems, a job that completes processing may need to be held in an output buffer until the customer is ready to accept delivery. Buffer capacity is usually assumed to be infinite.We study a number of the best known single machine scheduling problems, under several alternative assumptions about the capacity of the output buffer. Specifically, we allow the buffer capacity to be either zero, fixed, or specified as part of problem input. We also consider situations in which all jobs have the same storage requirement in the buffer, and others where the storage requirement may vary. Further, we consider generalizations where there is a time interval within which a customer accepts delivery without cost to the producer.A classification scheme for these problems is provided. For each problem considered, we provide either an efficient algorithm or a proof that such an algorithm is unlikely to exist. Our results provide a mapping of the computational complexity of these problems which parallels those that are available for classical scheduling problems with infinite buffer capacity.", "e:keyword": ["Production/scheduling", "Scheduling with limited output storage capacity", "Deterministic", "Single machine sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.3.s98", "e:abstract": "We consider a model for managing a single stage that produces multiple items. The production rates are finite, and there are switchover times. The interarrival times and quantities of demands for the items are random, and demand may occur for a set of items. We consider <i>order focussed</i> measures: cost based on response times; service levels based on quoted lead times and Type-1 service.We operate the stage in the following manner: (1) there is a cyclic schedule that determines the sequence of items and the number of times a particular item is produced in a cycle; (2) given a cyclic schedule, production of each item follows a modified base-stock policy or a (<i>s</i>, <i>S</i>) policy. We present a simulation based procedure to obtain good values for the base stock levels or <i>S</i> (for any fixed <i>S</i> − <i>s</i>) for each of the above performance measures. Numerical results indicate that good solutions can be obtained with modest computational effort. We also report on a real world implementation of this model.", "e:keyword": ["Inventory/production", "Multi-item", "Stochastic economic lot scheduling with response time criteria", "Simulation", "Applications", "Simulation based optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.433", "e:abstract": "This paper describes the formulation of the Russell-Yasuda Kasai financial planning model, including the motivation for the model. The presentation complements the discussion of the technical details of the financial modeling process and the managerial impact of its use to help allocate the firm's assets over time discussed in Cariño et al. (1994, 1998, respectively). The multistage stochastic linear program incorporates Yasuda Kasai's asset and liability mix over a five-year horizon followed by an infinite horizon steady-state end-effects period. The objective is to maximize expected long-run profits less expected penalty costs from constraint violations over the infinite horizon. Scenarios are used to represent the uncertain parameter distributions. The constraints represent the institutional, cash flow, legal, tax, and other limitations on the asset and liability mix over time.", "e:keyword": ["Finance", "Management", "Asset and liability", "Financial institutions", "Insurance", "Japanese", "Property and casualty", "Programming", "Stochastic", "Multiperiod", "Linear"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.450", "e:abstract": "This paper discusses technical aspects of the Russell-Yasuda Kasai financial planning model. These include the models for the discrete distribution scenario generation processes for the uncertain parameters of the model, the mathematical approach used to develop the infinite-horizon end-effects part of the model, a comparison of algorithms used in the model's solution, and a comparison of the multistage stochastic linear programming model with the previous technology, static mean-variance analysis. Experience and benefits of the model in Yasuda-Kasai's financial planning process is also discussed.", "e:keyword": ["Finance", "Management", "Asset and liability", "Financial Institutions", "Insurance", "Japanese property and casualty", "Programming", "Stochastic", "Multiperiod", "Linear"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.463", "e:abstract": "A new algorithm for the solution of the groundwater quality management problem is presented. The method assumes that most of the computational effort in such problems involves evaluation of the concentrations and their derivatives with respect to the pumping rates at the control points. The methodology proposed herein is a combination of the cutting plane method and the primal method. In this approach each line search moves from a feasible point towards the solution of a subproblem with linear constraints and the same objective function as the original problem. Sensitivity analysis is used to compute the derivative of a single concentration with respect to all the pumping rates by solving only one linear system at each time step. The method has been tested with analytical convex problems and a model example. A comparison of the algorithm's performance was conducted using MINOS 5.1. For the case of a linear cost function the method shows a computational growth rate almost linearly proportional to the number of decision variables.", "e:keyword": ["Water resources", "Groundwater quality management problem", "Nonlinear algorithms", "Primal method"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.474", "e:abstract": "We present DRIVE (Dynamic Routing of Independent VEhicles), a planning module to be incorporated in a decision support system for the direct transportation at Van Gend and Loos BV. Van Gend and Loos BV is the largest company providing road transportation in the Benelux, with about 1400 vehicles transporting 160,000 packages from thousands of senders to tens of thousands of addressees per day. The heart of DRIVE is a branch-and-price algorithm. Approximation and incomplete optimization techniques as well as a sophisticated column management scheme have been employed to create the right balance between solution speed and solution quality. DRIVE has been tested by simulating a dynamic planning environment with real-life data and has produced very encouraging results.", "e:keyword": ["Transportation", "Vehicle routing", "a dynamic pickup and delivery problem", "Programming", "Integer", "Heuristic", "a branch-and-price algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.491", "e:abstract": "Judgments are needed in medical diagnosis to determine what tests to perform given certain symptoms. For many diseases, what information to gather on symptoms and what combination of symptoms lead to a given disease are not well known. Even when the number of symptoms is small, the required number of experiments to generate adequate statistical data can be unmanageably large. There is need in diagnosis for an integrative model that incorporates both statistical data and expert judgment. When statistical data are present but no expert judgment is available, one property of this model should be to reproduce results obtained through time honored procedures such as Bayes theorem. When expert judgment is also present, it should be possible to combine judgment with statistical data to identify the disease that best describes the observed symptoms. Here we are interested in the Analytic Hierarchy Process (AHP) framework that deals with dependence among the elements or clusters of a decision structure to combine statistical and judgmental information. It is shown that the posterior probabilities derived from Bayes theorem are part of this framework, and hence that Bayes theorem is a sufficient condition of a solution in the sense of the AHP. An illustration is given as to how a purely judgment-based model in the AHP can be used in medical diagnosis. The application of the model to a case study demonstrates that both statistics and judgment can be combined to provide diagnostic support to medical practitioner colleagues with whom we have interacted in doing this work.", "e:keyword": ["Decision analysis", "Theory", "Applications", "Diagnosis", "Dependent symptoms", "Bayes theorem", "Analytic Hierarchy Process"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.503", "e:abstract": "In recent years approximation algorithms based on primal-dual methods have been successfully applied to a broad class of discrete optimization problems. In this paper, we propose a generic primal-dual framework to design and analyze approximation algorithms for integer programming problems of the covering type that uses valid inequalities in its design. The worst-case bound of the proposed algorithm is related to a fundamental relationship (called strength) between the set of valid inequalities and the set of minimal solutions to the covering problems. In this way, we can construct an approximation algorithm simply by constructing the required valid inequalities. We apply the proposed algorithm to several problems, such as covering problems related to totally balanced matrices, cyclic scheduling, vertex cover, general set covering, intersections of polymatroids, and several network design problems attaining (in most cases) the best worst-case bound known in the literature.", "e:keyword": ["Programming", "Integer", "Heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.515", "e:abstract": "The evolution of communication technology and the competitive software design environment dictates that performance characterization involves iterations between real-world design and its representation for modeling purposes. In this paper, models and analytical techniques are developed to evaluate the performance of single-server polling computer and communication systems, e.g., multiplexers or local area networks with a single token passing medium access protocol. We present a novel methodology for the design and control of such communication systems and computer systems. Our approach allows the handling of smooth, correlated, and bursty traffic, whereas traditional analytical approaches have depended on a Poisson traffic model. The methodology also provides a unified treatment of several existing methodologies for controlling single-server polling computer and communication systems.", "e:keyword": ["Telecommunications", "Polling systems", "Queues", "Linear-quadratic model of uncertainty", "Linear-quadratic control rule"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.532", "e:abstract": "We analyze two scheduling problems for a queueing system with a single server and two customer classes. Each class has its own renewal arrival process, general service time distribution, and holding cost rate. In the first problem, a setup cost is incurred when the server switches from one class to the other, and the objective is to minimize the long-run expected average cost of holding customers and incurring setups. The setup cost is replaced by a setup time in the second problem, where the objective is to minimize the average holding cost. By assuming that a recently derived heavy traffic principle holds not only for the exhaustive policy but for nonexhaustive policies, we approximate (under standard heavy traffic conditions) the dynamic scheduling problems by diffusion control problems. The diffusion control problem for the setup cost problem is solved exactly, and asymptotics are used to analyze the corresponding setup time problem. Computational results show that the proposed scheduling policies are within several percent of optimal over a broad range of problem parameters.", "e:keyword": ["Production/scheduling", "Stochastic scheduling with setups", "Queues", "Diffusion models of control problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.548", "e:abstract": "D.-c. programming is a recent technique of global optimization that allows the solution of problems whose objective function and constraints can be expressed as differences of convex (i.e., d.-c.) functions. Many such problems arise in continuous location theory. The problem first considered is to locate a known number of source facilities to minimize the sum of weighted Euclidean distances between a user's fixed location and the source facility closest to the location of each user. We also apply d.-c. programming to the solution of the conditional Weber problem, an extension of the multisource Weber Problem, in which some facilities are assumed to be already established. In addition, we consider a generalization of Weber's problem, the facility location problem with limited distances, where the effective service distance becomes a constant when the actual distance attains a given value. Computational results are reported for problems with up to 10,000 users and two new facilities, 50 users and three new facilities, 1,000 users, 20 existing facilities and one new facility or 200 users, 10 existing and two new facilities.", "e:keyword": ["Facilities/equipment planning", "Continuous location theory", "Nonlinear programming", "d.-c. programming", "Concave minimization", "Algorithm", "Application", "Linear programming", "Vertex enumeration"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.563", "e:abstract": "We consider optimal load balancing in a distributed computing environment consisting of homogeneous unreliable processors. Each processor receives its own sequence of tasks from outside users, some of which can be redirected to the other processors. Processing times are independent and identically distributed with an arbitrary distribution. The arrival sequence of outside tasks to each processor may be arbitrary as long as it is independent of the state of the system. Processors may fail, with arbitrary failure and repair processes that are also independent of the state of the system. The only information available to a processor is the history of its decisions for routing work to other processors, and the arrival times of its own arrival sequence.We prove the optimality of the round-robin policy, in which each processor sends all the tasks that can be redirected to each of the other processors in turn. We show that, among all policies that balance workload, round robin <i>stochastically</i> minimizes the <i>n</i>th task completion time for all <i>n</i>, and minimizes response times and queue lengths in a separable increasing convex sense for the entire system. We also show that if there is a single centralized controller, round-robin is the optimal policy, and a single controller using round-robin routing is better than the optimal distributed system in which each processor routes its own arrivals. Again “optimal” and “better” are in the sense of stochastically minimizing task completion times, and minimizing response time and queue lengths in the separable increasing convex sense.", "e:keyword": ["Probability", "Stochastic model applications", "Distributed routing", "Queues", "Cyclic", "Optimality of round-robin routing", "Dynamic programming/optimal control", "Semi-Markow"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.574", "e:abstract": "A simulation-based quantile estimator measures the level of system performance that can be delivered with a prespecified probability. To estimate selected quantiles of the response of a finite-horizon simulation, we develop procedures based on correlation induction techniques for variance reduction, with emphasis on antithetic variates and Latin hypercube sampling. These procedures achieve improved precision by controlling the simulation's random-number inputs as an integral part of the experimental design. The proposed multiple-sample quantile estimator is the average of negatively correlated quantile estimators computed from disjoint samples of the simulation response, where negative correlation is induced between corresponding responses in different samples while mutual independence of responses is maintained within each sample. The proposed single-sample quantile estimator is computed from negatively correlated simulation responses within one all-inclusive sample. The single-sample estimator based on Latin hypercube sampling is shown to be asymptotically normal and unbiased with smaller variance than the comparable direct-simulation estimator based on independent replications. Similar asymptotic comparisons of the multiple-sample and direct-simulation estimators focus on bias and mean square error. Monte Carlo results suggest that the proposed procedures can yield significant reductions in bias, variance, and mean square error when estimating quantiles of the completion time of a stochastic activity network.", "e:keyword": ["Simulation", "Efficiency", "Variance reduction techniques", "Simulation", "Design of experiments", "Antithetic variates", "Latin hypercube sampling", "Simulation", "Statistical analysis", "Single- multiple-sample quantile estimators"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.4.592", "e:abstract": "We study echelon-stock (<i>R</i>, <i>nQ</i>) policies in a multistage, serial inventory system with compound Poisson demand. We provide a simple method for determining near-optimal control parameters. This is achieved in two steps. First, we establish lower and upper bounds on the cost function by over- and under-charging a penalty cost to each upstream stage for holding inadequate stock. Second, we minimize the bounds, which are simple, separable functions of the control parameters, to obtain heuristic solutions. We also provide an algorithm that guarantees an optimal solution at the expense of additional computational effort. A numerical study suggests that the heuristic solutions are easy to compute (even for systems with many stages) and are close to optimal. It also suggests that a traditional approach for determining the order quantities can be seriously suboptimal. All the results can be easily extended to the discrete-time case with independent, identically distributed demands.", "e:keyword": ["Inventory/production", "Multiechelon", "Lot-sizing", "Stochastic", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.609", "e:abstract": "In this paper we propose a methodology to set prices of perishable items in the context of a retail chain with coordinated prices among its stores and compare its performance with actual practice in a real case study. We formulate a stochastic dynamic programming problem and develop heuristic solutions that approximate optimal solutions satisfactorily. To compare this methodology with current practices in the industry, we conducted two sets of experiments using the expertise of a product manager of a large retail company in Chile. In the first case, we contrast the performance of the proposed methodology with the revenues obtained during the 1995 autumn-winter season. In the second case, we compare it with the performance of the experienced product manager in a “simulation-game” setting. In both cases, our methodology provides significantly better results than those obtained by current practices.", "e:keyword": ["Perishable/aging items", "Short season no replenishment", "Marketing/pricing", "Tradeoff of revenue and profit", "Shochastic", "Heuristic approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.625", "e:abstract": "The transport of hazardous materials is an important strategic and tactical decision problem. Risks associated with this activity make transport planning difficult. Although most existing analytical approaches for hazardous materials transport account for risk, there is no agreement among researchers on how to model the associated risks. This paper provides an overview of the prevailing models, and addresses the question “Does it matter how we quantify transport risk?” Our empirical analysis on the U.S. road network suggests that different risk models usually select different “optimal” paths for a hazmat shipment between a given origin-destination pair. Furthermore, the optimal path for one model could perform very poorly under another model. This suggests that researchers and practitioners must pay considerable attention to the modeling of risks in hazardous materials transport.", "e:keyword": ["Transportation", "Travel route choice", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.643", "e:abstract": "This paper develops a methodology to select a least cost mix of transport policy measures in different regions of the European Union to attain air quality standards by the year 2010. Two fundamental characteristics of air quality problems are addressed: their variation across regions and the interregional linkages resulting from ozone pollution. We propose a column generation approach to answer the question of <i>which</i> policies should be introduced <i>where</i> to arrive at a least cost solution for the Union as a whole satisfying the emissions reduction constraints. Our methodology can be applied to a wide range of environmental policy questions with a spatial dimension characterized by interdependencies across policy instruments and environmental processes. Improvements in vehicle technology are important for meeting air quality standards. Triggering such changes through economic instruments is attractive because this accommodates the geographical variation of air quality problems throughout the European Union. Different clean fuels are suggested for different regions. Taxation on road use also plays an important role in the mix of policies to curb air pollution. Furthermore, emissions reduction from nontransport sources are of major importance in all scenarios studied.", "e:keyword": ["Cost analysis", "Cost effectiveness of transport policy measures", "Integer programming", "Algorithms", "Decomposition", "Dantzig-Wolfe"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.655", "e:abstract": "A fundamental <i>experimental design</i> problem is to select a most informative subset, having prespecified size, from a set of correlated random variables. Instances of this problem arise in many applied domains such as meteorology, environmental statistics, and statistical geology. In these applications, observations can be collected at different locations and, possibly, at different times. Information is measured by “entropy.” Practical situations have further restrictions on the design space. For example, budgetary limits, geographical considerations, as well as legislative and political considerations may restrict the design space in a complicated manner. Using techniques of linear algebra, combinatorial optimization, and convex optimization, we develop upper and lower bounds on the optimal value for the Gaussian case. We describe how these bounds can be integrated into a branch-and-bound algorithm for the exact solution of these design problems. Finally, we describe how we have implemented this algorithm, and we present computational results for estimated covariance matrices corresponding to sets of environmental monitoring stations in the Ohio Valley of the United States.", "e:keyword": ["Design of experiments", "Maximum entropy sampling", "Integer programming", "Branch-and-bound", "Nonlinear programming", "Discrete variables"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.665", "e:abstract": "We consider a class of models for multicomponent systems in which components can break down and be repaired in a dependent manner and where breakdown and repair times can be arbitrarily distributed. The problem of calculating the equilibrium distribution and, from this, the expected performability for these models is intractable unless certain assumptions are made about breakdowns and repairs. In this paper we show that the performability of multicomponent systems that do not satisfy these rules can be bounded by tractable modifications. Our results are proved by stochastic comparability arguments and a Markov reward technique, which is of interest in itself as it enables one to prove that the equilibrium distribution of one process can be bounded by that of another even when the sample paths of the process are not. This is illustrated by an example.", "e:keyword": ["Multicomponent systems", "Strong stochastic bounds", "Markov reward theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.675", "e:abstract": "We consider operation assignment problems arising from a printed circuit board assembly process. The research was inspired by an application at Hewlett Packard where hundreds of types of printed circuit boards require the insertion of a number of components. The components can be inserted manually or by semiautomated insertion machines. The machines are limited in terms of the number of different component types that they can hold.We investigate how to assign the boards and components to the machines and manual process so as to minimize cost. An optimal solution technique is developed for the single-machine case and for the multiple-machine case where boards are not allowed to be set up on more than one process. In addition, a heuristic is developed which gives near-optimal solutions (within 0.3%) with much less computational effort.Although the problem this paper specifically addresses is that of partially automated PC board assembly, the results apply to a more general set of problems. Other applications include completely automated PC board assembly, flexible manufacturing systems, and general operation assignment problems.", "e:keyword": ["Production scheduling", "Applications", "PC board manufacturing", "Flexible manufacturing", "Programming", "Integer", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.690", "e:abstract": "We consider a stock cutting problem for a paper plant that produces sheets of various sizes for a finished goods inventory that services random customer demand. The controller decides when to shut down and restart the paper machine and how to cut completed paper rolls into sheets of paper. The objective is to minimize long-run expected average costs related to paper waste (from inefficient cutting), shutdowns, backordering, and holding finished goods inventory. A two-step procedure (linear programming in the first step and Brownian control in the second step) is developed that leads to an effective, but suboptimal, solution. The linear program greatly restricts the number of cutting configurations that can be employed in the Brownian analysis, and hence the proposed policy is easy to implement, and the resulting production process is considerably simplified. In an illustrative numerical example using representative data from an industrial facility, the proposed policy outperforms several policies that use a larger number of cutting configurations. Finally, we discuss some alternative production settings where this two-step procedure may be applicable.", "e:keyword": ["Production/scheduling", "Cutting stock under stochastic demand", "Queues", "Brownian models of scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.702", "e:abstract": "In this paper we define and analyze a class of two-dimensional location-allocation problems that can be solved with a one-dimensional dynamic programming algorithm. We define a criterion that must be satisfied in order that a problem can be classified as having a one-dimensional intrinsic property. An algorithm is developed to test any given problem to see if it possesses this property. We then show that any problem possessing the intrinsic property can be solved by means of an efficient dynamic programming algorithm developed earlier by one of the authors.", "e:keyword": ["Facilities/equipment planning", "Location-continuous"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.710", "e:abstract": "Monte Carlo sampling-based algorithms hold much promise for solving stochastic programs with many scenarios. A critical component of such algorithms is a stopping criterion to ensure the quality of the solution. In this paper, we develop a stopping rule theory for a class of algorithms that estimate bounds on the optimal objective function value by sampling. We provide rules for selecting sample sizes and terminating the algorithm under which asymptotic validity of confidence intervals for the quality of the proposed solution can be verified. Empirical coverage results are given for a simple example.", "e:keyword": ["Programming", "Stochastic", "Sampling-based algorithms", "Statistics", "Sampling", "Stopping rules for stochastic programming algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.719", "e:abstract": "We study a problem that has arisen recently in the design of telecommunications transmission networks at France Telecom. Given a set of centers in a city or conglomeration linked together on a ring architecture, given the expected demands between the centers and an essentially unlimited availability of rings of fixed capacity on the network, assign demand pairs and corresponding add/drop multiplexers to the rings so as to satisfy the demands and minimize the number of “costly” multiplexers installed.Heuristics based on simulated annealing have been developed for the basic problem and several variants. France Telecom is particularly interested in validating the effectiveness of the heuristics. An exact algorithm based on integer column generation is shown to provide tight performance guarantees, and in most cases to give provable minimum cost solutions. Column generation is used even though the subproblems are strongly <i>NP</i>-hard and are solved by standard mixed integer programming software. In addition, the Master Problems involve both integer columns and integer variables, whereas these are 0–1 in most reported applications.", "e:keyword": ["Communications", "Assignment of equipment", "Comparison of safety policies", "Integer programming", "Heuristics and decomposition algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.729", "e:abstract": "In this paper we consider a class of parallel machine scheduling problems and their associated set-partitioning formulations. We show that the tightness of the linear programming relaxation of these formulations is directly related to the performance of a class of heuristics called parameter list scheduling heuristics. This makes it possible to characterize the worst possible gap between optimal solutions for the scheduling problems and the corresponding linear programming relaxations. In the case of the classical parallel machine weighted completion time model we also show that the solution to the linear programming relaxation of the set-partitioning formulation is asymptotically optimal under mild assumptions on the distribution of job weights and processing times. Finally, we extend most of the results to the time-discretized formulation of machine scheduling problems.", "e:keyword": ["Scheduling", "Deterministic", "Single machine", "Programming", "Integer", "Set-partitioning", "Analysis of algorithms", "Worst-case and probabilistic analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.5.742", "e:abstract": "A well-known technique for solving scheduling problems with two identical parallel processors and unit execution time jobs under a makespan minimization criteria involves finding maximum cardinality matchings in the graph associated with the problem, and then using these matchings to create optimal schedules. For some problem instances, however, this method will not work, since the problems are NP-hard. In this note, a previously open problem involving additional resource constraints is shown to have a polynomial algorithm using cardinality matching method.", "e:keyword": ["Analysis of algorithms", "Computational complexity", "Networks/graphs", "Matchings", "Scheduling", "Sequencing", "Deterministic"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.749", "e:abstract": "This paper describes an application of multiattribute utility theory to support the selection of a technology for the disposition of surplus weapons-grade plutonium by the Department of Energy (DOE). This analysis evaluated 13 alternatives, examined the sensitivity of the recommendations to the weights and assumptions, and quantified the potential benefit of the simultaneous deployment of several technologies. The measures of performance that were identified through the creation of a hierarchy of objectives helped to organize the information collected during the evaluation process, and the results of the analysis were presented to DOE on several occasions. This analysis supported the final DOE recommendation to pursue a strategy of the parallel development of two of the most preferred technologies.", "e:keyword": ["Utility/preference", "Multiattribute", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.763", "e:abstract": "Fiacco and McCormick's 1968 book is a classic. It arose out of the authors' activities at the Research Analysis Corporation, and as such it is the record of an ambitious and productive collaboration. But it is more than that. The topics they studied and the situations they encountered are representative of the experiences of other researchers in the 1960s. At a deeper level, Fiacco and McCormick's book illustrates the influence of two great traditions of research: one based on linear programming, and the other based on the calculus of variations. The work of Fiacco and McCormick remains influential, as do the influences of these two traditions. An understanding of these influences illuminates not only the history of nonlinear programming, but also its present and future.", "e:keyword": ["Programming", "Nonlinear", "History", "Optimal control", "History"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.776", "e:abstract": "We consider a processor-sharing service system, where the service rate to individual customers decreases as the load increases. Each arriving customer may observe the current load and should then choose whether to join the shared system. The alternative is a constant-cost option, modeled here for concreteness as a private server (e.g., a personal computer that serves as an alternative to a central mainframe computer). The customers wish to minimize their individual service times (or an increasing function thereof). However, the optimal choice for each customer depends on the decisions of subsequent ones, through their effect on the future load in the shared server. This decision problem is analyzed as a noncooperative dynamic game among the customers. We first show that any Nash equilibrium point consists of threshold decision rules and establish the existence and uniqueness of a <i>symmetric</i> equilibrium point. Computation of the equilibrium threshold is demonstrated for the case of Poisson arrivals, and some of its properties are delineated. We next consider a reasonable dynamic learning scheme, which converges to the symmetric Nash equilibrium point. In this model customers simply choose the better option based on available performance history. Convergence of this scheme is illustrated here via a simulation example and is established analytically in subsequent work.", "e:keyword": ["Queues", "Optimization", "Processor sharing", "Games", "Stochastic", "Dynamic equilibrium and learning"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.785", "e:abstract": "The goal of this paper is to understand choices of networks and schedules by a profit maximizing airline. By “network” we mean the routing pattern for planes and by “schedule” we mean the frequency of service between cities <i>and</i> the amount of time put into the schedule to assure on-time arrival. This paper analyzes network and schedule choice using an “idealized” model that permits derivation of analytic, closed form expressions for airline and passenger costs. Many important conclusions are obtained. It is optimal for a profit maximizing airline to design its network and schedule to minimize the sum of airline and passenger costs. Profit maximizing choice of schedule frequency depends on the network. Direct service has lower schedule frequency than other networks.Parametric studies are performed on the effect of distance between cities, demand rate, and the number of cities served on the choice of the network. Some conclusions are: (1) If the distance between cities is very small, then direct service is optimal; otherwise, other networks, such as hub and spoke are optimal. (2) Similarly, for very high demand rates, direct service is optimal; and for intermediate values, hub and spoke is optimal. (3) If the number of cities is small, direct service dominates; and if it is large, hub and spoke is optimal.We note that any airline's schedule includes safety time as a buffer against delays, and we demonstrate that schedule reliability is highest for direct routing. Surprisingly, the amount of time that is added to the schedule to buffer delays is relatively less in direct networks than in other networks. This can explain the superior on-time performance and high equipment utilization of direct carriers such as Southwest Airlines.", "e:keyword": ["Networks", "Design of airline networks", "Transportation", "Economics", "Economics", "Consumer choice", "Production cost"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.805", "e:abstract": "Consider a multiperiod airline overbooking problem that relates to a single-leg flight and a single service class. Passengers may cancel their reservations at any time, including being <i>no-shows</i> at flight-time. At that time, the airline <i>bumps</i> passengers in excess of flight capacity and pays a penalty for so doing. We give conditions on the fares, refunds, and distributions of passenger demand for reservations and cancellations in each period, and on the bumping penalty function, that ensure that a <i>booking-limit policy</i> is optimal, i.e., in each period the airline accepts reservation requests up to a <i>booking limit</i> if the number of initial reservations is less than that booking limit, and declines reservation requests otherwise. The optimal booking limits are easily computed. We give conditions under which the optimal booking limits are monotone in the time to flight departure. The model is applied to the discount allocation problem in which lower fare classes book prior to higher fare classes.", "e:keyword": ["Dynamic programming", "Markov", "Finite state", "Markov decision model for airline overbooking", "Inventory/production", "Uncertain yield and demand", "Transportation", "Airline seat inventory control", "Yield management"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.820", "e:abstract": "The <i>Crew Rostering Problem</i> (CRP) aims at determining an optimal sequencing of a given set of duties into rosters satisfying operational constraints deriving from union contract and company regulations. Previous work on CRP addresses mainly urban mass-transit systems, in which the minimum number of crews to perform the duties can easily be determined, and the objective is to evenly distribute the workload among the crews. In typical railway applications, however, the roster construction has to take into account more involved sequencing rules, and the main objective is the minimization of the number of crews needed to perform the duties. In this paper we propose a basic model for CRP, and describe a Lagrangian lower bound based on the solution of an assignment problem on a suitably defined graph. The information obtained through the lower bound computation is used to drive an effective algorithm for finding a tight approximate solution to the problem. Computational results for real-world instances from railway applications involving up to 1,000 duties are presented, showing that the proposed approach yields, within short computing time, lower and upper bound values that are typically very close. The code based on the approach we propose won the FARO competition organized by the Italian railway company, Ferrovie dello Stato SpA, in 1995.", "e:keyword": ["Networks/graphs", "Matchings", "Programming", "Heuristics", "Relaxations", "Applications", "Transportation", "Network models", "Personnel scheduling", "Railways"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.831", "e:abstract": "A customer order to a multi-item inventory system typically consists of several different items in different amounts. The probability of satisfying an arbitrary demand within a prespecified time window, termed the <i>order fill rate</i>, is an important measure of customer satisfaction in industry. This measure, however, has received little attention in the inventory literature, partly because its evaluation is considered a hard problem. In this paper, we study this performance measure for a base-stock system in which the demand process forms a multivariate compound Poisson process and the replenishment leadtimes are constant. We show that the order fill rate can be computed through a series of convolutions of one-dimensional compound Poisson distributions and the batch-size distributions. This procedure makes the exact calculation faster and much more tractable. We also develop simpler bounds to estimate the order fill rate. These bounds require only partial order-based information or merely the item-based information. Finally, we investigate the impact of the standard independent demand assumption when the demand is actually correlated across items.", "e:keyword": ["Inventory/production", "Multi-item", "Operating characteristics", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.846", "e:abstract": "Businesses frequently have to decide which of their existing equipment to replace, taking into account future changes in capacity requirements. The significance of this decision becomes clear when one notes that expenditure on new plant and equipment is a significant proportion of the GDP in the United States. The equipment replacement literature has focused on the replacement issue, usually ignoring aspects such as future demand changes and economies of scale. On the other hand, the capacity expansion literature has focused on the expansion of equipment capacity to meet demand growth, considering economies of scale but ignoring the replacement aspect. This paper attempts to unify the two streams of research by developing a general model that considers replacement of capacity as well as expansion and disposal, together with scale economy effects. Even special cases of the problems discussed here, such as the parallel machine replacement problem, have been considered difficult so far. However, we show that the problem can be solved efficiently by formulating it in a novel, disaggregate manner and using a dual-based solution procedure that exploits the structure of the problem. We also provide computational results to affirm that optimal or near-optimal solutions to large, realistic problems can be determined efficiently. We demonstrate the robustness of this approach by showing how other realistic features such as quantity discounts in purchases, alternative technology types or suppliers, and multiple equipment types can be incorporated.", "e:keyword": ["Facilities/equipment planning", "Capacity expansion", "Equipment replacement", "Integer programming applications", "Model for equipment planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.858", "e:abstract": "This paper studies the trade-off between inventory levels and the delivery leadtime offered to customers in achieving a target level of service. It addresses the question of how much a delivery leadtime can be reduced, per unit increase in inventory, at a fixed fill rate. We show that for a class of assemble-to-order models with stochastic demands and production intervals there is a simple <i>linear</i> trade-off between inventory and delivery leadtime, in a limiting sense, at high fill rates. The limiting slope is easy to calculate and can be interpreted as the approximate marginal rate for trading off inventory against leadtime at a constant level of service. We also investigate how various model features affect the trade-off—in particular, the impact of orders for multiple units of a single item and of orders for multiple units of different items.", "e:keyword": ["Production-inventory", "Assemble-to-order", "Fill rate", "Trade-off", "Exponential tail probability", "Delivery leadtime"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.872", "e:abstract": "In the flow shop weighted completion time problem, a set of jobs has to be processed on <i>m</i> machines. Every machine has to process each one of the jobs, and every job has the same routing through the machines. The objective is to determine a sequence of the jobs on the machines so as to minimize the sum of the weighted completion times of all jobs on the final machine. In this paper, we present a characterization of the asymptotic optimal solution value for general distributions of the job processing times and weights. In particular, we show that the optimal objective value of this problem is asymptotically equivalent to certain single and parallel machine scheduling problems. This characterization leads to a better understanding of the effectiveness of the celebrated weighted shortest processing time algorithm, as well as to the development of an effective algorithm closely related to the profile fitting heuristic, which was previously utilized for flow shop makespan problems. Computational results show the effectiveness of WSPT and this modified profile fitting heuristic on a set of random test problems.", "e:keyword": ["Production/scheduling", "Multiple machine sequencing", "Flow shop weighted completion time problem", "Analysis of algorithm", "Probabilistic analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.883", "e:abstract": "In this paper we address periodic base-stock policies for stochastic economic lot scheduling problems. These represent manufacturing settings in which multiple items compete for the availability of a common capacity source, in the presence of setup times and/or costs, incurred when switching between items, and in the presence of uncertainty regarding demand patterns, production, and setup times. Under periodic base-stock policies, items are produced according to a given periodic item-sequence. This paper derives effective heuristics for the design of a periodic item-sequence minimizing system-wide costs. This sequence is constructed based on desirable production frequencies for the items, obtained as the solution of lower bound mathematical programs. An extensive numerical study gauges the quality of the proposed heuristics.", "e:keyword": ["Inventory/production", "Multi-item/echeon/stage", "Inventory/production", "Stochastic", "Production/scheduling", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.899", "e:abstract": "For a single product, single-stage capacitated production-inventory model with stochastic, periodic (cyclic) demand, we find the optimal policy and characterize some of its properties. We study the finite-horizon, the discounted infinite-horizon and the infinite-horizon average cases. A simulation based optimization method is provided to compute the optimal parameters. Based on a numerical study, several insights into the model are also provided.", "e:keyword": ["Inventory/production", "Stochastic", "Optimal policies", "Nonstationary", "Simulation", "Infinitesimal perturbation analysis", "Dynamic programming", "Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.912", "e:abstract": "A new bounding procedure for the Quadratic Assignment Problem (QAP) is described which extends the Hungarian method for the Linear Assignment Problem (LAP) to QAPs, operating on the four dimensional cost array of the QAP objective function. The QAP is iteratively transformed in a series of equivalent QAPs leading to an increasing sequence of lower bounds for the original problem. To this end, two classes of operations which transform the four dimensional cost array are defined. These have the property that the values of the transformed objective function Z′ are the corresponding values of the “old” objective function Z, shifted by some amount C. In the case that all entries of the transformed cost array are nonnegative, then C is a lower bound for the initial QAP. If, moreover, there exists a feasible solution U to the QAP, such that its value in the transformed problem is zero, then C is the optimal value of Z and U is an optimal solution for the original QAP. The transformations are iteratively applied until no significant increase in constant C as above is found, resulting in the so called Dual Procedure (DP).Several strategies are listed for appropriately determining C, or equivalently, transforming the cost array. The goal is the modification of the elements in the cost array to obtain new equivalent problems that bring the QAP closer to solution. In some cases the QAP is actually solved, though solution is not guaranteed. The close relationship between the DP and the Linear Programming formulation of Adams and Johnson is presented. The DP attempts to solve Adams and Johnson's CLP, a continuous relaxation of a linearization of the QAP. This explains why the DP produces bounds close to the optimum values for CLP calculated by Johnson in her dissertation and by Resende, et al. in their Interior Point Algorithm for Linear Programming.The benefit of using DP within a branch-and-bound algorithm is described. Then, two versions of DP are tested on the Nugent test instances from size 5 to size 30, as well as several other test instances from QAPLIB. These compare favorably with earlier bounding methods.", "e:keyword": ["Integer programming", "Bounding method for Quadratic Assignment Problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.923", "e:abstract": "A key central tenet of decision theory is that decomposing an uncertain event into sub-events should not change the overall probability assigned to that uncertain event. As we show, both quantum physics and behavioral decision theory appear to systematically violate this principle in very similar ways. These results suggest that the structuring phase of decision analysis—which specifies how various events are decomposed—helps shape the subjective probabilities which will ultimately be assigned to those events.", "e:keyword": ["Decision analysis", "Theory", "Philosophy of modeling", "Probability", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.927", "e:abstract": "We are concerned with a queueing network, described by a continuous-time Markov chain, in which each node is quasi-reversible. A new class of local balance equations is derived for the Markov chain with respect to a product-form distribution, which simultaneously provides an alternative and short proof for product form results of queueing networks with customers and signals. Furthermore, if each node is internally balanced, i.e., the total arrival rate equals the total departure rate for each node, the Markov chain is locally balanced in the ordinary sense. Our arguments reveal a close relationship between quasi-reversibility and local balance, and provide further insights into how transition rates are balanced in product form queueing networks.", "e:keyword": ["Probability", "Markov processes", "Local balances", "Queues", "Network", "Quasi-reversibility", "Product form solutions"]}, {"@id": "http://dx.doi.org/10.1287/opre.46.6.934", "e:abstract": "In this paper we consider the Newsvendor Problem. Intuition may lead to the hypothesis that in this stochastic inventory problem a higher demand variability results in larger variances and in higher costs. In a recent paper, Song (1994a) has proved that the intuition is correct for many demand distributions that are commonly used in practice, such as for the normal distribution function. However, this paper shows that there exist demand distributions for which the intuition is misleading, i.e., for which larger variances occur in combination with lower costs. To characterize these demand distributions we use stochastic dominance relations.", "e:keyword": ["Inventory/production", "Stochastic uncertainty", "Probability", "Distribution comparisons"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.1", "e:abstract": "Many firms in the oil and gas business have long used decision analysis techniques to evaluate exploration and development opportunities and have looked at recent development in option pricing theory as potentially offering improvements over the decision analysis approach. Unfortunately, it is difficult to discern the benefits of the options approach from the literature on the topic: Most of the published examples greatly oversimplify the kinds of projects encountered in practice, and comparisons are typically made to traditional discounted cash flow analysis, which, unlike the option pricing and decision analytic approaches, does not explicitly consider the uncertainty in project cash flows. In this paper, we provide a tutorial introduction to option pricing methods, focusing on how they relate to and can be integrated with decision analysis methods, and describe some lessons learned in using these methods to evaluate some real oil and gas investments.", "e:keyword": ["Decision analysis", "Applications", "Evaluating oil and gas investments", "Finance", "Investments", "Real options"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.102", "e:abstract": "An oligopoly with spatially dispersed generators and consumers and with multi-period demand is modeled in this paper. The producers are assumed to behave in a Cournot manner with regulated transmission prices. A (generalized) Nash equilibrium is sought. The story of the game is as follows. Each generator takes its rivals' output (generation, supply, and flows) and the prices for transmission services as fixed when it decides upon its output to maximize its profit; the transmission firm takes the quantities of transmission services demanded by the generators as fixed when it determines the transmission prices according to certain regulatory rules. An equilibrium of the model is a set of generation output at which no generator will obtain more profit if it unilaterally modifies its output from this set, and a set of transmission prices satisfying certain regulatory requirements. A variational inequality approach is used for computing the equilibria of the model. Using the same approach, two variants of the model, respectively based on average-cost and marginal-cost pricing for transmission services, are also formulated. This model is applied to simulate a long-run electricity market where transmission prices are regulated.", "e:keyword": ["Industries", "Electric", "Modeling a deregulated electric industry", "Games", "Noncooperative", "Nash equilibrium of the Cournot game", "Programming", "Complementarity", "Complementarity approach"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.113", "e:abstract": "In this paper we study the weighted tardiness job-shop scheduling problem, taking into consideration the presence of random shop disturbances. A basic thesis of the paper is that global scheduling performance is determined primarily by a subset of the scheduling decisions to be made. By making these decisions in an a priori static fashion, which maintains a global perspective, overall performance efficiency can be achieved. Further, by allowing the remaining decisions to be made dynamically, flexibility can be retained in the schedule to compensate for unforeseen system disturbances. We develop a decomposition method that partitions job operations into an ordered sequence of subsets. This decomposition identifies and resolves a “crucial subset” of scheduling decisions through the use of a branch-and-bound algorithm. We conduct computational experiments that demonstrate the performance of the approach under deterministic cases, and the robustness of the approach under a wide range of processing time perturbations. We show that the performance of the method is superior, particularly for low to medium levels of disturbances.", "e:keyword": ["Production/scheduling", "Approximations/heuristic", "Job-shop scheduling problem", "Decision analysis", "Risk", "Robust optimization and control", "Programming", "Integer", "Algorithms", "Branch-and-bound", "Combinatorial optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.125", "e:abstract": "This study introduces a generalization of reversibility called <i>partition-reversibility</i>. A Markov jump process is partition-reversible if the average numbers of its transitions between sets that partition the state space are equal. In this case, its stationary distribution is obtainable by solving the balance equations separately on the sets. We present several characterizations of partition-reversibility and identify subclasses of treelike, starlike, and circular partition-reversible processes. A new circular birth-death process is used in the analysis. The results are illustrated by a queueing model with controlled service rate, a multitype service system with blocking, and a parallel-processing model. A few comments address partition-reversibility for non-Markovian processes.", "e:keyword": ["Probability", "Markov processes", "Partition-reversible"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.131", "e:abstract": "We study a multicomponent, multiproduct production and inventory system in which individual components are made to stock but final products are assembled to customer orders. Each component is produced by an independent production facility with finite capacity, and the component inventory is controlled by an independent base-stock policy. For any given base-stock policy, we derive the key performance measures, including the probability of fulfilling a customer order within any specified time window. Computational procedures and numerical examples are also presented. A similar approach applies to the generic multi-item make-to-stock inventory systems in which a typical customer order consists of a kit of items.", "e:keyword": ["Inventory/production", "Multi-item", "Operating characteristics", "Stochastic", "Probability", "Applications", "Markov processes", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.150", "e:abstract": "We analyze an (<i>s</i>, <i>S</i>) continuous review perishable inventory system with a general renewal demand process and instantaneous replenishments. Though continuous review systems seem more amenable to optimization analysis than do periodic review systems, the existing literature addressing this type of model is rather limited. This limitation motivated us to seek greater understanding of this important class of inventory models. Using a Markov renewal approach, we obtain closed-form solutions for the steady state probability distribution of the inventory level and system performance measures. We then construct a closed-form expected cost function. Useful analytical properties for the cost function are identified and extensive computations are conducted to examine the impact of different parameters. The numerical results illustrate the system behavior and lead to managerial insights into controlling such inventory systems.", "e:keyword": ["Inventory/production", "Perishable", "Continuous review", "Markov renewal"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.159", "e:abstract": "Combining parallel multiple recursive sequences provides an efficient way of implementing random number generators with long periods and good structural properties. Such generators are statistically more robust than simple linear congruential generators that fit into a computer word. We made extensive computer searches for good parameter sets, with respect to the spectral test, for combined multiple recursive generators of different sizes. We also compare different implementations and give a specific code in C that is faster than previous implementations of similar generators.", "e:keyword": ["Simulation", "Random number generation", "Multiple recursive", "Combined generators", "Lattice structure", "Spectral test"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.16", "e:abstract": "We provide a decision analysis perspective on the decision making process leading to the merger of The Institute of Management Sciences (TIMS) and the Operations Research Society of America (ORSA) to form the Institute for Operations Research and the Management Sciences (INFORMS). Throughout the merger negotiation era from 1989 until the merger in 1995, discussion regarding a possible merger was framed in an objectives-oriented manner characteristic of decision analysis methods. In addition, as ORSA and TIMS officers, we applied multiobjective decision analysis and financial analysis methods in portions of the planning and negotiation process leading to the formation of INFORMS. The merger process serves as an instructive case study of the uses and limitations of formal decision analysis methods in strategy formulation and implementation.", "e:keyword": ["Decision analysis", "Applications", "Merger of ORSA and TIMS", "Utility/preference", "Multiattribute", "Evaluating INFORMS merger options", "Professional", "OR/MS policy/standards", "Formation of INFORMS"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.165", "e:abstract": "We consider the routing-scheduling version of the flow-shop problem, where <i>n</i> jobs located at different nodes of a transportation network must be executed by <i>m</i> machines (servers) travelling between the jobs. The objective is to minimize the makespan. For this problem, we present a simple heuristic and analyze its worst-case performance.", "e:keyword": ["Approximations/heuristics", "Heuristic for routing-scheduling flow-shop"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.29", "e:abstract": "The recent HIV incidence rate (or hazard rate for infection) is an important quantity for use in monitoring the HIV/AIDS epidemic, evaluating HIV prevention programs, and allocating HIV prevention resources. Direct measurement of HIV incidence is difficult and time consuming, while estimating HIV incidence via backcalculation (deconvolution) using AIDS incidence data and the (presumed known) HIV incubation time distribution yields little information about recent infection. We propose a method for estimating recent HIV incidence in a population via a single sample at a single point in time. The method relies on understanding the progression of certain markers such as CD4 counts following infection. The actual formulas derived for the point and interval estimates of HIV incidence are simple and easy to use while the sample sizes needed to implement our approach are reasonable. We present two applications of our approach, comparing our results to those obtained from more conventional methods where possible.", "e:keyword": ["Health care", "Epidemiology", "Probability", "Applications", "Statistics", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.38", "e:abstract": "In stochastic dynamic programming (SDP) with continuous state and decision variables, the future value function is computed at discrete points in the state space. Interpolation can be used to approximate the values of the future value function between these discrete points. However, for large dimensional problems the number of discrete points required to obtain a good approximation of the future value function can be prohibitively large. Statistical methods of experimental design and function estimation may be employed to overcome this “curse of dimensionality.” In this paper, we describe a method for estimating the future value function by multivariate adaptive regression splines (MARS) fit over a discretization scheme based on orthogonal array (OA) experimental designs. Because orthogonal arrays only grow polynomially in the state-space dimension, our OA/MARS method is accurately able to solve higher dimensional SDP problems than previously possible. To our knowledge, the most efficient method published prior to this work employs tensor-product cubic splines to approximate the future value function (Johnson et al. 1993). The computational advantages of OA/MARS are demonstrated in comparisons with the method using tensor-product cubic splines for applications of an inventory forecasting SDP with up to nine state variables computed on a small workstation. In particular, the storage of an adequate tensor-product cubic spline for six dimensions exceeds the memory of our workstation, and the run time for an accurate OA/MARS SDP solution would be at least an order of magnitude faster than using tensor-product cubic splines for higher than six dimensions.", "e:keyword": ["Dynamic programming", "Applications", "Numerical solution of continuous-state dynamic program", "Dynamic programming", "Markov", "Finite-state", "Approximation of the value function in high dimensions", "Statistics", "Design of experiments", "Selection of discretization points in a continuous space"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.54", "e:abstract": "Part I of a two-part series, this paper recites a parable and formulates a stochastic optimization model that determines optimal link tolls on a road network whose users' value of time is a random variable. The parable, introducing the problem, demonstrates the importance of the variability of the value of time. The model, cast as a variational inequality, becomes a specialized form of a bicriterion user-equilibrium traffic assignment. Its solution is a set of efficient tolls for all links in the network. These tolls induce an equilibrium traffic flow that is at once system-optimal and user-optimal—for all trips, regardless of their value of time. Part II develops a solution algorithm, gives examples, and provides performance statistics.", "e:keyword": ["Transportation", "Toll-road planning", "Determining optimal toll policy", "Network optimization", "Stochastic modeling"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.65", "e:abstract": "Material handling systems are almost always intertwined with the production system they serve, but problems at this interface are rarely considered together. This paper develops a model that explicitly considers the costs of material handling and inventory to simultaneously design a material handling system and set shop floor inventory policy. The research was inspired by material handling problems we observed at computer manufacturing companies such as Apple Computer and Hewlett Packard Company. Shop floor inventory policies are often a result of trial and error or simple usage calculations—and material handling system design is usually addressed given an existing inventory policy. However, a tradeoff exists between shop floor inventory policy (reorder quantities and safety stock amounts) and demands for material handling. Inventory decisions can have a particularly large impact on cost when an automated material handling system, such as an automated guided vehicle system, is used to deliver materials. This paper develops a material handling/inventory system design model that is a nonlinear mixed integer program with nonlinear constraints, and presents a solution approach based on decomposition. Computational results show that simultaneous consideration of inventory policy and material handling system design can lead to significant reductions in overall production cost.", "e:keyword": ["Manufacturing", "Automated systems", "Facilities/equipment planning", "Design inventory/production", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.81", "e:abstract": "We consider networks in which a cost is associated with each arc or edge and a transition cost is associated with each node. This last cost is related to the presence of two technologies on the network and is incurred only when a flow enters and leaves the corresponding node on arcs of different types. The problem we consider consists in finding two node disjoint paths with minimum total cost. We show that it is strongly NP-complete. Then we propose two heuristics, study their worst case behavior, provide a lower bounding procedure based on Lagrangean relaxation, and finally embed those elements in a branch and bound procedure.", "e:keyword": ["Communications", "Telecommunications networks", "Network/graphs", "Flow algorithms", "Disjoint paths", "Programming", "Integer", "Algorithm for bifurcated routing with transition costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.1.93", "e:abstract": "This article proposes large-scale convex optimization problems to be solved via saddle points of the standard Lagrangian. A recent approach for saddle point computation is specialized, by way of a specific perturbation technique and unique scaling method, to convex optimization problems with differentiable objective and constraint functions. In each iteration the update directions for primal and dual variables are determined by gradients of the Lagrangian. These gradients are evaluated at perturbed points that are generated from current points via auxiliary mappings. The resulting algorithm suits massively parallel computing, though in this article we consider only a serial implementation. We test a version of our code embedded within GAMS on 16 nonlinear problems, which are mainly large. These models arise from multistage optimization of economic systems. For larger problems with adequate precision requirements, our implementation appears faster than MINOS.", "e:keyword": ["Convex programming", "Large scale", "Saddle points", "Parallel computation", "Stochastic optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.175", "e:abstract": "For training National Guard units, the U.S. Army National Guard will field 21 combat vehicle training simulators called <i>mobile trainers</i>. Each National Guard unit must train at a station that is not farther than a specified maximum travel distance from its armory. We address the problem of finding:• the optimum locations for the <i>home bases</i> for the mobile trainers,• the locations of <i>secondary training sites</i> to which the mobile trainers will travel to provide training, and• the actual routes that the mobile trainers will take to cover all these secondary training sites.The aim is to allocate each National Guard unit to a training site within the maximum travel distance from its armory, while simultaneously minimizing the mobile trainer fleet mileage and the total distance traveled by all units.The problem is too large and complex to solve as a single model. We apply a heuristic decomposition strategy to break the overall problem into manageable stages, developing suitable substitute objective functions for each. This approach led to a solution in which the mobile trainer fleet mileage is 72,850 miles per year: about 70% smaller than the 231,000 miles per year in the original Army's procurement plan. Our solution has been implemented (with minor modifications) by the Army.", "e:keyword": ["Integer programming", "Applications", "Optimizing training facility location and operation"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.183", "e:abstract": "In the newsvendor problem, a decision maker facing random", "e:keyword": ["Inventory/production", "Newsvendor"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.195", "e:abstract": "We analyze the problem of debt issuance through the sale of innovative financial products. The problem is broken down to questions of <i>designing</i> the financial products, specifying the <i>debt structure</i> with the amount issued in each product, and determining an optimal level of financial <i>leverage</i>. We formulate a hierarchical optimization model to integrate these three issues and provide constructive answers. Input data for the models are obtained from Monte Carlo simulation procedures that generate scenarios of holding period returns of the designed products.The hierarchical optimization model is specialized for the problem of issuing a portfolio of callable bonds to fund mortgage assets. The upper level optimization program is multimodal, and a tabu search procedure is developed for its solution. Empirical results illustrate the efficacy of the developed models in designing the appropriate structure of the callable bonds and making optimal allocations of equity and debt among the designed products. Computational results with the implementation of tabu search—on both serial and parallel computers—are also presented.", "e:keyword": ["Finance", "Security design and pricing", "Innovation", "Marketing", "New product design and pricing", "Programming", "Nonlinear utility maximization"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.209", "e:abstract": "This paper proposes an optimization model of incorporating competence sets of group decision makers to maximize the total benefit of the whole group. Such an incorporation model is formulated as finding a deduction graph linked from the nodes of existing competencies to the nodes of desired competencies. Compared with other methods treating competence set problems (Yu and Zhang 1991, Li and Yu 1994, and Shi and Yu 1996), the proposed model can solve problems involving multiple decision makers; in addition it allows the network to be cyclic and to contain compound nodes.", "e:keyword": ["Decision analysis", "Inference using integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.221", "e:abstract": "Introduction of product designs and process innovation requires a company to evaluate complex cost and environmental tradeoffs. In the past, these have not included environmental costs. This paper describes the first known analytical approach to capture comprehensively measurable corporate environmental impact considerations for the product life cycle. A mixed integer programming model is developed to select product and process alternatives while considering tradeoffs of yield, reliability, and business-focused environmental impacts. Explicit constraints for environmental impacts such as material consumption, energy consumption, and process waste generation are modeled for specified assembly and disassembly periods. The constraint sets demonstrate a new way to define the relationship between disassembly configurations and assembly activities through take-back rates. Use of the model as an industry decision tool is demonstrated with an electronics assembly case study.", "e:keyword": ["Environment", "Environmentally conscious design and manufacturing", "Production", "Strategic production planning", "Programming", "Mixed integer programming model"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.235", "e:abstract": "Product development has become the focal point of industrial competition and is the cornerstone of long-term survival for most firms. One of the major management challenges in product development is to deal with development risk in the design process. In this paper we provide a strategic guideline as to how the design process should be managed and controlled. We describe how design reviews and engineering resources can be scheduled as the control mechanisms to operationally manage development risk. The methodologies developed are an integral part of a project to fundamentally restructure product design processes at Rocketdyne Division of Rockwell International, which designs and develops liquid-propellant rocket propulsion systems.", "e:keyword": ["Design", "Reliability", "Inspection", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.247", "e:abstract": "This article describes a method for solving the crew rostering problem in air transportation. This problem consists of constructing personalized schedules that assign pairings, days off, and other activities to airline crew members. A generalized set partitioning model and a method using column generation have been used. This method has been adapted in a number of ways to take advantage of the nature of the problem and to accelerate solution. Numerical tests on problems from Air France have demonstrated that this method is capable of solving very large scale problems with thousands of constraints and hundreds of subproblems. The tests have also shown that these adaptations are capable of reducing solution time by a factor of about a thousand. Finally, results from this method are compared with those obtained with the method currently used at Air France.", "e:keyword": ["Programming", "Integer", "Large-scale systems", "Column generation", "Transportation", "Airline application", "Rostering", "Scheduling", "Personnel", "Airline crew members"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.264", "e:abstract": "In assemble-to-order production systems, and others of a similar flavor, it is often the case that orders for components of various types are placed simultaneously, but that these components are manufactured or assembled at separate facilities. The order process introduces correlation among the jobs at separate facilities. The purpose of this paper is to study the effect of this correlation on a variety of system performance measures.Consider a system that consists of <i>s</i> parallel servers, where each server has a finite buffer and is dedicated to a separate job type. Multiple classes of customer orders arrive to the system, where each class is composed of one or more unique job types. Upon the arrival of an order, each job in the order is separately routed to its designated buffer; if the buffer is full, that job is blocked and lost; otherwise, it enters the buffer and is served according to the FCFS discipline. Under Markovian assumptions, we systematically examine the impact of arrival correlations on <i>system-based</i> performance measures such as the queue length vector and the workload vector and <i>class-based</i> performance measures such as the waiting time vector and the order response time. Among other things, we establish several stochastic orders between performance vectors with different degrees of arrival correlations. We also show that greater arrival correlation can stochastically improve the worst component in a performance vector (e.g., the longest queue, the heaviest workload), reduce the expected sum of the <i>j</i> longest queues, 1 ≤ <i>j</i> ≤ <i>s</i>, and, for any given order type, increase its entering probability and reduce its order response time. Our results can also be extended to the compound Poisson arrival process, where each order contains multiple units of several job types.", "e:keyword": ["Probability", "Distribution comparisons", "Correlation structure of multiserver queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.277", "e:abstract": "In this paper, we present and solve a single-period, multiproduct, downward substitution model. Our model has one raw material as the production input and produces <i>N</i> different products as outputs. The demands and yields for the products are random. We determine the optimal production input and allocation of the <i>N</i> products to satisfy demands. The problem is modeled as a two-stage stochastic program, which we show can be decomposed into a parameterized network flow problem. We present and compare three different solution methods: a stochastic linear program, a decomposition resulting in a series of network flow subproblems, and a decomposition where the same network flow subproblems are solved by a new greedy algorithm.", "e:keyword": ["Inventory: uncertainty", "Random yield", "Inventory: multi-item", "Substitution", "Programming", "Stochastic", "Algorithms", "Greedy"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.291", "e:abstract": "Given a solution <i>x</i>* and an a priori estimated cost vector <b>c</b>, the inverse optimization problem is to identify another cost vector <b>d</b> so that <i>x</i>* is optimal with respect to the cost vector <b>d</b> and its deviation from <b>c</b> is minimum. In this paper, we consider the inverse spanning tree problem on an undirected graph <i>G</i> = (<i>N</i>, <i>A</i>) with <i>n</i> nodes and <i>m</i> arcs, and where the deviation between <b>c</b> and <b>d</b> is defined by the rectilinear distance between the two vectors, that is, <i>L</i><sub>1</sub> norm. We show that the inverse spanning tree problem can be formulated as the dual of an assignment problem on a bipartite network <i>G</i><sup>0</sup> = (<i>N</i><sup>0</sup>, <i>A</i><sup>0</sup>) with <i>N</i><sup>0</sup> = <i>N</i><sup>1</sup> ∪ <i>N</i><sup>2</sup> and <i>A</i><sup>0</sup> ⊆ <i>N</i><sup>1</sup> × <i>N</i><sup>2</sup>. The bipartite network satisfies the property that |<i>N</i><sup>1</sup>| = (<i>n</i> − 1), |<i>N</i><sup>2</sup>| = (<i>m</i> − <i>n</i> + 1), and |<i>A</i><sup>0</sup>| = <i>O</i>(<i>nm</i>). In general, |<i>N</i><sup>1</sup>| ≪ |<i>N</i><sup>2</sup>|. Using this special structure of the assignment problem, we develop a specific implementation of the successive shortest path algorithm that runs in <i>O</i>(<i>n</i><sup>3</sup>) time. We also consider the weighted version of the inverse spanning tree problem in which the objective function is to minimize the sum of the weighted deviations of arcs. The weighted inverse spanning tree can be formulated as the dual of the transportation problem. Using a cost scaling algorithm, this transportation problem can be solved in <i>O</i>(<i>n</i><sup>2</sup><i>m</i> log(<i>nC</i>)) time, where <i>C</i> denotes the largest arc cost in the data. Finally, we consider a minimax version of the inverse spanning tree problem and show that it can be solved in <i>O</i>(<i>n</i><sup>2</sup>) time.", "e:keyword": ["Networks/graphs", "Flow algorithms", "Minimum cost flow problem", "Networks/graphs", "Tree algorithms", "Inverse spanning tree problem", "Programming", "Linear", "Application of duality theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.299", "e:abstract": "We study estimators for the variance parameter σ<sup>2</sup> of a stationary process. The estimators are based on weighted Cramér-von Mises statistics, and certain weightings yield estimators that are “first-order unbiased” for σ<sup>2</sup>. We derive an expression for the asymptotic variance of the new estimators; this expression is then used to obtain the first-order unbiased estimator having the smallest variance among fixed-degree polynomial weighting functions. Our work is based on asymptotic theory; however, we present exact and empirical examples to demonstrate the new estimators' small-sample robustness. We use a single batch of observations to derive the estimators' asymptotic properties, and then we compare the new estimators among one another. In real-life applications, one would use more than one batch; we indicate how this generalization can be carried out.", "e:keyword": ["Simulation", "Statistical analysis", "Statistics", "Estimation", "Time series"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.310", "e:abstract": "This paper considers an <i>M</i>/<i>M</i>/1 queueing system with dynamically controlled arrival and service rates. At each arrival or service completion epoch, a decision maker chooses a pair of arrival and service rates from a finite set, and the system operates under these rates until the next arrival or service completion. There is a switching cost for changing the rates, and there is a cost per unit time of holding customers and using the arrival and service rates. The results describe natural conditions on the costs under which an optimal policy for either the discounted-cost or average-cost criterion is a hysteretic policy. Such a policy increases the service rate and decreases the arrival rate as the queue length increases. Furthermore, the control exhibits a stickiness or resistance to change, called hysteresis, due to the switching costs.", "e:keyword": ["Dynamic programming/optimal control", "Markovian queueing control"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.313", "e:abstract": "Power-tail distributions are those for which the reliability function is of the form <i>x</i><sup>−α</sup> for large <i>x</i>. Although they look well behaved, they have the singular property that E(<i>X</i><sup>ℓ</sup>) = ∞ for all ℓ ≥ α. Thus it is possible to have a distribution with an infinite variance, or even an infinite mean. As pathological as these distributions seem to be, they occur everywhere in nature, from the CPU time used by jobs on main-frame computers to sizes of files stored on discs, earthquakes, or even health insurance claims. Recently, traffic on the “electronic super highway” was revealed to be of this type, too.In this paper we first describe these distributions in detail and show their suitability to model self-similar behavior, e.g., of the traffic stated above. Then we show how these distributions can occur in computer system environments and develop a so-called truncated analytical model that in the limit is power-tail. We study and compare the effects on system performance of a GI/M/1 model both for the truncated and the limit case, and demonstrate the usefulness of these approaches particularly for Markov modeling with LAQT (Linear Algebraic Approach to Queueing Theory, Lipsky 1992) techniques.", "e:keyword": ["Queues", "Applications", "Powertail distributions in Markov modeling", "Queues", "Limit theorems", "Heavy load limit for G/M/1 queue", "Queues", "Transient results", "Sample sizes for stable results in powertail systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.327", "e:abstract": "The conclusion of a two-part series, this paper devises an algorithm that finds a system of optimal tolls in a road network whose trips have a stochastic value of time. As formulated in Part I, the model is a variational inequality, equivalent to a specialized bicriterion equilibrium traffic assignment whose solution reflects a traffic flow simultaneously user- and system-optimal. To compute these optimal tolls, our algorithm uses restricted simplicial decomposition. It solves the subproblem (direction step) with a novel multipath traffic assignment that obviates path enumeration. It solves the master problem (averaging step) using nonlinear complementarity. For real-world applications, where the algorithm's precondition that every congested arc may have a toll is impractical, this paper enhances the model to include link-specific upper and lower bounds on tolls. This more realistic model is solved with an heuristic using the optimization algorithm to its advantage. As our performance statistics show, the algorithm's speed makes it a practical planning tool. Experiments with the heuristic support the welcome hypothesis that a few well placed and properly priced tolls can reduce traffic congestion dramatically.", "e:keyword": ["Transportation equilibrium", "Bicriterion user- and system-optimized solution", "Optimization", "Simplicial decomposition", "Nonlinear complementarity"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.2.337", "e:abstract": "This article presents a risk-sensitive pricing model to maximize sales revenue of perishable commodities with fixed capacity and finite sales horizon. The model assumes a pair of predetermined prices and the Poisson demand process whose intensity is a decreasing function of price. When optimizing the expected revenue, management takes business risk into account by adding a penalty (or premium) to the objective function. We solve the continuous-time model with the exact solution in closed form. We further analyze the influence of risk attitude on optimal policies.", "e:keyword": ["Inventory/production", "Perishable/aging items", "Maximize revenue of perishable products", "Decision analysis", "Risk behavior of risk-averse management", "Probability: stochastic model applications", "Dynamic point process"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.345", "e:abstract": "Strategic asset-liability management is a primary concern in today's banking environment. In this paper, we present a methodology to assist in the process of asset-liability selection in a stochastic interest rate environment. In our approach, a quadratic optimizer is embedded in a simulation model and used to generate patterns of dividends, market value and duration of capital, for randomly generated interest rate scenarios. This approach can be used to formulate, test, and refine asset-liability strategies. We present results of applying this methodology to data from the Federal Home Loan Bank of New York.", "e:keyword": ["Finance", "Asset-liability management", "Financial institutions", "Banks"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.361", "e:abstract": "In this expository paper, we review a variety of resource allocation problems in which it is desirable to allocate limited resources <i>equitably</i> among competing activities. Applications for such problems are found in diverse areas, including distribution planning, production planning and scheduling, and emergency services location. Each activity is associated with a performance function, representing, for example, the weighted shortfall of the selected activity level from a specified target. A resource allocation solution is called <i>equitable</i> if no performance function value can be improved without either violating a constraint or degrading an already equal or worse-off (i.e., larger) performance function value that is associated with a different activity. A <i>lexicographic minimax</i> solution determines this equitable solution; that is, it determines the lexicographically smallest vector whose elements, the performance function values, are sorted in nonincreasing order. The problems reviewed include large-scale allocation problems with multiple knapsack resource constraints, multiperiod allocation problems for storable resources, and problems with substitutable resources. The solution of large-scale problems necessitates the design of efficient algorithms that take advantage of special mathematical structures. Indeed, efficient algorithms for many models will be described. We expect that this paper will help practitioners to formulate and solve diverse resource allocation problems, and motivate researchers to explore new models and algorithmic approaches.", "e:keyword": ["Programming", "Large-scale systems", "Resource allocation algorithms", "Programming", "Multiple criteria", "Lexicographic minimax objective", "Production/scheduling", "Applications", "Resource allocation models"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.379", "e:abstract": "We consider the problem of maximizing a weighted sum of expected rewards in steady-state in multiclass loss networks under dynamic routing and admission control, with Poisson arrivals and exponentially distributed holding times. By aggregating the underlying Markov decision process, we derive linear programming relaxations that contain the achievable performance region under all admissible policies and lead to a series of progressively tighter upper bounds. These relaxations allow stronger bounds at the expense of higher computational times. We further propose a series of routing and admission control policies from the relaxations that outperform, in computational experiments, other heuristic policies, such as the dynamic alternative routing with trunk reservation and the least busy alternative routing, variations of which are used in practice. The suboptimality guarantees defined as best bound/best policy range from 0 to 2.5% under symmetry conditions (symmetric network topology, arrival rates, link capacities, rewards), and from 4% to 10% under asymmetry conditions. We discuss the qualitative behavior of these policies and obtain insights about their performance.", "e:keyword": ["Programming", "Dynamic/optimal control", "Upper bounds for performance of loss network", "Communication heuristics", "Policies for dynamic routing"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.395", "e:abstract": "This paper develops an optimization model and methodology to support tactical planning in a high mix, low volume electronics assembly environment. The model assigns product families to parallel surface mount assembly lines to minimize setup cost on the placement machines while ensuring that the facilities are not overloaded. To capture the impact of product assignment decisions on setup cost, we consider a partial setup policy of mounting some components permanently on each placement machine and loading other components as needed for each product. We formulate the tactical planning problem as an integer program, and show that even the special case of minimizing the setup cost on a single placement machine, for a given assignment of products, is NP-hard. Our solution method for the general problem, based on column generation, provides both heuristic solutions and lower bounds. Two subproblems that we solve—a product selection subproblem and a setup optimization subproblem—might be independently useful for short-term production planning. Computational experience shows that the approach is effective, providing solutions that are within 1.5% of optimality on average and reducing setup costs considerably compared to a complete setup policy.", "e:keyword": ["Industry", "Computer/electronic", "Planning model for electronics assembly", "Production/scheduling", "Planning", "Parallel electronic assembly lines", "Programming", "Integer algorithms", "Column generation for technical production planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.410", "e:abstract": "We study the inspection process in the context of multistage batch manufacturing, focusing on interstage coordination under capacity limits. The problem is formulated as a constrained Markov decision program. We establish the optimality of a sequential policy that is characterized by a sequence of thresholds, with certain randomization at the thresholds. We further show that such an optimal policy can be completely derived through solving a linear program, and that randomization is needed at no more than two threshold values. We discuss an application in semiconductor wafer fabrication, which motivates our study.", "e:keyword": ["Dynamic programming", "Markov-finite state", "Constrained Markov decision processes", "Probability", "Stochastic model applications", "K-submodularity and threshold optimal policy", "Reliability", "Quality control", "Sequential inspection with capacity constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.422", "e:abstract": "This paper addresses a stochastic scheduling problem in which a set of independent jobs are to be processed by a number of identical parallel machines under a common deadline. Each job has a processing time, which is a random variable with an arbitrary distribution. Each machine is subject to stochastic breakdowns, which are characterized by a Poisson process. The deadline is an exponentially distributed random variable. The objective is to minimize the expected costs for earliness and tardiness, where the cost for an early job is a general function of its earliness while the cost for a tardy job is a fixed charge. Optimal policies are derived for cases where there is only a single machine or are multiple machines, the decision-maker can take a static policy or a dynamic policy, and job preemptions are allowed or forbidden. In contrast to their deterministic counterparts, which have been known to be NP-hard and are thus intractable from a computational point of view, we find that optimal solutions for many cases of the stochastic problem can be constructed analytically.", "e:keyword": ["Production/scheduling", "Sequencing", "Earliness/tardiness", "Multiple machines", "Production/scheduling", "Stochastic", "Random processing times", "Machine breakdowns", "Deadline"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.438", "e:abstract": "We consider here the optimal routing of customers, arriving to a system consisting of two heterogeneous parallel servers. The service times of the two servers have an increasing hazard rate. The arrival process is a general renewal process. The cost of holding <i>x</i> customers in the system per time unit is a nondecreasing and convex function. The objective is to minimize the expected discounted holding cost. We show some monotonicity properties of the optimal policy. Then we show that the optimal policy routes an arriving customer to the fastest server whenever this server has the lowest workload.", "e:keyword": ["Queues", "Optimization", "Optimal routing", "Dynamic programming/optimal control", "Semi-Markov"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.445", "e:abstract": "We present an <i>O</i>(<i>nm</i>)-time network simplex algorithm for finding a tree of shortest paths from a given node to all other nodes in a network of <i>n</i> nodes and <i>m</i> directed arcs or finding a directed cycle of negative length. The worst-case running time of this algorithm is as fast as that proved for any strongly polynomial algorithm and faster than that proved for any previously proposed simplex algorithm for this problem. We also show that this algorithm can be implemented in <i>O</i>(<i>nlogn</i>) time using <i>O</i>((<i>m</i>/<i>logn</i>) + <i>n</i>) exclusive read–exclusive write processors of a parallel random access machine.", "e:keyword": ["Networks/graphs", "Distance algorithms", "General shortest paths", "Programming", "Linear algorithms", "Network simplex", "Analysis of algorithms", "Computational complexity", "Strongly polynomial"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.449", "e:abstract": "Consider an assignment problem in which persons are qualified for some but usually not all of the jobs. Moreover, assume persons belong to given seniority classes and jobs have given priority levels. Seniority constraints impose that the solution be such that no unassigned person can be given a job unless an assigned person with the same or higher seniority becomes unassigned. Priority constraints specify that the solution must be such that no unassigned job can become assigned without a job with the same or higher priority becoming unassigned. It is shown that: (i) adding such constraints does not reduce and may even increase the number of assigned persons in the optimal solution; (ii) using a greedy heuristic for constrained assignment (as often done in practice) may reduce the number of assigned persons by half, and (iii) an optimal solution to the assignment problem with both types of constraints can be obtained by solving a classical assignment problem with adequately modified coefficients.", "e:keyword": ["Health care", "Hospitals", "Network/graphs", "Theory", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.454", "e:abstract": "This paper addresses the simultaneous determination of pricing and inventory replenishment strategies in the face of demand uncertainty. More specifically, we analyze the following single item, periodic review model. Demands in consecutive periods are independent, but their distributions depend on the item's price in accordance with general stochastic demand functions. The price charged in any given period can be specified dynamically as a function of the state of the system. A replenishment order may be placed at the beginning of some or all of the periods. Stockouts are fully backlogged. We address both finite and infinite horizon models, with the objective of maximizing total expected discounted profit or its time average value, assuming that prices can either be adjusted arbitrarily (upward or downward) or that they can only be decreased. We characterize the structure of an optimal combined pricing and inventory strategy for all of the above types of models. We also develop an efficient value iteration method to compute these optimal strategies. Finally, we report on an extensive numerical study that characterizes various qualitative properties of the optimal strategies and corresponding optimal profit values.", "e:keyword": ["Inventory/production", "Uncertainty", "Stochastic", "Operating characteristics", "Planning horizons", "Marketing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.476", "e:abstract": "This paper uses computational complexity theory to assess the difficulty of various discrete event simulation problems. More specifically, accessibility of states, ordering of events, noninterchangeability of model implementations, and execution stalling for discrete event simulations are formally stated as search problems and proven to be <i>NP</i>-hard. The consequences of these results cover a wide range of modeling and analysis problems in simulation. For example, problems associated with certain variance reduction techniques, model verification, model validation, and the applicability of infinitesimal perturbation analysis, among others, are deemed intractable.", "e:keyword": ["Simulation", "Modeling", "Validation", "Verification", "Computational complexity"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.482", "e:abstract": "We formulate and solve a location problem that determines where to position punt returners to maximize the number of punts caught. The problem is unusual within the location literature because it includes the dimension of time as well as Euclidean distance. The parameters of the model are estimated from actual punt return data. Our major finding is that the standard horizontal configuration of two punt returners results in only a small increase in the percentage of punts fielded over the case where a single returner is used. Moreover if a punter is not “directionalizing,” the vertical configuration of two returners (the punter and two returners are colinear) outperforms a horizontal configuration.", "e:keyword": ["Location", "Decision analysis", "Probability"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.3.488", "e:abstract": "We describe all possible asymptotic behavior of “bucket brigade” production lines with two or three workers, each characterized by a constant work velocity. The results suggest wariness in interpreting simulation results. They also suggest a strategy for partitioning a workforce into effective teams to staff the lines.", "e:keyword": ["Production/scheduling", "Line balancing", "Self-balancing bucket brigade lines", "Mathematics", "Fixed points", "Asymptotic behavior of bucket brigade lines"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.495", "e:abstract": "This paper addresses a real-life unidimensional cutting stock problem. The objective is not only to minimize trim loss, as in traditional cutting stock problems, but also to minimize cutting time. A variety of technical constraints are taken into account. These constraints often arise in the iron and steel cutting industry. Since cutting stock problems are well known to be NP-hard, it is prohibitive to obtain optimal solutions. We develop approximation algorithms for different purposes: quick response algorithms for individual customer requirement planning to build a quotation, and elaborate algorithms to provide a production plan for the next day. These latter algorithms are submitted to less strict computation time limitations. Computational results show that our algorithms improve by 8% the performance of our partner company where the cutting plan had been carried out manually by very experienced people. Numerical comparison for small sized problems shows that these algorithms provide solutions very close to optimal. These algorithms have been implemented in the company.", "e:keyword": ["Cutting stock/trim", "Approximation algorithms to solve cutting stock problems", "Approximations", "Heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.509", "e:abstract": "We propose a new estimator of steady-state blocking probabilities for simulations of stochastic loss models that can be much more efficient than the natural estimator (ratio of losses to arrivals). The proposed estimator is a convex combination of the natural estimator and an indirect estimator based on the average number of customers in service, obtained from Little's law (<i>L</i> = λ<i>W</i>). It exploits the known offered load (product of the arrival rate and the mean service time). The variance reduction is dramatic when the blocking probability is high and the service times are highly variable. The advantage of the combination estimator in this regime is partly due to the indirect estimator, which itself is much more efficient than the natural estimator in this regime, and partly due to strong correlation (most often negative) between the natural and indirect estimators. In general, when the variances of two component estimators are very different, the variance reduction from the optimal convex combination is about 1 − ρ<sup>2</sup>, where ρ is the correlation between the component estimators. For loss models, the variances of the natural and indirect estimators are very different under both light and heavy loads. The combination estimator is effective for estimating multiple blocking probabilities in loss networks with multiple traffic classes, some of which are in normal loading while others are in light and heavy loading, because the combination estimator does at least as well as either component estimator, and it provides improvement as well.", "e:keyword": ["Simulation", "Efficiency", "Variance reduction for estimates of blocking probabilities", "Queues", "Simulation", "Efficient simulation estimators for loss models", "Communications", "Efficient simulation of loss networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.524", "e:abstract": "We analyze the performance of a tandem queueing network populated by two customer types. The interarrival times of each type and the service times of each type at each station are independent random variables with general distributions, but the load on each station is assumed to be identical. A setup time is incurred when a server switches from one customer type to the other, and each server employs an exhaustive polling scheme. We conjecture that a time scale decomposition, which is known to occur at the first station under heavy traffic conditions, holds for the entire tandem system, and we employ heavy traffic approximations to compute the sojourn time distribution for a customer that arrives to find the network in a particular state. When setup times are zero (except perhaps at the first station) and additional “product-form” type assumptions are imposed, we find the steady-state sojourn time distribution for each customer type.", "e:keyword": ["Queues", "Heavy traffic approximations", "Inventory/production", "Multi-item", "Multi-stage systems with lot-sizing"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.535", "e:abstract": "This work is concerned with manufacturing systems with two failure-prone tandem machines. The production is regulated by a continuous version of buffer control. Our goal is to obtain an optimal buffer-control policy to minimize a long-run average cost function. Concentrating on threshold type of control policies, our effort is devoted to parameter optimization problems for the continuous material produce-to-stock models. We estimate the gradients of the cost function with respect to the parameter using perturbation analysis techniques, and approximate the optimal value of the parameter via a constant step-size stochastic approximation algorithm. An analysis for error accumulation in perturbation propagation is undertaken, and a sufficient condition for breaking the propagation chain is derived. In addition, we show that the event of breaking the perturbation propagation chain is recurrent if the system has sufficient capacity, derive the consistency of the gradient estimators, and establish the convergence of the iterative algorithm. We also treat non-Markovian models with the machine repair time following an Erlang distribution, and provide numerical examples to illustrate the proposed algorithm.", "e:keyword": ["Production", "Stochastic", "Manufacturing", "Performance/productivity"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.550", "e:abstract": "Two people are placed randomly and independently on a street of unit length. They attempt to find each other in the shortest possible expected time. We solve this problem, assuming each searcher knows where he or she is on the street, for monotonic density functions for the initial placement (this includes the uniform pdf as a special case). This gives an example of a rendezvous search problem where there is no advantage in being allowed to use asymmetric strategies. We also solve some corresponding problems for the circle when asymmetric strategies are permitted: One of these shows that it can sometimes be optimal for one player to wait for the other to find him.", "e:keyword": ["Search and surveillance", "Rendezvous search on the interval and the circle"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.559", "e:abstract": "In a multiproduct, discrete-item manufacturing environment, it is often more economical to have one “flexible” machine to produce several products rather than to have a dedicated machine for the production of each product. We consider the problem of scheduling multiple products on a single-finite-capacity resource when faced with product-dependent, but sequence-independent, changeover costs and times. The problem horizon is assumed finite and the demand time-varying and known over this horizon. The objective is to find a feasible schedule that meets all demand on time and minimizes the total changeover cost.With the “delivery-on-time-or-else” mentality and the trend toward shorter order-to-delivery lead times, scheduling and managing changeovers has become just that much more important. We first present analytical results to reduce the feasible solution space and then develop a branch-and-bound algorithm to solve this NP-complete problem. Computational results are presented that show that the algorithm can solve realistic size problems in a reasonable amount of time.", "e:keyword": ["Production/scheduling", "Sequencing", "Deterministic", "Single machine", "Changeover costs and times"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.570", "e:abstract": "Since Balas and Zemel in the 1980s introduced the so-called core problem as an efficient tool for solving the Knapsack Problem, all the most successful algorithms have applied this concept. Balas and Zemel proved that if the weights in the core are uniformly distributed then there is a high probability for finding an optimal solution in the core. Items outside the core may be fathomed because of reduction rules.This paper demonstrates that generally it is not reasonable to assume a uniform distribution of the weights in the core, and it is experimentally shown that the heuristic proposed by Balas and Zemel does not find as good solutions as expected. Also, other algorithms that solve some kind of core problem may be stuck by difficult cores. This behavior has apparently not been noticed before because of unsufficient testing.Capacities leading to difficult problems are identified for several categories of instance types, and it is demonstrated that the hitherto applied test instances are easier than the average. As a consequence we propose a series of new randomly generated test instances and show how recent algorithms behave when applied to these problems.", "e:keyword": ["Programming", "Integer", "Algorithms", "Knapsack problem", "Analysis of algorithms", "Expected hardness", "Statistics", "Cluster analysis", "Applied to Knapsack problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.576", "e:abstract": "Nonstationary inventory problems with set-up costs, proportional ordering costs, and stochastic demands occur in a large number of industrial, distribution, and service contexts. It is well known that nonstationary (<i>s</i>, <i>S</i>) policies are optimal for such problems. In this paper, we propose a simple, myopic heuristic for computing the policies. The heuristic involves approximating the future problem at each period by a stationary one and obtaining the solution to the corresponding stationary problem. We numerically compare our heuristic with an earlier myopic heuristic and the optimal dynamic programming solution procedure. Over all problems tested, the new heuristic averaged 1.7% error, compared with 2.0% error for the old procedure, and is on average 399 times as fast as the D.P. and 2062 as fast as the old heuristic. Moreover, our heuristic, owing to its myopic nature, requires the demand data only a few periods into the future, while the dynamic programming solution needs the demand data for the entire time horizon—which are typically not available in most practical situations.", "e:keyword": ["Inventory/production", "Heuristics", "Myopic heuristics", "Inventory", "Stochastic", "Random demands", "Inventory", "Review/lead times", "Periodic review and ordering set-up costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.585", "e:abstract": "We analyze the performance of a <i>splitting</i>technique for the estimation of rare event probabilities by simulation. A straightforward estimator of the probability of an event evaluates the proportion of simulated paths on which the event occurs. If the event is rare, even a large number of paths may produce little information about its probability using this approach. The method we study reinforces promising paths at intermediate thresholds by splitting them into subpaths which then evolve independently. If implemented appropriately, this has the effect of dedicating a greater fraction of the computational effort to informative runs. We analyze the method for a class of models in which, roughly speaking, the number of states through which each threshold can be crossed is bounded. Under additional assumptions, we identify the optimal degree of splitting at each threshold as the rarity of the event increases: It should be set so that the expected number of subpaths reaching each threshold remains roughly constant. Thus implemented, the method is provably effective in a sense appropriate to rare event simulations. These results follow from a branching-process analysis of the method. We illustrate our theoretical results with some numerical examples for queueing models.", "e:keyword": ["Simulation", "Efficiency", "Simulation of rare events by splitting", "Queues", "Simulation", "Simulation of rare events in queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.601", "e:abstract": "Contemporary management theories such as Just-in-Time and Total Quality Management emphasize variance reduction as a critical step in improving system performance. But little is said about how such efforts should be directed. Suppose a manager has only limited resources for variance reduction efforts. How should she allocate them among a set of competing activities? Which activity should receive the highest priority? We explore such questions in the context of a synchronous assembly line where processing times are variable, incomplete jobs are reworked at the end of the line, and the objective is to minimize the total expected work overload. Our results indicate that the station with the highest variance may not always be the best choice for variance reduction. Identifying the set of stations that should receive variance reduction in an optimal solution is not trivial. Moreover, the variances at these stations may not be reduced by the same amount or to the same level. We establish that the remaining variances among stations that receive variance reduction must conform to one of two preferred structures: equal variance or spike-shaped. Dominance results are presented to identify the set of stations and the amount of reduction in an optimal solution.", "e:keyword": ["Inventory/production", "Multistage", "Continuous improvement of assembly lines", "Inventory/production", "Uncertainty", "Variance and variability reduction", "Production/scheduling", "Line balancing", "Stochastic processing times"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.619", "e:abstract": "We define a family of random trees in the plane. Their nodes of level <i>k</i>, <i>k</i>= 0, …, <i>m</i>are the points of a homogeneous Poisson point process Π<i><sub>k</sub></i>, whereas their arcs connect nodes of level <i>k</i>and <i>k</i>+ 1, according to the least distance principle: If <i>V</i>denotes the Voronoi cell w.r.t. Π<i><sub>k</sub></i><sub>+1</sub>with nucleus <i>x</i>, where <i>x</i>is a point of Π<i><sub>k</sub></i><sub>+1</sub>, then there is an arc connecting <i>x</i>to all the points of Π<i><sub>k</sub></i>that belong to <i>V</i>. This creates a family of stationary random trees rooted in the points of Π<i><sub>m</sub></i>. These random trees are useful to model the spatial organization of several types of hierarchical communication networks. In relation to these communication networks, it is natural to associate various cost functions with such random trees. Using point process techniques, like the exchange formula between two Palm measures, and integral geometry techniques, we show how to compute these average costs as functions of the intensity parameters of the Poisson processes. The formulas derived for the average value of these cost functions can then be exploited for parametric optimization purposes. Several applications to classical and mobile cellular communication networks are presented.", "e:keyword": ["Probabiltiy", "Stochastic model applications", "Networks/graphs", "Stochastic", "Communications", "Transportation", "Network models"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.4.632", "e:abstract": "We study a single period multiproduct inventory problem with substitution and proportional costs and revenues. We consider<i>N</i>products and<i>N</i>demand classes with full downward substitution, i.e., excess demand for class<i>i</i>can be satisfied using product<i>j</i>for<i>i</i>≥<i>j</i>. We first discuss a two-stage profit maximization formulation for the multiproduct substitution problem. We show that a greedy allocation policy is optimal. We use this to write the expected profits and its first partials explicitly. This in turn enables us to prove additional properties of the profit function and several interesting properties of the optimal solution. In a limited computational study using two products, we illustrate the benefits of solving for the optimal quantities when substitution is considered at the ordering stage over similar computations without considering substitution while ordering. Specifically, we show that the benefits are higher with high demand variability, low substitution cost, low profit margins (or low price to cost ratio), high salvage values, and similarity of products in terms of prices and costs.", "e:keyword": ["Inventory/production", "Stochastic", "Substitution", "Multiproduct"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.651", "e:abstract": "British Columbia Gas, a major utility, was required by the British Columbia Utilities Commission (BCUC) to develop an integrated resource plan that addressed multiple objectives and involved the participation of stakeholders. To assist BC Gas, we elicited values separately from most of the senior executives at BC Gas, members of the BCUC, and representatives of several stakeholder groups. Based on these values, we structured a set of objectives and associated performance measures for integrated resource planning (IRP) at BC Gas. A multistakeholder process then provided judgments about appropriate value tradeoffs among these objectives. This information was used in several ways in the IRP process. It fostered improved communication and served as a guide for designing more attractive plans and identifying future information needs. It also provided the basis for a quantitative evaluation of alternative plans and resources. Both the IRP process and the chosen integrated resource plan were reviewed by lawyers representing intervenors at a quasi-judicial hearing of the BCUC. Their concerns are informative in providing lessons for the use of the elicited values in the context of regulatory hearings.", "e:keyword": ["Decision analysis", "Applications", "Regulatory contexts", "Planning", "Corporate", "Stakeholder values", "Natural resources", "Energy", "Natural gas"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.663", "e:abstract": "In this paper we look at alternative ways of teaching linear programming model formulation and show that the material needs to be presented within the context of a structure that improves the ability of students to apply the models to subsequent problems. This work builds on the cognitive psychology literature on formulating algebra word problems. We show that presenting a conceptual framework prior to teaching linear programming examples is better then either no conceptual framework or presenting the framework after teaching the examples.", "e:keyword": ["Linear programming", "Cognition and LP modeling", "Philosophy of modeling"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.675", "e:abstract": "We analyze a general model of dynamic vehicle dispatching systems in which congestion is the primary measure of performance. In the model, a finite collection of tours are dynamically dispatched to deliver loads that arrive randomly over time. A load waits in queue until it is assigned to a tour. This representation, which is analogous to classical set-covering models, can be used to study a variety of dynamic routing and load consolidation problems. We characterize the optimal work in the system in heavy traffic using a lower bound from our earlier work (Gans and van Ryzin 1997) and an upper bound which is based on a simple batching policy. These results give considerable insight into how various parameters of the problem affect system congestion. In addition, our analysis suggests a practical heuristic which, in simulation experiments, significantly outperforms more conventional dispatching policies. The heuristic uses a few simple principles to control congestion, principles which can be easily incorporated within classical, static routing algorithms.", "e:keyword": ["Transportation", "Freight/materials handling", "Queueing analysis of dynamic load", "Consolidation problems", "Transportation", "Vehicle routing", "Queueing analysis of dynamic routing problems", "Queues", "Applications", "Dynamic vehicle dispatching"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.693", "e:abstract": "We study a network", "e:keyword": ["Communications", "Telecommunication", "Networks/graphs", "Design and path packing", "Combinatorics", "Polyhedral combinatorics"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.703", "e:abstract": "The cost of producing electricity during a given time interval is a random variable that depends both on the availability of the generating units during the study horizon and on the magnitude of the load. Based upon a Markov model, we present a recursive scheme for estimating the asymptotic mean and variance of the production cost. These computations are difficult because the state space for a typical power generation system is very large and because the asymptotic variance depends upon the fundamental matrix. Its computation requires the inversion of a matrix whose dimension depends on the size of the state space. The recursion relations given here preclude the need for such matrix inversion and provide approximate estimates that compare very favorably with a realistic Monte Carlo simulation.", "e:keyword": ["Stochastic model applications", "Markov process", "Fundamental matrix", "Electric power industry", "Production casting models"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.713", "e:abstract": "We consider a single", "e:keyword": ["Inventory/production", "Stochastic", "Random yields and demands", "Inventory/production", "Heuristics", "Myopic"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.723", "e:abstract": "This paper applies multicriteria de novo programming to formulate and solve problems of system design that involve multiple decision makers and a possible debt. In the framework of the system design model, each involved decision maker has his or her own preference for the budget availability level associated with multicriteria under consideration. If the possible debt occurs in the design time, the model allows flexibility for decision makers to borrow additional money from a bank with a fixed interest rate so as to keep the production process feasible. A contingency plan therefore can be constructed to deal with the debt situation. A solution procedure is developed to design the optimal system with a certain range of budget availability levels. Numerical examples are used to illustrate the procedure.", "e:keyword": ["Decision analysis", "Multiple criteria", "Multiple decision makers", "Systems", "Theory", "Programming", "Linear", "Algorithm", "De novo problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.730", "e:abstract": "We present a Lagrangian-based heuristic for the well-known <i>Set Covering Problem</i> (SCP). The algorithm was initially designed for solving very large scale SCP instances, involving up to 5,000 rows and 1,000,000 columns, arising from crew scheduling in the Italian Railway Company, Ferrovie dello Stato SpA. In 1994 Ferrovie dello Stato SpA, jointly with the Italian Operational Research Society, organized a competition, called FASTER, intended to promote the development of algorithms capable of producing good solutions for these instances, since the classical approaches meet with considerable difficulties in tackling them. The main characteristics of the algorithm we propose are (1) a dynamic pricing scheme for the variables, akin to that used for solving large-scale LPs, to be coupled with subgradient optimization and greedy algorithms, and (2) the systematic use of column fixing to obtain improved solutions. Moreover, we propose a number of improvements on the standard way of defining the step-size and the ascent direction within the subgradient optimization procedure, and the scores within the greedy algorithms. Finally, an effective refining procedure is proposed. Our code won the first prize in the FASTER competition, giving the best solution value for all the proposed instances. The algorithm was also tested on the test instances from the literature: in 92 out of the 94 instances in our test bed we found, within short computing time, the optimal (or the best known) solution. Moreover, among the 18 instances for which the optimum is not known, in 6 cases our solution is better than any other solution found by previous techniques.", "e:keyword": ["Programming", "Integer", "Set covering", "Heuristic algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.744", "e:abstract": "Chen (1994) develops an attractive variant of the classical problem of preemptively scheduling independent jobs with release dates and due dates. Chen suggests that in practice one can often pay to reduce the processing requirement of a job. This leads to two parametric max flow problems. Serafini (1996) considers scheduling independent jobs with due dates on multiple machines, where jobs can be split among machines so that pieces of a single job can execute in parallel. Minimizing the maximum tardiness again gives a parametric max flow problem. A third problem of this type is deciding how many more games a baseball team can lose part way through a season without being eliminated from finishing first (assuming a best possible distribution of wins and losses by other teams). A fourth such problem is an extended selection problem of Brumelle et al. (1995a), where we want to discount the costs of “tree-structured” tools as little as possible to be able to process all jobs at a profit.It is tempting to try to solve these problems with the parametric push-relabel max flow methods of Gallo et al. (GGT) (1989). However, all these applications appear to violate the conditions necessary to apply GGT. We extend GGT in three ways that allow it to be applied to all four of the above applications. We also consider some other applications where these ideas apply. Our extensions to GGT yield faster algorithms for all these applications.", "e:keyword": ["Production/scheduling", "Multiple machine sequencing with premption", "Networks/graphics", "Parametric flow algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.757", "e:abstract": "A fully polynomial approximation scheme for the problem of scheduling <i>n</i> jobs on a single machine to minimize total weighted earliness and tardiness is presented. A new technique is used to develop the scheme. The main feature of this technique is that it recursively computes lower and upper bounds on the value of partial optimal solutions. Therefore, the scheme does not require any prior knowledge of lower and upper bounds on the value of a complete optimal solution. This distinguishes it from all the existing approximation schemes.", "e:keyword": ["Analysis of algorithms", "Suboptimal algorithms", "Fully polynomial approximation scheme", "Production/scheduling", "Approximations/heuristic", "Single machine", "Weighted earliness–tardiness"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.762", "e:abstract": "Simulation experiments for analysing the steady-state behaviour of queueing systems over a range of traffic intensities are considered, and a procedure is presented for improving their design. In such simulations the mean and variance of the response output can increase dramatically with traffic intensity; the design has to be able to cope with this complication. A regression metamodel of the likely mean response is used consisting of two factors, namely, a low-degree polynomial and a factor accounting for the exploding mean as the traffic intensity approaches its saturation. The best choice of traffic intensities at which to make simulation runs depends on the variability of the simulation output, and this variability is estimated using analytical heavy traffic results. The optimal numbers of customers simulated at each traffic intensity are built up using a multistage procedure. The asymptotic properties of the procedure are investigated theoretically. The procedure is shown to be robust and to be more efficient than more naive procedures. A result of note is that even when the range of interest includes high traffic intensities, the highest traffic load simulated should remain well away from its upper limit; but the number of customers simulated should be concentrated at the higher traffic intensities used. Empirical results are included for simulations of a single server queue with different priority rules and for a complicated queueing network. These results support the theoretical results, demonstrating that the proposed procedure can increase the accuracy of the estimated metamodel significantly compared with more naive methods.", "e:keyword": ["Queues", "Simulation", "Simulation of nearly saturated queues", "Simulation", "Efficiency", "Optimal design of experiments with queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.778", "e:abstract": "Balking is the act of not joining a queue because the prospective arriving customer judges the queue to be too long. We analyze queues in the presence of balking, using only the service start and stop data utilized in Larson's Queue Inference Engine (Q.I.E.). Using an extension of Larson's congestion probability calculation to include balking we present new maximum likelihood, nonparametric, and Bayesian methods for inferring the arrival rate and balking functions. The methodology is applicable to businesses that wish to estimate lost sales because of balking arising from queuing-type congestion. The techniques are applied to a small transactional data set for illustrative purposes.", "e:keyword": ["Statistics", "Bayesian estimation", "Transactional data analysis", "Queues", "Balking", "Statistical inference", "Busy period analysis", "Marketing", "Buyer behavior"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.785", "e:abstract": "Recently, Ahmadi and Tang (1991) demonstrated how various manufacturing problems can be modeled and solved as graph partitioning problems. They use Lagrangian relaxation of two different mixed integer programming formulations to obtain both heuristic solutions and lower bounds on optimal solution values. In this note, we point to certain inconsistencies in the reported results. Among other things, we show analytically that the first bound proposed is trivial (i.e., it can never have a value greater than zero) while the second is also trivial for certain sparse graphs. We also present limited empirical results on the behavior of this second bound as a function of graph density.", "e:keyword": ["Manufacturing", "Cell formation", "Tool loading", "VLSI design", "Networks/graphs", "Partitioning", "Programming", "Integer", "Lagrangian relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.5.789", "e:abstract": "This paper presents the Facet Generation Procedure (FGP) for solving 0/1 integer programs. The FGP seeks to identify a hyperplane that represents a facet of an underlying polytope to cut off the fractional solution to the linear programming relaxation of the integer programming problem. A set of standard problems is used to provide insight into the computational characteristics of the procedure.", "e:keyword": ["Programming", "Integer", "Cutting plane/facet", "Generating facets computationally"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.795", "e:abstract": "Manufacturing managers often address capacity and inventory decisions separately, thus ignoring the interaction between capacity and inventory within a manufacturing system. The separation of these two decisions can lead to an imbalance of capacity and inventory investment. We develop a model that simultaneously plans capacity investment, inventory investment, and the production schedule using return on assets as the objective to maximize. An algorithm is developed that optimizes a fractional objective function for a mixed-integer program. The model was applied at an electronics manufacturer and at a manufacturer of office supplies.", "e:keyword": ["Production scheduling", "Planning", "Capacity and inventory planning", "Aggregate planning", "Inventory production", "Applications", "Capacity and inventory policies", "Programming/fractional", "Integer", "Mixed-integer", "Fractional"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.807", "e:abstract": "Product guarantees or warranties have been around for generations, but formal approaches for establishing and examining warranties have been considered only during the past 20 years. A review of the literature on warranty models and analysis methods is provided, along with some suggestions for further research.", "e:keyword": ["Probability", "Stochastic model applications to warranties", "Reliability", "Replacement/renewal decisions", "Cost analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.821", "e:abstract": "This paper considers scheduling problems arising in robot-served manufacturing cells in which the machines are configured in a flowshop that repetitively produces a family of similar parts. We study the problem of determining the robot move cycle and the part sequence that jointly minimize the average steady-state cycle time required for the repetitive production of a minimal part set, or equivalently maximize the long-run throughput rate. Three earlier related papers provide algorithms, or proofs of intractability, for a variety of cell configurations. We use the intuition developed there to design and test simple heuristic procedures for the part sequencing problem under different robot move cycles in three-machine cells where that problem is intractable. This enables us to develop a heuristic procedure for a general three-machine cell. A methodology for extending this heuristic to four-machine cells is described and tested. Ideas for extension to even larger cells are also discussed. We also describe and test two heuristics for a cell design problem that involves partitioning machines into cells as well as determining the sequence of robot moves and parts. Finally, we provide a list of open research problems.", "e:keyword": ["Production/scheduling: scheduling robot served manufacturing cells", "Sequencing", "Deterministic", "Multiple machine", "Design and computational evaluation of heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.836", "e:abstract": "A bounding-based method is developed for estimating the expected operation cost of a multiarea electric power system in which transmission capacity limits interarea flows. Costs include the expense of power generation and losses suffered by consumers because of supply shortfalls, averaged over random generator outage states and varying demand levels. The calculation of this expectation, termed the <i>distribution problem</i>, is a large-scale stochastic programming problem. Rather than solving this problem directly, lower and upper bounds to the expected cost are created using two more easily solved models. The lower bound is from a deterministic model based on the expected value of the uncertain inputs. The upper bound results from a linear program with recourse whose structure permits relatively quick solution by Benders decomposition. The Benders subproblems use probabilistic production costing, which can be viewed as a stochastic greedy algorithm, to consider random outages and demands. These bounds are iteratively tightened by partitioning realizations of the random variables into subsets based on the status of larger generators and a cluster analysis of demands. Computational examples are described and application issues addressed.", "e:keyword": ["Stochastic programming", "Application of the distribution problem to electric power systems", "Natural resources", "Energy", "Calculation of expected electric power generation costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.849", "e:abstract": "Two players are placed on a line at a distance <i>d</i>which is drawn from a known distribution <i>F</i>. The players have no common notion of direction on the line, and each has a resources bound on the total distance he can travel. If <i>F</i>is bounded and the resources are sufficiently large, then the players can ensure a meeting. The expected time minimization problem in that case has been studied by the authors in a previous paper. Aside from that case the most the players can do is maximize the probability that they meet. This is the problem studied here, for general and specific distributions. This problem generalizes that of Foley et al. (1991), where one of the players is stationary (zero resources).", "e:keyword": ["Search and surveillance", "Rendezvous search"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.862", "e:abstract": "Parallel machine scheduling problems concern the scheduling of <i>n</i>jobs on <i>m</i>machines to minimize some function of the job completion times. If preemption is not allowed, then most problems are not only (N-script)(P-script)-hard, but also very hard from a practical point of view. In this paper, we show that strong and fast linear programming lower bounds can be computed for an important class of machine scheduling problems with additive objective functions. Characteristic of these problems is that on each machine the order of the jobs in the relevant part of the schedule is obtained through some priority rule. To that end, we formulate these parallel machine scheduling problems as a set covering problem with an exponential number of binary variables, <i>n</i>covering constraints, and a single side constraint. We show that the linear programming relaxation can be solved efficiently by column generation because the pricing problem is solvable in pseudo-polynomial time. We display this approach on the problem of minimizing total weighted completion time on <i>m</i>identical machines. Our computational results show that the lower bound is singularly strong and that the outcome of the linear program is often integral. Moreover, they show that our branch-and-bound algorithm that uses the linear programming lower bound outperforms the previously best algorithm.", "e:keyword": ["Production/scheduling", "Sequencing", "Deterministic", "Multiple machine", "Column generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.873", "e:abstract": "The crew scheduling problem (CSP) appears in many mass transport systems (e.g., airline, bus, and railway industry) and consists of scheduling a number of crews to operate a set of transport tasks satisfying a variety of constraints. This problem is formulated as a set partitioning problem with side constraints (SP), where each column of the SP matrix corresponds to a feasible <i>duty</i>, which is a subset of tasks performed by a crew. We describe a procedure that, without using the SP matrix, computes a lower bound to the CSP by finding a heuristic solution to the dual of the linear relaxation of SP. Such dual solution is obtained by combining a number of different bounding procedures.The dual solution is used to reduce the number of variables in the SP in such a way that the resulting SP problem can be solved by a branch-and-bound algorithm. Computational results are given for problems derived from the literature and involving from 50 to 500 tasks.", "e:keyword": ["Programming algorithms", "Dual ascent procedures", "Transportation", "Personnel scheduling", "Mass transit"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.889", "e:abstract": "Motivated by make-to-order cable manufacturing, we describe a remnant inventory system in which orders arrive for units of raw material that are produced-to-stock. As orders are satisfied, the partially consumed units of material, or remnants, are either scrapped or returned to inventory for future allocation to orders. We present a linear program that minimizes the long-run average scrap rate. Its dual prices exhibit many rational properties, including monotonicity and superadditivity. We use these prices in an integer-programming-based control scheme, which we simulate and compare with an existing control scheme previously used in practice.", "e:keyword": ["Production/scheduling", "Cutting stock", "Dynamic", "Networks", "Generalized networks", "Duality theory", "Programming", "Integer", "Applications", "Fiber-optic cable manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.899", "e:abstract": "This paper addresses the optimal allocation of reliability among components that are to be assembled into a system. While it is a generally accepted notion that a component's cost is an increasing function of its reliability, most research to date adopts exponentially increasing, closed-form functions to relate cost and reliability. However, in practice such functions are often unknown or difficult to construct, and it is often more reasonable to describe cost-reliability relationships via discrete data sets. We consider such situations, where each component is available at several reliability levels with corresponding costs. The design optimization problem results in a nonlinear integer program. Because every system configuration has an equivalent representation as either a series connection of parallel subsystems or a parallel connection of series subsystems, we provide formulations, linear relaxations, and algorithms for these two.", "e:keyword": ["Reliability", "Allocation and optimization", "Programming", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.907", "e:abstract": "One may consider a discrete-event simulation as a Markov chain evolving on a suitably rich state space. One way that regenerative cycles may be constructed for general state-space Markov chains is to generate auxiliary coin-flip random variables at each transition, with a regeneration occurring if the coin-flip results in a success. The regenerative cycles are therefore <i>randomized</i>with respect to the sequence of states visited by the Markov chain. The point estimator for a steady-state performance measure does not depend on the cycle structure of the chain, but the variance estimator (that defines the width of a confidence interval for the performance measure) does. This implies that the variance estimator is randomized with respect to the visited states. We show how to “derandomize” the variance estimator through the use of conditioning. A new variance estimator is obtained that is consistent and has lower variance than the standard estimator.", "e:keyword": ["Simulation", "Statistical analysis", "Probability: regenerative processes", "Markov processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.917", "e:abstract": "This paper considers the nonpreemptive priority queue with <i>MAP</i>(Markovian Arrival Process) arrivals. Since <i>MAP</i>is weakly dense in the class of stationary point processes, it is a fairly general arrival process. Service times of customers of each priority class are independent and identically distributed according to a general distribution function that may differ among priority classes. Using both the generating function technique and the matrix analytic method, we derive various formulas for the marginal queue length distribution of each class. Further, we provide the delay cycle analysis of the waiting time distribution of each class and characterize its Laplace-Stieltjes transform.", "e:keyword": ["Queue", "Algorithm", "Algorithmic analysis of a queue with MAP arrivals", "Queue", "Priority", "Nonpreemptive priority", "MAP/G/1 queue"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.928", "e:abstract": "We introduce a permit-based adaptive control scheme for regulating traffic admission in high-speed connectionless data networks, such as the internet. Permits are awarded to potential customers arriving from outside and travel with them towards their destinations, where the permits are assigned to the local controllers. The controllers randomly distribute the permits among the entry gates at the nodes. Customers from outside are not allowed to enter the network unless there are permits available at the entrance node, thus the model is that of a closed queueing network if we model the dynamics of the permits. The goal is to find the permit distribution strategy that maximizes network performance subject to the restrictions of the network topology. A traffic balance approach is used to establish nonuniqueness of the optimal distribution probabilities for the decentralized operation. We exploit nonuniqueness introducing the concept of the <i>automata actions</i>, focusing on two strategies for the actions. For each strategy, a learning automaton is implemented at the controllers using the Kuhn–Tucker conditions for optimality. Our first learning algorithm converges weakly to a unique limit point, which is optimal, while the limit behaviour of our second learning algorithm may be suboptimal. We illustrate our results using computer simulations in order to compare the two strategies for the same network.", "e:keyword": ["Automatic control", "Applications", "Stochastic control using learning automata methods", "Telecommunications", "Optimal flow control", "Routing", "Queues", "Optimization", "Optimal routing in networks toward global equilibria"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.943", "e:abstract": "We consider the problem of bounding the expected value of a linear program (LP) containing random coefficients, with applications to solving two-stage stochastic programs. An upper bound for minimizations is derived from a restriction of an equivalent, penalty-based formulation of the primal stochastic LP, and a lower bound is obtained from a restriction of a reformulation of the dual. Our “restricted-recourse bounds” are more general and more easily computed than most other bounds because random coefficients may appear anywhere in the LP, neither independence nor boundedness of the coefficients is needed, and the bound is computed by solving a single LP or nonlinear program. Analytical examples demonstrate that the new bounds can be stronger than complementary Jensen bounds. (An upper bound is “complementary” to a lower bound, and vice versa). In computational work, we apply the bounds to a two-stage stochastic program for semiconductor manufacturing with uncertain demand and production rates.", "e:keyword": ["Programming", "Stochastic", "Bounds", "Networks/graphs", "Stochastic", "Facilities/equipment planning", "Capacity expansion"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.957", "e:abstract": "This paper explores a seemingly paradoxical phenomenon associated with the use of expected-utility theory in capital-budgeting and risk-sharing decisions under uncertainity. As an investment prospect becomes better and better, decision makers using classic decision-analysis techniques may, in fact, prefer less and less of it. We explore this phenomenon in the very real context of petroleum company drilling-investment decisions and demonstrate that the phenomenon is pervasive. When we examined the prospect inventory of a major oil company, we found that an increase in the upside payoff would lead to a lower optimal working interest for the grand majority of its prospects. We also explore the underlying factors leading to this phenomenon to develop both intuition and understanding. These factors have to do with degree of risk aversion (the more risk averse you are, the less likely you are to increase your holdings of an improving prospect) and managerial perspectives on risk. Once understood, the results no longer seem surprising.", "e:keyword": ["Design analysis", "Utility/performance", "Natural resources"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.966", "e:abstract": "We consider an inventory system in which both the arrival of items and the demand for those items are Poisson processes. The stored items have two phases of shelf-life. If the item has not been taken by a demand during the first phase it is inspected. With probability <i>q</i>it is removed and with probability <i>p</i>= 1 − <i>q</i>it is transferred to the second phase. The blood bank model inspired this study. We compute ergodic limits for the number of items in the system, the lost demands, and the two types of outdating.", "e:keyword": ["Markov processes", "Perishable inventory systems", "Queues with restricted accessibility", "Busy period analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.47.6.974", "e:abstract": "We present two new results for the asymmetric rendezvous problem on the line. We first show that it is never optimal for one player to be stationary during the entire search period in the two-player rendezvous. Then we consider the meeting time of<i>n</i>-players in the worst case and show that it has an asymptotic behavior of<i>n</i>/2 +<i>O</i>(log<i>n</i>).", "e:keyword": ["Military", "Search/surveillance", "Rendezvous"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.1.12445", "e:abstract": "Increased competition and more demanding customers have forced companies to offer a wide variety of products. Component commonality can help companies reduce the cost of providing product variety to their customers. However, determining the extent to which component commonality should be used is difficult. In this paper we present an approach to determine the optimal level of component commonality for end-product components that do not differentiate models from the customer's perspective. The work was inspired by and applied to a wire-harness design problem faced by a major automobile manufacturer. We model the component design problem as a mathematical program that considers production, inventory holding, setup, and complexity costs (the cost in indirect functions caused by component variety). We develop two approaches to solve the problem: a branch-and-bound algorithm that can solve small- and medium-size problems optimally, and a simulated annealing algorithm that can solve large-size problems heuristically. We apply both algorithms to the wireharness design problem faced by the automobile manufacturer and to a number of randomly generated test problems. We show that an optimal design achieves high cost savings by using significantly fewer variants than a no-commonality design but significantly more variants than a full-commonality design. We apply sensitivity analysis to identify extreme conditions under which the no-commonality and full-commonality designs perform well, and we identify the key cost drivers for our application. Finally, we describe the impact of our analysis on the company's subsequent component design decisions.", "e:keyword": ["Manufacturing: component design", "Programming: nonlinear programming applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.111.12453", "e:abstract": "Dantzig-Wolfe decomposition as applied to an integer program is a specific form of problem reformulation that aims at providing a tighter linear programming relaxation bound. The reformulation gives rise to an integer master problem, whose typically large number of variables is dealt with implicitly by using an integer programming column generation procedure, also known as branch-and-price algorithm. There is a large class of integer programs that are well suited for this solution technique. In this paper, we propose to base the Dantzig-Wolfe decomposition of an integer program on the discretization of the integer polyhedron associated with a subsystem of constraints (as opposed to its convexification). This allows us to formulate the integrality restriction directly on the master variables and sets a theoretical framework for dealing with specific issues such as branching or the introduction of cutting planes in the master. We discuss specific branching schemes and their effect on the structure of the column generation subproblem. We give theoretical bounds on the complexity of the separation process and the extent of the modifications to the column generation subproblem. Our computational tests on the cutting stock problem and a generalisation---the cutting strip problem---show that, in practice, all fractional solutions can be eliminated using branching rules that preserve the tractability of the subproblem, but there is a trade-off between branching efficiency and subproblem tractability.", "e:keyword": ["Integer programming algorithms: decomposition", "Column generation", "Branch-and-price", "Production/scheduling: cutting stock/trim"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.129.12455", "e:abstract": "The Capacitated Arc Routing Problem arises in several contexts where streets or roads must be traversed for maintenance purposes or for the delivery of services. A tabu search is proposed for this difficult problem. On benchmark instances, it outperforms all known heuristics and often produces a proven optimum.", "e:keyword": ["Transportation: arc routing problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.136.12448", "e:abstract": "We consider two queueing control problems that are stochastic versions of the economic lot scheduling problem: A single server processes <i>N</i> customer classes, and completed units enter a finished goods inventory that services exogenous customer demand. Unsatisfied demand is backordered, and each class has its own general service time distribution, renewal demand process, and holding and backordering cost rates. In the first problem, a setup cost is incurred when the server switches class, and the setup cost is replaced by a setup time in the second problem. In both problems we employ a long-run average cost criterion and restrict ourselves to a class of dynamic cyclic policies, where idle periods and lot sizes are state-dependent, but the <i>N</i> classes must be served in a fixed sequence. Motivated by existing heavy traffic limit theorems, we make a time scale decomposition assumption that allows us to approximate these scheduling problems by diffusion control problems. Our analysis of the approximating setup cost problem yields a closed-form dynamic lot-sizing policy and a computational procedure for an idling threshold. We derive structural results and an algorithmic procedure for the setup time problem. A computational study compares the proposed policy and several alternative policies to the numerically computed optimal policy.", "e:keyword": ["Inventory/production: stochastic multi-item lot-sizing", "Queues: diffusion approximation of scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.155.12457", "e:abstract": "The finite horizon stochastic knapsack combines a secretary problem with an integer knapsack problem. It is useful for optimizing sales of perishable commodities with low marginal costs to impatient customers. Applications include yield management for airlines, hotels/motels, broadcasting advertisements, and car rentals. In these problems, <i>K</i> types of customers arrive stochastically. Customer type, <i>k</i>, has an integer weight <i>w<sub>k</sub></i>, a value <i>b<sub>k</sub></i>, and an arrival rate (lambda)<i><sub>k</sub></i> (<i>t</i>) (which depends on time). We consider arrivals over a continuous time horizon [0; <i>T</i>] to a “knapsack” with capacity <i>W</i>. For each arrival that fits in the remaining knapsack capacity, we may (1) accept it, receiving <i>b<sub>k</sub></i>, while giving up capacity <i>w<sub>k</sub></i>; or (2) reject it, forgoing the value and not losing capacity. The choice must be immediate; a customer not accepted on arrival is lost. We model the problem using continuous time, discrete state, finite horizon, dynamic programming. We characterize the optimal return function and the optimal acceptance strategy for this problem, and we give solution methods. We generalize to multidimensional knapsack problems. We also consider the special case where <i>w<sub>k</sub></i> = 1 for all <i>k</i>. This is the classic airline yield problem. Finally, we formulate and solve a new version of the secretary problem.", "e:keyword": ["Dynamic programming/Optimal control models: stochastic", "Finite horizon", "Continuous time", "Finite states", "Probability", "Stochastic model applications", "Transportation: airline yield management"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.173.12454", "e:abstract": "The optimal allocation for stratification, parameterized by the respective sampling strategy to use in each stratum, is derived directly from the notion of efficiency. Especially with simulation, there are often opportunities to maximize efficiency (myopically) within each stratum. To maximize efficiency globally, first maximize the efficiency of the sampling strategy for each stratum separately and then use the optimal allocation given these respective maximizers. Given any other allocation, maximizing the efficiency of the sampling strategy in each stratum separately does not give the highest efficiency attainable with that allocation except in degenerate cases. Given a class (C-script) of deterministic rounding strategies, the rounding of the (continuous) optimal allocation over (C-script), which maximizes efficiency, cannot be improved by a strategy that randomizes over (C-script).", "e:keyword": ["Simulation: efficiency", "Stratification", "Statistics: design of experiments", "Sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.177.12451", "e:abstract": "We consider a blocking (i.e., bufferless) flowshop that repetitively processes a minimal part set to minimize its cycle time, or equivalently to maximize its throughput rate. The best previous heuristic procedure solves instances with 9 machines and 25 jobs, with relative errors averaging about 3% but sometimes as much as 10%. The idea of deliberately slowing down the processing of operations (i.e., increasing their processing times) establishes a precise mathematical connection between this problem and a no-wait flowshop. This enables a very effective heuristic for the no-wait flowshop to be adapted as a heuristic for the blocking flowshop. Our computational results show relative errors that average less than 2% for instances with 20 machines and 250 jobs.", "e:keyword": ["Production/scheduling: sequencing", "Blocking flowshop", "Manufacturing: Traveling salesman problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.181.12442", "e:abstract": "This paper addresses a discrete time inventory model where the maximum amount of supply from which instantaneous replenishment orders can be made is a random variable. Optimal ordering policies are characterized as critical number policies with monotone increasing optimal critical values. A sufficient condition is then presented for myopic ordering policies to be optimal.", "e:keyword": ["Dynamic programming", "Optimal control", "Inventory/production", "Operating characteristics: supply uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.185.12440", "e:keyword": ["Military", "Cost effectiveness shadow pricing", "Military force effectiveness: force-on-force models"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.20.12441", "e:abstract": "Sensitivity analysis, combined with parametric optimization, is often presented as a way of checking if the solution of a deterministic linear program is reliable---even if some of the parameters are not fully known but are instead replaced by a best guess, often a sample mean. It is customary to claim that if the region over which a certain basis is optimal is large, one is fairly safe by using the solution of the linear program. If not, the parametric analysis will provide us with alternative solutions that can be tested. This way, sensitivity analysis is used to facilitate decision making under uncertainty by means of a deterministic tool, namely parametric linear programming. We show in this note that this basic idea of stability has little do with optimality of an optimization problem where the parameters are uncertain.", "e:keyword": ["Decision analysis", "Risk: sensitivity analysis", "What-if analysis", "Programming", "Linear", "Parametric: random parameters", "Programming stochastic: can sensitivity analyses be used?"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.26.12456", "e:abstract": "In this paper, we establish a sufficient condition for the stability of a multiclass fluid network and queueing network under priority service disciplines. The sufficient condition is based on the existence of a linear Lyapunov function, and it is stated in terms of the feasibility of a set of linear inequalities that are defined by network parameters. In all the networks we have tested, this sufficient condition actually gives a necessary and sufficient condition for their stability.", "e:keyword": ["Stochastic model applications: Stability of multiclass queueing networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.38.12446", "e:abstract": "This paper presents a queueing network description of a cellular mobile communications network. These cellular networks are characterised by the call lengths, the fresh call arrival rate, and the handover rates, as well as the capacity restrictions on the number of calls because of limited bandwidth. In the resulting queueing network the routing probabilities depend on the age of the customers. This queueing network is amenable to analysis via known methods. In particular, insensitivity results are generalised to also include capacity restrictions under age-dependent routing and to include call holding time distributions of customers moving from one queue to another.", "e:keyword": ["Communications", "Queues: networks", "Multichannel"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.50.12443", "e:abstract": "Customers for retail merchandise can often be satisfied with one of several items. Accounting for demand substitution in defining customer service influences the choice of items to stock and the optimal inventory level for each item stocked. Further, when certain items are not stocked, the resulting substitutions increase the demand for other items, which also affects the optimal stock levels. In this paper, we develop a probabilistic demand model for items in an assortment that captures the effects of substitution and a methodology for selecting item inventory levels so as to maximize total expected profit, subject to given resource constraints. Illustrative examples are solved to provide insights concerning the behavior of the optimal inventory policies, using the negative binomial demand distribution, which has performed well in fitting retail sales data.", "e:keyword": ["Inventory: multi-item with substitution", "Marketing: retailing", "Assortment planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.65.12449", "e:abstract": "The problem of optimizing a linear function over the efficient set of a multiple objective linear programming problem is studied. The decomposition of the efficient set into efficient faces is used as the basis of a search-based algorithm to solve this problem. The faces of the feasible region are characterized by the set of constraints that hold as equality in that face. The search is conducted over the indices of the constraints in a way that explores faces of possibly higher dimension first. Computational tests are performed to establish the behavior of the algorithm when the objective function is built according to different schemes that have received attention in the literature. The results indicate that different objective function types may lead to varying computation time requirements. In general, computational requirements of the algorithm increase significantly with problem size. A heuristic modification of the algorithm is proposed to solve large problems within reasonable time limits. Tests to measure the quality of the heuristic solutions show that the heuristic approach constitutes a practical alternative for finding good solutions for the problem.", "e:keyword": ["Multiple criteria programming: effcient set", "Nonlinear programming: global optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.73.12452", "e:abstract": "We consider the problem of approximating the expected recourse function for two-stage stochastic programs. Our problem is motivated by applications that have special structure, such as an underlying network that allows reasonable approximations to the expected recourse function to be developed. In this paper, we show how these approximations can be improved by combining them with sample gradient information from the true recourse function. For the case of strictly convex nonlinear approximations, we prove convergence for this hybrid approximation. The method is attractive for practical reasons because it retains the structure of the approximation.", "e:keyword": ["Programming", "Stochastic", "Networks", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.80.12444", "e:abstract": "We develop a mathematical programming approach for the classical <i>PSPACE-hard</i> restless bandit problem in stochastic optimization. We introduce a hierarchy of <i>N</i> (where <i>N</i> is the number of bandits) increasingly stronger linear programming relaxations, the last of which is exact and corresponds to the (exponential size) formulation of the problem as a Markov decision chain, while the other relaxations provide bounds and are efficiently computed. We also propose a priority-index heuristic scheduling policy from the solution to the firstorder relaxation, where the indices are defined in terms of optimal dual variables. In this way we propose a policy and a suboptimality guarantee. We report results of computational experiments that suggest that the proposed heuristic policy is nearly optimal. Moreover, the second-order relaxation is found to provide strong bounds on the optimal value.", "e:keyword": ["Probability: Stochastic models", "Programming: Stochastic optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.91.12450", "e:abstract": "Many production problems involve facility setups that lead to integer variables, production decisions that are continuous, and demands that are likely to be random. While these problems can be quite difficult to solve, we propose a model and an efficient solution technique for this basic class of stochastic mixed-integer programs. We use a set of scenarios to reflect uncertainty. The resulting mathematical model is solved using Lagrangian relaxation. We show that the duality gap of our relaxation is bounded above by a constant that depends on the cost function and the number of branching points in the scenario tree. We apply our technique to the problem of generating electric power. Numerical results indicate significant savings when the stochastic model is used instead of a deterministic one.", "e:keyword": ["Production/Scheduling", "Stochastic: mixed-integer programs with varying right-hand side", "Industries", "Electric: scheduling power generators"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.1.99.12447", "e:abstract": "In the context of production scheduling, inserted idle time (IIT) occurs whenever a resource is deliberately kept idle in the face of waiting jobs. IIT schedules are particularly relevant in multimachine industrial situations where earliness costs and/or dynamically arriving jobs with due dates come into play. We provide a taxonomy of environments in which IIT scheduling is relevant, review the extant literature on IIT scheduling, and identify areas of opportunity for future research.", "e:keyword": ["Production/scheduling: Single and multiple machine scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.189.12380", "e:abstract": "As part of its growth strategy, Caterpillar Inc. is launching a new P2000 product line of “compact” construction equipment and worktools. In anticipation of this, they asked the authors to construct and analyze potential P2000 supply chain configurations. Using decomposition and results from network flow theory, inventory theory, and simulation theory, we were able to provide solutions to this problem for different supply chain scenarios provided by Caterpillar. Novel features of our model include expedited deliveries, partial backlogging of orders, and realized sales that are responsive to service. Caterpillar made their decision regarding the P2000 supply chain based on our recommendations.", "e:keyword": ["Professional: OR/MS implementation", "Inventory/production: applications", "Industries: machinery"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.205.12383", "e:abstract": "This historical account of operations research at Bell Laboratories was drafted in the late 1970s when the authors were part of the Operations Research Center within Bell Laboratories at AT&T; it has not previously appeared in the open literature. We have added a few references to later publications that describe particular aspects of the period covered. Discussions of technological practices and organizational arrangements expressed in the present tense represent a viewpoint of about 1980, before “divestiture” split up the Bell System. The principal topics in this issue of <i>Operations Research</i> are the organizational history, and teletraffic theory and engineering. Upcoming issues of <i>Operations Research</i> will include parts II and III of this account, comprising capacity expansion in telephone networks, operations analysis, applied statistics, and related OR activities. A comprehensive table of contents for parts I, II, and III can be found in the Online Collection of the <i>Operations Research</i> Home Page, at <ext-link ext-link-type=\"uri\"  href=\"http://www.informs.org/pubs\">www.informs.org/pubs</ext-link>.", "e:keyword": ["Communications", "Queues: applications", "Research and development"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.216.12376", "e:abstract": "We develop a supply network model that takes as input the bill of materials, the (nominal) lead times, the demand and cost data, and the required customer service levels. In return, the model generates the base-stock level at each store---the stocking location for a part or an end-product, so as to minimize the overall inventory capital throughout the network and to guarantee the customer service requirements. The key ingredient of the model is a detailed, albeit approximate, analysis of the <i>actual</i> lead times at each store and the associated demand over such lead times, along with a characterization of the operation at each store via an inventory-queue model. The gradients are derived in explicit forms, and a conjugate gradient routine is used to search for the optimal solution. Several numerical examples are presented to validate the model and to illustrate its various features.", "e:keyword": ["Inventory/production", "Multiechelon: inventory optimization in complex supply chains", "Queues", "Applications: queueing approximations for inventory applications", "Programming", "Nonlinear", "Applications: constrained nonlinear optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.233.12384", "e:abstract": "We consider a dynamic network flow problem where the arc capacities are random variables. This gives a multistage stochastic linear program. We describe the randomness using a multi-scenario approach. Because of the multilayered structure, there are several ways to decompose the linear program. We classify different decomposition schemes, and we develop a scheme called <i>compath decomposition</i>, which is derived from path decomposition for network flows. We give a polynomial-time algorithm to find a cheapest compath that can solve the subproblems resulting from compath decomposition. Finally, compath decomposition is extended to multicommodity flow problems.", "e:keyword": ["Networks", "Graphs", "Stochastic: dynamic network flow with uncertain arc capabilities"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.243.12377", "e:abstract": "Many stochastic optimization problems are solved using the renewal reward theorem (RRT). Once a regenerative cycle is identified, the objective function is formed as the ratio of the expected cycle cost to the expected cycle time and optimized using the standard techniques. Application of the RRT requires only the first moments of the cycle-related random variables. However, if the start of a cycle corresponds to an important event, e.g., end of a period of shortages in an inventory problem, knowing only the <i>expected</i> time---and the cost---of the cycle may not give enough information on the functioning of the stochastic system. For example, it may be useful to know the probability that the cycle cost, or more importantly, the average cost per unit time will exceed predetermined levels. In this paper we provide a complete description of the cycle-related random variables for a stochastic inventory problem with supply interruptions. We assume a general phase-type distribution for the supplier's availability (ON) periods and an exponential distribution for the OFF periods. The first passage time of an embedded Markov chain of the ON/OFF process is used to develop the expressions for the exact distribution and the moments of the cycle time and cycle cost random variables. We then describe a method for computing the probability that the average cost per unit time will exceed a predetermined level. This method is used to construct an “efficient frontier” for the two criteria of (i) average cost and (ii) the probability of exceeding it. The efficient frontier is used to find a solution to the multiple-criteria optimization problem.", "e:keyword": ["Probability: cycle distributions", "Inventory/production: supply uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.256.12386", "e:abstract": "The problem addressed in this paper is that of orthogonally packing a given set of rectangular-shaped items into the minimum number of three-dimensional rectangular bins. The problem is strongly NP-hard and extremely difficult to solve in practice. Lower bounds are discussed, and it is proved that the asymptotic worst-case performance ratio of the continuous lower bound is 1/8. An exact algorithm for filling a single bin is developed, leading to the definition of an exact branch-and-bound algorithm for the three-dimensional bin packing problem, which also incorporates original approximation algorithms. Extensive computational results, involving instances with up to 90 items, are presented: It is shown that many instances can be solved to optimality within a reasonable time limit.", "e:keyword": ["Production/scheduling: cutting stock/trim", "Transportation: freight/material handling", "Programming: integer", "Branch-and-bound"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.268.12379", "e:abstract": "The electric power industry is going through deregulation. As a result, the load on the generating units of a utility is becoming increasingly unpredictable. Furthermore, electric utilities may need to buy power or sell their production to a power pool that serves as a spot market for electricity. These trading activities expose utilities to volatile electricity prices. In this paper, we present a stochastic model for the unit commitment that incorporates power trading into the picture. Our model also accounts for fuel constraints and prices that may vary with electricity prices and demand. The resulting model is a mixed-integer program that is solved using Lagrangian relaxation and Bender's decomposition. Using this solution approach, we solve problems with 729 demand scenarios on a single processor to within 0.1% of the optimal solution in less than 10 minutes. Our numerical results indicate that significant savings can be achieved when the spot market is entered into the problem and when stochastic policy is adopted instead of a deterministic one.", "e:keyword": ["Energy deregulation of the electric power industry", "Stochastic programming: using a set of scenarios to model uncertainty", "Scheduling: unit commitment problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.281.12387", "e:abstract": "We consider a one-warehouse, <i>N</i>-identical-retailer model. Random demands occur at the retailers with complete backlogging. The retailers replenish their inventories from the warehouse, which in turn orders from an outside supplier with unlimited stock. Each retailer places an order every <i>N</i> periods according to a base-stock policy, and the reorder intervals of the retailers are staggered so that only one retailer places an order in each period. The warehouse orders according to an (<i>s, S</i>) policy based on its own inventory position. We consider two allocation policies, past priority allocation (PPA) and current priority allocation (CPA), which specify how the retailer orders are filled at the warehouse. For the PPA model, we provide an exact procedure for computing the long-run average total cost. Based on the exact procedure, we develop an approximate model that can be used to determine near-optimal control parameters for both the PPA and the CPA model. We conduct a computational study to test the effectiveness of the approximate model and to compare the performance of the two allocation policies.", "e:keyword": ["Inventory/production: multi-echelon", "Stochastic", "Coordination", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.294.12382", "e:abstract": "Many location models involve distances and demand points in their objective function. In urban contexts, there can be millions of demand points. This leads to demand point aggregation, which produces error. We identify a general model structure that includes most such location models, and present a means of obtaining error bounds for all models with this structure. The error bounds suggest how to do the demand point aggregation so as to keep the error small.", "e:keyword": ["Facilities Location: demand point aggregation error"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.308.12385", "e:abstract": "We study statistical tests of uniformity based on the <i>L<sub>p</sub></i>-distances between the <i>m</i> nearest pairs of points, for <i>n</i> points generated uniformly over the <i>k</i>-dimensional unit hypercube or unit torus. The number of distinct pairs at distance no more than <i>t</i>, for <i>t</i> (ge) 0, is a stochastic process whose initial part, after an appropriate transformation and as <i>n</i> (rightarrow) (infinity), is asymptotically a Poisson process with unit rate. Convergence to this asymptotic is slow in the hypercube as soon as <i>k</i> exceeds 2 or 3, due to edge effects, but is reasonably fast in the torus. We look at the quality of approximation of the exact distributions of the tests statistics by their asymptotic distributions, discuss computational issues, and apply the tests to random number generators. Linear congruential generators fail decisively certain variants of the tests as soon as <i>n</i> approaches the square root of the period length.", "e:keyword": ["Simulation", "Random number generation", "Statistical tests", "Goodness-of-fit", "Spatial statistics"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.318.12378", "e:abstract": "We present a column-generation model and branch-and-price-and-cut algorithm for origin-destination integer multicommodity flow problems. The origin-destination integer multicommodity flow problem is a constrained version of the linear multicommodity flow problem in which flow of a commodity (defined in this case by an origin-destination pair) may use only one path from origin to destination. Branch-and-price-and-cut is a variant of branch-and-bound, with bounds provided by solving linear programs using column-and-cut generation at nodes of the branch-and-bound tree. Because our model contains one variable for each origin-destination path, for every commodity, the linear programming relaxations at nodes of the branch-and-bound tree are solved using column generation, i.e., implicit pricing of nonbasic variables to generate new columns or to prove LP optimality. We devise a new branching rule that allows columns to be generated efficiently at each node of the branch-and-bound tree. Then, we describe cuts (cover inequalities) that can be generated at each node of the branch-and-bound tree. These cuts help to strengthen the linear programming relaxation and to mitigate the effects of problem symmetry. We detail the implementation of our combined column-and-cut generation method and present computational results for a set of test problems arising from telecommunications applications. We illustrate the value of our branching rule when used to find a heuristic solution and compare branch-and-price and branch-and-price-and-cut methods to find optimal solutions for highly capacitated problems.", "e:keyword": ["Networks/graphs: multicommodity", "Programming", "Integer: branch-and-bound", "Programming", "Linear: column generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.327.13375", "e:abstract": "We consider the problem of determining the optimal policy for staffing a queueing system over multiple periods, using a model that takes into account transient queueing effects. Formulating the problem in a dynamic programming setting, we show that the optimal policy follows a monotone optimal control by establishing the submodularity of the objective function with respect to the staffing level and initial queue size in a period. In particular, this requires proving that the system occupancy in a <i>G/M/s</i> queue is submodular in the number of servers and initial system occupancy.", "e:keyword": ["Dynamic programming", "Applications: staffing problem", "Queues", "Transient results: submodularity", "Optimal control: monotone policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.332.13373", "e:abstract": "It is a common practice for industries to price the same products at different levels. For example, airlines charge various fares for a common pool of seats. Seasonal products are sold at full or discount prices during different phases of the season. This article presents a model that reflects this yield management problem. The model assumes that (1) products are offered at multiple predetermined prices over time; (2) demand is price sensitive and obeys the Poisson process; and (3) price is allowed to change monotonically, i.e., either the markup or markdown policy is implemented. To maximize the expected revenue, management needs to determine the optimal times to switch between prices based on the remaining season and inventory. Major results in this research include (1) an exact solution for the continuous-time model; (2) piecewise concavity of the value function with respect to time and inventory; and (3) monotonicity of the optimal policy. The implementation of optimal policies is fairly facile because of the existence of threshold points embedded in the value function. The value function and time thresholds can be solved with a reasonable computation effort. Numerical examples are provided.", "e:keyword": ["Inventory/production: perishable/aging items", "Maximize revenue of perishable products", "Probability: stochastic model dynamic point process"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.2.344.12381", "e:abstract": "This paper clarifies the role of non-Archimedean infinitesimal (epsilon) in DEA models so that the associated linear programs may be infeasible (for the multiplier side) and unbounded (for the envelopment side) for certain values of (epsilon). It is shown that the bound of (epsilon) proposed by Ali and Seiford (1993) is invalid for feasibility and boundedness of the linear programs. A procedure is presented for determining an assurance interval of (epsilon). It is also shown that an assurance value for (epsilon) can be found using a single linear program.", "e:keyword": ["Measuring efficiency: data development analysis", "Computational issue: non-Archimedean infinitesimal"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.351.12426", "e:abstract": "This historical account of operations research at Bell Laboratories was drafted in the late 1970s when the authors were part of the Operations Research Center within Bell Laboratories at AT&T; it has not previously appeared in the open literature. We have added a few references to later publications that describe particular aspects of the period covered. Discussions of technological practices and organizational arrangements expressed in the present tense represent a viewpoint of about 1980, before “divestiture” split up the Bell System. A comprehensive table of contents for parts I, II, and III can be found in the Online Collection of the <i>Operations Research</i> Home Page, at <ext-link ext-link-type=\"uri\"  href=\"www.informs.org/pubs\">www.informs.org/pubs</ext-link>.We describe the work done in the Bell System up to about 1980, mainly at Bell Labs, both in developing the techniques of operations research and in applying them to the telecommunications business. This account does not include military OR or direct support of other federal government activities. Part II covers capacity expansion and operations analysis.", "e:keyword": ["Communications", "Facilities/equipment planning: capacity expansion", "Research & Development"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.362.12434", "e:abstract": "We examine two competing technologies for delivering business information to professional subscribers: first, a package service that delivers information using physical media, such as CD-ROMs; second, an online service that allows subscribers to access information over online networks. We model the information services market as a duopoly, where each information service provider is equipped with either packaged or online information delivery technology. They compete for potential subscribers characterized by their usage volume. Subscribers may also choose “self-service,” wherein they collect and collate information directly from the source. Service design for information service providers must specify their information delivery technology and the size of the database they provide. Service providers first commit to a service design and then enter a price competition phase, where they choose the parameters of their feasible pricing strategies. Specifically, a package service provider chooses a fixed charge for subscription. An online provider chooses both a fixed charge and a marginal charge, because it is possible to meter usage in that technology. We show that the package provider serves high volume subscribers while the online provider serves low volume subscribers. We also derive bounds on equilibrium database sizes, prices, and market shares for each provider.", "e:keyword": ["Industrial marketing: pricing", "Segmentation", "Competitive strategy", "Computers: databases", "Technologies for packaging and distributing information", "Noncooperative games: competitive models of information services"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.376.12427", "e:abstract": "In many production/distribution systems, materials flow from one stage to another in fixed lot sizes. For example, a retailer orders a full truckload from a manufacturer to qualify for a quantity discount; a factory has a material handling system that moves full containers of parts from one production stage to the next. In this paper, we derive optimal policies for multi-stage serial and assembly systems where materials flow in fixed batches. The optimal policies have a simple structure, and their parameters can be easily determined. This research extends the multi-echelon inventory theory in several ways. It generalizes the Clark-Scarf model by allowing batch transfers of inventories. Rosling (1989) shows that assembly systems can be interpreted as serial systems under the assumption that there are no setup costs. We show that the series interpretation still holds when materials flow in fixed batches which satisfy a certain regularity condition. Finally, Veinott (1965) identifies an optimal policy for a single-location inventory system with batch ordering. This paper generalizes his result to multi-echelon settings.", "e:keyword": ["Inventory/production: multi-stage/item", "Stochastic", "Fixed lot-sizes", "Optimal policies", "Manufacturing: assembly systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.390.12436", "e:abstract": "We propose a new randomized method for solving global optimization problems. This method, the Nested Partitions (NP) method, systematically partitions the feasible region and concentrates the search in regions that are the most promising. The most promising region is selected in each iteration based on information obtained from random sampling of the entire feasible region and local search. The method hence combines global and local search. We first develop the method for discrete problems and then show that the method can be extended to continuous global optimization. The method is shown to converge with probability one to a global optimum in finite time. In addition, we provide bounds on the expected number of iterations required for convergence, and we suggest two stopping criteria. Numerical examples are also presented to demonstrate the effectiveness of the method.", "e:keyword": ["Programming: randomized global optimization", "Mathematics", "Combinatorics: optimization", "Probability", "Markov processes: convergence"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.408.12435", "e:abstract": "We consider a long term contractual agreement between buyer and seller in which <i>Q</i> units are delivered to the buyer at regular time intervals. It must be true that the delivery quantity, <i>Q</i>, is less than the mean demand per period. In order to manage the inventory, the buyer has the option of adjusting the delivery quantity upwards just prior to a delivery, but must pay a premium to do so. Demand is assumed random, and we model the system in a continuous review setting. We show that the equations one must solve to find optimal adjustment strategies are intractable. A diffusion approximation is developed which when coupled with the solution to an even simpler deterministic version of the problem yields very simple but effective approximations. Extensive computations are included to compare the performance of the optimal and approximate policies. We also empirically derive a formula for computing <i>Q</i> whose accuracy is established computationally. We prove that the fixed delivery contract results in lower variance of orders to the seller. We also include a computational study to find the unit cost discount that equalizes the expected costs for the fixed delivery contract and the base stock contract for a large parameter set.", "e:keyword": ["Inventory/Production: Fixed contract with adjustments"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.424.12429", "e:abstract": "In this paper, we present a variational inequality framework for the modeling, qualitative analysis, and computation of equilibrium patterns in multiproduct, multipollutant oligopolistic markets with marketable pollution permits in the presence of transaction costs. The model deals explicitly with spatial differentiation and also guarantees that the imposed environmental quality standards are met through the initial allocation of licenses. An algorithm is proposed, with convergence results, to compute the profit-maximized quantities of the oligopolistic firms' products and the quantities of emissions, along with the equilibrium allocation of licenses and their prices. Numerical examples are included to illustrate this approach.", "e:keyword": ["Environment: marketable pollution permits", "Economics: oligopolistic markets with transaction costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.436.12437", "e:abstract": "We consider the combined problem of pricing and ordering for a perishable product with unknown demand distribution and censored demand observations resulting from lost sales, faced by a monopolistic retailer. We develop an adaptive pricing and ordering policy with the asymptotic property that the average realized profit per period converges with probability one to the optimal value under complete information on the distribution. The pricing mechanism is modeled as a multiarmed bandit problem, while the order quantity decision, made after the price level is established, is based on a stochastic approximation procedure with multiplicative updates.", "e:keyword": ["Inventory/production", "Perishable/aging items: Ordering and pricing under unknown demand", "Probability", "Stochastic models applications: Stochastic approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.444.12431", "e:abstract": "The multisource Weber problem is to locate simultaneously <i>m</i> facilities in the Euclidean plane to minimize the total transportation cost for satisfying the demand of <i>n</i> fixed users, each supplied from its closest facility. Many heuristics have been proposed for this problem, as well as a few exact algorithms. Heuristics are needed to solve quickly large problems and to provide good initial solutions for exact algorithms. We compare various heuristics, i.e., alternative location-allocation (Cooper 1964), projection (Bongartz et al. 1994), Tabu search (Brimberg and Mladenović 1996a), <i>p</i>-Median plus Weber (Hansen et al. 1996), Genetic search and several versions of Variable Neighbourhood search. Based on empirical tests that are reported, it is found that most traditional and some recent heuristics give poor results when the number of facilities to locate is large and that Variable Neighbourhood search gives consistently best results, on average, in moderate computing time.", "e:keyword": ["Facilities/equipment planning", "Location", "Continuous", "Heuristic solution methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.461.12439", "e:abstract": "The capacitated network design problem is a multicommodity minimal cost network flow problem with fixed charges on the arcs and is well known to be NP-hard. The problem type is very common in the context of transportation networks, telecommunication networks, etc. In this paper we propose an efficient method for this problem, based on a Lagrangian heuristic within a branch-and-bound framework. The Lagrangian heuristic uses a Lagrangian relaxation to obtain easily solved subproblems and solves the Lagrangian dual by subgradient optimization. It also includes techniques for finding primal feasible solutions. The Lagrangian heuristic is then embedded into a branch-and-bound scheme that yields further primal improvements. Special penalty tests and cutting criteria are developed. The branch-and-bound scheme can either be an exact method that guarantees the optimal solution of the problem or be a quicker heuristic. The method has been tested on networks of various structures and sizes. Computational comparisons between this method and a state-of-the-art mixed-integer code are presented. The method is found to be capable of generating good feasible solutions to large-scale problems within reasonable time and data storage limits.", "e:keyword": ["Programming", "Integer", "Algorithms", "Relaxation/subgradient", "Branch-and-bound: for the capacitated network design problem", "Networks/graphs", "Multicommodity: fixed charges"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.482.12428", "e:abstract": "We consider a multi-class production system without setups where many job classes share a single server. The traditional performance measure used for scheduling these systems is that of mean throughput time (i.e., the time spent in the system). However, mean throughput time may not be the only measure of importance in real systems. In particular, throughput time variance and the outer percentiles of throughput time may be equally important. We present two heuristics for scheduling multi-class single-server queues that are based on heavy-traffic analysis and perform well with respect to these nontraditional measures in a wide variety of cases. An approximation is given for the throughput time distribution under both scheduling methods.", "e:keyword": ["Production scheduling", "Sequencing", "Stochastic: multi-item single-machine", "Production scheduling", "Approximation/heuristic: heavy-traffic", "Multi-item single-machine"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.490.12438", "e:abstract": "In service industries, operations are often split into the “front office” and the “back room.” The front office deals directly with customers, whereas the back room engages in manufacturing-like operations. While such an arrangement may enjoy many benefits including reduced customer participation and back room efficiency, it gives rise to the possibility of communication and coordination problems between the front office and the back room. To address such coordination problems, we work from one popular piece of case material in operations management, the National Cranberry Cooperative (NCC) case, and develop and analyze a service facility start-up and capacity model based on a finite-horizon, multistage queueing system. The model allows for general variability that can be either stationary or nonstationary, predictable or unpredictable. We derive the relationships between job start and finish times and primitive parameters and control variables using a sample path construction. We then show that two performance measures, the expected waiting and operating costs, have desirable convexity properties with respect to the control variables; namely, the starting time and the processing capacity of each stage. We also show how to perform marginal analysis on start-up times. Numerical examples based on the NCC case are presented and discussed. The model can be extended and applied to many service and manufacturing situations.", "e:keyword": ["Inventory/production: multistage: start-up time and processing capacity", "Queues", "Nonstationary: finite horizon"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.498.12432", "e:abstract": "Suppose two blind agents with unit speed are placed a distance <i>H</i> apart on an infinite line, and faced in random directions. Their initial distance <i>H</i> is picked from a distribution <i>F</i> with finite mean (mu). We present a pair of rendezvous strategies which do not depend on the distribution <i>F</i> and ensure a meeting in expected time less than 5:514(mu). This improves the bound of 5:74(mu) given by Baston and Gal. Furthermore, the bound we give is best possible for strategies of our type.", "e:keyword": ["Search and surveillance: rendezvous search"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.3.0.12433", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.505.12425", "e:abstract": "We introduce a new problem that arises from operations planning in the process industry. This problem involves matching an order book against surplus inventory before production planning. It can be formulated by generalizing the multiple knapsack problem along three dimensions: (i) adding assignment restrictions on items that can be assigned to a knapsack, (ii) adding a new attribute (called “color” in this paper) to an item and then adding the associated “color” constraints that restrict the number of distinct colors that can be assigned to a knapsack, and (iii) considering multiple objectives for optimization. We formulate the problem, provide a result regarding its complexity, and report on our computational experience with solving a set of real instances based on data from the operations of a large steel plant. We then propose a network-flow---based heuristic that yields solutions within 3% of optimal (or the best known feasible solution). This system has been successfully deployed and is now used daily in the mill operations.", "e:keyword": ["Production/scheduling: applications", "Planning", "Approximations/heuristics", "Inventory/production: approximations/heuristics", "Applications", "Industry", "Mining", "Metals"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.517.12419", "e:abstract": "This historical account of operations research at Bell Laboratories was drafted in the late 1970s when the authors were part of the Operations Research Center within Bell Laboratories at AT&T; it has not previously appeared in the open literature. We have added a few references to later publications that describe particular aspects of the period covered. Discussions of technological practices and organizational arrangements expressed in the present tense represent a viewpoint of about 1980, before “divestiture” split up the Bell System. A comprehensive table of contents for parts I, II, and III can be found in the Online Collection of the <i>Operations Research</i> Home Page, at <ext-link ext-link-type=\"uri\"  href=\"http://or.pubs.informs.org\">http://or.pubs.informs.org</ext-link>.", "e:keyword": ["Communications", "Economics", "Statistics"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.527.12415", "e:abstract": "Reisman and Kirschnick (1995) have defined seven process categories among OR/MS research strategies and analysed the contents of U.S. flagship OR/MS journals in 1992 to examine how often OR/MS workers use these processes. We have applied their method of analysis to the 1994 contents of the U.K. flagship journal, the <i>Journal of the Operational Research Society.</i> As well as providing some insight into the nature of OR/MS research in the United Kingdom, the results enable a comparison to be made with the results from the U.S. flagship journals. The findings confirm that the ripple process is dominantly used in theoretical research, and the transfer-of-technology process is the one most frequently used in true applications in the United Kingdom as well as the United States. Because the literature is dominated by theoretical research using a ripple strategy, the discrimination achieved by the classification is limited. To provide further discrimination an alternative categorisation is used to analyse the nature of research output, distinguishing between the object (material, social, personal) and the objective (knowledge, function, purpose) of the research. The paper proposes that the analysis of the content of research, using a method such as Reisman and Kirschnick's categorisation, could be used for the evaluation of OR/MS research output to complement citation analysis and peer reviews.", "e:keyword": ["Professional: OR/MS education", "Metaresearch", "Professional: OR/MS philosophy: research on research", "Epistomology of management science"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.535.12424", "e:abstract": "We study usage-sensitive charging schemes for broadband communications networks. We argue that a connection's “effective bandwidth” is a good proxy for the quantity of network resource that the connection consumes and can be the basis for a usage charge. The determination of effective bandwidth can be problematic, however, since it involves the moment-generating function of the cell arrival process, which may be difficult to model or measure. This article describes methods of computing usage charges from simple measurements and relating these to bounds on the effective bandwidth. Thus we show that charging for usage on the basis of effective bandwidths can be approximated well by charges based on simple measurements.", "e:keyword": ["Communications: measurement-based charging"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.549.12418", "e:abstract": "The crux of the kidney allocation problem is the trade-off between clinical efficiency and equity. We consider a dynamic resource allocation problem with the tri-criteria objective of maximizing the quality-adjusted life expectancy of transplant candidates (clinical efficiency) and minimizing two measures of inequity: a linear function of the likelihood of transplantation of the various types of patients, and a quadratic function that quantifies the differences in mean waiting times across patient types. The dynamic status of patients is modeled by a set of linear differential equations, and an approximate analysis of the optimal control problem yields a dynamic index policy. We construct a large-scale simulation model using data from over 30,000 transplants, and the simulation results demonstrate that, relative to the organ allocation policy currently employed in the United States, the dynamic index policy increases the quality-adjusted life expectancy and reduces the mean waiting time until transplantation for all six demographic groups (two sexes, races, and age groups) under consideration.", "e:keyword": ["Health care: Organ allocation", "Queues: networks with reneging", "Dynamic programming: applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.570.12422", "e:abstract": "We investigate multiproduct, multilocation production/distribution networks with deterministic, stationary demands. Most research on such systems assumes zero leadtimes. Methods using fixed reorder intervals that are power-of-two multiples of a base planning period have proved to be very successful for such systems. The same methods apply to networks with positive leadtimes, provided the leadtimes are <i>balanced</i> in a specific sense. We explore networks with general, <i>unbalanced</i> leadtimes. A key result is a lower bound on the cost of any feasible policy. Other results concern policy heuristics and their performance. For a large class of networks, we construct a policy whose cost is within 45% of the lower bound. For general networks, the performance guarantee is 1:02(sqrt)(1+(eta)), where (eta) is a number dependent on the network topology only. In general, the best performance bound is obtained by systematically reducing the order intervals derived from the corresponding zero-leadtime system.", "e:keyword": ["Inventory/production: multi-item/echelon/stage", "Approximations/heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.578.12417", "e:abstract": "Both the single-airport ground-holding problem (GH) and the multi-airport ground-holding problem can be extended by the addition of banking constraints to accommodate the hubbing operations of major airlines. These constraints enforce the desire of airlines to land certain groups of flights, called banks, within fixed time windows, thus preventing the propagation of delays throughout their entire operation. GH can be formulated as a transportation problem and readily solved. But in the presence of banking constraints, GH becomes a difficult integer programming problem. In this paper, we construct five different models of the single-airport ground-holding problem with banking constraints (GHB). The models are evaluated both computationally and analytically. For two of the models, we show that the banking constraints induce facets of the convex hull of the set of integer solutions. In addition, we explore a linear transformation of variables and a branching technique.", "e:keyword": ["Programming: integer", "Algorithms", "Transportation: models", "Air traffic management"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.591.12413", "e:abstract": "We develop heuristics for a problem that models the static balancing of turbine fans: load point masses at regularly spaced positions on the periphery of a circle so that the residual unbalance about the center---which corresponds to the axis of rotation of the fan---is as small as possible. We give worst-case guarantees for our heuristics in terms of residual unbalance. For the case of an even number of blades, we show that one of our heuristics provides the same worst-case guarantee (with respect to the ideal of perfect balance) as does total enumeration. Furthermore, computational tests show that our heuristics are orders of magnitude faster and not far from optimum on average.", "e:keyword": ["Industries", "Machinery: assembly of turbine engines", "Analysis of algorithms", "Suboptimal: worst-case residual unbalance"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.603.12416", "e:abstract": "In this study, we formulate the railroad blocking problems as a network design problem with maximum degree and flow constraints on the nodes and propose a heuristic Lagrangian relaxation approach to solve the problem. The newapproach decomposes the complicated mixed integer programming problem into two simple subproblems so that the storage requirement and computational effort are greatly reduced. A set of inequalities are added to one subproblem to tighten the lower bounds and facilitate generating feasible solutions. Subgradient optimization is used to solve the Lagrangian dual. An advanced dual feasible solution is generated to speed up the convergence of the subgradient method. The model is tested on blocking problems from a major railroad, and the results show that the blocking plans generated have the potential to reduce the railroad's operating costs by millions of dollars annually.", "e:keyword": ["Transportation network design: column and cut generation", "Lagrangian relaxation", "Railroad blocking"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.615.12423", "e:abstract": "Consider a flow shop with <i>M</i> machines in series, through which a set of jobs are to be processed. All jobs have the same routing, and they have to be processed in the same order on each of the machines. The objective is to determine such an order of the jobs, often referred to as a permutation schedule, so as to minimize the total completion time of all jobs on the final machine. We show that when the processing times are statistically exchangeable across machines and independent across jobs, the Shortest ProcessingTime first (SPT) scheduling rule, based on the total service requirement of each job on all <i>M</i> machines, is asymptotically optimal as the total number of jobs goes to infinity. This extends a recent result of Kaminsky and Simchi-Levi (1996), in which a crucial assumption is that the processing times on all <i>M</i> machines for all jobs must be i.i.d.. Our work provides an alternative proof using martingales, which can also be carried out directly to show the asymptotic optimality of the weighted SPT rule for the Flow Shop Weighted Completion Time Problem.", "e:keyword": ["Asymptotically optimal scheduling: flow shop average completion time problem", "Flow shop scheduling: average completion time", "Asymptotic optimality", "Tandem queueing: asymptotic optimality of SPT"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.623.12420", "e:abstract": "A new method of sensitivity analysis for mixed integer/linear programming (MILP) is derived from the idea of inference duality. The inference dual of an optimization problem asks how the optimal value can be deduced from the constraints. In MILP, a deduction based on the resolution method oftheorem proving can be obtained from the branch-and-cut tree that solves the primal problem. One can then investigate which perturbations ofthe problem leave this proof intact. On this basis it is shown that, in a minimization problem, any perturbation that satisfies a certain system of linear inequalities will reduce the optimal value no more than a prespecified amount. One can also give an upper bound on the increase in the optimal value that results from a given perturbation. The method is illustrated on two realistic problems.", "e:keyword": ["Sensitivity analysis: sensitivity analysis for mixed integer programming", "Integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.635.12414", "e:abstract": "We consider the problem of servicing a number of objects in a discrete time environment. In each period, we may select an object that will receive a service in the period. Each time an object is serviced, we incur a servicing cost dependent on the time since the object's last service. Problems of this type appear in many contexts, e.g., multiproduct lot-sizing, machine maintenance, and several problems in telecommunications. We assume that at most one object can be serviced in a given period. For the general problem with <i>m</i> objects, which is known to be (N-script)(P-script)-Hard, we describe properties of an optimal policy, and for the specific case of <i>m</i> = 2 objects, we determine an optimal policy.", "e:keyword": ["Inventory/production: policies", "Maintenance/replacement", "Mathematics: Convexity", "Analysis of algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.4.646.12421", "e:abstract": "This paper studies a continuous review two-echelon inventory system with a supplier serving multiple retailers. Each location uses a decentralized (<i>Q, R</i>) policy based on installation stocks. Each retailer may set a unique reorder quantity as a multiple of a basic packaging size <i>Q</i>. The supplier, in turn, adopts a similar batch replenishment policy. Our objective is to derive the exact steady-state performance for the supplier. We show that the inventory position at the supplier can be easily characterized by a uniform distribution. As a result, the closed-form expected cost function is tractable. We also examine the performance of approximation schemes such as the classic Poisson approximation. Our numerical results show that even though the Poisson approximation performs reasonably well in estimating steady-state performance, the cost penalty for using it in optimization may be significant.", "e:keyword": ["Inventory/production: operating characteristics: supplier performance evaluation: multi-echelon: two echelons: uncertainty", "Stochastic: random demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.661.12402", "e:abstract": "This paper is derived from the author's sponsored history of Operations Research in Britain from its formal inception in the later 1930s. It is inspired by the knowledge that any history of OR in Britain that ignores the interrelationship with OR in the United States would be grossly incomplete. This relates not only to the period of military collaboration in World War II but also to the profound influence on British operations researchers of American-derived techniques and methods. The Anglo-American perspective serves also to highlight major contrasts, as well as striking similarities in the development of OR in both countries. The paper advocates the need for ongoing research into the history of OR as an essential complement to the advancing frontier of knowledge, both in theory and in practice. Virtually all disciplines worthy of university-level study have their chroniclers, and this should apply with no less force to OR in the light of its impressive trajectory of development and early acknowledgment of its utilitarian value in a wide variety of settings in the public and private sectors.", "e:keyword": ["History of OR/MS: Anglo/U.S. comparisons and contrasts", "OR/MS philosophy: Anglo/U.S. comparisons and contrasts"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.671.12410", "e:abstract": "The <i>Uncapacitated Facility Location Problem with Client Matching</i> (LCM) is an extension of the <i>Uncapacitated Facility Location Problem</i> (UFLP), where two clients allocated to a facility can be matched. As in the UFLP, facilities can be opened at any of <i>m</i> predefined locations with given fixed costs, and <i>n</i> clients have to be allocated to the open facilities. In classical location models, the allocation cost is the distance between a client and an open facility. In the LCM, the allocation cost is either the cost of a return trip between the facility and the client, or the length of a tour containing the facility and two clients. The similarities of the LCM with the classical UFLP and the matching problem are exploited to derive valid inequalities, optimality cuts, and polyhedral results. A greedy heuristic and a branch-and-cut algorithm are developed, and several separation procedures are described. Computational experiments confirm the efficiency of the proposed approach.", "e:keyword": ["Facilities", "Location", "Discrete: client matching", "Transportation", "Vehicle routing: depot location", "Programming", "Integer", "Branch and cut", "Algorithms", "Cutting plane", "Facet generation"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.686.12403", "e:abstract": "We consider a two-level inventory system with one central warehouse and <i>N</i> retailers. All installations apply different continuous review installation stock (<i>R,Q</i>) policies. The retailers face independent compound Poisson demand processes. Transportation times are constant. We present a method for exact evaluation of control policies that provides the complete probability distributions of the retailer inventory levels.", "e:keyword": ["Inventory/production", "Multi-echelon: batch-ordering policies", "Continuous review", "Inventory/production", "Stochastic: compound Poisson demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.697.12411", "e:abstract": "Interest in protecting natural areas is increasing as development pressures and conflicting land uses threaten and fragment ecosystems. A variety of quantitative approaches have been developed to help managers select sites for biodiversity protection. The problem is often formulated to select the set of reserve sites that maximizes the number of species or ecological communities that are represented, subject to an upper bound on the number or area of selected sites. Most formulations assume that information about the presence or absence of species in the candidate sites is known with certainty. Because complete information typically is lacking, we developed a reserve selection formulation that incorporates probabilistic presence-absence data. The formulation was a discrete 0/1 optimization model that maximized the number of represented vegetation communities subject to a budget constraint, where a community was considered represented if its probability of occurrence in the set of selected sites exceeded a specified minimum reliability threshold. Although the formulation was nonlinear, a log transformation allowed us to represent the problem in a linear format that could be solved using exact optimization methods. The formulation was tested using a moderately sized reserve selection problem based on data from the Superior National Forest in Minnesota.", "e:keyword": ["Environment: protected natural reserves on national forests", "Programming", "Integer: discrete 0/1 by exact solution methods", "Probability", "Applications: presence-absence data of species in reserves"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.709.12401", "e:abstract": "In this paper, we address the problem of admission control and sequencing in a production system that produces two classes of products. The first class of products is made-to-stock, and the firm is contractually obliged to meet demand for this class of products. The second class of products is made-to-order, and the firm has the option to accept (admit) or reject a particular order. The problem is motivated by suppliers in many industries who sign contracts with large manufacturers to supply them with a given product and also can take on additional orders from other sources on a make-to-order basis.We model the joint admission control/sequencing decision in the context of a simple two-class <i>M</i>/<i>M</i>/1 queue to gain insight into the following problems: 1. How should a firm decide (a) when to accept or reject an additional order, and (b) which type of product to produce next? 2. How should a firm decide what annual quantity of orders to commit to when signing a contract to produce the make-to-stock products?", "e:keyword": ["Inventory/production: inventory policies for make-to-stock/make-to-order systems", "Sequencing: stochastic sequencing"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.721.12408", "e:abstract": "This paper studies the fluid models of two-station multiclass queueing networks with deterministic routing. A fluid model is globally stable if the fluid network eventually empties under each nonidling dispatch policy. We explicitly characterize the global stability region in terms of the arrival and service rates. We show that the global stability region is defined by the nominal workload conditions and the “virtual workload conditions,” and we introduce two intuitively appealing phenomena---virtual stations and push starts---that explain the virtual workload conditions. When any of the workload conditions is violated, we construct a fluid solution that cycles to infinity, showing that the fluid network is unstable. When all the workload conditions are satisfied, we solve a network flow problem to find the coefficients of a piecewise linear Lyapunov function. The Lyapunov function decreases to zero, proving that the fluid level eventually reaches zero under any nonidling dispatch policy. Under certain assumptions on the inter arrival and service time distributions, a queueing network is stable or positive Harris recurrent if the corresponding fluid network is stable. Thus, the workload conditions are sufficient to ensure the global stability of two-station multiclass queueing networks with deterministic routing.", "e:keyword": ["Queueing networks", "Fluid models", "Stability", "Piecewise linear Lyapunov functions", "Network flows"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.745.12412", "e:abstract": "In this article we propose a mixed 0-1 linear programming model for the topological network design problem with modular switches such as the ones that will be used in asynchronous transfer mode (ATM) frame relay and other broadband networks. The model includes the location of switches, their configuration with respect to ports and multiplexers, the design of an access network with a star topology, and a backbone network with a fixed topology (ring or tree). To obtain a solution, we propose a greedy heuristic that provides a good starting solution, and a tabu search heuristic to improve the solution. Finally, we present an example of the application of the heuristics and results for a set of randomly generated problems with up to 500 users and 30 potential switch sites. For the hundreds of problems generated, the tabu algorithm produced solutions that were, on average, within 1.5% of the optimal solution, and in the worst case within 4.95% of the optimal solution.", "e:keyword": ["Communications: topological network design"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.761.12406", "e:abstract": "This study considers a situation where a contractor receives an order that it commits to satisfy in full. The fulfillment of the contract requires manufacturing and inspection. Because the number of defective units within a produced lot is not known in advance, it is possible that after examining the lot, it is learned that the number of conforming units is short of the demand. If so, further manufacturing and inspection are required. Once enough conforming units are found, the inspection terminates, and the remaining uninspected units, as well as all defectives, are scrapped.Whereas previous “multiple production runs” studies implicitly assumed that inspection costs are negligible, we include these costs as a key part of the problem. It turns out that the optimal production lot size depends on the inspection cost. Our model is very general: We provide a framework to calculate the optimal batch and the expected number of inspections for any yield pattern, as well as for any inspection procedure. We also provide results and numerical examples concerning specific yield patterns that are common in practice.", "e:keyword": ["Random yield production", "Inspection", "Lot sizing"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.768.12400", "e:abstract": "In this paper we address the problem of determining optimal portfolios that may include options in a framework of return maximization with risk constraints relative to a benchmark, as well as in terms of absolute returns. The model we propose allows for deterministic constraints as well as probabilistic constraints. We derive properties of optimal and feasible portfolios and present a linear programming model to solve the problem. The optimal portfolios have payoff functions that reflect a gambling policy. We show that optimal solutions to a large class of portfolio models that maximize expected return subject to downside risk constraints are driven by this casino effect and present tractable conditions under which it occurs in our model. We propose to control the casino effect by using chance constraints. Using results from financial theory, we formulate an LP model that maximizes expected return subject to worst-case return constraints and chance constraints on achieving prespecified levels of return. The results are illustrated with real-life data on the S&P 500 index.", "e:keyword": ["Portfolio optimization", "Options", "Downside risk", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.776.12409", "e:abstract": "We prove strong laws of large numbers and central limit theorems for some permuted estimators from regenerative simulations. These limit theorems provide the basis for constructing asymptotically valid confidence intervals for the permuted estimators.", "e:keyword": ["Simulation", "Efficiency: variance reduction techniques", "Simulation", "Statistical analysis: regenerative simulation", "Probability", "Regenerative processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.788.12405", "e:abstract": "When a production shop has a large number of identical parts, the parts are often recorded by a part description and quantity. This differs from the type of description used by standard scheduling problems, which assume that all parts or jobs are unique. In high-multiplicity scheduling problems, identical jobs are encoded in an efficient format similar to that of the production shop. The input describes one of the jobs and the number of such identical jobs. We consider single-machine, high-multiplicity problems with earliness and tardiness weights. We investigate three categories of weights: unit, common, and job-specific. For the unit and common weights problems, a polynomial time algorithm is developed. The algorithm takes advantage of identical jobs and finds solutions faster than by standard methods.We provide a new method for creating a lower bound for the standard encoding of the job-specific weights problem, which is NP-complete. We disaggregate each job into identical sub jobs with unit processing times. Then, using high-multiplicity encoding for this disaggregated problem, we create a lower bound on the optimal objective function value of the original problem in polynomial time. Heuristic solutions are generated using a randomized rounding technique on the lower bound solution. These results are used in a branch-and-bound solution method. Analytical and computational results are presented. Our combination of disaggregation and high-multiplicity encoding provides a new method for creating lower bounds on the objective functions of NP-complete problems.", "e:keyword": ["Production/scheduling", "Deterministic Sequencing", "Single machine: High multiplicity problems with an Earliness-Tardiness objective", "Production/scheduling", "Approximations/heuristic: disaggregate and then use high multiplicity methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.801.12407", "e:abstract": "In this paper we consider the Split Delivery Vehicle Routing Problem (SDVRP), a relaxation of the known Capacitated Vehicle Routing Problem (CVRP) in which the demand of any client can be serviced by more than one vehicle. We define a feasible solution of this problem, and we show that the convex hull of the associated incidence vectors is a polyhedron (<i>P</i><sub>SDVRP</sub>), whose dimension depends on whether a vehicle visiting a client must service, or not, at least one unit of the client demand. From a partial and linear description of <i>P</i><sub>SDVRP</sub> and a new family of valid inequalities, we develop a lower bound whose quality is exhibited in the computational results provided, which include the optimal resolution of some known instances---one of them with 50 clients. This instance is, as far as we know, the biggest one solved so far.", "e:keyword": ["Transportation", "Vehicle routing: a case with split demands", "Study of the polyhedron", "Programming", "Integer", "Cutting plane: based on the polyhedral study", "Computational results are reported"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.5.811.12404", "e:abstract": "We consider the problem of dynamically allocating production capacity between two products to minimize the average inventory and backorder costs per unit time in a make-to-stock single machine system. Using sample path comparisons and dynamic programming, we give a characterization of the optimal hedging point policy for a certain region of the state space. The characterization is simple enough to lead to easily implementable heuristics and provides a formal justification of some of the earlier heuristics proposed.", "e:keyword": ["Production/scheduling", "Stochastic: multi-item", "Queues: make-to-stock system", "Inventory/production: optimal policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.823.12397", "e:abstract": "Handling freight in a crossdocking terminal is labor intensive and therefore costly because workers must unload, sort, and transfer a wide variety of freight from incoming to outgoing trailers. The efficiency of workers depends in large part on how trailers are assigned to doors around the dock; that is, on its layout. A good layout reduces travel distances without creating congestion, but until now no tools have been available to construct such layouts. We describe models of travel cost and three types of congestion typically experienced in crossdocking terminals, and we use them to construct layouts that minimize the labor cost of transferring freight. We report on the use of our models in the less-than-truckload trucking industry, including an implementation at a terminal in Stockton, California that improved productivity by more than 11%.", "e:keyword": ["Material handling: freight terminals", "Congestion", "Layout", "Facilities", "Design: layout of freight terminals", "Congestion"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.833.12394", "e:abstract": "Under present arrangements, U.S. commercial planes do not travel “as the crow flies” from origin to destination; rather, they are generally restricted to paths within a grid. New technologies, however, raise the possibility of moving to a “free-flight” regime under which planes could fly directly from point to point. Striving for general insight rather than definitive conclusions, we use geometrical probability to assess how free-flight could affect the safety and efficiency of en route air traffic operations. We work with two air traffic control sectors: one hypothetical and the other based on actual traffic patterns over Albany, New York. Though tentative, the results suggest that---so long as certain operational constraints are retained---the changed geometry of flight paths after a transition to free-flight might tend in itself to diminish mid-air collision risk. Much depends, however, on whether the human/technological capabilities of future air traffic control can match the extraordinary effectiveness of the existing system.", "e:keyword": ["Transportation", "Safety: aviation collision risk modeling", "Reliability", "Failure models: aviation collision risk"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.846.12388", "e:abstract": "We discuss an integer linear programming modeling system based on relational algebra. In this system, all modeling related activities, such as model formulation, model instantiation, and model and instance management, are done using simple operations such as selection, projection, and predicated join.", "e:keyword": ["Information systems: analysis and design", "Philosophy of modeling"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.858.12396", "e:abstract": "Increasingly shorter product life cycles impel firms to design, develop, and market more products in less time than ever before. Overlapping of design and development stages is commonly regarded as the most promising strategy to reduce product development times. However, overlapping typically requires additional resources and can be costly.Our research addresses the trade-off between product development time and costs and introduces an algorithm to determine an appropriate overlapping strategy under different scenarios. The methodology developed was successfully employed at Rocketdyne Division of Rockwell International.", "e:keyword": ["Design", "Optimization", "Algorithm", "Product development"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.866.12390", "e:abstract": "We study the problem of designing at minimum cost a two-connected network such that the shortest cycle to which each edge belongs (a “mesh”) does not exceed a given length <i>K</i>. This problem arises in the design of fiber-optic-based backbone telecommunication networks. A Branch-and-Cut approach to this problem is presented for which we introduce several families of valid inequalities and discuss the corresponding separation algorithms. Because the size of the problems solvable to optimality by this approach is too small, we also develop some heuristics. The computational performances of these exact and approximate methods are then thoroughly assessed both on randomly generated instances as well as instances suggested by real applications.", "e:keyword": ["Communication: survivable network design/model for communication network design", "Programming", "Integer: cutting plane and heuristics/ILP: branch-and-cut heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.878.12399", "e:abstract": "After reformulating Clark and Scarf's (1960) classical serial multi-echelon model so that the lead time between adjacent echelons is one week (period), the option to expedite between each resulting echelon is added. Thus, each week requires a decision to be made at each echelon on how many units to expedite in from the next upstream echelon (to be received immediately) and how many to regular order (to be received in one week), with the remainder detained (left as is). The model can be interpreted as addressing dynamic lead time management, in which the (remaining) effective lead time for each ordered unit can be dynamically reduced by expediting and/or extended. Use of Clark and Scarf's (1960) idea of echelon stocks reduces a complex, multidimensional stocking problem to the analysis of a series of one-dimensional subproblems. What are called <i>top-down base stock policies</i>, which are readily amenable to managerial interpretation, are shown to be optimal. Myopic policies are shown to be optimal in the stationary, in1nite horizon case. The results are illustrated numerically.", "e:keyword": ["Inventory/production", "Multi-echelon: decomposition into sequence of one-dimensional problems", "Inventory/production", "Review/lead times: dynamically managed expediting", "Dynamic programming", "Applications: optimality of top-down base stock policies under stochastic demands"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.894.12392", "e:abstract": "The open-pit mining problem is to determine the contours of a mine, based on economic data and engineering feasibility requirements, to yield maximum possible net income. This practical problem needs to be solved for very large data sets. In practice, moreover, it is necessary to test multiple scenarios, taking into account a variety of realizations of geological predictions and forecasts of ore value.The industry is experiencing computational difficulties in solving the problem. Yet, the problem is known to be equivalent to the minimum cut or maximum flow problem. For the maximum flow problem there are a number of very efficient algorithms that have been developed over the last decade. On the other hand, the algorithm that is most commonly used by the mining industry has been devised by Lerchs and Grossmann (LG). This algorithm is used in most commercial software packages for open-pit mining.This paper describes a detailed study of the LG algorithm as compared to the maximum flow “push-relabel” algorithm. We propose here an efficient implementation of the push-relabel algorithm adapted to the features of the open-pit mining problem. We study issues such as the properties of the mine and ore distribution and how they affect the running time of the algorithm. We also study some features that improve the performance of the LG algorithm. The proposed implementations offer significant improvements compared to existing algorithms and make the related sensitivity analysis problem practically solvable.", "e:keyword": ["Industries/mining/metals: planning for mining operations", "Networks/graphs", "Applications: application of minimum cut to mining", "Networks/graphs", "Flow algorithms: implementing the preflow and another minimum cut algorithm"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.915.12391", "e:abstract": "The cutting stock problem is that of finding a cutting of stock material to meet demands for small pieces of prescribed dimensions while minimising the amount of waste. Because changing over from one cutting pattern to another involves significant setups, an auxiliary problem is to minimise the number of different patterns that are used. The pattern minimisation problem is significantly more complex, but it is of great practical importance. In this paper, we propose an integer programming formulation for the problem that involves an exponential number of binary variables and associated columns, each of which corresponds to selecting a fixed number of copies of a specific cutting pattern. The integer program is solved using a column generation approach where the subproblem is a nonlinear integer program that can be decomposed into multiple bounded integer knapsack problems. At each node of the branch-and-bound tree, the linear programming relaxation of our formulation is made tighter by adding super-additive inequalities. Branching rules are presented that yield a balanced tree. Incumbent solutions are obtained using a rounding heuristic. The resulting branch-and-price-and-cut procedure is used to produce optimal or approximately optimal solutions for a set of real-life problems.", "e:keyword": ["Integer programming algorithms: decomposition", "Branch-and-bound", "Branch-and-price", "Column generation", "Cutting plane", "Heuristic", "Relaxation", "Production/scheduling: cutting stock/trim"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.927.12398", "e:abstract": "The problem of designing a contract mechanism to allocate the component subprojects of a large project to a pool of contractors has important implications for project success. Our research analytically addresses issues involved in diversifying risk for the project owner by partitioning the project and assigning the subprojects to multiple contractors whose performance characteristics are imperfectly known. We begin by giving a precise analytical treatment of the effect of activity variance on expected project duration, characterizing the cases when an increase in activity variance pushes up the expected project duration. In the case of a homogeneous project consisting of serial subprojects, we show that disaggregating the project and assigning the subprojects to the contractors on a piecemeal basis reduces variance of project duration while leaving the mean unchanged. On the other hand, in the case of a homogeneous project consisting of parallel subprojects, aggregating the subprojects and assigning the aggregated project to one of the contractors reduces mean project duration.", "e:keyword": ["Project management: contract design", "Probability: stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.939.12393", "e:abstract": "This study concerns a generic model-free stochastic optimization problem requiring the minimization of a risk function defined on a given bounded domain in a Euclidean space. Smoothness assumptions regarding the risk function are hypothesized, and members of the underlying space of probabilities are presumed subject to a large deviation principle; however, the risk function may well be nonconvex and multimodal. A general approach to finding the risk minimizer on the basis of decision/observation pairs is proposed. It consists of repeatedly observing pairs over a collection of design points. Principles are derived for choosing the number of these design points on the basis of an observation budget, and for allocating the observations between these points in both prescheduled and adaptive settings. On the basis of these principles, large-deviation type bounds of the minimizer in terms of sample size are established.", "e:keyword": ["Simulation: stochastic optimization", "Design of experiments", "Programming: stochastic", "Adaptive"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.951.12389", "e:abstract": "We consider a production/inventory system consisting of <i>M</i> machines and <i>K (K(le)M)</i> repair crews in which machines are subject to time dependent failures. The repair operations on each machine require one repair crew during the whole operation. In this production/inventory system, each machine is assigned to produce a different item according to a make-to-stock routine. Inventories of each item service a Poisson demand process, and the unsatisfied demands are lost. The objective is to minimize the sum of the average holding and lost-sales penalty costs.We formulate the joint problem of (1) allocating the limited number of repairmen to failed machines and (2) deciding how much finished goods inventory to keep as a Markov decision process. We show that the optimal policy has a very complicated structure. We introduce two models to compute optimal base-stock policies in systems with identical machines, where first break first repair (FBFR) or preemptive priority (PPRI) repair policies are used. Then we present two heuristics to perform the same optimization analysis in systems with different machines. Finally, we compare the combination of the optimal base-stock policy (as the production policy) and the FBFR and PPRI policies (as the repair policies) to the optimal dynamic policy, and through numerical examples we show that this integration creates a solution that is close to the optimal dynamic policy. The results indicate that simple policies for determining finished goods inventory levels and repair crew assignment to failed machines can work well as long as the two problems are addressed in a coordinated manner.", "e:keyword": ["Inventory/production: maintenance/replacement", "Manufacturing: performance"]}, {"@id": "http://dx.doi.org/10.1287/opre.48.6.965.12395", "e:abstract": "Given that the total service effort in a multiple-server environment is fixed, it is generally known that the single-server system yields the minimum time a customer spends in the system. However, in many manufacturing as well as service applications the waiting time in the queue is more significant than total time in the system. We consider several such queueing design problems and show that the results for minimizing the waiting time in the queue are markedly different from those for minimizing total time in the system.", "e:keyword": ["Queues: optimization", "Application", "Probability: stochastic model application"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.1.11187", "e:abstract": "We review Stigler's diet problem, its impact on linear programming and operations research, and we determine minimum cost diets using updated nutritional and cost data. We also discuss how Stigler's diet problem formulation and its extensions have, over the years, influenced dietitians and nutritionists in their search for more wholesome but cost-effective diets.", "e:keyword": ["Agriculture/food: human diet", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.107.11191", "e:abstract": "We consider rendezvous problems in which two players move on the plane and wish to cooperate to minimise their first meeting time. We begin by considering the case where both players are placed such that the vector difference is chosen equiprobably from a finite set. We also consider a situation in which they know they are a distance <i>d</i> apart, but they do not know the direction of the other player. Finally, we give some results for the case in which player 1 knows the initial position of player 2, while player 2 is given information only on the initial distance of player 1.", "e:keyword": ["Military", "Search/surveillance: Marking strategies on the geometric plane"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.119.11183", "e:abstract": "We consider a model of a multiclass make-to-stock manufacturing system. External demand for each product class is met from the available finished goods inventory; unsatisfied demand is backlogged. The objective is to devise a production policy that minimizes inventory costs subject to guaranteeing stockout probabilities to stay bounded above by given constants (epsilon)<sub><i>j</i></sub>, for each product class <i>j</i> (<i>service level guarantees</i>). Such a policy determines whether the facility should be producing (<i>idling decisions</i>), and if it should, which product class (<i>sequencing decisions</i>). Approximating the original system, we analyze a corresponding <i>fluid model</i> to make sequencing decisions and employ <i>large deviations techniques</i> to make idling ones. We consider both linear and quadratic inventory cost structures to obtain a <i>priority-based</i> and a <i>generalized longest queue first-based</i> production policy, respectively. An important feature of our model is that it accommodates autocorrelated demand and service processes, both critical features of modern failure-prone manufacturing systems.", "e:keyword": ["Inventory/Production: multi-item", "Make-to-stock systems", "Queues", "Approximations: large deviations", "Fluid models"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.134.11192", "e:abstract": "In most classical scheduling models, it is assumed that a job is dispatched to a customer immediately after its processing completes. In many practical situations, however, a set of delivery dates may be fixed before any jobs are processed. This is particularly relevant where delivery is an expensive or complicated operation, for example, as with heavy machinery. A similar situation arises where customers find deliveries disruptive and thus require them to be made within a limited time interval that repeats periodically. A third possibility is that a periodic business function, for example, the supplier's billing cycle, effectively defines a delivery date, and includes all jobs that have been completed since the previous billing cycle. These situations are not adequately represented by classical scheduling models. We consider a variety of deterministic scheduling problems in which a job is dispatched to a customer at the earliest fixed delivery date that is no earlier than the completion time of its processing. Problems where the number of delivery dates is constant, and others where it is specified as part of data input, are studied. For almost all problems considered, we either provide an efficient algorithm or establish that such an algorithm is unlikely to exist. By doing so, we permit comparisons between the solvability of these fixed delivery date problems and of the corresponding classical scheduling problems.", "e:keyword": ["Production/scheduling: scheduling with fixed delivery dates", "Sequencing", "Deterministic: algorithms and complexity results"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.14.11195", "e:abstract": "The National Energy Modeling System (NEMS) is a large-scale mathematical model that computes equilibrium fuel prices and quantities in the U.S. energy sector and is currently in use at the U.S. Department of Energy (DOE). At present, to generate these equilibrium values, NEMS iteratively solves a sequence of linear programs and nonlinear equations. This is a nonlinear Gauss-Seidel approach to arrive at estimates of market equilibrium fuel prices and quantities. In this paper, we present existence and uniqueness results for NEMS-type models based on a nonlinear complementarity/variational inequality problem format. Also, we document mathematically, for the first time, how the inputs and the outputs for each NEMS module link together.", "e:keyword": ["Mathematical programming", "Complementarity: energy-economic application for the nonlinear complementarity problem", "Government", "Energy policies: general framework for determining energy policies."]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.145.11186", "e:abstract": "The classical model of the preemptive repeat priority queue assumes that there is a single distribution from which service times for a given class are selected, regardless of the number of preemptions. This paper presents three alternatives for improving service progressively to preempted customers: (1) changing the service distribution, (2) preventing further preemptions, and (3) promoting customers to the next higher priority class. The Laplace-Stieltjes transform of the flow time distribution is found for each class in a model that can employ these three alternatives in arbitrary combinations.", "e:keyword": ["Queues", "Priority: flow times in preemptive-repeat different queues with three methods to reduce customer delay"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.157.11197", "e:abstract": "We consider a periodic review inventory system with demand arriving simultaneously from a deterministic source and a random source. The deterministic demand has to be satisfied immediately and the stochastic demand can be backordered. Assuming that the stochastic demand is never backlogged if there is stock in the system, we prove that a modified (<i>s, S</i>) policy is optimal under general conditions if there is a setup cost. If there is a smoothing cost instead of the setup cost, we observe that the problem corresponds to a standard model with one source of demand.", "e:keyword": ["Inventory/production: deterministic and stochastic demand", "Contract", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.163.11193", "e:abstract": "Nemhauser and Trick presented the problem of finding a timetable for the 1997/1998 Atlantic Coast Conference (ACC) in basketball. Their solution, found with a combination of integer programming and exhaustive enumeration, was accepted by the ACC.Finite-domain constraint programming is another programming technique that can be used for solving combinatorial search problems such as sports tournament scheduling. This paper presents a solution of round-robin tournament planning based on finite-domain constraint programming. The approach yields a dramatic performance improvement, which makes an integrated interactive software solution feasible.", "e:keyword": ["Recreation and sports: tournament scheduling. Programming: constraint."]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.169.11190", "e:abstract": "A number of methods for multiple-objective optimization problems (MOP) give as solution to MOP the set of optimal solutions for some single-objective optimization problems associated with it. Well-known examples of these single-objective optimization problems are the minsum and the minmax. In this note, we propose a new parametric single-objective optimization problem associated with MOP by means of Goal Programming ideas. We show that the minsum and minmax are particular instances, so we are somehow combining minsum and minmax by means of a parameter. Moreover, such parameter has a clear meaning in the value space. Applications of this parametric problem to classical models in Locational Analysis are discussed.", "e:keyword": ["Decision analysis: multiple criteria theory", "Facilities: Continuous location/discrete location"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.175.11184", "e:abstract": "We examine two coefficient adjustment methods proposed in the literature, and we show by example that the second is not valid. We provide a modified approach that subsumes both methods, generalizing the first and correcting the second.", "e:keyword": ["Integer programming", "Theory: coefficient adjustment using special ordered sets", "Cutting planes", "Continuous relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.26.11185", "e:abstract": "A resource allocation problem, called the dynamic and stochastic knapsack problem (DSKP), is studied. A known quantity of resource is available, and demands for the resource arrive randomly over time. Each demand requires an amount of resource and has an associated reward. The resource requirements and rewards are unknown before arrival and become known at the time of the demand's arrival. Demands can be either accepted or rejected. If a demand is accepted, the associated reward is received; if a demand is rejected, a penalty is incurred. The problem can be stopped at any time, at which time a terminal value is received that depends on the quantity of resource remaining. A holding cost that depends on the amount of resource allocated is incurred until the process is stopped. The objective is to determine an optimal policy for accepting demands and for stopping that maximizes the expected value (rewards minus costs) accumulated. The DSKP is analyzed for both the infinite horizon and the finite horizon cases. It is shown that the DSKP has an optimal policy that consists of an easily computed threshold acceptance rule and an optimal stopping rule. A number of monotonicity and convexity properties are studied. This problem is motivated by the issues facing a manager of an LTL transportation operation regarding the acceptance of loads and the dispatching of a vehicle. It also has applications in many other areas, such as the scheduling of batch processors, the selling of assets, the selection of investment projects, and yield management.", "e:keyword": ["Dynamic programming/optimal control", "Markov: resources allocation", "Transportation", "Scheduling: load acceptance dispatching", "Production/scheduling", "Cutting stock: dynamic stock cutting"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.42.11196", "e:abstract": "The truss design problem is to find the optimal placement and size of structural bars that can support a given load. The problem is nonlinear and, in the version addressed here, the bars must take certain discrete sizes. It is shown that a logic-based method that dispenses with integer variables and branches directly on logical disjunctions can solve substantially larger problems than mixed integer programming, even though the nonlinearities disappear in the mixed integer model. A primary purpose of the paper is to investigate whether advantages of logic-based branching that have been demonstrated elsewhere for linear problems extend to nonlinear programming.", "e:keyword": ["Programming", "Integer", "Nonlinear: logic-based method", "Engineering: structural design"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.52.11189", "e:abstract": "This paper discusses the problem of batching and scheduling of certain kinds of batch processors. Examples of these processors include heat treatment facilities, particularly in the steel and ceramics industries, as well as a variety of operations in the manufacture of integrated circuits. In general, for our problem there is a set of jobs waiting to be processed. Each job is associated with a given family and has a weight or delay cost and a volume. The scheduler must organize jobs into batches in which each batch consists of jobs from a single family and in which the total volume of jobs in a batch does not exceed the capacity of the processor. The scheduler must then sequence all the batches. The processing time for a batch depends only on the family and not on the number or the volume of jobs in the batch. The objective is to minimize the mean weighted flow time.The paper presents an integer programming formulation for this problem, generates a lower bound from a partial LP relaxation, provides a polynomial algorithm to solve a special case, and tests a set of heuristics on the general problem. The ability to pack jobs into batches is the key to efficient solutions and is the basis of the different solution procedures in this paper. The heuristics include a greedy heuristic, a successive knapsack heuristic, and a generalized assignment heuristic. Optimal solutions are obtained by complete enumeration for small problems.The conclusions of the computational study show that the successive knapsack and generalized assignment heuristics perform better than the greedy. The generalized assignment heuristic does slightly better than the successive knapsack heuristic in some cases, but the latter is substantially faster and more robust. For problems with few jobs, the generalized assignment heuristic and the knapsack heuristic almost always provide optimal solutions. For problems with more jobs, we compare the heruistic solutions' values to lower bounds; the computational work suggests that the heuristics continue to provide solutions that are optimal or close to the optimal. The study also shows that the volume of the job relative to the capacity of the facility and the number of jobs in a family affect the performance of the heuristics, whereas the number of families does not. Finally, we give a worst-case analysis of the greedy heuristic.", "e:keyword": ["Production/scheduling: approximations/heuristic: batch processors", "Manufacturing: automated systems: batch processors in semiconductor", "Steel and ceramic industries"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.66.11181", "e:abstract": "We consider policies for deciding which cells will be lost or dropped when losses occur at a finite buffer asynchronous transfer mode (ATM) node. The performance criteria of interest are the delay of transmitted (nonlost) cells, the jitter (or variability in the delay of transmitted cells), and the burstiness of lost cells. We analyze the performance trade-offs for various cell dropping policies. We show that “rear dropping,” in which cells that arrive to a full buffer are lost, stochastically maximizes delay, whereas “front dropping,” in which cells at the front of the buffer are lost, stochastically minimizes delay. On the other hand, rear dropping stochastically minimizes the jitter. We also propose policies that have both stochastically smaller delay and less lost cell burstiness in a stochastic majorization sense than the rear dropping policy.", "e:keyword": ["Communications: cell dropping with finite buffers", "Probability/applications: ATM networks", "Cell dropping", "Queues/applications: G/D/1 queues with finite buffers"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.79.11188", "e:abstract": "This paper studies a two-echelon supply chain with stochastic and discrete consumer demand, batch order quantities, periodic inventory review, and deterministic transportation times. Reorder point policies manage inventories at every location. Average inventory, backorders and fill rates are evaluated exactly for each location. Safety stock is evaluated exactly at the lower echelon and a good approximation is detailed for the upper echelon. Numerical data are presented to demonstrate the model's utility. It is found that system costs generally increase substantially if the upper echelon is restricted to carry no inventory, of if the upper echelon is required to provide a high fill rate. In many cases it is optimal to set the upper echelon's reorder point to yield near zero safety stock, yet in some cases this simple heuristic can significantly increase supply chain operating costs. Finally, policies selected under the assumption of continuous inventory review can perform poorly if implemented in an environment with periodic review.", "e:keyword": ["Inventory/Production: multi-echelon", "Stochastic demand", "Periodic review heuristic"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.1.99.11194", "e:abstract": "We consider a manufacturing system that is controlled by a fixed-cycle smoothed production policy. This policy, which is becoming increasingly common in repetitive manufacturing environments, is characterized by a production rate that is updated on a periodic basis. We model the system as a stochastic process that includes parameters for vendor responsiveness, plant responsiveness, randomness in production yield rates, nonstationarity and randomness in market demand, demand forecast error, operating cost rates, and safety stock. Properties of the model expose structural relationships between expected system performance and system parameters.", "e:keyword": ["Manufacturing: performance/productivity", "Inventory/production: smoothing"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.181.13529", "e:abstract": "The paper describes a scientific experiment about a contentious policy issue: What costs and disruptions might arise if U.S. domestic airlines adopted positive passenger bag-match (PPBM), an antiterrorist measure aimed at preventing baggage unaccompanied by passengers from traveling in aircraft luggage compartments? The heart of the effort was a two-week live test of domestic bag-match that involved 11 airlines, 8,000 flights, and nearly 750,000 passengers. Working with the Federal Aviation Administration, the authors played a major role in designing, monitoring, and analyzing the live test. However, the live test provided “raw materials” for an assessment of PPBM rather than the assessment itself. As we discuss, there are difficulties in extrapolating from a short experiment involving 4% of domestic flights to the steady-state consequences of systemwide bag-match.Our findings challenge the widely held industry view that PPBM would have grave impacts on domestic operations. We ultimately estimated that, under usual operating conditions, PPBM would delay domestic departures by an average of approximately 1 minute per flight. (Approximately one-seventh of flights would suffer bag-match departure delays, which would average about 7 minutes apiece.) Implementing bag-match would cost the airlines roughly 40 cents per passenger enplanement, and would require virtually no reduction in the number of flights performed. Restricting bag-match to 5% of passengers chosen under a security profile would cut these delays by about 75% and these dollar costs by about 50%.", "e:keyword": ["Transportation: air", "Cost analysis: system safety", "Statistics: design of experiments"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.196.13531", "e:abstract": "Recently the application of contingent claims analysis and dynamic programming to project evaluation has attracted a lot of attention. These techniques are required, for example, if the value of a project develops stochastically over time and the decision to invest into this project can be postponed. Yet, so far there are no considerations regarding how this perception of projects reflects on a capital budgeting situation. We propose two approaches that integrate these methods with traditional capital budgeting models. A simple capital budgeting model can be formulated as the problem of finding the portfolio of options that has maximal value and fulfils the capital expenditure constraint. However, this model has some shortcomings regarding its applicability in traditional budgeting situations. Therefore, we define an alternative optimisation model that uses scenarios to depict a set of possible future states. The optimal portfolio is then equivalent to a dynamic investment strategy that determines a number of state-dependent optimal portfolios.", "e:keyword": ["Finance/Capital budgets: scenario-based optimization for capital budgeting", "Finance/Portfolio: optimal portfolios for capital budgeting", "Programming", "Integer/application: solution of 0-1 integer programming by Lagrangian decomposition and composite relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.207.13535", "e:abstract": "We propose a new heuristic for pure 0--1 programs, which finds feasible integer points by enumerating extended facets of the octahedron, the outer polar of the unit hypercube. We give efficient algorithms to carry out the enumeration, and we explain how our heuristic can be embedded in a branch-and-cut framework. Finally, we present computational results on a set of pure 0--1 programs taken from MIPLIB and other sources.", "e:keyword": ["Integer programming: pure 0-1 programming", "Heuristics: facet enumeration"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.226.13528", "e:abstract": "This paper considers a multistage serial inventory system with Markov-modulated demand. Random demand arises at Stage 1, Stage 1 orders from Stage 2, etc., and Stage <i>N</i> orders from an outside supplier with unlimited stock. The demand distribution in each period is determined by the current state of an exogenous Markov chain. Excess demand is backlogged. Linear holding costs are incurred at every stage, and linear backorder costs are incurred at Stage 1. The ordering costs are also linear. The objective is to minimize the long-run average costs in the system. The paper shows that the optimal policy is an echelon base-stock policy with state-dependent order-up-to levels. An efficient algorithm is also provided for determining the optimal base-stock levels. The results can be extended to serial systems in which there is a fixed ordering cost at stage <i>N</i> and to assembly systems with linear ordering costs.", "e:keyword": ["Inventory/Production: multi-echelon", "Markov-modulated infinite horizon", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.235.13537", "e:abstract": "Nonconvex nonlinear programming (NLP) problems arise frequently in water resources management, e.g., reservoir operations, groundwater remediation, and integrated water quantity and quality management. Such problems are usually large and sparse. Existing software for global optimization cannot cope with problems of this size, while current local sparse NLP solvers, e.g., MINOS (Murtagh and Saunders 1987), or CONOPT (Drud 1994) cannot guarantee a global solution. In this paper, we apply the Generalized Benders Decomposition (GBD) algorithm to two large nonconvex water resources models involving reservoir operations and water allocation in a river basin, using an approximation to the GBD cuts proposed by Floudas et al. (1989) and Floudas (1995). To ensure feasibility of the GBD subproblem, we relax its constraints by introducing elastic slack variables, penalizing these slacks in the objective function. This approach leads to solutions with excellent objective values in run times much less than the GAMS NLP solvers MINOS5 and CONOPT2, if the complicating variables are carefully selected. Using these solutions as initial points for MINOS5 or CONOPT2 often leads to further improvements.", "e:keyword": ["Natural Resources: water resources management modeling", "Nonlinear Programming: nonconvexivity in large nonlinear models", "Algorithm: Generalized Benders Decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.246.13530", "e:abstract": "This paper examines how setups, due dates, and the mix of standardized and customized products affect the scheduling of a single machine operating in a dynamic and stochastic environment. We restrict ourselves to the class of dynamic cyclic policies, where the machine busy/idle policy and lot-sizing decisions are controlled in a dynamic fashion, but different products must be produced in a fixed sequence. As in earlier work, we conjecture that an averaging principle holds for this queueing system in the heavy traffic limit, and optimize over the class of dynamic cyclic policies. The results allow for a detailed discussion of the interactions between the due-date, setup, and product mix facets of the problem.", "e:keyword": ["Inventory/production: dynamic lot-sizing", "Production/scheduling: sequencing jobs with due dates", "Queues: diffusion models of scheduling problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.271.13533", "e:abstract": "In many chemical process applications, a large mix of products is produced by blending them from a much smaller set of basic grades. The basic grades themselves are typically produced on the same process equipment and inventoried in batches. Decisions that arise in this process include selecting the set of basic grades, determining how much of each basic grade to produce, and how to blend basic grades to meet final product demand. We model this problem as a nonlinear mixed-integer program, which minimizes total grade inclusion, batching, blending, and quality costs subject to meeting quality and demand constraints for these products. Heuristics and lower bounds are developed and tested. The methods are applied to data from Europe's leading manufacturer of wheat- and starch-based products. Our results suggest that this model could potentially reduce annual costs by a minimum of 7%, translates to annual savings of around $5 million.", "e:keyword": ["Production planning: blending and grading in process industries", "Manufacturing productivity: optimizing cost", "And conformance quality", "Industries: chemical", "Food", "And petrochemical sectors"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.281.13529", "e:abstract": "Traditional Data Envelope Analysis (DEA) neglects uncertainty for the input-output variables by treating the observations as if they were the true input-output variables to select reference units for efficiency estimation and performance benchmarking. In stochastic environments, the traditional framework may include stochastically dominated reference units and exclude stochastically undominated ones. To incorporate uncertainty for the input-output variables in DEA, we propose a mean-variance framework derived from the theory of stochastic dominance. From that framework an extension to the traditional model is derived that prevents the selection of stochastically dominated reference units. In addition, within the mean-variance approach, variance restrictions can be specified that reduce the uncertainty for the performance of the evaluated unit relative to its reference unit.", "e:keyword": ["Utility theory: DEA", "Stochastic dominance", "Mean-variance analysis", "Effectiveness/Performance: DEA", "Stochastic dominance", "Mean-variance analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.293.13536", "e:abstract": "In the flow shop mean completion time problem, a set of jobs has to be processed on <i>m</i>-machines. Every machine has to process each one of the jobs, and every job has the same routing through the machines. The objective is to determine a sequence of the jobs on the machines so as to minimize the sum of the completion times of all jobs on the final machine. In this paper, we prove the asymptotic optimality of the Shortest Processing Time algorithm for any continuous, independent, and identically distributed job processing times.", "e:keyword": ["Production/scheduling: multiple machine sequencing", "Flow shop weighted completion time problem", "Shortest processing time dispatch rule", "Analysis of algorithms: probabilistic analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.305.13527", "e:abstract": "We study flexible processing systems where there are multiple paths for a certain type of customer to follow through the system. This paper analyzes an open processing network model with discretionary routing. The underlying methodology is the heavy traffic approximation, which has been well publicized by many researchers. Allowing priority sequencing in addition to dynamic routing as our control policy, we show that the balanced workload routing rule does not necessarily give us an optimal solution. In general unbalanced workload routing with priority sequencing is shown to give us better performance than a balanced one. Through some simulation results we compare our recommendation with other scheduling policies for a flexible processing network.", "e:keyword": ["Queues: dynamic scheduling for a queueing network using Brownian approximation"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.2.316.13534", "e:abstract": "Inventory and backorder cost functions in the classical Wagner-Whitin economic lot size (ELS) models are typically period-pair-independent (<i>pp-independent</i>) in the sense that inventoried units carried (or backorders in existence) in a given period are treated the same regardless of the periods in which they are produced (placed) or the periods in which they are used (filled). We consider versions of the problem where inventory and backorder costs are <i>pp-dependent</i>, as well as versions where backorder costs, but not inventory costs, are pp-dependent. Recognizing that the problems considered are NP-hard, we provide cases where the cost structure allows polynomial solvability via dynamic programming.", "e:keyword": ["Dynamic Programming", "Applications: Solving economic lot size problems", "Inventory/production", "Perishable/aging: ECS problems with holding and backorder costs", "Production--scheduling", "Planning: lot sizing models over a finite horizon"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.325.11213", "e:abstract": "Many in Operations Research/Management Science (OR/MS) have long predicted a major contraction in the field. Given the recent trends in OR/MS hiring, this contraction may have finally arrived, both in academia, as well as in industry. This study maintains that part of this contraction is because we drifted from the field's broad, real-world roots due to a limited philosophical foundation of the field, our realist view of reality. If our commonly held realist philosophy has, in the ironic fashion described herein, given rise to the “academic drift” in the field, the recognition of a relativist philosophy that emphasizes understanding over problem solving could help return the field to its roots.", "e:keyword": ["Philosophy of modeling: understanding vs. problem solving", "Professional OR/MS philosophy: ontological basis of OR/MS"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.334.11210", "e:abstract": "We analyze a single-period, stochastic inventory model (newsboy-like model) in which a sequence of heterogeneous customers dynamically substitute among product variants within a retail assortment when inventory is depleted. The customer choice decisions are based on a natural and classical utility maximization criterion. Faced with such substitution behavior, the retailer must choose initial inventory levels for the assortment to maximize expected profits.Using a sample path analysis, we analyze structural properties of the expected profit function. We show that, under very general assumptions on the demand process, total sales of each product are concave in their own inventory levels and possess the so-called <i>decreasing differences</i> property, meaning that the marginal value of an additional unit of the given product is decreasing in the inventory levels of all other products. For a continuous relaxation of the problem, we then show, via counterexamples, that the expected profit function is in general not even quasiconcave. Thus, global optimization may be difficult. However, we propose and analyze a stochastic gradient algorithm for the problem, and prove that it converges to a stationary point of the expected profit function under mild conditions. Finally, we apply the algorithm to a set of numerical examples and compare the resulting inventory decisions to those of some simpler, naive heuristics. The examples show that substitution effects can have a significant impact on an assortment's gross profits. The examples also illustrate some systematic distortions in inventory decisions if substitution effects are ignored. In particular, under substitution one should stock relatively more of popular variants and relatively less of unpopular variants than a traditional newsboy analysis indicates.", "e:keyword": ["Inventory/production: consumer substitution", "Marketing/Choice Models: choice and inventory decisions"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.352.11215", "e:abstract": "There has been considerable debate about what share of drug control resources should be allocated to treatment vs. enforcement. Most of the debate has presumed that there is one answer to that question, but it seems plausible that the mix of interventions should vary as the size of the problem changes. We formulate the choice between treatment and enforcement as an optimal control problem and reach the following conclusions. If initiation into drug use is an increasing function of the current number of users and control begins early, then it is optimal to use very large amounts of both enforcement and treatment to cut short the epidemic. Otherwise the optimal policy is not to stop the growth of the epidemic, but rather to moderate it. Initially this should be done primarily with enforcement. Over time, enforcement spending should increase, but not nearly so fast as treatment spending. Hence, treatment should receive a larger share of control resources when a drug problem is mature than when it is first growing. If initiation rates subsequently decline, enforcement's budget share should drop further in the ensuing declining stage of the epidemic.", "e:keyword": ["Dynamic programming/optimal control: applications to drug policy", "Judicial/legal: crime and drug policy"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.363.11211", "e:abstract": "In this paper, we discuss the use of mixed integer rounding (MIR) inequalities to solve mixed integer programs. MIR inequalities are essentially Gomory mixed integer cuts. However, as we wish to use problem structure, we insist that MIR inequalities be generated from constraints or simple aggregations of constraints of the original problem. This idea is motivated by the observation that several strong valid inequalities based on specific problem structure can be derived as MIR inequalities.Here we present and test a separation routine for such MIR inequalities that includes a heuristic row aggregation procedure to generate a single knapsack plus continuous variables constraint, complementation of variables, and finally the generation of an MIR inequality. Inserted in a branch-and-cut system, the results suggest that such a routine is a useful additional tool for tackling a variety of mixed integer programming problems.", "e:keyword": ["Integer programming", "Cutting planes: valid inequalities and solution for mixed integer programming problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.372.11218", "e:abstract": "Given a European derivative security with an arbitrary payoff function and a corresponding set of underlying securities on which the derivative security is based, we solve the optimal-replication problem: Find a self-financing dynamic portfolio strategy---involving only the underlying securities---that most closely approximates the payoff function at maturity. By applying stochastic dynamic programming to the minimization of a mean-squared error loss function under Markov-state dynamics, we derive recursive expressions for the optimal-replication strategy that are readily implemented in practice. The approximation error or “(epsilon)” of the optimal-replication strategy is also given recursively and may be used to quantify the “degree” of market incompleteness. To investigate the practical significance of these (epsilon)-arbitrage strategies, we consider several numerical examples, including path-dependent options and options on assets with stochastic volatility and jumps.", "e:keyword": ["Finance: derivatives", "Pricing", "And hedging", "Dynamic programming/optimal control: stochastic optimization of hedging errors"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.398.11219", "e:abstract": "New dynamic programming methods are developed to solve stochastic control problems with a larger number of state variables than previously possible. These methods apply accurate interpolation to numerical approximation of continuous cost-to-go functions, greatly reducing the number of discrete states that must be evaluated. By efficiently incorporating information on first and second derivatives, the approximation reduces computational effort by several orders of magnitude over traditional methods. Consequently, it is practical to apply dynamic programming to complex stochastic problems with a larger number of state variables than traditionally possible. Results are presented for hypothetical reservoir control problems with up to seven state variables and two random inputs.", "e:keyword": ["Programming", "Stochastic: algorithms", "Dynamic programming", "Markov", "Infinite state: efficient approximation of cost-to-go functions", "Natural resources", "Water resources: optimal reservoir operations"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.413.11209", "e:abstract": "The theory of standardized time series, initially proposed to estimate a single steady-state mean from the output of a simulation, is extended to the case where more than one steady-state mean is to be estimated simultaneously. Under mild assumptions on the stochastic process representing the output of the simulation, namely a functional central limit theorem, we obtain asymptotically valid confidence regions for a (multivariate) steady-state mean based on multivariate standardized time series. We provide examples of multivariate standardized time series, including the multivariate versions of the batch means method and Schruben's standardized sum process. Large-sample properties of confidence regions obtained from multivariate standardized time series are discussed. We show that, as in the univariate case, the asymptotic expected volume of confidence regions produced by standardized time series procedures is larger than that obtained from a consistent estimation procedure. We present and discuss experimental results that illustrate our theory.", "e:keyword": ["Simulation", "Statistical analysis: multivariate steady-state output analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.423.11217", "e:abstract": "In this paper we deal with the problem of finding the first <i>K </i>shortest paths from a single origin node to all other nodes of a directed graph. In particular, we define the necessary and sufficient conditions for a set of distance label vectors, on the basis of which we propose a class of methods which can be viewed as an extension of the generic label-correcting method for solving the classical single-origin all-destinations shortest path problem. The data structure used is characterized by a set of <i>K </i>lists of candidate nodes, and the proposed methods differ in the strategy used to select the node to be extracted at each iteration. The computational results show that: 1. some label-correcting methods are generally much faster than the double sweep method of Shier (1979); 2. the most efficient node selection strategies, used for solving the classical single-origin all-destinations shortest path problem, have proved to be effective also in the case of the <i>K </i>shortest paths.", "e:keyword": ["Network/Graphs: algorithms for the K shortest paths problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.430.11214", "e:abstract": "We study a production-inventory system with multiple unreliable supply sources. Through inspection and rework, the system can improve the quality of the units received from the supply sources. There are two interleaved decisions: the replenishment quantities from the sources and the inspection-rework quantities among the units received. We show the optimal solution to the replenishment decision can be efficiently derived from a greedy algorithm, and inspection-rework is optimally applied to a single source identified by the algorithm. Furthermore, in the case of linear cost functions, it is optimal to place orders from two supply sources, i.e., dual sourcing. The results extend to the infinite-horizon case, where an order-up-to policy is optimal. The model also readily adapts to situations in which the supply imperfection takes the form of a reduced delivery quantity (yield loss).", "e:keyword": ["Inventory/production: multiple sourcing", "Optimal replenishment", "Inspection", "Rework"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.444.11220", "e:abstract": "Stochastic multicriteria acceptability analysis (SMAA) is a multicriteria decision support method for multiple decision makers in discrete problems. In SMAA, the decision makers need not express their preferences explicitly or implicitly. Instead, the method is based on exploring the weight space in order to describe the valuations that would make each alternative the preferred one. Inaccurate or uncertain criteria values are represented by probability distributions from which the method computes confidence factors describing the reliability of the analysis. In this paper we introduce the SMAA-2 method, which extends the original SMAA by considering all ranks in the analysis. In situations where the “elitistic” SMAA may assess large acceptability only for extreme alternatives without sufficient majority support, the more holistic SMAA-2 analysis can be used to identify good compromise candidates. The results are presented graphically. We consider also situations where partial preference information is available. We demonstrate the new method using a real-life decision problem.", "e:keyword": ["Decision analysis", "Multiple criteria"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.3.455.11216", "e:abstract": "We study a perishable inventory system under a fixed-critical number order policy. By using an appropriate transformation of the state vector, we derive several key sample-path relations. We obtain bounds on the limiting distribution of the number of outdates in a period, and we derive families of upper and lower bounds for the long-run number of outdates per unit time. Analysis of the bounds on the expected number of outdates shows that at least one of the new lower bounds is always greater than or equal to previously published lower bounds, whereas the new upper bounds are sometimes lower than and sometimes higher than the existing upper bounds. In addition, using an expected cost criterion, we compare optimal policies and different choices of critical-number policies.", "e:keyword": ["Inventory", "Perishable: heuristics", "Performance bounds", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.469.11231", "e:abstract": "This exposition on the Analytic Hierarchy Process (AHP) has the following objectives: (1) to discuss why AHP is a general methodology for a wide variety of decision and other applications, (2) to present brief descriptions of successful applications of the AHP, and (3) to elaborate on academic discourses relevant to the efficacy and applicability of the AHP vis-a-vis competing methodologies. We discuss the three primary functions of the AHP: structuring complexity, measurement on a ratio scale, and synthesis, as well as the principles and axioms underlying these functions. Two detailed applications are presented in a linked document at <ext-link ext-link-type=\"uri\"  href=\"http://mdm.gwu.edu/FormanGass.pdf\">http://mdm.gwu.edu/FormanGass.pdf</ext-link>.", "e:keyword": ["Decision analysis: multiple criteria"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.487.11223", "e:abstract": "The two critical factors distinguishing inventory management in a multifirm supply-chain context from the more traditional centrally planned perspective are <i>incentive conflicts</i> and <i>information asymmetries</i>. We study the well-known order quantity/reorder point (<i>Q, r</i>) model in a two-player context, using a framework inspired by observations during a case study. We show how traditional allocations of decision rights to supplier and buyer lead to inefficient outcomes, and we use principal-agent models to study the effects of information asymmetries about setup cost and backorder cost, respectively.We analyze two “opposite” models of contracting on inventory policies. First, we derive the buyer's optimal menu of contracts when the supplier has private information about setup cost, and we show how consignment stock can help reduce the impact of this information asymmetry. Next, we study consignment and assume the supplier cannot observe the buyer's backorder cost. We derive the supplier's optimal menu of contracts on consigned stock level and show that in this case, the supplier effectively has to <i>overcompensate</i> the buyer for the cost of each stockout.Our theoretical analysis and the case study suggest that consignment stock helps reduce <i>cycle stock</i> by providing the supplier with an additional incentive to decrease batch size, but simultaneously gives the buyer an incentive to increase <i>safety stock</i> by exaggerating backorder costs. This framework immediately points to practical recommendations on how supply-chain incentives should be realigned to overcome existing information asymmetries.", "e:keyword": ["Inventory/production: asymmetric information supply chains", "Game Theory: inventory control under asymmetric information", "Inventory: lot-sizing and stochastic demand"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.501.11227", "e:abstract": "We consider the problem of searching <i>m</i> branches which, with the exception of a common source <i>s</i>, are disjoint (hereafter called concurrent branches). A searcher, starting at <i>s</i>, must find a given “exit” <i>t</i> whose location, unknown to the searcher, is on one of the <i>m</i> branches. The problem is to find a strategy that minimizes the worst-case ratio between the total distance traveled and the length of the shortest path from <i>s</i> to <i>t</i>. Additional information may be available to the searcher before he begins his search.In addition to finding optimal or near optimal deterministic online algorithms for these problems, this paper addresses the “value” of getting additional information before starting the search.", "e:keyword": ["Networks/graph", "Distance algorithms online searching", "Search and surveillance: target discovery in graphs"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.516.11221", "e:abstract": "We provide a new approach to the numerical computation of moments of the exit time distribution of Markov processes. The method relies on a linear programming formulation of a process exiting from a bounded domain. The LP formulation characterizes the evolution of the process through the moments of the induced occupation measure and naturally provides upper and lower bounds for the exact values of the moments. The conditions the moments have to satisfy are derived directly from the generator of the Markov process and are not based on some approximation of the process. Excellent software is readily available because the computations involve finite dimensional linear programs.", "e:keyword": ["Application of linear programming", "60H35: Computation method for Markov processes", "60J25: Exit times of Markov processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.531.11226", "e:abstract": "The problem of assigning locomotives and cars to trains is a complex task for most railways. In this paper, we propose a multicommodity network flow-based model for assigning locomotives and cars to trains in the context of passenger transportation. The model has a convenient structure that facilitates the introduction of maintenance constraints, car switching penalties, and substitution possibilities. The large integer programming formulation is solved by a branch-and-bound method that relaxes some of the integrality constraints. At each node of the tree, a mixed-integer problem is solved by a Benders decomposition approach in which the LP relaxations of multicommodity network flow problems are optimized either by the simplex algorithm or by Dantzig-Wolfe decomposition. Some computational refinements, such as the generation of Pareto-optimal cuts, are proposed to improve the performance of the algorithm. Computational experiments performed on two sets of data from a railroad show that the approach can be used to produce optimal solutions to complex problems.", "e:keyword": ["Transportation", "Scheduling", "Vehicles: assignment of locomotives and cars", "Transportation", "Models", "Network: rail", "Programming", "Integer", "Algorithm", "Benders decomposition: application"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.549.11228", "e:abstract": "This paper evaluates the practice of determining staffing requirements in service systems with random cyclic demands by using a series of stationary queueing models. We consider Markovian models with sinusoidal arrival rates and use numerical methods to show that the commonly used “stationary independent period by period” (SIPP) approach to setting staffing requirements is inaccurate for parameter values corresponding to many real situations. Specifically, using the SIPP approach can result in staffing levels that do not meet specified period by period probability of delay targets during a significant fraction of the cycle. We determine the manner in which the various system parameters affect SIPP reliability and identify domains for which SIPP will be accurate. After exploring several alternatives, we propose two simple modifications of SIPP that will produce reliable staffing levels in models whose parameters span a broad range of practical situations. Our conclusions from the sinusoidal model are tested against some empirical data.", "e:keyword": ["Service systems", "Staffing: Use of queueing models", "Queueing systems", "Cyclic: accuracy of stationary models", "Call centers"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.565.11224", "e:abstract": "In a scheduling problem where agents can opt out, we show that the familiar random priority (RP) mechanism can be improved upon by another mechanism dubbed probabilistic serial (PS). Both mechanisms are nonmanipulable in a strong sense, but the latter is Pareto superior to the former and serves a larger (expected) number of agents. The PS equilibrium outcome is easier to compute than the RP outcome; on the other hand, RP is easier to implement than PS. We show that the improvement of PS over RP is significant but small: at most a couple of percentage points in the relative welfare gain and the relative difference in quantity served. Both gains vanish when the number of agents is large; hence both mechanisms can be used as a proxy of each other.", "e:keyword": ["Production/scheduling: probabilistic sequencing", "Random priority", "Opting out", "Games/group decisions: Pareto improvement", "Strategy-proof report of utility"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.578.11229", "e:abstract": "Recent papers have developed analytical models to explain and quantify the benefits of delayed differentiation and quick response programs. These models assume that while demands in each period are random, they are independent across time and their distribution is perfectly known, i.e., sales forecasts do not need to be updated as time progresses. In this paper, we characterize these benefits in more general settings, where parameters of the demand distributions fail to be known with accuracy or where consecutive demands are correlated. Here it is necessary to revise estimates of the parameters of the demand distributions on the basis of observed demand data. We analyze these systems in a Bayesian framework, assuming that our initial information about the parameters of the demand distributions is characterized via prior distributions. We also characterize the structure of close-to-optimal ordering rules in these systems, for a variety of types of order cost functions.", "e:keyword": ["Inventory/Production: multi-item/echelon/stage", "Inventory/Production: operating characteristics", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.599.11222", "e:abstract": "We study the classical multistage lot sizing problem that arises in distribution and inventory systems. A celebrated result in this area is the 94% and 98% approximation guarantee provided by power-of-two policies. In this paper, we propose a simple randomized rounding algorithm to establish these performance bounds. We use this new technique to extend several results for the capacitated lot sizing problems to the case with submodular ordering cost. For the joint replenishment problem under a fixed base period model, we construct a 95.8% approximation algorithm to the (possibly dynamic) optimal lot sizing policy. The policies constructed are stationary but not necessarily of the power-of-two type. This shows that for the fixed based planning model, the class of stationary policies is within 95.8% of the optimum, improving on the previously best known 94% approximation guarantee.", "e:keyword": ["Production/scheduling: lot sizing", "Programming/integer: randomized rounding"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.4.609.11225", "e:abstract": "We address the problem of scheduling a multiclass <i>M/M/m</i> queue with Bernoulli feedback on <i>m </i>parallel servers to minimize time-average linear holding costs. We analyze the performance of a heuristic priority-index rule, which extends Klimov's optimal solution to the single-server case: servers select preemptively customers with larger Klimov indices. We present closed-form suboptimality bounds (<i>approximate optimality</i>) for Klimov's rule, which imply that its suboptimality gap is uniformly bounded above with respect to (i) external arrival rates, as long as they stay within system capacity; and (ii) the number of servers. It follows that its <i>relative</i> suboptimality gap vanishes in a heavy-traffic limit, as external arrival rates approach system capacity (<i>heavy-traffic optimality</i>). We obtain simpler expressions for the special no-feedback case, where the heuristic reduces to the classical c(mu) rule. Our analysis is based on comparing the expected cost of Klimov's rule to the value of a strong linear programming (LP) relaxation of the system's region of achievable performance of mean queue lengths. In order to obtain this relaxation, we derive and exploit a new set of <i>work decomposition laws </i>for the parallel-server system. We further report on the results of a computational study on the quality of the c(mu) rule for parallel scheduling.", "e:keyword": ["Queues/optimization: multiclass queues", "Parallel servers", "Dynamic programming: performance guarantees for heuristic policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.629.10611", "e:abstract": "The ingot formation or “melt” process is the first step in many steel-making operations. This process involves melting steel and alloys in the desired chemical composition, then pouring it into a variety of ingot molds. Complex technological and resource constraints can make the planning and scheduling of such processes extremely challenging. In this paper, we report our experience in developing solution methods for this “melt scheduling” problem at BethForge, a division of the Bethlehem Steel Corporation, and a manufacturer of custom-made heavy steel forgings. We describe the main issues associated with generic melt-scheduling problems as well as constraints that are specific to BethForge. The problem at BethForge is particularly challenging, because of the need to keep the ingot at a high temperature before forging, their large product variety, and the need to consider trade-offs between two conflicting objectives. We first formulate the base melt-scheduling problem as a mixed-integer program. We then decouple the scheduling decisions into two levels and develop a local search algorithm based on Storer and Wu's problem space neighborhood. Our aim is to generate a family of efficient schedules that allow decision makers to balance the trade-off between two criteria. Computational experiments are performed using data from BethForge. The melt-scheduling procedure developed here has been implemented and installed at BethForge. It has made fundamental improvement in their melt-scheduling process.", "e:keyword": ["Production/scheduling: melt scheduling problem", "Programming", "Integer", "Heuristic: problem space search", "Professional: OR/MS implementation"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.646.10603", "e:abstract": "We analyze a model of inventory competition among <i>n</i> firms that provide competing, substitutable goods. Each firm chooses initial inventory levels for their good in a single period (newsboy-like) inventory model. Customers choose dynamically based on current availability, so the inventory levels at one firm affect the demand of all competing firms. This creates a strategic interaction among the firms' inventory decisions.Our work extends earlier work on variations of this problem by Karjalainen (1992), Lippman and McCardle (1997) and Parlar (1988). Specifically, we model demand in a more realistic way as a stochastic sequence of heterogeneous consumers who choose dynamically from among the available goods (or choose not to purchase) based on a utility maximization criterion. We also use a sample path analysis, so minimal assumptions are imposed on this demand process. We characterize the Nash equilibrium of the resulting stocking game and prove it is unique in the symmetric case. We show there is a bias toward overstocking caused by competition; specifically, reducing the quantity stocked at any equilibrium of the game increases total system profits, and at any joint-optimal set of stocking levels, each firm has an individual incentive to increase its own stock. For the symmetric case, we show that as the number of competing firms increases, the overstocking becomes so severe that total system (and individual firm) profits approach zero. Finally, we propose a stochastic gradient algorithm for computing equilibria and provide several numerical examples.", "e:keyword": ["Inventory/production: policies---marketing/pricing", "Marketing: buyer behavior", "Games/group decisions: noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.658.10611", "e:abstract": "This paper studies the competitive and cooperative selection of inventory policies in a two-echelon supply chain with one supplier and <i>N</i> retailers. Stochastic demand is monitored continuously. Retailers incur inventory holding and backorder penalty costs. The supplier incurs holding costs for its inventory and backorder penalty costs for backorders at the retailers. The latter cost reflects the supplier's desire to maintain adequate availability of its product to consumers. Previous research finds the supply chain cost minimizing reorder point policies, the cooperative solution. The competitive solution is a Nash equilibrium, a set of reorder points such that no firm can deviate from the equilibrium and lower its cost. It is shown that Nash equilibria exist and a method is presented to find all of them. In some settings the cooperative solution is a Nash equilibrium; competition does not necessarily lead to supply chain inefficiency. In other settings, competition leads to costs that are substantially higher than optimal. Usually (but not always), the competitive supply chain carries too little inventory. Three cooperation strategies are discussed: change incentives, change equilibrium, and change control. A set of contracts is provided that changes the firms' incentives so that the optimal policy is a Nash equilibrium. An equilibrium change can improve performance but does not guarantee optimal performance. To change control, the firms let the supplier choose all reorder points, a key component in vendor managed inventory. That change leads to optimal supply chain performance.", "e:keyword": ["Inventory/production: multiechelon stochastic demand", "Games/group decisions: noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.675.10604", "e:abstract": "We study options on short-term leases for capital-intensive equipment performing specific functions and services, such as leases for semi-submersible drilling rigs, marine seismic services, corporate real estate leasing, retail space leasing, and apartment leasing. We quantify the effect of an important factor in pricing options on these services: <i>idle time</i> between consecutive lease contracts. We show that while the expected, discounted value for a contract with options is unique, option prices and option exercise prices must be given with respect to a payment structure for the whole contract. We prove that there exist payment schemes in which prices do not depend on exercise probabilities. We use a simple analytic model to derive closed-form solutions for option prices and illustrate our methodology by pricing options for leasing oil-drilling services in the North Sea.", "e:keyword": ["Finance: securities: real options", "Industries: petroleum: offshore drilling", "Rig leasing", "Cost analysis: opportunity cost of idle time"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.690.10610", "e:abstract": "This paper develops an operational risk management model for evaluating production efforts in manufacturing and mining industries where the resource to be exploited is nonhomogenous. Using a <i>contingent claims</i> methodology now commonly encountered in financial applications, we formulate a production control model in an environment characterized by <i>market</i> and <i>process</i> uncertainty. In our analysis, market risk is depicted by the output price while process uncertainty is captured by the random variability inherent in the output's yield. In this light, adjustments to the rate of production are viewed as a sequence of (nested) real options affording operating flexibility. We account for an optimal sequence of production adjustments, over a preestablished production horizon, by taking the production rate as an adapted positive real-valued process. Accordingly, techniques of stochastic control theory and contingent claims analysis (CCA) are employed to ensure value maximizing production policies are rendered in a manner consistent with an equilibrium price structure.", "e:keyword": ["Inventory/production: policies and uncertainty.", "Dynamic programming/optimization control: applications", "Finance: options investments capital budgeting"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.700.10616", "e:abstract": "This paper considers a continuous-time non-Markovian parallel queueing system subject to external disturbances. The servers are mutually interfering in that their service rates are nonlinearly interdependent functions of the controls applied by the servers, and external discrete-valued continuous-time random disturbances. At certain time epochs, namely, every (Delta) time units, the servers may adjust their service rates by changing the values of their controls; however, the system may change its state several times between successive decision epochs. The stability region of the system is established and a service rate control policy (pi)* is provided, where an arrival rate vector in the interior of the region is sufficient for stability under (pi)*, and a vector in the closure is necessary for stability under any policy. The stability region depends on (Delta) and the variations of the disturbances between decision epochs, and (pi)* does not require knowledge of the arrival rates. The stability region is not in general monotonic in (Delta), but under perfect continuous control ((Delta) = 0) the stability region is a superset of that under (Delta) > 0. This queueing model captures essential features of resource allocation and stochastic control problems encountered in a number of telecommunication, transportation, and manufacturing systems.", "e:keyword": ["Queues", "Multichannel: Optimal control of dependent servers", "Queues", "Optimization: Parallel queues with parallel dependent servers", "Communications : power control in wireless networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.710.10609", "e:abstract": "“Bucket brigades” are a way of sharing work on a flow line that results in the spontaneous emergence of balance and consequent high throughput. All this happens without a work-content model or traditional assembly line balancing technology. Here we show that bucket brigades can be effective even in the presence of variability in the work content. In addition, we report confirmation at the national distribution center of a major chain retailer, which experienced a 34% increase in productivity after the workers began picking orders by bucket brigade.", "e:keyword": ["Probability", "Stochastic model/applications: stochastic model of bucket brigade", "Production", "Stochastic: bucket brigade order picking"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.720.10605", "e:abstract": "We consider a single-server queue with Poisson arrivals, where holding costs are continuously incurred as a nondecreasing function of the queue length. The queue length evolves as a birth-and-death process with constant arrival rate (lambda) = 1 and with state-dependent service rates (mu)<sub>n</sub> that can be chosen from a fixed subset A of [0, (infinity)). Finally, there is a nondecreasing cost-of-effort function <i>c</i>(·) on A, and service costs are incurred at rate <i>c</i>((mu)<sub>n</sub>) when the queue length is <i>n</i>. The objective is to minimize average cost per time unit over an infinite planning horizon. The standard optimality equation of average-cost dynamic programming allows one to write out the optimal service rates in terms of the minimum achievable average cost (zeta)*. Here we present a method for computing (zeta)* that is so fast and so transparent it may be reasonably described as an explicit solution for the problem of service rate control. The optimal service rates are nondecreasing as a function of queue length and are bounded if the holding cost function is bounded. From a managerial standpoint it is natural to compare (zeta)*, the minimum average cost achievable with state-dependent service rates, against the minimum average cost achievable with a single fixed service rate. The difference between those two minima represents the economic value of a responsive service mechanism, and numerical examples are presented that show it can be substantial.", "e:keyword": ["Queues: dynamic control", "Dynamic programming: service rate control in queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.732.10615", "e:abstract": "Standard “indifference-zone” procedures that allocate computer resources to infer the best of a finite set of simulated systems are designed with a statistically conservative, least favorable configuration assumption consider the probability of correct selection (but not the opportunity cost) and assume that the cost of simulating each system is the same. Recent Bayesian work considers opportunity cost and shows that an average case analysis may be less conservative but assumes a known output variance, an assumption that typically is violated in simulation. This paper presents new two-stage and sequential selection procedures that integrate attractive features of both lines of research. They are derived assuming that the simulation output is normally distributed with unknown mean and variance that may differ for each system. We permit the reduction of either opportunity cost loss or the probability of incorrect selection and allow for different replication costs for each system. The generality of our formulation comes at the expense of difficulty in obtaining exact closed-form solutions. We therefore derive a bound for the expected loss associated potentially incorrect selections, then asymptotically minimize that bound. Theoretical and empirical results indicate that our approach compares favorably with indifference-zone procedures.", "e:keyword": ["Simulation--statistical analysis: selecting the best simulated system", "Statistics--Bayesian: Bayesian inference for ranking and selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.744.10606", "e:abstract": "A number of authors have identified problematic issues with techniques used in current simulation practice for selecting probability distributions and their parameters for input to stochastic simulations. A major goal of this paper is to address some of those issues by presenting a self-consistent evaluation of the uncertainty about the mean value of the simulation output, when there is uncertainty in both the parameters and functional form of input distributions (structural uncertainty), and uncertainty due to the stochastic nature of simulation output (stochastic uncertainty), as is common in simulation practice. The analysis leads to an algorithm for randomly sampling input distributions and parameters before each simulation replication, using a Bayesian posterior distribution for input distributions and parameters, given historical data. Mechanisms for addressing issues of importance to the discrete-event simulation community are illustrated by example, such as the specification of prior distributions, and analysis for shifted distributions.", "e:keyword": ["Simulation", "Statistical Analysis: input distribution selection", "Uncertainty analysis", "Statistics", "Bayesian: model selection", "Prior distribution assessment"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.759.10614", "e:abstract": "This paper develops a method for determining sequencing policies to effectively control a multistation closed queueing network. We assume that several job classes, with different service time distributions, share each server and should be sequenced to maximize the long-run throughput rate of the system. A Brownian control problem that approximates the original queueing network is formulated and used to develop a dynamic sequencing policy that seeks to prevent idleness, unless the system is at a face of the <i>workload imbalance polytope</i> that arises in the Brownian formulation. Several examples are used to demonstrate the benefit of this policy over the static prioritization proposed by Chevalier and Wein (1993), which has previously been shown to dominate traditional scheduling policies in a closed network setting.", "e:keyword": ["Production scheduling", "Sequencing", "Stochastic: Priority sequencing in a multistation closed network", "Queues", "Networks: scheduling rules based on approximate Brownian Models"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.771.10607", "e:abstract": "In this paper, we study inverse optimization problems defined as follows. Let <b>S</b> denote the set of feasible solutions of an optimization problem <b>P</b>, let <i>c</i> be a specified cost vector, and <i>x</i><sup>0</sup> be a given feasible solution. The solution <i>x</i><sup>0</sup> may or may not be an optimal solution of <b>P</b> with respect to the cost vector <i>c</i>. The inverse optimization problem is to perturb the cost vector <i>c</i> to <i>d</i> so that <i>x</i><sup>0</sup> is an optimal solution of <b>P</b> with respect to <i>d</i> and ||<i>d</i>-<i>c</i>||<sub>p</sub> is minimum, where ||<i>d</i>-<i>c</i>||<sub>p</sub> is some selected <i>L</i><sub>p</sub> norm. In this paper, we consider the inverse linear programming problem under <i>L</i><sub>1</sub> norm (where ||<i>d</i>-<i>c</i>||<sub>p</sub> = (Sigma)<sub>i(in)J</sub> w<sub>j</sub>|d<sub>j</sub>-c<sub>j</sub>|, with <i>J</i> denoting the index set of variables <i>x<sub>j</sub></i> and <i>w<sub>j</sub></i> denoting the weight of the variable <i>j</i>) and under <i>L</i><sub>(infinity)</sub> norm (where ||<i>d</i>-<i>c</i>||<sub>p</sub> = max<sub>j(in)J</sub>{w<sub>j</sub>|d<sub>j</sub>-c<sub>j</sub>|}). We prove the following results: (i) If the problem <b>P</b> is a linear programming problem, then its inverse problem under the <i>L</i><sub>1</sub> as well as <i>L</i><sub>(infinity)</sub> norm is also a linear programming problem. (ii) If the problem <b>P</b> is a shortest path, assignment or minimum cut problem, then its inverse problem under the <i>L</i><sub>1</sub> norm and unit weights can be solved by solving a problem of the same kind. For the nonunit weight case, the inverse problem reduces to solving a minimum cost flow problem. (iii) If the problem <b>P</b> is a minimum cost flow problem, then its inverse problem under the <i>L</i><sub>1</sub> norm and unit weights reduces to solving a unit-capacity minimum cost flow problem. For the nonunit weight case, the inverse problem reduces to solving a minimum cost flow problem. (iv) If the problem <b>P</b> is a minimum cost flow problem, then its inverse problem under the <i>L</i><sub>(infinity)</sub> norm and unit weights reduces to solving a minimum mean cycle problem. For the nonunit weight case, the inverse problem reduces to solving a minimum cost-to-time ratio cycle problem. (v) If the problem <b>P</b> is polynomially solvable for linear cost functions, then inverse versions of <b>P</b> under the <i>L</i><sub>1</sub> and <i>L</i><sub>(infinity)</sub> norms are also polynomially solvable.", "e:keyword": ["Programming", "Linear: inverse linear programming problems", "Networks/graphs", "Flow algorithms: inverse shortest path", "Minimum cut", "Minimum cost flows", "Transportation", "Mass transit: toll pricing in mass transportation networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.784.10601", "e:abstract": "We consider the problem of minimizing (Sigma)<sub>j(in)N</sub> C<sub>j</sub>(x<sub>j</sub>), subject to the following chain constraints x<sub>1</sub> (le) x<sub>2</sub> (le) x<sub>3</sub> (le) ... (le) x<sub>n</sub>, where C<sub>j</sub>(x<sub>j</sub>) is a convex function of x<sub>j</sub> for each j (in) N = {1,2, ... ,n}. This problem is a generalization of the <i>isotonic regression problems</i> with complete order, an important class of problems in regression analysis that has been examined extensively in the literature. We refer to this problem as the <i>generalized isotonic regression problem</i>. In this paper, we focus on developing a fast-scaling algorithm to obtain an integer solution of the generalized isotonic regression problem. Let <i>U</i> denote the difference between an upper bound on an optimal value of x<sub>n</sub> and a lower bound on an optimal value of x<sub>1</sub>. Under the assumption that the evaluation of any function C<sub>j</sub> (x<sub>j</sub>) takes <i>O</i>(1) time, we show that the generalized isotonic regression problem can be solved in <i>O</i>(n log <i>U</i>) time. This improves by a factor of n the previous best running time of <i>O</i>(n<sup>2</sup> log <i>U</i>) to solve the same problem. In addition, when our algorithm is specialized to the <i>isotonic median regression problem</i> (where C<sub>j</sub>(x<sub>j</sub>) = c<sub>j</sub>|x<sub>j</sub>-a<sub>j</sub>|) for specified values of <i>c<sub>j</sub>s</i> and <i>a<sub>j</sub>s</i>, the algorithm obtains a real-valued optimal solution in <i>O</i>(n log <i>n</i>) time. This time bound matches the best available time bound to solve the isotonic median regression problem, but our algorithm uses simpler data structures and may be easier to implement.", "e:keyword": ["Statistics", "Data analysis: isotonic regression problem", "Programming", "Nonlinear: convex programming subject to chain constraints"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.790.10613", "e:abstract": "The paper considers the optimal control of a single-item continuous-review inventory system with random demand and discount opportunities. Items can always be purchased with the regular order setup and variable costs. However, when a discount opportunity occurs, they can also be purchased with a different setup cost and a lower variable cost. Demands for individual items and discount opportunities occur according to independent Poisson processes. The paper proposes an algorithm to compute the parameters of the optimal replenishment policy, which has been shown to be an (<i>r, R, d, D</i>) policy where <i>r</i> < <i>R</i>, <i>r</i> (le) <i>d</i>, and <i>d</i> < <i>D</i>. Under an (<i>r, R, d, D</i>) policy, when the inventory position drops to the level <i>r</i>, an order is placed with the regular costs to increase the inventory position to <i>R</i>; and when a discount opportunity occurs at or below <i>d</i>, an order is placed with the discount costs to increase the inventory position to <i>D</i>. The algorithm is based on a bisection search procedure to minimize the long-run average cost and is finitely convergent.", "e:keyword": ["Inventory/Production: continuous review", "Stochastic models: Poisson demand and random discount offers", "Algorithms: bisection method"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.5.796.10608", "e:abstract": "The paper considers the single vehicle routing problem with stochastic demands. While most of the literature has studied the a priori solution approach, this work focuses on computing a reoptimization-type routing policy. This is obtained by sequentially improving a given a priori solution by means of a rollout algorithm. The resulting rollout policy appears to be the first computationally tractable algorithm for approximately solving the problem under the reoptimization approach. After describing the solution strategy and providing properties of the rollout policy, the policy behavior is analyzed by conducting a computational investigation. Depending on the quality of the initial solution, the rollout policy obtains 1% to 4% average improvements on the a priori approach with a reasonable computational effort.", "e:keyword": ["Dynamic programming: heuristics", "Network/graphs: stochastic model", "Transportation", "Vehicle routing: dynamic and stochastic model"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.807.10022", "e:abstract": "Data Envelopment Analysis (DEA) models, as ordinarily employed, assume that the data for all inputs and outputs are known exactly. In some applications, however, a number of factors may involve imprecise data, which take forms such as ordinal rankings and knowledge only of bounds. Here we provide an example involving a Korean mobile telecommunication company. The Imprecise Data Envelopment Analysis (IDEA) method we use permits us to deal not only with imprecise data and exact data but also with weight restrictions as in the (now) widely used “Assurance Region” (AR) and “cone-ratio envelopment” approaches to DEA. We also show how to transform AR bounds on the <i>variables</i>, obtained from managerial assessments, for instance, into <i>data</i> adjustments. This involves an extended IDEA model, which we refer to as AR-IDEA. All these uses are illustrated by an example application directed to evaluate efficiencies of branch offices of a telecommunication company in Korea.", "e:keyword": ["Telecommunications: performance evaluation", "Organization: employee performance evaluation", "Linear programming: applications---algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.821.10017", "e:abstract": "Different equilibrium concepts have been proposed by various authors (Schweppe et al., Hogan et al., Chao and Peck, Wu et al.) to analyse competitive electricity systems. We establish correspondences between these different models through a single framework and provide additional interpretations of these equilibrium concepts. This unifying conceptual view also provides a computationally feasible approach to simulate the market. It also opens the way to the modeling of some imperfect markets.", "e:keyword": ["Economics: equilibrium models of restructured electricity systems", "Electric industries: equivalence between restructuring models", "Complementary programming: models of restructured electricity systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.839.10020", "e:abstract": "This paper integrates pricing and replenishment decisions for the following prototypical two-echelon distribution system with deterministic demands. A supplier distributes a single product to multiple retailers, who in turn sell it to consumers. The retailers serve geographically dispersed, heterogeneous markets. The demand in each retail market arrives continuously at a constant rate, which is a general decreasing function of the retail price in the market. The supplier replenishes its inventory through orders (purchases, production runs) from a source with ample capacity. The retailers replenish their inventories from the supplier. We develop efficient algorithms to determine optimal pricing and replenishment strategies for the following three channel structures. The first is the vertically integrated channel, where the system-wide pricing and replenishment strategies are determined by a central planner whose objective is to maximize the system-wide profits. The second structure is that of a vertically integrated channel in which pricing and operational decisions are made sequentially by separate functional departments. The third channel structure is decentralized, i.e., the supplier and the retailers are independent, profit-maximizing firms with the supplier acting as a Stackelberg game leader. We apply our algorithms to a set of numerical examples to quantify the supply chain inefficiencies due to functional segregation or uncoordinated decision making in a decentralized channel. We also gain insight into systematic differences in the associated pricing and operational patterns.", "e:keyword": ["Inventory/Production: multi-echelon", "Marketing/pricing policies", "Marketing: Channels of distribution", "Retailing", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.854.10014", "e:abstract": "The operations research literature provides little guidance about how data should be generated for the computational testing of algorithms or heuristic procedures. We discuss several widely used data generation schemes, and demonstrate that they may introduce biases into computational results. Moreover, such schemes are often not representative of the way data arises in practical situations. We address these deficiencies by describing several principles for data generation and several properties that are desirable in a generation scheme. This enables us to provide specific proposals for the generation of a variety of machine scheduling problems. We present a generation scheme for precedence constraints that achieves a target density which is uniform in the precedence constraint graph. We also present a generation scheme that explicitly considers the correlation of routings in a job shop. We identify several related issues that may influence the design of a data generation scheme. Finally, two case studies illustrate, for specific scheduling problems, how our proposals can be implemented to design a data generation scheme.", "e:keyword": ["Simulation", "Random variable generation: methods for generating random data", "Production/scheduling: experimental data for testing algorithms and heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.866.10021", "e:abstract": "The Generalized Assignment Problem (GAP) is the problem of finding the minimal cost assignment of jobs to machines such that each job is assigned to exactly one machine, subject to capacity restrictions on the machines. We propose a new stochastic model for the GAP. A tight condition on this stochastic model under which the GAP is feasible with probability one when the number of jobs goes to infinity is derived. This new stochastic model enables us to analyze the adequacy of most of the random generators given for the GAP in the literature. We demonstrate that the random generators commonly used to test solution procedures for the GAP tend to create easier problem instances when the number of machines <i>m</i> increases. We describe a greedy heuristic for the GAP, and use it to illustrate the results from the paper.", "e:keyword": ["Programming", "Integer: generalized assignment problem", "Statistics: generation of random data"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.879.10015", "e:abstract": "Financial institutions require sophisticated tools for risk management. For companywide risk management, both sides of the balance sheet should be considered, resulting in an integrated asset-liability management approach. Stochastic programming models suit these needs well and have already been applied in the field of asset-liability management to improve financial operations and risk management. The dynamic aspect of the financial planning problems inevitably leads to multiple decision stages (trading dates) in the stochastic program and results in an explosion of dimensionality. In this paper we show that dedicated model generation, specialized solution techniques based on decomposition and high-performance computing, are the essential elements to tackle these large-scale financial planning problems. It turns out that memory management is a major bottleneck when solving very large problems, given an efficient solution approach and a parallel computing facility. We report on the solution of an asset-liability management model for an actual Dutch pension fund with 4,826,809 scenarios; 12,469,250 constraints; and 24,938,502 variables; which is the largest stochastic linear program ever solved. A closer look at the optimal decisions reveals that the initial asset mix is more stable for larger models, demonstrating the potential benefits of the high-performance computing approach for ALM.", "e:keyword": ["Finance", "Investment: asset-liability management models", "Programming", "Stochastic: decomposition methods for stochastic programming", "Computer science: high-performance computing"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.892.10024", "e:abstract": "We consider transmission policies for multiple users sharing a single wireless link to a base station. The noise, and hence the probability of correct transmission of a packet, depends on the state of the user receiving the packet. The state for each user is independent of the states of the other users and changes according to a two-state (good/bad) Markov chain. The state of a user is observed only when it transmits. We give conditions under which the optimal policy is the myopic policy, in which a packet is transmitted to the user that is most likely to be in the better of the two states. We do this by showing that the optimal value function is marginally linear in each of the users' probabilities of being in the good state. Our model also may be applied to flexible manufacturing systems with unreliable tools and networked computer systems.", "e:keyword": ["Communications: transmission policies for noisy channels", "Dynamic programming/optimal control: models", "Scheduling with partial information"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.900.10016", "e:abstract": "The method of importance sampling is widely used for efficient rare-event simulation of stochastic systems. This method involves simulating the system under a new distribution that accentuates the probability along the most likely paths to the rare event. Traditionally, insights from large deviations theory are used to identify the distribution emphasizing these most likely paths. In this paper we develop an intuitive <i>cyclic approach</i> for selecting such a distribution. The key idea is to select a distribution under which the event of interest is no longer rare and the probability of occurrence of a cycle in any sample path remains equal to the original probability of that cycle. We show that only an exponentially twisted distribution can satisfy this <i>equiprobable cycle</i> condition. Using this approach we provide an elementary derivation of the asymptotically optimal change of measure for level crossing probability for Markov-additive processes. To demonstrate its ease of use for more complex stochastic systems, we apply it to determine the asymptotically optimal change of measure for estimating buffer overflow probability of a single-server queue subject to server interruptions.", "e:keyword": ["Markov processes", "Simulation: importance sampling"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.913.10023", "e:abstract": "In this paper we consider a class of discrete resource-allocation problems with a <i>min-max-sum</i> objective function. We first provide several examples of practical applications of this problem. We then develop a branch-and-bound procedure for solving the general case of this computationally intractable problem. The proposed solution procedure employs a surrogate relaxation technique to obtain lower and upper bounds on the optimal objective function value of the problem. To obtain the multipliers of the surrogate relaxation, two alternative approaches are discussed. We also discuss a simple approximation algorithm with a tight bound. Our computational results support the effectiveness of the branch-and-bound procedure for fairly large-size problems.", "e:keyword": ["Resource allocation: min-max-sum resource allocation", "Optimization: integer optimization", "Min-max optimization", "Robust optimization", "Mathematical programming: nonlinear integer programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.923.10018", "e:abstract": "Pricing financial options often requires Monte Carlo methods. One particular case is that of barrier options, whose payoff may be zero depending on whether or not an underlying asset crosses a barrier during the life of the option. This paper develops variance reduction techniques that take advantage of the special structure of barrier options, and are appropriate for general simulation problems with similar structure. We use a change of measure at each step of the simulation to reduce the variance arising from the possibility of a barrier crossing at each monitoring date. The paper details the theoretical underpinnings of this method, and evaluates alternative implementations when exact distributions conditional on one-step survival are available and when not available. When these one-step conditional distributions are unavailable, we introduce algorithms that combine change of measure and estimation of conditional probabilities simultaneously. The methods proposed are more generally applicable to terminal reward problems on Markov processes with absorbing states.", "e:keyword": ["Simulation", "Efficiency: Variance reduction", "Finance", "Asset pricing: Computational methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.938.10026", "e:abstract": "We consider an airline seat inventory control problem with multiple origins, one hub, and one destination. Passengers from the origins fly to the destination via the hub. Seat capacity of the hub-destination flight is fixed. Demands at origins are assumed to obey the Poisson process. To maximize the expected revenue, management faces decisions on allocating seats among competing origin-destination routes. This study presents a stochastic control model and develops optimal control rules. The basic model is subsequently distended to consider multiple fares on each route, time-dependent demands, and booking control on an extended network. A numerical example shows that optimal seat control is simple and efficient.", "e:keyword": ["Industries", "Transportation/shipping: airline route revenue management", "Inventory/production", "Perishable/aging items: yield management.", "Dynamic programming/optimal control", "Applications: optimal seat inventory control"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.950.10019", "e:abstract": "In this paper, we address the problem of finding the simulated system with the best (maximum or minimum) expected performance when the number of alternatives is finite, but large enough that ranking-and-selection (R&S) procedures may require too much computation to be practical. Our approach is to use the data provided by the first stage of sampling in an R&S procedure to screen out alternatives that are not competitive, and thereby avoid the (typically much larger) second-stage sample for these systems. Our procedures represent a compromise between standard R&S procedures---which are easy to implement, but can be computationally inefficient---and fully sequential procedures---which can be statistically efficient, but are more difficult to implement and depend on more restrictive assumptions. We present a general theory for constructing combined screening and indifference-zone selection procedures, several specific procedures and a portion of an extensive empirical evaluation.", "e:keyword": ["Simulation", "Design of experiments: two-stage procedures", "Simulation", "Statistical analysis: finding the best alternative", "Statistics", "Design of experiments"]}, {"@id": "http://dx.doi.org/10.1287/opre.49.6.964.10013", "e:abstract": "Recently, Nelson et al. (2001a, b) formulated a class of combined screening-and-selection procedures for identifying the simulated system with optimal expected response when the number of alternatives is finite, but large enough to render conventional ranking-and-selection procedures impractical. Under a certain key assumption, they derived an <i>additive decomposition</i> lemma that provides a lower bound on the correct-selection probability when either the original or group-screening version of their combined screening-and-selection procedure is applied to randomly sampled normal populations with unknown and unequal variances. For both these procedures, we establish an improved lower bound on the correct-selection probability that is the product of (a) the probability that the best alternative will survive the first-stage screening procedure, and (b) the probability that the second-stage sampling-and-selection procedure will correctly identify the best alternative starting from the full set of alternatives. This <i>multiplicative decomposition</i> property offers a different perspective on the probabilistic structure of the entire class of combined screening-and-selection procedures developed by Nelson et al., and it does not require the key assumption of their additive decomposition lemma.", "e:keyword": ["Simulation", "Design of experiments: screening", "Selection", "And multiple comparison procedures"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.0.17795", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.1.17785", "e:keyword": ["Inventory/production: dynamic inventory models under uncertainty. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.100.17788", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.103.17786", "e:abstract": "This paper emphasizes OR's utility to the U.S. Navy, as seen by its customers. Like many naval officers who subspecialized in operations research, the author was both producer and consumer of analysis toward improving fleet operations, Pentagon planning, and training effectiveness. Many of OR's unnoticed heroes are officers and Navy civilians who, then and now, could put operations analysis to best practical use. The paper reaches three conclusions, which (briefly) are: (1) The U.S. Navy could shift much analytical talent to improve fleet readiness and scarcely notice a loss of quality in Washington, (2) the benefit of Navy OR in the Pentagon was not so much in formal decision making as it was in educating a stream of future leaders about the state of the Navy and cost-constrained possible future states, and (3) the distinguishing contribution of all OR has been and still is in helping executives make better, timely decisions by applying our special art of quantitative analysis, and only incidentally in the fidelity or complexity of the models and other tools we employ.", "e:keyword": ["Military: defense systems", "Force effectiveness", "Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.112.17801", "e:keyword": ["Professional: comments on. Queues: networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.114.17800", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.122.17811", "e:keyword": ["Decision analysis: applications", "Theory", "Looking back", "Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.125.17772", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.132.17784", "e:keyword": ["Judicial legal/crime: police and prosecution models. Professional: comments on. Hypercube queueing model", "Queue inference engine. Operations research"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.132.17787", "e:keyword": ["Professional: comments on", "Programming: nonlinear"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.146.17799", "e:abstract": "Morse was the many-sided founding father of operations research in the United States. Following his pioneering wartime effort, he vigorously promoted the nonmilitary development of the field. A glimpse into his activities and accomplishments helps explain the vitality and high intellectual standards of the field.", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.149.17796", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.154.17774", "e:abstract": "In 1989 I was pleased and honored to be awarded the ORSA/TIMS (now INFORMS) John von Neumann Theory Prize for my work in portfolio theory, sparse matrices, and SIMSCRIPT. The following is a retrospective on my work in these fields.", "e:keyword": ["Finance", "Portfolio: origins of portfolio theory. Professional: comments on. Programming", "Linear: sparse matrices. Simulation", "Languages: SIMSCRIPT"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.16.17794", "e:keyword": ["Crime. Criminal justice: penal system. Incapacitation. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.161.17790", "e:abstract": "Simulation is introduced in terms of its different forms and uses, but the focus on discrete event modeling for systems analysis is dominant as it has been during the evolution of the technique within operations research and the management sciences. This evolutionary trace of over almost fifty years notes the importance of bidirectional influences with computer science, probability and statistics, and mathematics. No area within the scope of operations research and the management sciences has been affected more by advances in computing technology than simulation. This assertion is affirmed in the review of progress in those technical areas that collectively define the art and science of simulation. A holistic description of the field must include the roles of professional societies, conferences and symposia, and publications. The closing citation of a scientific value judgment from over 30 years in the past hopefully provides a stimulus for contemplating what lies ahead in the next 50 years.", "e:keyword": ["Professional: comments on. Simulation: discrete event simulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.173.17802", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.179.17797", "e:keyword": ["Decision analysis: theory and practice", "Risk", "Sequential. Games and group decisions: negotiations. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.186.17773", "e:keyword": ["Inventory/production", "Personal reflections. Professional", "Comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.192.17789", "e:keyword": ["Professional", "Comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.197.17783", "e:keyword": ["Professional", "Comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.217.17777", "e:keyword": ["Forecasting", "Inventory system effectiveness. Inventory/production", "Impact of forecasts on. Professional", "Comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.227.17792", "e:keyword": ["Probability: applications", "Historical survey and bibliography. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.25.17804", "e:abstract": "This paper provides some of my historical perspectives on Operations Research (OR) in the U.S. Army. It is based on my 40+ years of personal experience and, thus, focuses on the modeling and analysis (M&A) aspects of OR in the Army. I have attempted to highlight the changing problems and growth of M&A in the Army over the past 40 years. Although I refer to approaches taken by others for some of this growth, more information is provided on those of Vector Research,Incorporated (VRI),since all of my experience has been with VRI since 1969. The paper has four main sections. Since OR in the Army started before 1960 and my activities interacted with many others in the Army, the first section briefly reviews the lineage of some of the Army's main OR organizations. The second and third summarize my M&A activities and perspectives for the periods 1960--1989 (the “Cold War” era) and 1990--2000, respectively. Based on this experience, I offer some “lessons learned” for today's military M&A community in the concluding section.", "e:keyword": ["Military: Army OR history", "Combat models and simulations", "Military analyses", "Military decision issues", "Model development", "Military experimentation", "Process modeling", "M&A lessons learned. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.3.17780", "e:abstract": "This paper is an invited contribution to the 50th anniversary issue of the journal <i>Operations Research</i>, published by the Institute of Operations Research and Management Science (INFORMS). It describes one person's perspective on the development of computational tools for linear programming. The paper begins with a short personal history, followed by historical remarks covering the some 40 years of linear-programming developments that predate my own involvement in this subject. It concludes with a more detailed look at the evolution of computational linear programming since 1987.", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.35.17778", "e:abstract": "This paper describes a research strategy and its results, which guided a 40+ year collaboration between Abraham Charnes and William W. Cooper, which was initiated in a research center established at the (then) new Graduate School of Industrial Administration at Carnegie Institute of Technology (now Carnegie Mellon University). Initiated in collaboration with Bob Mellon of Gulf Oil Company, this strategy involved on-site collaborations with company personnel in more than 100 different companies and government agencies. An appendix to this paper describes the efforts of another team working from the same research center. This team, consisting of Charles Holt, Franco Modigliani, John Muth, and Herbert A. Simon, used a different research strategy with an accompanying application that also had important impacts on operations research/management science and other disciplines.", "e:keyword": ["Philosophy of modeling: modeling for use. Professional: comments on. Professional: OR/MS implementation. Programming: linear"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.42.17798", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.48.17791", "e:keyword": ["Dynamic programming: history. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.52.17805", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.61.17781", "e:abstract": "We review the early history of linear programming with respect to the solution of linear equations, computer developments, and its origins within the federal government.", "e:keyword": ["Linear programming: on the solution of linear equations", "Computers", "And their relationship to linear programming. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.69.17776", "e:keyword": ["Decision analysis: driver behavior in traffic flow. Organizational studies: performance and improvement of traffic systems. Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.78.17793", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.82.17771", "e:abstract": "This paper identifies a number of events and circumstances, which contributed in Great Britain during the period 1938--1963 to the development and adoption of OR outside the military and defence concerns. I show how the OR club formed a focus for industrial activity and the development of the subject. The early growth in education and training provision will also be traced to indicate its influence on the direction of OR and its acceptance as an academic discipline.", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.89.17803", "e:abstract": "Energy policy modeling owes a great debt to the disciplines of operations research. Valuable modeling tools were available when the energy crisis struck unexpectedly. In turn, the immediate response to problem-driven policy modeling produced methodological challenges and innovations that have application outside the domain of energy. The early days of the explosive growth of energy modeling for policy studies provide illustrations of the interaction of problem identification, model formulation, problem analysis, and policy implementation in the tradition of operations research.", "e:keyword": ["Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.1.96.17779", "e:keyword": ["Dynamic programming/optimal control/applications. Inventory/production: applications. Philosophy of modeling: corporate", "Professional: comments on"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.249.425", "e:abstract": "This paper describes a decision support system for paper production scheduling. This is the first system to provide an integrated solution to paper production scheduling and to consider interactions between different stages of the manufacturing and distribution process. Using a multicriteria optimization approach, the system generates multiple enterprisewide schedules to reveal tradeoffs between the multiple, often competing, objectives. The large portfolio of algorithms used by the system is embedded in an agent-based decision support framework, called Asynchronous Team (A-Team). Successful implementations of the system in several paper mills in North America have resulted in significant savings and improved customer satisfaction.", "e:keyword": ["Production/scheduling: multiple machine", "Cutting stock/firm", "Decision analysis: multiple criteria", "Computers/computer science: agents"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.260.436", "e:abstract": "Multicommodity network flow models arise in a wide variety of contexts, typical among which is the dimensioning of telecommunication networks. In this paper, we present various approaches based on specialization of the simplex algorithm and interior-point methods to solve nonoriented multicommodity flowproblems. Algorithms are tested with data from the France-Telecom Paris district transmission network. First, we focus on a specialization for the node-arc formulation of the problem. A Primal simplex and Dual Affine Scaling algorithms exploiting the particular structure of the constraint matrix are presented and compared. Numerical results are provided for problems up to about 800,000 constraints and 6,000,000 variables. However, much more powerful approaches based on specialized decomposition methods can be implemented for solving the problem. A Dantzig-Wolfe decomposition method is designed and compared with a specialized implementation of the Analytic Center Cutting Plane Method (ACCPM). Partitioning techniques are used to exploit the structure of the master programs involved in those methods.", "e:keyword": ["Networks/graphs", "Multicommodity: solution of network routing problems", "Programming/linear", "Large scale systems: solution of multicommodity flow problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.277.426", "e:abstract": "Credit scoring is one of the most successful applications of quantitative analysis in business. This paper shows how using survival-analysis tools from reliability and maintenance modeling allows one to build credit-scoring models that assess aspects of profit as well as default. This survival-analysis approach is also finding favor in credit-risk modeling of bond prices. The paper looks at three extensions of Cox's proportional hazards model applied to personal loan data. A new way of coarse-classifying of characteristics using survival-analysis methods is proposed. Also, a number of diagnostic methods to check adequacy of the model fit are tested for suitability with loan data. Finally, including time-by-characteristic interactions is proposed as a way of possible improvement of the model's predictive power.", "e:keyword": ["Risk: estimating credit risk for personal loans", "Failure models: Survival analysis applied to credit scoring models"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.290.435", "e:abstract": "Optimum burn-in times have been determined for a variety of criteria such as mean residual life and conditional survival. In this paper we consider a residual coefficient of variation that balances mean residual life with residual variance. To study this quantity, we develop a general result concerning the preservation ofbathtub distributions. Using this result, we give a condition so that the residual coefficient of variation is bathtub-shaped. Furthermore, we show that it attains its optimum value at a time that occurs after the mean residual life function attains its optimum value, but not necessarily before the change point of the failure rate function.", "e:keyword": ["Reliability: burn-in", "Bathtub curves"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.297.429", "e:abstract": "This paper discusses using real options to value power plants with unit commitment constraints over a short-term period. We formulate the problem as a multistage stochastic problem and propose a solution procedure that integrates forward-moving Monte Carlo simulation with backward-moving dynamic programming. We assume that the power plant operator maximizes expected profit by deciding in each hour whether or not to run the unit, that a certain lead time for commitment and decommitment decisions is necessary to start up and shut down a unit, and that these commitment decisions, once made, are subject to physical constraints such as minimum uptime and downtime. We also account for the costs associated with starting up and shutting down a unit. Last, we assume that there are hourly markets for both electricity and the fuel used by the generator and that their prices follow Ito processes. Using numerical simulation, we show that failure to consider physical constraints may significantly overvalue a power plant.", "e:keyword": ["Natural resources: energy", "Decision analysis: applications", "Finance: investment"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.311.433", "e:abstract": "There is an extensive theory regarding optimal continuous path search for a mobile or immobile “target.” The traditional theory assumes that the target is one of three types: (i) an object with a known distribution of paths, (ii) a mobile or immobile hider who wants to avoid or delay capture, or (iii) a rendezvouser who wants to find the searcher. This paper introduces a new type of search problem by assuming that aims of the target are not known to the searcher. The target may be either a type (iii) cooperator (with a known cooperation probability <i>c</i>) or a type (ii) evader. This formulation models search problems like that for a lost teenager who may be a “runaway,” or a lost intelligence agent who may be a defector. In any given search context, it produces a continuum of search problems (tau)(<i>c</i>), 0 (le) <i>c</i> (le) 1, linking a zero-sum search game (with <i>c</i> = 0) to a rendezvous problem (with <i>c</i> = 1). These models thus provide a theoretical bridge between two previously distinct parts of search theory, namely search games and rendezvous search.", "e:keyword": ["Search and surveillance: search for agent of unknown aims"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.324.427", "e:abstract": "A common problem encountered in paper-production facilities is that of allocating customer orders to machines so as to minimize the total cost of production. It can be formulated as a <i>dual-angular</i> integer program, with identical machines inducing symmetry. While the potential advantages of decomposing large mathematical programs into smaller subproblems have long been recognized, the solution of decomposable <i>integer</i> programs remains extremely difficult. Symmetry intensifies the difficulty. This paper develops an approach, based on the construction of tight subproblem bounds, to solve decomposable dual-angular integer programs and successfully applies it to solve the problem from the paper industry. This method is of particular interest as it significantly reduces the impact of symmetry.", "e:keyword": ["Production/scheduling", "Cutting stock: multimachine order allocation", "Programming", "Integer", "Applications: decomposition"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.333.423", "e:abstract": "This paper addresses the problem of designing a multicommodity network using several facilities with different costs and capacities. The problem is addressed in a special context of designing private telecommunications networks using Fractional-T1 services. The algorithm starts from any given solution of the problem and gradually improves it by solving a series of subproblems, arriving at a local minimum. The subproblem is defined over a subset of links, called the subnetwork, by using one of the links as the base link. It is shown that the subproblem can be formulated as a multiple choice knapsack problem that is solved by dynamic programming. Computational results and lower bounds are reported on problems of up to 20 nodes and up to 3 facilities. On most problems, the algorithm produces solutions within about 5% of lower bound on the average. Although it was not possible to compute lower bounds for larger problems, heuristic solutions and running times are reported for problems of up to 99 nodes and four facilities.", "e:keyword": ["Networks", "Multicommodity: telecommunications network design"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.345.430", "e:abstract": "Experience shows that document caching by a web browser is a cheap and effective way to improve the performance of the World Wide Web. This study analyzes a LRU (Least Recently Used) policy for cache management in a web browser. In this policy, the cache is filled with documents based upon a document's “age,” defined as the time elapsed since the document was last accessed. The user's preference for a document is modeled as a general function that declines with the document's age. Two popular measures---the expected delay per document access, and the hit-ratio---are used to evaluate the LRU policy. Unlike many previous studies that evaluate caching policies using simulation methods, this study derives analytical expressions to evaluate performance. The study also presents an approximate, easy-to-compute method to evaluate performance. Numerical tests show this approximation to be extremely accurate. A variety of other numerical results are presented that help describe the behavior ofthe LRU policy under different situations (e.g., when the documents need to be updated periodically). We also compare the LRU policy with other caching policies (both static and dynamic) for small problems. Our comparison suggests that finding a good caching policy that is conscious of document size and delay may be difficult.", "e:keyword": ["Information systems: analysis and design", "Computers/computer science: system design and operation", "Probability: stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.358.424", "e:abstract": "The idea of investigating the relation of option and stock prices based just on the no-arbitrage assumption, but without assuming any model for the underlying price dynamics, has a long history in the financial economics literature. We introduce convex and, in particular semidefinite optimization methods, duality, and complexity theory to shed new light on this relation. For the single stock problem, given moments of the prices of the underlying assets, we show that we can find best-possible bounds on option prices with general payoff functions efficiently, either algorithmically (solving a semidefinite optimization problem) or in closed form. Conversely, given observable option prices, we provide best-possible bounds on moments of the prices of the underlying assets, as well as on the prices of other options on the same asset by solving linear optimization problems. For options that are affected by multiple stocks either directly (the payoff of the option depends on multiple stocks) or indirectly (we have information on correlations between stock prices), we find nonoptimal bounds using convex optimization methods. However, we show that it is NP-hard to find best possible bounds in multiple dimensions. We extend our results to incorporate transactions costs.", "e:keyword": ["Finance: asset pricing", "Options", "Optimization: semidefinite", "Convex optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.375.428", "e:abstract": "We consider a firm that provides multiple services using both specialized and flexible capacity. The problem is formulated as a two-stage, single-period stochastic program. The firm invests in capacity before the actual demand is known and optimally assigns capacity to customers when demand is realized. Sample applications include a car rental company's use of mid-sized cars to satisfy unexpectedly high demand for compact cars and an airline's use of business-class seats to satisfy economy-class demand. We obtain an analytical solution for a particular case, when services may be upgraded by one class. The simple form of the solution allows us to compare the optimal capacities explicitly with a solution that does not anticipate flexibility. Given that demand follows a multivariate normal distribution, we analytically characterize the effects of increasing demand correlation on the optimal solution. For the case with two customer classes, the effects of demand correlation are intuitive: Increasing correlation induces a shift from flexible to dedicated capacity. When there are three or more classes, there are also adjustments to the resources not directly affected by the correlation change. As correlation rises, these changes follow an alternating pattern (for example, if the optimal capacity of one resource rises, then the optimal capacity of the adjacent resource falls). These results make precise conjectures based on numerical experiments that have existed in the literature for some time.", "e:keyword": ["Facilities/equipment planning: application to services", "Inventory/production", "Stochastic", "Multiproduct: substitution and demand correlation"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.2.389.422", "e:abstract": "This paper considers the inverse problem of estimating time-varying attrition coefficients in Lanchester's square law with reinforcements, using observed data on some or all of the battle's strength histories and the reinforcement schedules. The method employed is a nonparametric extension of the parametric conjugate gradient method (P-CGM). We use hypothetical strength histories and reinforcement schedules that are known to be without error at several points in time to illustrate the method. However, the method has application in other circumstances. The problem of estimating the time-dependent attrition coefficients that best fit a set of given strength histories is inherently a nonparametric inverse problem. In this paper we cast it into a nonlinear optimization problem, and show how to solve it numerically by using a nonparametric conjugate gradient method (NP-CGM). Two numerical test cases are provided to illustrate the application of the method.", "e:keyword": ["Military: warfare models"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.399.7749", "e:abstract": "Farmers on the Central Plateau of Burkina Faso in West Africa cultivate under precarious conditions. Rainfall variability is extremely high in this area and accounts for much of the uncertainty surrounding the farmers' decision-making process. Strategies to cope with these risks are typically dynamic. Sequential decision making is one of the most important ways to cope with risk due to uncertain rainfall. In this paper, a stochastic programming model is presented to describe farmers' sequential decisions in reaction to rainfall. The model describes farmers' strategies of production, consumption, selling, purchasing, and storage from the start of the growing season until one year after the harvest period. This dynamic model better describes farmers' strategies than do static models that are usually applied. This study draws important policy conclusions regarding reorientation of research programs and illustrates how operations research techniques can be usefully applied to study grass root problems in developing countries.", "e:keyword": ["Decision analysis: sequential: farmers' response to uncertain rain fall", "Programming", "Stochastic: application of recourse model"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.415.7751", "e:abstract": "The classical Vehicle Routing Problem consists ofdetermining optimal routes for <i>m</i> identical vehicles, starting and leaving at the depot, such that every customer is visited exactly once. In the capacitated version (CVRP) the total demand collected along a route cannot exceed the vehicle capacity. This article considers the situation where some ofthe demands are stochastic. This implies that the level of demand at each customer is not known before arriving at the customer. In some cases, the vehicle may thus be unable to load the customer's demand, even ifthe expected demand along the route does not exceed the vehicle capacity. Such a situation is referred to as a failure. The capacitated vehicle routing problem with stochastic demands (SVRP) then consists ofminimizing the total cost ofthe planned routes and of expected failures. Here, penalties for failures correspond to return trips to the depot. The vehicle first returns to the depot to unload, then resumes its trip as originally planned. This article studies an implementation of the Integer <i>L</i>-shaped method for the exact solution of the SVRP. It develops new lower bounds on the expected penalty for failures. In addition, it provides variants of the optimality cuts for the SVRP that also hold at fractional solutions. Numerical experiments indicate that some instances involving up to 100 customers and few vehicles can be solved to optimality within a relatively short computing time.", "e:keyword": ["Transportation: stochastic vehicle routing", "Programming: stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.424.7743", "e:abstract": "A job shop has to deliver a given number of custom-order items. Production is performed in “lots” that require costly setup, and output quality is stochastic. The size of each lot must be set before output quality is observed. We study production processes whose yield is distributed according to a generalized truncated geometric distribution: Production can randomly go “out of control,” in which case all subsequent output of that lot is defective. The “generalization” allows both hazard rates and marginal production costs to vary as production progresses. Our results characterize the optimal lot sizes. In particular, we show that for small demands the optimal lot size equals the outstanding demand, and for larger demands it is less than the outstanding demand, with all lot sizes being uniformly bounded. For sufficiently large demands, we identify conditions under which the optimal lots are precisely those that minimize the ratio of production cost to the expected number of good items. A tighter characterization is given for the standard case, where hazard rates and marginal costs are constant.", "e:keyword": ["Production planning: single machine", "Lot sizes", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.433.7739", "e:abstract": "In this paper, we study the capacitated Euclidean and <i>l<sub>p</sub></i> distance location-allocation problems. There exists no global optimization algorithm that has been developed and tested for this class of problems, aside from a total enumeration approach. Beginning with the Euclidean distance problem, we design a branch-and-bound algorithm based on a partitioning of the allocation space that finitely converges to a global optimum for this nonconvex problem. For deriving lower bounds at node subproblems in this partial enumeration scheme, we employ two types of procedures. The first approach computes a lower bound via a projected location space subproblem. The second approach derives a significantly enhanced lower bound by using a Reformulation-Linearization Technique (RLT) to transform an equivalent representation of the original nonconvex problem into a higher dimensional linear programming relaxation. In addition, certain cut-set inequalities are generated in the allocation space, and objective function based cuts are derived in the location space to further tighten the lower bounding relaxation. The RLT procedure is then extended to the general <i>l<sub>p</sub></i> distance problem, for <i>p</i> > 1. Computational experience is provided on a set of test problems to investigate both the projected location space and the RLT-lower bounding schemes. The results indicate that the proposed global optimization approach using the RLT-based scheme offers a promising viable solution procedure. In fact, among the problems solved, for the only two test instances available in the literature for the Euclidean distance case, we report significantly improved solutions.", "e:keyword": ["Facilities planning: location continuous", "Programming: nonlinear algorithms", "Applications", "Nonconvex", "Transportation: models/location"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.449.7747", "e:abstract": "Auctions are a common price-setting mechanism in many areas of the economy. Certain auctions, for example those in deregulated electric power networks, require that there exist sufficient capacity along the power lines connecting the buyers and the sellers. We investigate how auction participants modify their bidding strategies depending on the influence and behavior of a transmission line owner. We also investigate optimal strategic behavior when multiple buyers and sellers are separated by a possibly-constrained channel, and show that both sides’ strategies converge to truth-telling behavior as the number of market participants increases, and price-taking behavior also emerges as the number of participants increases. We show that limited transmission capacity increases participants’ misrepresentation and increases auction inefficiency, as players modify their bidding strategies in an attempt to increase their profit.", "e:keyword": ["Games/group decisions: bidding/auctions", "Industries: restructuring of electric power"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.462.7738", "e:abstract": "Portfolio selection techniques must provide decision makers with a dynamic model framework that incorporates realistic assumptions regarding financial markets, risk preferences, and required portfolio characteristics. Unfortunately, multistage stochastic programming (SP) models for portfolio selection very quickly become intractable as assumptions are relaxed and uncertainty is introduced. In this paper, I present an alternative model framework for portfolio selection, stochastic convergence (SC), that systematically incorporates uncertainty under a realistic assumption set. The optimal portfolio is derived through an iterative procedure, where portfolio plans are evaluated under many possible future scenarios then revised until the model converges to the optimal plan. This approach allows for scenario analysis over all stochastic components, requires no limitation on the structural form of the objective or constraints, and permits evaluation over any length planning horizon while maintaining model tractability by aggregating the scenario tree at each stage in the solution process. Through focused aggregation schemes, the SC approach allows for the implementation of a lower partial variance riskmetric, which is preferred in investment selection. In simulated tests, the SC model, with scenario aggregation, generated portfolios exhibiting performance similar to those generated using the SP model form with no aggregation. Empirical tests using historical fund returns show that a multiperiod SC decision strategy outperforms various benchmarkstrate gies over a long-term test horizon under both asset-liability matching and return maximization frameworks.", "e:keyword": ["Finance", "Portfolio: stochastic optimization selection model", "Programming", "Stochastic: convergence model for portfolio selection", "Finance", "Investment: stochastic optimization for portfolio selection"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.477.7748", "e:abstract": "In this paper, we study strategies for generators making offers into electricity markets in circumstances where demand is unknown in advance. We concentrate on a model with smooth supply functions and derive conditions under which a single supply function can represent an optimal response to the offers of the other market participants over a range of demands. In order to apply this approach in practice, it may be necessary to approximate the supply functions of other players. We derive bounds on the loss in revenue that occurs in comparison with the exact supply function response, when a generator uses an approximation both for its own supply function and for the supply functions of other players. We also demonstrate the existence of symmetric supply-function equilibria.", "e:keyword": ["Government energy policies: electricity markets", "Industries", "Electric: optimising generation", "Games/group decisions", "Noncooperative", "Stochastic: equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.490.7741", "e:abstract": "An important problem in image analysis is to segment an image into regions with different class labels. This is relevant in applications in medicine and cartography. In a proper statistical framework this problem may be viewed as a discrete optimization problem. We present two integer linear programming formulations of the problem and study some properties of these models and associated polytopes. Different algorithms for solving these problems are suggested and compared on some realistic data. In particular, a Lagrangian algorithm is shown to have a very promising performance. The algorithm is based on the technique of cost splitting and uses the fact that certain relaxed problems may be solved as shortest path problems.", "e:keyword": ["Integer programming: applications", "Image analysis", "Networks/graphs: applications", "Statistics: Bayesian"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.501.7745", "e:abstract": "A practical method is presented for giving unlimited, deterministically correct, numerical responses to ad-hoc queries to an online database, while not compromising confidential numerical data. The method is appropriate for any size database, and no assumptions are needed about the statistical distribution of the confidential data. Responses are in the form of a number plus a guarantee, so the user can determine an interval that is sure to contain the exact answer. Virtually any imaginable query type can be answered, and in the absence of insider information, collusion among the users presents no problem. Experimental analysis supports the practical viability of the proposed method.", "e:keyword": ["Computers", "Databases: protecting confidential data", "Information systems", "Decision support systems: answering ad-hoc queries"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.517.7752", "e:abstract": "This paper investigates the effect of demand censoring on the optimal policy in newsvendor inventory models with general parametric demand distributions and unknown parameter values. We show that the newsvendor problem with <i>observable</i> lost sales reduces to a sequence of single-period problems, while the newsvendor problem with <i>unobservable</i> lost sales requires a dynamic analysis. Using a Bayesian Markov decision process approach we show that the optimalin ventory level in the presence of censored demand is <i>higher</i> than would be determined using a Bayesian myopic policy. We explore the economic rationality for this observation and illustrate it with numerical examples.", "e:keyword": ["Inventory/production: unknown demand", "Lost sales", "Censoring", "Optimal policies", "Dynamic programming: Bayesian Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.528.7744", "e:abstract": "Using the net present value is the standard methodology in theoretical analysis, and the most frequently used method for making financial decisions. However, net present value is rarely used in production and inventory decisions. The main reasons appear to be the complexity of the formulae and the robustness of the EOQ model. We investigate the general multiproduct, multistage production and inventory model using the net present value of its total cost as the objective function. A power-of-two heuristic gives us a near optimal solution to this problem. If the base period is fixed (or varied), the solution based on the best power-of-two heuristic will be within 6.2% (or 2.1% ) of the optimal. This result is surprisingly similar to models using the long-term average cost. The average cost does not reflect the time value of money. Does this mean that decisions based on average cost are significantly inferior to those based on net present value? The answer is quite surprising. If we include discounted production cost in the holding cost, it turns out that the decision based on average cost is only 9.6% (in terms of the net present value of the total cost) worse than the decision based on the net present value. However, the reorder interval based on the average cost could be much longer than that derived using net present value. This result shows that average cost is a good approximation to the net present value when the demands are deterministic.", "e:keyword": ["Inventory/production", "Approximations/heuristics: near optimal solution using net present value", "Inventory/production", "Multi-item/echelon/stage: mineral production and inventory network"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.538.7737", "e:abstract": "Much interest exists in broadband network services to deliver a variety of products to consumers, such as Internet access, telephony, interactive TV, and video on demand. Due to its cost efficiency, Hybrid Fiber Coaxial (HFC) technology is currently being considered by most Telcos and cable companies as the technology to deliver these products. The topological HFC network design problem as implemented by several major companies is a form of the capacitated tree-star network design problem. We propose a new formulation for this problem and present a heuristic based on hierarchical decomposition of the problem. The proposed solution methodology exploits an Adaptive Reasoning Technique (ART), embedded as a meta-heuristic over specialized heuristics for the subproblems. In this context, we demonstrate the dynamic use of an exact solution technique within ART. The generalizability of the proposed solution methodology is demonstrated by applying it to a second problem, the Traveling Salesman Problem (TSP). Computational results are presented for both the HFC network design problem and the TSP, indicating high-quality solutions expending a very modest computational effort. The proposed solution method is found to be effective, and is shown to be easily adaptable to new problems without much crafting, and as such, has a broad appeal to the general operations research community.", "e:keyword": ["HFC network design: configuration of HFC topology", "Tree-star: network architecture", "Heuristic search: Solutions to problems in short time"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.552.7750", "e:abstract": "In this study, an adaptive Bayesian decision model is developed to determine the optimal replacement age for the systems maintained according to a general age-replacement policy. It is assumed that when a failure occurs, it is either <i>critical</i> with probability <i>p</i> or <i>noncritical</i> with probability 1 -- <i>p</i>, independently. A maintenance policy is considered where the noncritical failures are corrected with minimal repair and the system is replaced either at the first critical failure or at age (tau), whichever occurs first. The aim is to find the optimal value of (tau) that minimizes the expected cost per unit time. Two adaptive Bayesian procedures that utilize different levels of information are proposed for sequentially updating the optimal replacement times. Posterior density/mass functions of the related variables are derived when the time to failure for the system can be expressed as a Weibull random variable. Some simulation results are also presented for illustration purposes.", "e:keyword": ["Maintenance/replacement: maintenance with minimal repair", "Decision analysis: Bayesian updating"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.559.7742", "e:abstract": "We show that the “exponential decay parameter” of the waiting time in a Markov-modulated <i>M/G</i>/1 queue is no larger than that of the corresponding <i>M/G</i>/1 queue with “averaged” parameters, and we give a necessary and sufficient condition for equality. We also explore the effect of speeding up the modulation process. A key tool is a Markov-modulated fluid model.", "e:keyword": ["Queues", "Approximations and limit theorems: effect of modulation on probability of a long wait"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.3.566.7740", "e:keyword": ["Simulation: statistical analysis", "Statistics: sampling", "Bayesian"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.571.2852", "e:abstract": "Online models for real-time operations planning face a host of implementation issues that do not arise in more strategic arenas. We use the seemingly simple problem of assigning drivers to loads in the truckload motor carrier industry as an instance to study the issues that arise in the process of implementing a real-time dispatch system. Although the project was moderately successful, our focus is not on documenting the benefits, but rather on summarizing the challenges that arose. The most significant theme running through the implementation hurdles we encountered was the lack of information available to the model. Computers are very good at processing vast quantities of information; humans are very good at challenging the information that is in the computer and augmenting computer-provided data with head knowledge. Our study includes a careful comparison of actual decisions with model recommendations, using a six-month database of actual transactions. This comparison is the first we have seen of its kind and provides the most rigorous evaluation of an online dispatch model that we have seen. Although the model was well used, the results demonstrate that significant improvements could have been obtained if the level of model utilization had been even higher.", "e:keyword": ["Transportation: model implementation issues"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.582.2864", "e:abstract": "We describe a large-scale linear programming model for optimizing strategic (intercontinental) airlift capability. The model routes cargo and passengers through a specified transportation network with a given fleet of aircraft subject to many physical and policy constraints. The time-dynamic model captures a significant number of the important aspects of an airlift system in a large-scale military deployment, including aerial refueling, tactical (intracontinental) aircraft shuttles, and constraints based on crew availability. The model is designed to provide insight into issues associated with designing and operating an airlift system. We describe analyses for the U.S. Air Force system concerning fleet modernization and concerning the allocation of resources that affect the processing capacity of airfields.", "e:keyword": ["Military logistics: optimizing strategic airlift. Large-scale linear programming: multiperiod air transportation model for cargo and passengers"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.603.2862", "e:abstract": "We present an approach to the admission control and resource allocation problem in connection-oriented networks that offer multiple services to users. users' preferences are summarized by means of their utility functions, and each user is allowed to request more than one type of service. Multiple types of resources are allocated at each link along the path of a connection. We assume that the relation between Quality of Service (QoS) and resource allocation is given, and we incorporate it as a constraint into a static optimization problem. The objective of the optimization problem is to determine the amount of and required resources for each type of service to maximize the sum of the users' utilities. We prove the existence of a solution of the optimization problem and describe a competitive market economy that implements the solution and satisfies the informational constraints imposed by the nature of the decentralized resource allocation problem. The economy consists of four different types of agents: resource providers, service providers, users, and an auctioneer that regulates the prices based on the observed aggregate excess demand. The goods that are sold are: (i) the resources at each link of the network, and (ii) services constructed from these resources and then delivered to users. We specify an iterative procedure that is used by the auctioneer to update the prices, and we show that it leads to an allocation that is arbitrarily close to a solution of the optimization problem in a finite number of iterations.", "e:keyword": ["Networks: pricing schemes for resource allocation. Programming: nonlinear", "Algorithms. Economics: resource allocation in integrated-services networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.617.2853", "e:abstract": "The network restoration problem is a specialized capacitated network design problem requiring the installation of spare capacity to fully restore disrupted network flows if any edge in a telecommunications network fails. We present a new mixed-integer programming formulation for a line restoration version of the problem using a single type of capacitated facility. We examine two different models, for distinct and integrated spare-capacity systems, reflecting technologies used in synchronous transfer mode (STM) and asynchronous transfer mode (ATM) networks. The problem is NP-complete in the strong sense. We study the problem's polyhedral structure to identify strong valid inequalities that tighten the problem formulation. Our computational results on several real and randomly generated problems show that these inequalities considerably reduce the integrality gap from an average of 10% to an average of under 1%. These results indicate that strong cutting planes combined with branch-and-bound can provide efficient algorithms for solving a class of real-world problems in the telecommunications industry.", "e:keyword": ["Communications: network restoration planning. Programming", "Integer: capacity planning for telecommunication networks. Facilities/equipment planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.636.2859", "e:abstract": "A new interior-point algorithm for solving the groundwater-pollution-control design problem is presented. The algorithm requires that the objective function is differentiable in the interior region. For minimization problems with nonlinear constraints and a concave objective function, the technique is shown to be similar to an active set gradient-projection method, where the tangent of the boundary between feasible and infeasible solutions is used to determine a search direction. In this new method, however, the search direction is translated into the interior space of the feasible region. This process allows progress to be made toward improving the objective function while remaining in the feasible space and ultimately converges to a stationary point. Although the solution technique was developed to solve a groundwater control formulation with a linear objective function and nonlinear constraints, the method has been successfully applied to an unconstrained nonconcave/nonconvex formulation and may be applicable to a wide variety of problems.", "e:keyword": ["Nonlinear algorithms", "Interior point method", "Water resources: groundwater contamination control"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.645.2854", "e:abstract": "This paper develops a new nonparametric model for efficiency estimation. In contrast to Data Envelopment Analysis (DEA), it does not impose debatable production assumptions like free disposability and convexity, and it does not assume that the data are measured without error. The estimators are asymptotically unbiased and have an asymptotic variance that is comparable to that of stochastic frontier estimators (provided the latter use a correct specification of the functional form for the production relationships). In addition, the estimators can be computed using a simple enumeration algorithm.", "e:keyword": ["Econometrics: nonparametric efficiency analysis. Input-output analysis"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.656.2865", "e:abstract": "This paper is concerned with applying the <i>Reformulation-Linearization Technique</i> (RLT) to derive tighter relaxations for the <i>Asymmetric Traveling Salesman Problem</i> (ATSP) formulation that is based on the <i>Miller-Tucker-Zemlin</i> (MTZ) subtour elimination constraints. The MTZ constraints yield a compact representation for the Traveling Salesman Problem (TSP), and their use is particularly attractive in various routing and scheduling contexts that have an embedded ATSP structure. However, it is well recognized that these constraints yield weak relaxations, and with this motivation, Desrochers and Laporte (1991) have lifted the MTZ constraints into facets of the underlying ATSP polytope. We show that a novel application of the RLT process from a nonstandard MTZ representation of the ATSP reveals a new formulation of this problem that is compact, and yet theoretically as well as computationally dominates the lifted-MTZ formulation of Desrochers and Laporte. This approach is also extended to derive tight formulations for the <i>Precedence Constrained Asymmetric Traveling Salesman Problem</i> (PCATSP), based on the MTZ subtour elimination constraints. Additional classes of valid inequalities are also developed for both these versions of the ATSP, and further ideas for developing tighter representations are suggested for future investigations.", "e:keyword": ["Programming", "Integers: Reformulation-linearization technique", "Asymmetric traveling salesman problem", "Miller-Tucker-Zemlin", "Tight relaxations", "Precedence constrained asymmetric traveling salesman problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.670.2857", "e:abstract": "The multiple objective median problem (MOMP) involves locating a new facility with respect to a given set of existing facilities so that a vector of performance criteria is optimized. A variation of this problem is obtained if the existing facilities are situated on two sides of a linear barrier. Such barriers, like rivers, highways, borders, or mountain ranges, are frequently encountered in practice. In this paper, theory of an MOMP with line barriers is developed. As this problem is nonconvex but specially structured, a reduction to a series of convex optimization problems is proposed. The general results lead to a polynomial algorithm for finding the set of efficient solutions. The algorithm is proposed for bicriteria problems with different measures of distance.", "e:keyword": ["Facilities/equipment planning", "Location", "Continuous: location with a barrier and multiple criteria. Programming", "Multiple criteria: location with a barrier"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.680.2867", "e:abstract": "We formulate the dynamic product-cycling problem with yield uncertainty and buffer limits to determine how much product to produce at what time to minimize total expected switching, production, inventory storage, and backorder costs. A “restricted” Lagrangian technique is used to develop a lower bound and a model-based Lagrangian heuristic. We also develop an operational heuristic and a greedy heuristic. The operational heuristic has been implemented at seven refineries at Cerestar, Europe's leading manufacturer of wheat-and corn-based starch products in the food-processing industry . This has already reduced total costs by around 5 percent or $3 million annually at these sites. Tests of the Lagrangian heuristic on data from these refineries during this period have shown the potential to further reduce total costs by at least 2 percent or about $1 million. In addition, the Lagrangian heuristic has provided an objective basis to evaluate the economic impact of several strategic decisions involving issues such as buffer expansion, variability reduction, and product selection.", "e:keyword": ["Production planning: product cycling with yield uncertainty. Production application: process industry", "Food-processing sector . Production heuristics:"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.692.2860", "e:abstract": "We describe a simple online heuristic for scheduling job shops. We assume there is a fixed set of routes for the jobs, and many jobs, say <i>N</i>, on each route. The heuristic uses safety stocks and keeps the bottleneck machine busy at almost all times, while the other machines are paced by the bottleneck machine. We perform a probabilistic analysis of the heuristic, under some assumptions on the distributions of the processing times. We show that our heuristic produces makespan, which exceeds the optimal makespan by no more than <i>c</i>log<i>N</i> with a probability that exceeds 1-1/<i>N</i> for all <i>N (ge)</i> 1, where <i>c</i> is some constant independent of <i>N</i>.", "e:keyword": ["Production/scheduling", "Sequencing", "Stochastic: job shop", "Makespan", "Re-entrant. Production/scheduling", "Approximations", "Heuristic: fluid model", "Performance bound", "Bottleneck stations"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.708.2861", "e:abstract": "We study a scheduling problem with changeover costs and capacity constraints. The problem is NP-complete, and combinatorial algorithms for solving it have not performed well. We identify a general class of facets that subsumes as special cases some known facets from the literature. We also develop a cutting-plane-based procedure and reformulation for the problem, and we obtain optimal solutions to problem instances with up to 600 integer variables without resorting to branch-and-bound procedures.", "e:keyword": ["Valid inequalities", "Facets", "Lotsizing"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.720.2855", "e:abstract": "Revenue management has become an important tool in the airline, hotel, and rental car industries. We describe asymptotic properties of revenue management policies derived from the solution of a deterministic optimization problem. Our primary results state that, within a stochastic and dynamic framework, solutions arising out of a single well-known linear program can be used to generate allocation policies for which the normalized revenue converges in distribution to a constant upper bound on the optimal value. We also show similar asymptotic results for expected revenues. In addition, we describe counterintuitive behavior that can occur when allocations are updated during the booking process (updating allocations can lead to lower expected revenue). These results add to the understanding of allocation policies and help to make concrete the statement that simple policies from easy-to-solve formulations can be relatively effective, even when analyzed in the more realistic stochastic and dynamic framework.", "e:keyword": ["Inventory", "Perishable items: revenue/yield management", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.728.2866", "e:abstract": "Some problems in economics, operations research, and engineering may be approached by means of a pair of radial DEA models that are nested, i.e., that the set of constraints of one of them is included in that of the other. In this paper we have focused on analyzing the marginal role of a given variable, called <i>candidate</i>, with respect to the efficiency measured by means of a DEA model. First, we have defined a new <i>efficiency contribution measure (ECM)</i>, which finally compares the efficiency scores of the two radial DEA models differing in the candidate. This can be either one input or one output. Then,based on ECM, we have also approached the problem from a statistical point of view. To be precise, we have developed a statistical test that allows us to evaluate the significance of the observed efficiency contribution of the candidate. Eventually, solving this test may provide some useful insights in order to decide the incorporation or the deletion of a variable into/from a given DEA model, on the basis of the information supplied by the data. Two procedures for progressive selection of variables were designed by sequentially applying the test: a forward selection and a backward elimination. These can be very helpful in the initial selection of variables when building a radial DEA model.", "e:keyword": ["Linear programming: radial DEA models", "Nonparametric statistics: nonparametric DEA test of model specification"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.736.2863", "e:abstract": "This paper is concerned with the performance of multicommodity capacitated networks in a deterministic but time-dependent environment. For a given time-dependent origin-destination table, this paper asks if it is easy to find a way of regulating the input flows into the network to avoid queues from growing internally, i.e., to avoid capacity violations. Problems of this type are conventionally approached in the traffic/transportation field with variational methods such as control theory (if time is continuous) and with mathematical programming (if time is discrete). However, these approaches can only be expected to work well ifthe set off easible solutions is convex.Unfortunately, it is found in this paper that this is not the case. It is shown that continuous-time versions of the problem satisfying the smoothness conditions of control theory can have a finite but very large number off easible solutions. The same happens for the discrete time case. These difficulties arise even with the simplest versions of the problem (with unique origin-destination paths, perfect information, and deterministic travel times).The paper also shows that the continuous-time feasibility problem is NP-hard, and that if we restrict our attention to (practical) problems whose data can be described with a finite number ofbits (e.g., in discrete time), then the problem is NP-complete. These results are established by showing that the problem instances of interest can be related to the Directed Hamiltonian Path problem by a polynomial transformation.", "e:keyword": ["Traffic models: control of freeway systems", "Multicommodity networks: optimization with FIFO queues"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.4.744.2858", "e:keyword": ["Search theory"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.751", "e:abstract": "Large-scale industrial production processes face increasingly tight environmental constraints, which can be addressed through costly but relatively simple end-of-pipe solutions, or through cheaper but more subtle pollution prevention approaches. Achieving the process improvements necessary for pollution prevention is challenging due to the inherent complexity and unpredictability of several types of processes found in the food processing, pharmaceuticals, biotechnology, and specialty chemical industries. We propose an iterative procedure to achieve process improvements through model-based process redesign. This procedure is based on successive convex approximations of the process performance model, where product flows and process settings are optimized for a given configuration and the solution and dual variables of this optimization problem are used to update the process configuration following a greedy capacity reallocation procedure. We implemented this procedure over a five-year period at Cerestar, a major European producer of starch products, which led to a dramatic simplification in process configuration. Reduced energy and water consumption led to an estimated $3 million annual cost savings. Moreover, the reduction in environmental impacts allowed Cerestar to maintain current production levels without investing $100 million in additional wastewater treatment capacity to comply with new environmental constraints.", "e:keyword": ["Environment: pollution prevention", "Manufacturing: performance/productivity", "Nonlinear programming: iterative procedure"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.764.368", "e:abstract": "The Defense Information System Agency (DISA) has managerial and engineering responsibility for a circuit-switched network currently known as the Defense Information System Network---Voice. This network was originally implemented in the 1960s and was known as the Automatic Voice Network. Throughout this time, DISA has used a network design and analysis model to reconfigure the network. This model was developed in the early 1970s, has been continually enhanced, and is used on a weekly basis to engineer and manage the network. Since 1975, it has been used to save the United States (U.S.) government approximately $1,400,000,000.00. In this paper, we present a chronology of its development, enhancement, and use.", "e:keyword": ["Communications: circuit-switch network design and analysis", "Queues", "Algorithm: network algorithm. Military: defense systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.772.363", "e:abstract": "The rendezvous-search problem was posed by the author 25 years ago. In its basic form, it asks how two unit speed players can find each other in least expected time, when randomly placed in a known dark region. The problem received little attention until about 10 years ago. This article surveys the rapid progress that has been made since then, and also presents some new results.", "e:keyword": ["Search and surveillance: rendezvous search", "Games/group decisions: teams"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.796.365", "e:abstract": "In Markov models of sequential decision processes, one is often interested in showing that the value function is monotonic, convex, and/or supermodular in the state variables. These kinds of results can be used to develop a qualitative understanding of the model and characterize how the results will change with changes in model parameters. In this paper we present several fundamental results for establishing these kinds of properties. The results are, in essence, \"metatheorems\" showing that the value functions satisfy property <i>P</i> if the reward functions satisfy property <i>P</i> and the transition probabilities satisfy a stochastic version of this property. We focus our attention on closed convex cone properties, a large class of properties that includes monotonicity, convexity, and supermodularity, as well as combinations of these and many other properties of interest.", "e:keyword": ["Dynamic programming: properties of stochastic models", "Decision analysis: properties of sequential models"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.810.370", "e:abstract": "We study the use of binary variables in reformulating general mixed-integer linear programs. We show that binary reformulations result in problems for which almost all the binary variables replacing a general integer variable need to be explored during branching. We also give computational results on the performance of such reformulations in solving the mixed-integer programs, which support our theoretical results.", "e:keyword": ["Programming", "Integer: remodeling of general integer variables"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.820.364", "e:abstract": "There is a growing need for the ability to specify and generate correlated random variables as primitive inputs to stochastic models.Moti vated by this need, several authors have explored the generation of random vectors with specified marginals, together with a specified covariance matrix, through the use of a transformation of a multivariate normal random vector (the NORTA method).A covariance matrix is said to be feasible for a given set of marginal distributions if a random vector exists with these characteristics. We develop a computational approach for establishing whether a given covariance matrix is feasible for a given set of marginals. The approach is used to rigorously establish that there are sets of marginals with feasible covariance matrix that the NORTA method cannot match. In such cases, we show how to modify the initialization phase of NORTA so that it will exactly match the marginals, and approximately match the desired covariance matrix.An important feature of our analysis is that we show that for almost any covariance matrix (in a certain precise sense), our computational procedure either explicitly provides a construction of a random vector with the required properties, or establishes that no such random vector exists.", "e:keyword": ["Simulation", "Random variable generation: random vector generation", "Probability distributions: copulae"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.835.371", "e:abstract": "This paper presents the design and pricing of financial contracts for the supply and procurement of interruptible electricity service. While the contract forms and pricing methodology have broader applications, the focus of this work is on electricity market applications, which motivate the contract structures and price process assumptions. In particular, we propose a new contract form that bundles simple forwards with exotic call options that have two exercise points with different strike prices. Such options allow hedging and valuation of supply curtailment risk, while explicitly accounting for the notification lead time before curtailment.The proposed instruments are priced under the traditional GBM price process assumption and under the more realistic assumption (for electricity markets) of a mean reverting price process with jumps. The latter results employ state-of-the-art Fourier transforms techniques.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.851.362", "e:abstract": "The train timetabling problem aims at determining a periodic timetable for a set of trains that does not violate track capacities and satisfies some operational constraints. In particular, we concentrate on the problem of a single, one-way track linking two major stations, with a number of intermediate stations in between. Each train connects two given stations along the track (possibly different from the two major stations) and may have to stop for a minimum time in some of the intermediate stations. Trains can overtake each other only in correspondence of an intermediate station, and a minimum time interval between two consecutive departures and arrivals of trains in each station is specified.In this paper, we propose a graph theoretic formulation for the problem using a directed multigraph in which nodes correspond to departures/arrivals at a certain station at a given time instant. This formulation is used to derive an integer linear programming model that is relaxed in a Lagrangian way. A novel feature of our model is that the variables in the relaxed constraints are associated only with nodes (as opposed to arcs) of the aforementioned graph. This allows a considerable speed-up in the solution of the relaxation. The relaxation is embedded within a heuristic algorithm which makes extensive use of the dual information associated with the Lagrangian multipliers. We report extensive computational results on real-world instances provided from Ferrovie dello Stato SpA, the Italian railway company, and from Ansaldo Segnalamento Ferroviario SpA.", "e:keyword": ["Programming algorithms: Lagrangian relaxation", "Heuristics", "Transportation: train timetabling", "Railway"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.862.373", "e:abstract": "We motivate, derive, and implement a multilevel approach to the travelling salesman problem. The resulting algorithm progressively coarsens the problem, initialises a tour, and then employs either the Lin-Kernighan (LK) or the Chained Lin-Kernighan (CLK) algorithm to refine the solution on each of the coarsened problems in reverse order. In experiments on a well-established test suite of 80 problem instances we found multilevel configurations that either improved the tour quality by over 25% as compared to the standard CLK algorithm using the same amount of execution time, or that achieved approximately the same tour quality over seven times more rapidly. Moreover, the multilevel variants seem to optimise far better the more clustered instances with which the LK and CLK algorithms have the most difficulties.", "e:keyword": ["Networks/graphs", "Heuristics: meta-heuristic for TSP", "Mathematics", "Combinatorics: multilevel refinement for combinatorial problems", "Simulation", "Applications: optimization by multilevel refinement"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.878.361", "e:abstract": "In an attempt to improve the procedures for statistical process control many researchers have developed and proposed a variety of adaptive control charts in the last decade. The common characteristic of those charts is that one or more of the chart parameters (sampling interval, sample size,control limits) is allowed to change during operation, taking into account current sample information. Due to their flexibility, adaptive charts are more effective than their static counterparts but they are also more complex in terms of implementation. The purpose of this paper is to evaluate the economic performance of various adaptive control schemes to derive conclusions about their relative effectiveness. The analysis concentrates on Bayesian control charts used for monitoring the process mean in finite production runs. We present dynamic programming formulations and properties of the optimal solutions, which we then use to solve a number of numerical examples. The results from our comparative numerical study indicate that the chart parameter having the most positive impact on the economic performance by being adaptive is the sampling interval. It is therefore sufficient in most cases to use control charts with adaptive sampling intervals rather than other types of partially adaptive charts or the more complicated fully adaptive control charts.", "e:keyword": ["Reliability", "Quality control: statistical process control using adaptive charts", "Statistics", "Bayesian: updating the state of a monitored manufacturing process"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.889.372", "e:abstract": "We study a single-product assembly system in which the final product is assembled to order whereas the components (subassemblies) are built to stock. Customer demand follows a Poisson process, and replenishment lead times for each component are independent and identically distributed random variables. For any given base-stock policy, the exact performance analysis reduces to the evaluation of a set of <i>M</i>/<i>G</i>/(infinity) queues with a common arrival stream. We show that unlike the standard <i>M</i>/<i>G</i>/(infinity) queueing system, lead time (service time) variability degrades performance in this assembly system. We also show that it is desirable to keep higher base-stock levels for components with longer mean lead times (and lower unit costs). We derive easy-to-compute performance bounds and use them as surrogates for the performance measures in several optimization problems that seek the best trade-off between inventory and customer service. Greedy-type algorithms are developed to solve the surrogate problems. Numerical examples indicate that these algorithms provide efficient solutions and valuable insights to the optimal inventory/service trade-off in the original problems.", "e:keyword": ["Inventory/production: multi-item", "Operating characteristics", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.5.904.360", "e:abstract": "Decision making under uncertainty is a challenge faced by many decision makers. Stochastic programming is a major tool developed to deal with optimization with uncertainties which has found applications in, e.g., finance, such as asset--liability and bond--portfolio management. Computationally, however, many models in stochastic programming remain unsolvable because of overwhelming dimensionality. For a model to be well solvable, its special structure must be explored. Most of the solution methods are based on decomposing the data. In this paper we propose a new decomposition approach for two-stage stochastic programming, based on a direct application of the path-following method combined with the homogeneous self-dual technique. Numerical experiments show that our decomposition algorithm is very efficient for solving stochastic programs. In particular, we apply our decomposition method to a two-period portfolio selection problem using options on a stock index. In this model the investor can invest in a money-market account, a stock index, and European options on this index with different maturities. We experiment with our model with market prices of options on the S&P500.", "e:keyword": ["Programming", "Stochastic: decomposition and interior point methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.1007.346", "e:abstract": "", "e:keyword": ["Inventory/production: operating characteristics", "Service rates. Probability: renewal processes. Cost analysis: inventory shortage costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.1018.356", "e:abstract": "", "e:keyword": ["Facilities location: restricted location problems. Facilities layout: barriers to travel"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.1032.349", "e:abstract": "We study a multiclass open-queueing network with a set of single-server stations that operate under a combination of FIFO (first-in-first out) and priority service disciplines, and are subject to random breakdowns. Assuming that the primitive processes---in particular, external arrivals, service requirements, service capacities (up and down times), and the routing mechanism---follow two-moment approximations (based on functional central limit theorems), we develop a semi-martingale reflected Brownian motion (SRBM) approximation for the performance processes such as workload, queue lengths, and sojourn times. We illustrate through numerical examples in comparison against simulation that the SRBM approximation, while not always supported by a limit theorem, exhibits good accuracy in most cases. Through analyzing special networks, we also discuss the existence of the SRBM approximation in relation to the stability and the heavy traffic limits of the networks.", "e:keyword": ["Queues: approximations and diffusion models. Probability: diffusion. Production/scheduling: approximations/scheduling"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.1050.354", "e:abstract": "We consider the problem of locating a line or a line segment in three-dimensional space, such that the sum of distances from the facility represented by the line (segment) to a given set of points is minimized. An example is planning the drilling of a mine shaft, with access to ore deposits through horizontal tunnels connecting the deposits and the shaft. Various models of the problem are developed and analyzed, and efficient solution methods are given.", "e:keyword": ["Facilities equipment: linear facility location. Planning", "Location", "Continuous: three dimensions"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.1058.350", "e:abstract": "We consider an economic lot-sizing problem with a special class of piecewise linear ordering costs, which we refer to as the class of modified all-unit discount cost functions. Such an ordering cost function represents transportation costs charged by many less-than truckload carriers. We show that even special cases of the lot-sizing problem are NP-hard and therefore analyze the effectiveness of easily implementable policies. In particular, we demonstrate that there exists a zero-inventory-ordering(ZIO) policy, i.e., a policy in which an order is placed only when the inventory level drops to zero, whose total inventory and ordering cost is no more than 4/3 times the optimal cost. Furthermore, if the ordering cost function does not vary over time, then the cost of the best ZIO policy is no more than 5.6/4.6 times the optimal cost. These results hold for any transportation and holding cost functions that satisfy the following properties: (i) they are non decreasing functions, and (ii) the associated cost per unit is non increasing. Finally, we report on a numerical study that shows the effectiveness of ZIO policies on a set of test problems.", "e:keyword": ["Analysis of algorithms--computational complexity", "Economic lot-sizing problem", "Inventory/production--approximations for economic lot-sizing problem", "Transportation--less than truckload cost"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.1068.344", "e:abstract": "Consider a basin with remaining undiscovered fields, in which both field sizes and their number are uncertain. Assuming that the probability of finding a field is increasing in its size, we show that the expected size of the first remaining field discovered, which is of particular importance, is increasing in the variability of field sizes, and results from a simulation model of exploration illustrate this trend for the first several fields discovered. We also provide simple bounds on the expected size of the first field discovered. It seems that more information (less variability) leads to less value---something unexpected. We explore and explain this seemingly counterintuitive result.", "e:keyword": ["Industries", "Petroleum/natural gas: exploration. Natural resources", "Energy: discovery process. Probability models"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.1073.358", "e:abstract": "Multiple independent streams of random numbers are often required in simulation studies, for instance, to facilitate synchronization for variance-reduction purposes, and for making independent replications. A portable set of software utilities is described for uniform random-number generation. It provides for multiple generators (streams) running simultaneously, and each generator (stream) has its sequence of numbers partitioned into many long disjoint contiguous substreams. The basic underlying generator for this implementation is a combined multiple-recursive generator with period length of approximately 2<sup>191</sup>, proposed by L'Ecuyer (1999a). A C++ interface is described here. Portable implementations are available in C, C++, and Java", "e:keyword": ["Simulation: random number generation", "Random variable generation. Statistical analysis. Computers/computer science: software"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.923.348", "e:abstract": "This paper presents a decision support tool for solving a cutting and reuse problem arising in a European plant devoted to the production of gear belts. In this production, rectangular pieces of rubberised nylon are cut using machines employing parallel blades, so as to obtain rectangular components of identical height and (possibly) different width. A component is then used to produce a set of belts with the same girth; but, if necessary, the girth required can also be obtained by sewing together two components. The major objectives of optimisation are: trim loss minimisation, quality control, workload equalisation, setup minimisation. The problem, a particular one-dimensional cutting stock with both cutting and reuse decision variables, has been formulated in terms of integer linear programming and then efficiently solved by applying standard packages within a column generation scheme. A significant improvement of performance has been obtained in terms of both economic savings and product quality. This has convinced the management to implement the model in the plant operation.", "e:keyword": ["Industries: machinery", "Manufacturing: performance/productivity", "Production/scheduling: cutting"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.935.357", "e:abstract": "Value trade-offs define how much must be gained in the achievement of one objective to compensate for a lesser achievement on a different objective. Value trade-offs that adequately express a decision maker's values are essential both for good decision making in multiple-objective contexts and for insightful analyses of multiple-objective decisions. This paper identifies and illustrates 12 important mistakes frequently made that limit one's ability to determine useful value trade-offs. It then suggests how to avoid making these mistakes. The intent is to provide practical advice for making good value trade-offs, and hence, better decisions.", "e:keyword": ["Decision analysis", "Multiple criteria: assessing decision maker's value trade-offs. Utility/preference", "Multiattribute: quantifying preferences"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.946.351", "e:abstract": "We analyze the problem of maximizing the expected number of species in a nature reserve network, subject to a constraint on the number of sites in the network, given probabilistic information about species occurrences. The problem is a nonlinear binary integer program that is NP-hard. We develop a linear integer programming approximation that may be solved with standard integer programming software. We compare the approximation with two other approaches, an expected greedy approach and a probability hurdle approach, using probabilistic data on occurrences of terrestrial vertebrates in the state of Oregon. Results of the approximation and an exact algorithm are compared by using samples from the North American Breeding Bird Survey.", "e:keyword": ["Integer programming: coverage models", "Facility location: reserve site selection under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.956.345", "e:abstract": "In a probabilistic set-covering problem the right-hand side is a random binary vector and the covering constraint has to be satisfied with some prescribed probability. We analyze the structure of the set of probabilistically efficient points of binary random vectors, develop methods for their enumeration, and propose specialized branch-and-bound algorithms for probabilistic set-covering problems.", "e:keyword": ["Programming: stochastic", "Programming: integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.968.353", "e:abstract": "The radio link frequency assignment problem occurs when a network of radio links has to be established. Each link must be assigned an operating frequency from a given domain. The assignment has to satisfy certain restrictions so as to limit the interference between links. The number of frequencies used is to be minimized.Problems of this type were investigated within the CALMA project by a consortium consisting of research groups from Delft, Eindhoven, London, Maastricht, Norwich, and Toulouse. The participants developed optimization algorithms based on branch-and-cut and constraint satisfaction, and approximation techniques including a variety of local search methods, genetic algorithms, neural networks, and potential reduction. These algorithms were tested and compared on a set of real-life instances.", "e:keyword": ["Programming/integer: comparison of algorithms. Communications: frequency assignment"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.981.347", "e:abstract": "A cycle time-throughput curve quantifies the relationship of average cycle time to throughput rates in a manufacturing system. Moreover, it indicates the asymptotic capacity of a system. Such a curve is used to characterize system performance over a range of start rates. Simulation is a fundamental method for generating such curves since simulation can handle the complexity of real systems with acceptable precision and accuracy. A simulation-based cycle time-throughput curve requires a large amount of simulation output data; the precision and accuracy of a simulated curve may be poor if there is insufficient simulation data. To overcome these problems, sequential simulation experiments based on a nonlinear D-optimal design are suggested. Using the nonlinear shape of the curve, such a design pinpoints <i>p</i> starting design points, and then sequentially ranks the remaining <i>n</i> -- <i>p</i> candidate design points, where <i>n</i> is the total number of possible design points being considered. A model of a semiconductor wafer fabrication facility is used to validate the approach. The sequences of experimental runs generated can be used as references for simulation experimenters.", "e:keyword": ["Simulation: efficient estimation of cycle time-throughput curves. Queues: simulation of queueing networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.50.6.991.343", "e:abstract": "We study the employee staffing problem in a service organization that uses employee service capacity to meet random, nonstationary service requirements. The employees experience learning and turnover on the job, and we develop a Markov Decision Process (MDP) model which explicitly represents the stochastic nature of these effects. Theoretical results show that the optimal hiring policy is of a state-dependent “hire-up-to” type, similar to an inventory “order-up-to” policy. For two important special cases, a myopic policy is optimal. We also test a linear programming (LP) based heuristic, which uses average learning and turnover behavior, in stationary environments. In most cases, the LP-based policy performs quite well, within 1% of optimality. When flexible capacity---in the form of overtime or outsourcing---is expensive or not available, however, explicit modeling of stochastic learning and turnover effects may improve performance significantly.", "e:keyword": ["Dynamic programming/optimal control", "Applications: hierarchical model for manpower planning. Organizational studies", "Manpower planning: MDP staffing model withlearning and turnover"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.1.12797", "e:abstract": "In this paper we develop a policy model for pollution control investment and operational decisions in the copper industry. The system consists of (i) a nonlinear integer model to optimize smelter operations, including the investment decisions relating to smelting capacity and pollution control plants that complies with environmental regulations, and (ii) a network flow model to describe the economic behavior of the sulfuric acid market, which considers the sulfuric acid produced at the pollution abatement stages in the smelting process. This second model solves for an equilibrium among spatially separated markets, that determines the price and distribution of acid in each demand and supply region. The two models interact through the input each receives from the other. Thus, the smelter model uses the sulfuric acid price at each smelter to find optimal operational and investment decisions, whereas the sulfuric acid market model considers sulfuric acid output at the smelters as part of the supply input to find the price of this product at each smelter location. The solution given by the policy model is the global equilibrium obtained when this iterative process between the two models converges. Thus, the price of the sulfuric acid, which is the central component when deciding when and where to locate a sulfuric acid plant, is determined endogenously, rather than assumed exogenous as in most models of this type. Computational experiments show that expected profits associated with the copper industry can increase significantly when the problem is solved in aggregate, as compared with the smelters making their decisions independently. Several applications of the policy model are described.", "e:keyword": ["Industries", "Mining/metals: decontamination in copper production process", "Facilities/equipment planning"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.113.12793", "e:abstract": "Consider the following due-date scheduling problem in a multiclass, acyclic, single-station service system: Any class <i>k</i> job arriving at time <i>t</i> must be served by its due date <i>t</i>+<i>D<sub>k</sub></i>. Equivalently, its delay <i>(tau)<sub>k</sub></i> must not exceed a given delay or lead-time <i>D<sub>k</sub></i>. In a stochastic system, the constraint <i>(tau)<sub>k</sub></i>(le)<i>D<sub>k</sub></i> must be interpreted in a probabilistic sense. Regardless of the precise probabilistic formulation, however, the associated optimal control problem is intractable with exact analysis. This article proposes a new formulation which incorporates the constraint through a sequence of convex-increasing delay cost functions. This formulation reduces the intractable optimal scheduling problem into one for which the Generalized <i>c(mu)</i> (G<i>c(mu)</i>) scheduling rule is known to be asymptotically optimal. The G<i>c(mu)</i> rule simplifies here to a generalized longest queue (GLQ) or generalized largest delay (GLD) rule, which are defined as follows. Let <i>N<sub>k</sub></i> be the number of class <i>k</i> jobs in system, <i>(lambda)<sub>k</sub></i> their arrival rate, and <i>a<sub>k</sub></i> the age of their oldest job in the system. GLQ and GLD are dynamic priority rules, parameterized by <i>(theta)</i>: GLQ(<i>(theta)</i>) serves FIFO within class and prioritizes the class with highest index <i>(theta)<sub>k</sub>N<sub>k</sub></i>, whereas GLD(<i>(theta)</i>) uses index <i>(theta)<sub>k</sub>(lambda)<sub>k</sub>a<sub>k</sub></i>.The argument is presented first intuitively, but is followed by a limit analysis that expresses the cost objective in terms of the maximal due-date violation probability. This proves that GLQ(<i>(theta)</i><sub>*</sub>) and GLD(<i>(theta)</i><sub>*</sub>), where <i>(theta)</i><sub>*</sub>,k = 1/<i>(lambda)<sub>k</sub>D<sub>k</sub></i>, asymptotically minimize the probability of maximal due-date violation in heavy traffic. Specifically, they minimize <inline-formula><tex-math notation=\"LaTeX\">\\documentclass{aastex}\\usepackage{amsbsy}\\usepackage{amsfonts}\\usepackage{amssymb}\\usepackage{bm}\\usepackage{mathrsfs}\\usepackage{pifont}\\usepackage{stmaryrd}\\usepackage{textcomp}\\usepackage{portland,xspace}\\usepackage{amsmath,amsxtra}\\pagestyle{empty}\\DeclareMathSizes{10}{9}{7}{6}\\begin{document}$\\mbox{lim}\\, \\mbox{inf}_{n\\to\\infty}\\mbox{Pr}\\{\\max_k \\mbox{sup}_{s\\in [0,t]}\\tfrac{\\tau_k(ns)}{n^{1/2}D_k}\\geq x\\}$\\end{document}</tex-math></inline-formula> for all positive <i>t</i> and <i>x</i>, where <i>(tau)<sub>k</sub></i>(<i>s</i>) is the delay of the most recent class <i>k</i> job that arrived before time <i>s</i>. GLQ with appropriate parameter (theta)<sub>(alpha)</sub> also reduces “total variability” because it asymptotically minimizes a weighted sum of (alpha)th delay moments. Properties of GLQ and GLD, including an expression for their asymptotic delay distributions, are presented.", "e:keyword": ["Queues", "Optimization: optimal control to meet due-dates or lead-times", "Production/scheduling", "Sequencing", "Stochastic: lead-time constrained scheduling", "Inventory/production", "Policies", "Review/lead-times: production policies to guarantee lead-times"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.123.12802", "e:abstract": "In a batch-processing network, multiple jobs can be formed into a batch to be processed in a single service operation. The network is multiclass in that several job classes may be processed at a server. Jobs in different classes cannot be mixed into a single batch. A batch policy specifies which class of jobs is to be served next. Throughput of a batch-processing network depends on the batch policy used. When the maximum batch sizes are equal to one, the corresponding network is called a standard-processing network, and the corresponding service policy is called a dispatch policy. There are many dispatch policies that have been proven to maximize the throughput in standard networks. This paper shows that any <i>normal</i> dispatch policy can be converted into a batch policy that preserves key stability properties. Examples of normal policies are given. These include static buffer priority (SBP), first-in-first-out (FIFO), and generalized round robin (GRR) policies.", "e:keyword": ["Production/scheduling: sequencing", "Stochastic", "Queues: networks", "Batch/bulk"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.137.12796", "e:abstract": "When a customer requests a discount fare, the airline must decide whether to sell the seat at the requested discount or to hold the seat in hope that a customer will arrive later who will pay more. We model this situation for a single-leg flight with multiple fare classes and customers who arrive according to a semi-Markov process (possibly nonhomogeneous). These customers can request multiple seats (batch requests) and can be overbooked. Under certain conditions, we show that the value function decreases as departure approaches. If each customer only requests a single seat or if the requests can be partially satisfied, then we show that there are optimal booking curves which decrease as departure approaches. We also provide counterexamples to show that this structural property of the optimal policy need not hold for more general arrival processes if the requests can be for more than one seat and must be accepted or rejected as a whole.", "e:keyword": ["Dynamic programming/optimal control: semi-Markov", "Industries: Transportation/shipping", "Airline revenue management", "Probability: stochastic model application"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.149.12803", "e:abstract": "Data envelopment analysis (DEA) is a methodology that allows, in one way or other, the assignment of efficiency scores to members of a group of decision-making units. We call an efficiency measure “continuous” if small perturbations of the input-output data cause only small changes in the score. Continuity is a desirable property of an efficiency measure, in particular in the presence of measurement tolerances. Continuity is also desirable from a numerical point of view because the scores are computed by linear programming software.Focusing on convex production possibility sets, we give examples where radial DEA measures fail to be continuous, i.e., they “jump” under small data perturbations. We present necessary and sufficient conditions for continuity in terms of the data and show that these conditions are satisfied for “almost all” data. We also discuss continuity of nonradial measures and identify possible problems of “multistage approaches” to compute mix efficiencies.", "e:keyword": ["Economics: continuity of DEA efficiency measures", "Linear programming: continuity of the value function"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.160.12799", "e:abstract": "We characterize optimal policies of a dynamic lot-sizing/vehicle-dispatching problem under dynamic deterministic demands and stochastic lead times. An essential feature of the problem is the structure of the ordering cost, where a fixed cost is incurred every time a batch is initiated (or a vehicle is hired) regardless of the portion of the batch (or vehicle) utilized. Moreover, for every unit of demand not satisfied on time, holding and backorder costs are incurred. Under mild assumptions we show that the demand of a period is satisfied from at most three distinct production (dispatching) epochs. We devise a dynamic programming algorithm to compute the production/dispatching quantities and times.", "e:keyword": ["Inventory/production", "Planning horizons: stochastic lead times and stepwise fixed costs", "Transportation", "Scheduling", "Vehicles: dispatching under stochastic transit times"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.167.12795", "e:abstract": "In this paper, we analyze a generalization of a classic network-flow model. The generalization involves the replacement of deterministic demand with stochastic demand. While this generalization destroys the original network structure, we show that the matrix underlying the stochastic model is dual network. Thus, the integer program associated with the stochastic model can be solved efficiently using network-flow or linear-programming techniques. We also develop an application of this model to the ground-holding problem in air-traffic management. The use of this model for the ground-holding problem improves upon prior models by allowing for easy integration into the newly developed ground-delay program procedures based on the Collaborative Decision-Making paradigm.", "e:keyword": ["Transportation: air-traffic management", "Programming integer: embedded networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.17.12801", "e:abstract": "In an earlier study we examined the available evidence on the Edelman Prize finalist applications 1989--1998. This study concluded that 13 of the 42 private sector applications provide examples of strategic operations research (SOR) when SOR is defined as operations research that creates a sustainable competitive advantage. In a follow-up study we tested our classifications, gathering longitudinal information on the continued success of the Edelman applications. We contacted people who were familiar with all the private sector applications that were Edelman finalists 1989--1996 and had at least five years of history since the competition. We describe the post-Edelman history of these applications and use this data to reassess their strategic role. We found that the longitudinal data provides evidence to support our original classification, but also suggests that several additional applications were more strategic than was originally apparent. We conclude that almost 60% (20 of 34) of these applications created a sustainable competitive advantage for their firms and provide examples of SOR.", "e:keyword": ["Organizational studies", "Strategy: sustaining a competitive advantage through operations research"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.32.12794", "e:abstract": "As the 2000 election so vividly showed, it is Electoral College standings rather than national popular votes that determine who becomes President. But current pre-election polls focus almost exclusively on the popular vote. Here we present a method by which pollsters can achieve both point estimates and margins of error for a presidential candidate's electoral-vote total. We use data from both the 2000 and 1988 elections to illustrate the approach. Moreover, we indicate that the sample sizes needed for reliable inferences are similar to those now used in popular-vote polling.", "e:keyword": ["Government: elections", "Electoral college", "Statistics: Bayesian sampling", "Probability: stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.41.12804", "e:abstract": "We examine the mechanism-design problem for a single buyer to procure purchase options for a homogeneous good when that buyer is required to satisfy an unknown future demand. Suppliers have two-dimensional types in the form of commitment costs and production costs. The efficient schedule of options depends on the distribution of demand. To implement an efficient outcome, we introduce a class of mechanisms which are essentially pivotal mechanisms (Vickrey-Clarke-Groves) with respect to the expected costs of the suppliers. We show that the computational task of running such mechanisms is not burdensome. Our discussion uses electricity markets as an example.", "e:keyword": ["Games", "Bidding/auctions: procuring from capacity-constrained suppliers", "Programming", "Linear: formulating procurement as longest-path"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.52.12798", "e:abstract": "American-Asian options are average-price options that allow early exercise. In this paper, we derive structural properties for the optimal exercise policy, which are then used to develop an efficient numerical algorithm for pricing such options. In particular, we show that the optimal policy is a threshold policy: The option should be exercised as soon as the average asset price reaches a characterized threshold, which can be written as a function of the asset price at that time. By exploiting this and other structural properties, we are able to parameterize the exercise boundary, and derive gradient estimators for the option payoff with respect to the parameters of the model. These estimators are then incorporated into a simulation-based algorithm to price American-Asian options. Computational experiments carried out indicate that the algorithm is very competitive with other recently proposed numerical algorithms.", "e:keyword": ["Finance", "Securities: option pricing", "Simulation: perturbation analysis and stochastic approximation", "Dynamic programming", "Models: structure of optimal policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.67.12805", "e:abstract": "All-optical networks with wavelength division multiplexing (WDM) capabilities are prime candidates for future wide-area backbone networks. The simplified processing and management of these very high bandwidth networks make them very attractive. A procedure for designing low cost WDM networks is the subject of this investigation.In the literature, this design problem has been referred to as the routing and wavelength assignment problem. Our proposed solution involves a three-step process that results in a low-cost design to satisfy a set of static point-to-point demands. Our strategy simultaneously addresses the problem of routing working traffic, determining backup paths for single node or single link failures, and assigning wavelengths to both working and restoration paths.An integer linear program is presented that formally defines the routing and wavelength assignment problem (RWA) being solved along with a simple heuristic procedure. In an empirical analysis, the heuristic procedure successfully solved realistically sized test cases in under 30 seconds on a Compaq AlphaStation. CPLEX 6.6.0 using default settings required over 1,000 times longer to obtain only slightly better solutions than those obtained by our new heuristic procedure.", "e:keyword": ["Optical networks: wavelength routing", "Network design: provisioning"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.80.12800", "e:abstract": "This paper presents a modeling framework for analyzing competition between multiple firms that each possess a mixture of hydroelectric and thermal generation resources. Based upon the concept of a Cournot oligopoly with a competitive fringe, the model characterizes the Cournot equilibrium conditions of a multiperiod hydrothermal scheduling problem. Using data from the western United States electricity market, this framework is implemented as a mixed linear complementarity model. The results show that some firms may find it profitable to allocate considerably more hydro production to off-peak periods then they would under perfect competition. This strategy is a marked contrast to the optimal hydroschedules that would arise if no firms were acting strategically. These results highlight the need to explicitly consider profit-maximizing behavior when examining the impact of regulatory and environmental policies on electricity market outcomes.", "e:keyword": ["Games/group decisions", "Noncooperative: oligopoly competition", "Industries", "Electric: simulation of unregulated market outcomes"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.1.94.12791", "e:abstract": "Responding to varying customer needs for product customization, quantities, or lead times requires increased manufacturing flexibility. In certain contexts, the reverse is true, i.e., customer requirements are somewhat flexible, providing additional options in production planning. This paper considers such an opportunity faced by a specialty steel manufacturer whose customers allow flexibility in product specifications. We develop and test a model and solution methodology to exploit this flexibility during production planning. The model also applies to other situations in which manufacturers can select, within limits, the size or quantities of items produced using limited capacity. We formulate the problem as a profit-maximizing mixed-integer program with an interesting embedded packing subproblem using “expandable” items. We develop a composite solution method that combines model enhancement using strong valid inequalities, Lagrangian relaxation, and heuristic approaches. Computational results using real data from a specialty steel manufacturer and randomly generated test problems show that the algorithm generates solutions that are, on average, within 0.59% of optimality. For the problem instances based on real data, the model increases contribution to profit by over 7% relative to current practice at the steel manufacturing facility.", "e:keyword": ["Production/scheduling", "Applications: steel plate production planning", "Programming", "Integer applications: large-scale mixed integer optimization", "Industries", "Mining/metals: applied optimization in steel manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.175.12789", "e:abstract": "In many theme parks, stores are located within major attractions to sell related merchandise. Sales at such stores form a significant portion of theme park profits. Typically, store sales depend upon visitor flows through the attraction, customer satisfaction with the attraction, and the merchandise at the store. In addition, such stores constitute a unique retail environment, as visitor flows to attractions can be managed and stores are not competitors, but belong to the same parent company. This provides the opportunity to increase store sales by interfacing park operations, which manages visitor flows by setting schedules and capacity of attractions, with the store-level merchandising process, which determines which and how much product to order.Motivated by a study at Universal Studios Hollywood (USH), we develop a flow management model to link park operations with store-level merchandising. This model sets the capacities and schedules of the major attractions to increase visitor flows to high-profit retail areas subject to visitor satisfaction, capacity, scheduling, and flow-balance constraints. In addition, this model serves as an important tool to generate and evaluate various strategies aimed at increasing theme park profitability at USH.", "e:keyword": ["Industries", "Recreation: theme parks", "Schedules", "Applications: visitor flow management", "Programming", "Integer: heuristic and relaxation"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.185.12782", "e:abstract": "This paper develops an eigenfunction expansion approach to pricing options on scalar diffusion processes. All contingent claims are unbundled into portfolios of primitive securities called <i>eigensecurities</i>. Eigensecurities are eigenvectors (eigenfunctions) of the pricing operator (present value operator). All computational work is at the stage of finding eigenvalues and eigenfunctions of the pricing operator. The pricing is then immediate by the linearity of the pricing operator and the eigenvector property of eigensecurities. To illustrate the computational power of the method, we develop two applications:pricing vanilla, single- and double-barrier options under the constant elasticity of variance (CEV) process and interest rate knock-out options in the Cox-Ingersoll-Ross (CIR) term-structure model.", "e:keyword": ["Finance", "Asset pricing: option pricing", "CEV model", "CIR model", "Finance", "Securities: barrier options", "Probability", "Diffusion: spectral theory", "Barrier crossing", "Generalized Bessel process"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.210.12780", "e:abstract": "We consider a supply chain in which the underlying demand process can be described in a linear state space form. Inventory is managed at various points of the chain (members), based on local information that each member observes and continuously updates. The key feature of our model is that it takes into account the ability of the members to observe subsets of the underlying state vector, and adopt their forecasting and replenishment policies accordingly. This enables us to model situations in which the members are privy to certain explanatory variables of the demand, with the latter possibly evolving according to a vector autoregressive time series. For each member, we identify an associated demand evolution model, for which we propose an adaptive inventory replenishment policy that utilizes the Kalman filter technique. We then provide a simple methodology for assessing the benefits of various types of information-sharing agreements between members of the supply chain. We also discuss inventory positioning and cost performance assessment in the supply chain. Our performance metrics and inventory target levels are usually presented in matrix forms, allowing them to accommodate a relatively large spectrum of linear demand models, and making them simple to implement. Several illustrations for possible applications of our models are provided.", "e:keyword": ["Inventory/production: policies", "Uncertainty", "Applications", "Forecasting: time series"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.228.12786", "e:abstract": "We consider a given set of offshore platforms and onshore wells producing known (or estimated) amounts of oil to be connected to a port. Connections may take place directly between platforms, well sites, and the port, or may go through connection points at given locations. The configuration of the network and sizes of pipes used must be chosen to minimize construction costs. This problem is expressed as a mixed-integer program, and solved both heuristically by Tabu Search and Variable Neighborhood Search methods and exactly by a branch-and-bound method. Two new types of valid inequalities are introduced. Tests are made with data from the South Gabon oil field and randomly generated problems.", "e:keyword": ["Networks/graphs", "Applications: design problem-formulation and analysis", "Programming", "Integer", "Algorithms: interactive branch-and-bound with valid inequalities", "Industries", "Petroleum/natural gas: oil pipeline network design"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.240.12779", "e:abstract": "The notion of a data warehouse for integrating operational data into a single repository is rapidly becoming popular in modern organizations. An important issue in the integration process is how to deal with the identifier mismatch problem when combining similar data from disparate sources. A real-world entity may be represented using different identifiers in different operational data sources, and matching them may often be difficult using simple database operations expressed, say, as an SQL query. A record-by-record manual matching is also not practical because the databases may be large. A decision model is presented that combines probability-based automated matching with manual matching in a cost minimization formulation. A heuristic approach is proposed for solving the decision model. Both the model and the heuristic solution approach have been tested on real data. The results from the testing indicate that the model can be effectively used in real-world situations.", "e:keyword": ["Computers", "Databases: data warehousing", "Data consolidation", "Information systems", "Decision-support systems: record matching", "Programming", "Integer: algorithms", "Heuristic systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.255.12787", "e:abstract": "We consider a queueing system, commonly found in inbound telephone call centers, that processes two types of work. Type-H jobs arrive at rate (lambda)<i><sub>H</sub></i>, are processed at rate (mu)<i><sub>H</sub></i> , and are served first come, first served within class. A service-level constraint of the form E[delay] (le) (alpha) or <b>P</b> {delay (le) (beta)} (ge) limits the delay in queue that these jobs may face. An infinite backlog of type-L jobs awaits processing at rate (mu)<i><sub>H</sub></i>, and there is no service-level constraint on this type of work. A pool of <i>c</i> identical servers processes all jobs, and a system controller must maximize the rate at which type-L jobs are processed, subject to the service-level constraint placed on the type-H work.We formulate the problem as a constrained, average-cost Markov decision process and determine the structure of effective routing policies. When the expected service times of the two classes are the same, these policies are globally optimal, and the computation time required to find the optimal policy is about that required to calculate the normalizing constant for a simple <i>M/M/c</i> system. When the expected service times of the two classes differ, the policies are optimal within the class of priority policies, and the determination of optimal policy parameters can be determined through the solution of a linear program with O(<i>c</i><sup>3</sup>) variables and O(<i>c</i><sub>2</sub>) constraints.", "e:keyword": ["Dynamic programming", "Infinite state Markov: average cost", "Constrained MDP", "Queues", "Optimization: control of multiclass", "Markovian systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.272.12778", "e:abstract": "We present an approach for using parallel processors to simulate long sample paths of Markovian queueing networks with finite buffers and both loss and blocking stations. Parallelism is achieved by distributing the available processors among segments of the time domain of the simulation. We conduct the simulation in such a way that all sample paths of the system will eventually couple (i.e., become identical), regardless of their starting states. This coupling property is exploited to generate valid sample paths of the system by combining the information collected on consecutive time segments of the simulation. The efficiency of our approach depends heavily on the magnitude of the coupling times of the sample paths. We study how the expected coupling times depend on the system parameters through a variety of theoretical and numerical results. Our main results give conditions under which the expected coupling times grow slowly (no faster than linearly) with respect to the number of stations and buffer capacities in the system. These results suggest that our time segmentation approach is likely to perform well on a substantial class of finite Markovian queueing networks.", "e:keyword": ["Simulation", "Efficiency: analysis of the time segmentation parallel simulation method", "Queues", "Markovian: analysis of the expected coupling time of all sample paths"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.281.12790", "e:abstract": "The Rural Postman Problem (RPP) is a classic “edge-routing” problem. A mathematical programming formulation of the RPP that differs fundamentally from those in the literature was introduced, but not tested computationally, by Garfinkel and Webb (1999). A rudimentary algorithm that yields lower bounds via cutting planes and upper bounds via heuristics is developed and tested for a variation of that formulation. Computational results are encouraging, especially in terms of the relatively small number of added inequalities needed to get good lower bounds, and the fact that the vast majority of these have efficient, exact separation procedures. Note that a first algorithm based on this new formulation is computationally competitive, allowing the possibility of far more efficient and complex future realizations.", "e:keyword": ["Networks", "Applications: The Rural Postman Problem", "Networks", "Distance algorithms: tight upper and lower bounds"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.292.12781", "e:abstract": "We study an assemble-to-order system with stochastic leadtimes for component replenishment. There are multiple product types, of which orders arrive at the system following batch Poisson processes. Base-stock policies are used to control component inventories. We analyze the system as a set of queues driven by a common, multiclass batch Poisson input, and derive the joint queue-length distribution. The result leads to simple, closed-form expressions of the first two moments, in particular the covariances, which capture the dependence structure of the system. Based on the joint distribution and the moments, we derive easy-to-compute approximations and bounds for the order fulfillment performance measures. We also examine the impact of demand and leadtime variability, and investigate the value of advance demand information.", "e:keyword": ["Inventory/production: assemble-to-order", "Multi-item", "Operating characteristics", "Queues: simultaneous arrivals", "Infinite server", "Probability: generating functions", "Moments", "Stochastic comparison"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.309.12784", "e:abstract": "This paper addresses the parts-grouping problem that arises in automated manufacturing environments where appropriate cutting tools must be loaded on Computer Numerical Control (CNC) machines to process a variety of parts. Since tool-loading times may be considerably long and reduce available machine processing times, it is important to find a mutually exclusive grouping of parts such that the total number of tools required by each group does not exceed the tool-magazine capacity and the number of groups is minimized. We present an integer programming formulation of the problem and develop a lower bounding procedure using Lagrangean decomposition. We then introduce valid inequalities to improve the quality of the lower bounds obtained from the relaxed problem. Our solution procedure for the lower bounding problem requires only one set of Lagrange multipliers, which reduces the required computational effort significantly. We also modify the lower bound solution heuristically to find an upper bound. The lower and upper bounds are then incorporated in a branch-and-bound scheme to search for an optimal solution. Our computational comparisons with the best existing solution procedures from the literature show that our procedure performs better on average, and for the majority of test problems within the scope of our experiments.", "e:keyword": ["Programming", "Integer", "Algorithms", "Relaxation: bin-packing problem with sharing", "Lagrangean decomposition", "Manufacturing", "Automated systems: tool-loading decision"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.321.12777", "e:abstract": "This paper is concerned with a periodic review inventory system with fast and slow delivery modes, fixed ordering cost, and regular demand forecast updates. At the beginning of each period, on-hand inventory and demand information are updated. At the same time, decisions on how much to order using fast and slow delivery modes are made. Fast and slow orders are delivered at the end of the current period and at the end of the next period, respectively. A forecast-update-dependent (<i>s</i>, <i>S</i>)-type policy is shown to be optimal. Also shown are some monotonicity properties of the policy parameters with respect to the costs and information updates.", "e:keyword": ["Inventory/production", "Stochastic: fix lost", "Multiple delivery modes", "Forecast updates", "Dynamic programming: optimality of (s", "S)-type policies", "Forecasting: updating forecasts over time"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.329.12788", "e:abstract": "A standard problem in operations literature is optimal stocking of substitutable products. We consider a consumer-driven substitution problem with an arbitrary number of products under both centralized inventory management and competition. Substitution is modeled by letting the unsatisfied demand for a product flow to other products in deterministic proportions. We obtain analytically tractable solutions that facilitate comparisons between centralized and competitive inventory management under substitution. For the centralized problem we show that, when demand is multivariate normal, the total profit is decreasing in demand correlation.", "e:keyword": ["Inventory/production", "Stochastic", "Multiproduct: substitution", "Games/group decisions", "Noncooperative: Nash equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.2.336.12785", "e:abstract": "In this paper we consider an infinite horizon economic order quantity (EOQ) model with single announced price increase, with an option of placing a special order just before the price increase takes effect. We extend earlier work where it is assumed that the special order is an integral multiple of the new EOQ quantity. In the process, we show that when the assumption of integrality is not valid, the earlier approach of minimizing the cost difference over a finite horizon is no longer valid and establish the periodicity of cost difference function. Next, we show that the Cesaro limit of the function exists and utilize that to derive the optimal special-order quantity. We find that the optimal special-ordering policy is of (<i>s</i>,<i>S</i>) type.", "e:keyword": ["Inventory", "Lot sizing: economic order quantity", "Inventory", "Pricing: special ordering for price increases"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.343.14953", "e:abstract": "We describe an enterprise-wide tool for tactical planning at a grocery retailer. The tool enables a “total landed cost” perspective by coordinating decisions across functions of the supply chain. It dynamically optimizes across logistics, purchasing, and warehouse management, while considering the necessary joint replenishment economies and accounting for a wide variety of complexities, such as discounts on total order quantity, intermittent demand, multiple distribution centers, vendor deals and forward buys, and promotions. The application combines appropriate operations research techniques to solve the problem; furthermore, it is created in the newly available information technology infrastructure. The entire application “sits above” and coordinates existing execution tools from commercial vendors by setting key operating targets and providing operational guidance.Within weeks of a pilot, we observed a reduction in on-hand inventories, an increase in service levels, and substantial improvements in logistics decisions. The total landed cost is substantially lower---with an improvement of 20.8% of operating costs, or 11.6% of net profits---while providing superior fulfillment to the stores.", "e:keyword": ["Professional: OR/MS implementation", "Inventory/production: applications", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.354.14961", "e:abstract": "Managers of complex engineering development projects face a challenge when deciding how to allocate scarce resources to minimize the risks of project failure. As resource constraints become tighter, balancing these failure risks is more critical, less intuitive, and can benefit from the power of quantitative analysis. This paper describes the Advanced Programmatic Risk Analysis and Management model (APRAM), a decision-support framework for the management of the risk of failures of dependent engineering projects within programs. Our goal is to guide the management of the design, the development, and the budget of dependent projects. Considering first a single project, our approach is to optimize the use of the budget reserves and of the funds dedicated to the system itself for each possible budget allocation. This phase involves separate optimizations of the system design and a strategy for resolving development problems based on a chosen objective function. The model also allows checking that specified thresholds of maximum acceptable risks are met, and if not, indicates how much is required to satisfy them. It is then extended to include project dependencies within a program. Finally, it allows checking that the level of resources available is appropriate by computing the shadow “risk cost” of the budget constraint. The NASA Jet Propulsion Laboratory has supported the development of this model, so each step of APRAM is illustrated by the schematic case of an unmanned space program involving two dependent projects. Also, where applicable, we discuss our experiences working with the APRAM concepts for the management of unmanned space missions.", "e:keyword": ["Project management/resource constraints", "Decision analysis: risk management", "Space program"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.371.14954", "e:abstract": "This paper considers a principal-agent variant of the classical make-to-stock single-server queueing system. The principal incurs all costs for holding inventory and backordering demand. The agent dynamically controls the production rate at the server and incurs a convex production cost. The principal cannot monitor the production rate but can draw inference from increases in the inventory level. Furthermore, by making payments contingent on the inventory level, the principal motivates the agent to control the production rate in a manner that will minimize the principal's own total expected discounted cost. We show that an optimal incentive payment scheme consists of piece rates and inventory penalties that vary dynamically with the inventory level. This scheme coordinates the system if the agent is risk neutral. Otherwise, operational performance is degraded by the conflict in incentives between principal and agent. We identify some drivers of this agency loss: In addition to discounting and risk aversion in the agent's preferences, which are standard causes of friction in dynamic agency models, an increasing marginal cost of production and slack in the agent's capacity are also found to be lead contributors. Heavy traffic analysis supports these findings through closed-form expressions for the performance of the system under the optimal incentive scheme.", "e:keyword": ["Games", "Noncooperative: dynamic principal agent model", "VMI contract", "Inventory/production", "Stochastic: make-to-stock", "Dynamic programming: infinite horizon with risk aversion"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.387.14759", "e:abstract": "Crew costs are the second-largest operating expense faced by the airline industry, after fuel. Thus, even a small improvement in the quality of a crew schedule can have significant financial impact. Decisions made earlier in the airline planning process, however, can reduce the number of options available to the crew scheduler. We address this limitation by delaying some of these earlier planning decisions---specifically, key maintenance routing decisions---and incorporating them within the crew scheduling problem. We present an <i>extended crew pairing model</i> that integrates crew scheduling and maintenance routing decisions. We prove theoretical results that allow us to improve the tractability of this model by decreasing the number of variables needed and by relaxing the integrality requirement of many of the remaining variables. We discuss how to solve the model both heuristically and to optimality, providing the user with the flexibility to trade off solution time and quality. We present a computational proof-of-concept to support the tractability and effectiveness of our approach.", "e:keyword": ["Programming", "Integers", "Branch and bound: branch and price", "Transportation", "Models", "Assignment: crews and aircraft"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.397.14955", "e:abstract": "We consider a generator making offers of energy into an electricity pool market. For a given time period, it must submit an offer stack, consisting of a fixed number of quantities of energy and prices at which it wants these quantities dispatched. We assume that the generator cannot offer enough power to substantially affect the market price, so the optimal response would be to offer energy at marginal cost. However, the market rules do not permit an arbitrary function, so the problem is to find an offer stack approximating marginal cost in a way that maximizes its profit. We give optimality conditions for this problem and derive an optimization procedure based on dynamic programming. This procedure is illustrated by applying it to several examples with different costs of production.", "e:keyword": ["Natural resources", "Energy: electricity pool markets", "Dynamic programming", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.409.14957", "e:abstract": "We consider a two-echelon distribution system in which a supplier distributes a product to <i>N</i> competing retailers. The demand rate of each retailer depends on all of the retailers' prices, or alternatively, the price each retailer can charge for its product depends on the sales volumes targeted by all of the retailers. The supplier replenishes his inventory through orders (purchases, production runs) from an outside source with ample supply. From there, the goods are transferred to the retailers. Carrying costs are incurred for all inventories, while all supplier orders and transfers to the retailers incur fixed and variable costs. We first characterize the solution to the centralized system in which all retailer prices, sales quantities and the complete chain-wide replenishment strategy are determined by a single decision maker, e.g., the supplier. We then proceed with the decentralized system. Here, the supplier chooses a wholesale pricing scheme; the retailers respond to this scheme by each choosing all of his policy variables. We distinguish systematically between the case of Bertrand and Cournot competition. In the former, each retailer independently chooses his retail price as well as a replenishment strategy; in the latter, each of the retailers selects a sales target, again in combination with a replenishment strategy. Finally, the supplier responds to the retailers' choices by implementing his own cost-minimizing replenishment strategy. We construct a perfect coordination mechanism. In the case of Cournot competition, the mechanism applies a discount from a basic wholesale price, based on the <i>sum</i> of three discount components, which are a function of (1) annual sales volume, (2) order quantity, and (3) order frequency, respectively.", "e:keyword": ["Games/group decisions", "Noncooperative: price and quantity competition among nonidentical retailers", "Inventory/production", "Multi-item/echelon/stage: two-echelon supply chain with procurement setup costs", "Inventory/production", "Policies", "Marketing/pricing: coordinating mechanisms via discounting schemes"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.427.14951", "e:abstract": "An important issue in multiple objective mathematical programming is finding discrete representations of the efficient set. Because discrete points can be directly studied by a decision maker, a discrete representation can serve as the solution to the multiple objective problem at hand. However, the discrete representation must be of acceptable quality to ensure that a most--preferred solution identified by a decision maker is of acceptable quality. Recently, attributes for measuring the quality of discrete representations have been proposed. Although discrete representations can be obtained in many different ways, and their quality evaluated afterwards, the ultimate goal should be to find such representations so as to conform to specified quality standards. We present a method that can find discrete representations of the efficient set according to a specified level of quality. The procedure is based on mathematical programming tools and can be implemented relatively easily when the domain of interest is a polyhedron. The nonconvexity of the efficient set is dealt with through a coordinated decomposition approach. We conduct computational experiments and report results.", "e:keyword": ["Programming", "Multiple criteria: efficient set", "Discrete representations"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.437.14958", "e:abstract": "We consider a model of a capacitated single-class supply chain consisting of production facilities (stages) in tandem. External demand is met from the available finished goods inventory maintained in front of the most downstream facility; unsatisfied demand is backlogged. Every stage orders from its upstream facility, thus production is constrained by the local production capacity and the availability of upstream inventory. We propose production policies in two separate cases: (1) when each facility has information about its local inventory only, and (2) when each facility has knowledge of the total downstream inventory. In case (1) the proposed policy guarantees that stockout probabilities at each stage stay bounded below given constants (service level constraints). In case (2) the proposed policy minimizes total expected inventory cost subject to desirable service-level constraints. In both cases the parameters of the proposed policies are obtained analytically based on large deviations asymptotics, which leads to drastic computational savings compared to simulation. An important feature of our model is that it accommodates autocorrelated demand and service processes, both critical features of modern failure-prone manufacturing systems. We demonstrate that detailed distributional information on demand and service processes, which is incorporated into large deviations asymptotics, is critical in inventory control decisions. We discuss extensions to a multiclass setting and to a model where unsatisfied demand is lost instead of backordered.", "e:keyword": ["Inventory/production: service-level approximations/heuristics", "Inventory/production", "Uncertainty: correlations in demand and capacity processes", "Probability", "Applications: large deviations"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.461.14960", "e:abstract": "Planning for capacity expansion forms a crucial part of the strategic-level decision making in many applications. Consequently, quantitative models for economic capacity expansion planning have been the subject of intense research. However, much of the work in this area has been restricted to linear cost models and/or limited degree of uncertainty to make the problems analytically tractable. This paper addresses a stochastic capacity expansion problem where the economies-of-scale in expansion costs are handled via fixed-charge cost functions, and forecast uncertainties in the problem parameters are explicitly considered by specifying a set of scenarios. The resulting formulation is a multistage stochastic integer program. We develop a fast, linear-programming-based, approximation scheme that exploits the decomposable structure and is guaranteed to produce feasible solutions for this problem. Through a probabilistic analysis, we prove that the optimality gap of the heuristic solution almost surely vanishes asymptotically as the problem size increases.", "e:keyword": ["Analysis of algorithms: asymptotically optimal heuristics", "Facilities/equipment planning: capacity expansion", "Programming: stochastic", "Integer"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.472.14956", "e:abstract": "We develop two classes of optimization models to maximize revenue in a restaurant (while controlling average waiting time as well as perceived fairness) that may violate the first-come-first-serve (FCFS) rule. In the first class of models, we use integer programming, stochastic programming, and approximate dynamic programming methods to decide dynamically when, if at all, to seat an incoming party during the day of operation of a restaurant that does not accept reservations. In a computational study with simulated data, we show that optimization-based methods enhance revenue relative to the industry practice of FCFS by 0.11% to 2.22% for low-load factors, by 0.16% to 2.96% for medium-load factors, and by 7.65% to 13.13% for high-load factors, without increasing, and occasionally decreasing, waiting times compared to FCFS. The second class of models addresses reservations. We propose a two-step procedure: Use a stochastic gradient algorithm to decide a priori how many reservations to accept for a future time and then use approximate dynamic programming methods to decide dynamically when, if at all, to seat an incoming party during the day of operation. In a computational study involving real data from an Atlanta restaurant, the reservation model improves revenue relative to FCFS by 3.5% for low-load factors and 7.3% for high-load factors.", "e:keyword": ["Dynamic programming: revenue management", "Stochastic optimization", "Industries: restaurant"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.487.14949", "e:abstract": "In this paper a new time-oriented decomposition heuristic is proposed to solve the dynamic multi-item multilevel lot-sizing problem in general product structures with single and multiple constrained resources as well as setup times. While lot-sizing decisions are made sequentially within an internally rolling planning interval (or lot-sizing window), capacities are always considered over the entire planning horizon. For each submodel a model formulation based on the “Simple Plant Location” representation is developed. These mixed-integer linear submodels are solved by standard mathematical programming software even for relatively large test instances. Extensive computational tests show that the heuristic proposed provides a better solution quality than a well-known special purpose heuristic.", "e:keyword": ["Inventory/production", "Multi-item/stage: heuristic for production plans in a multi-item/stage setting including lot sizing", "Programming", "Integer", "Linear: standard code within the heuristic to generate solutions", "Lot sizing"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.3.503.14952", "e:abstract": "This note is concerned with the inventory models with finite horizon and cost changes. We presented more specific results on the optimal inventory policy than those in previous papers and showed that some of the results in Lev and Weiss' (1990) paper are not necessarily optimal.", "e:keyword": ["Inventory/production: ordering strategies for price increase"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.509.16104", "e:abstract": "We describe a broadcast scheduling system developed for a precision marketing firm specialized in location-sensitive permission-based mobile advertising using SMS (Short Message Service) text messaging. Text messages containing advertisements were sent to registered customers when they were shopping in one of two shopping centers in the vicinity of London. The ads typically contained a limited-time promotional offer. The company's problem was deciding which ads to send out to which customers at what particular time, given a limited capacity of broadcast time slots, while maximizing customer response and revenues from retailers paying for each ad broadcast. We solved the problem using integer programming with an interface in Microsoft Excel. The system significantly reduced the time required to schedule the broadcasts, and resulted both in increased customer response and revenues.", "e:keyword": ["Marketing", "Advertising and media: mobile marketing", "Production/scheduling", "Applications: advertisement broadcast scheduling", "Integer programming", "Applications: IP solver interfaced with Microsoft Excel"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.518.16099", "e:abstract": "The question of how to effectively manage items with heterogeneous attributes and differing service requirements has become increasingly important to supply chains that support the delivery of after-sales service. However, there has been little investigation to date on how organizations actually manage inventory levels under such circumstances. This study provides such an investigation, focusing on the logistic system used to manage consumable service parts for weapon systems in the U.S. military. Our findings, based on interviews and rigorous analysis of part attribute and performance data, suggest that in practice a part's service level is negatively affected by an item's cost and is less affected by attributes such as its priority code. We introduce a simple inventory model to explain our empirical findings and explore how variations in item attributes can interact with an inventory policy to affect system performance. Based on this model, we recommend using explicit service-level targets for priority categories to achieve performance consistent with part priority. We show, using military data, that a service differentiation strategy can be an effective way of allocating inventory investment by providing higher service for critical parts at the expense of accepting lower service levels for parts with less importance.", "e:keyword": ["Military", "Cost effectiveness: service differentiation", "Inventory", "Applications: service parts for weapon systems"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.531.16093", "e:abstract": "We investigate how performance scales in the standard <i>M/M/n</i> queue in the presence of growing congestion-dependent customer demand. We scale the queue by letting the potential (congestion-free) arrival rate be proportional to the number of servers, <i>n</i>, and letting <i>n</i> increase. We let the actual arrival rate with <i>n</i> servers be of the form (lambda)<i><sub>n</sub></i>=<i>f((xi)<sub>n</sub>)n</i>, where <i>f</i> is a strictly-decreasing continuous function and (xi)<i><sub>n</sub></i> is a steady-state congestion measure. We consider several alternative congestion measures, such as the mean waiting time and the probability of delay. We show, under minor regularity conditions, that for each <i>n</i> there is a unique equilibrium pair ((lambda)*<i><sub>n</sub></i>, (xi)*<i><sub>n</sub></i>) such that (xi)*<sub>n</sub> is the steady-state congestion associated with arrival rate (lambda)*<i>n</i> and (lambda)*<i>n</i>=<i>f((xi)*<sub>n</sub>)n</i>. Moreover, we show that, as<i> n</i> increases, the queue with the equilibrium arrival rate (lambda)*<i>n</i> is brought into heavy traffic, but the three different heavy-traffic regimes for multiserver queues identified by Halfin and Whitt (1981) each can arise depending on the congestion measure used. In considerable generality, there is asymptotic service efficiency: the server utilization approaches one as <i>n</i> increases. Under the assumption of growing congestion-dependent demand, the service efficiency can be achieved even if there is significant uncertainty about the potential demand, because the actual arrival rate adjusts to the congestion.", "e:keyword": ["Queues", "Multichannel: congestion-dependent demand", "Queues", "Limit theorems: heavy traffic", "Queues", "Markovian: multiserver"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.543.16101", "e:abstract": "Classical formulations of the portfolio optimization problem, such as mean-variance or Value-at-Risk (VaR) approaches, can result in a portfolio extremely sensitive to errors in the data, such as mean and covariance matrix of the returns. In this paper we propose a way to alleviate this problem in a tractable manner. We assume that the distribution of returns is partially known, in the sense that only <i>bounds</i> on the mean and covariance matrix are available. We define the worst-case Value-at-Risk as the largest VaR attainable, given the partial information on the returns' distribution. We consider the problem of computing and optimizing the worst-case VaR, and we show that these problems can be cast as semidefinite programs. We extend our approach to various other partial information on the distribution, including uncertainty in factor models, support constraints, and relative entropy information.", "e:keyword": ["Finance", "Portfolio: Value-at-Risk", "Portfolio optimization", "Programming", "Nonlinear: semidefinite programming", "Dualing", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.557.16094", "e:abstract": "This paper discusses mixed-integer programming formulations of variants of the discrete lot-sizing problem. Our approach is to identify simple mixed-integer sets within these models and to apply tight formulations for these sets. This allows us to define integral linear programming formulations for the discrete lot-sizing problem in which backlogging and/or safety stocks are present, and to give extended formulations for other cases. The results help significantly to solve test cases arising from an industrial application motivating this research.", "e:keyword": ["Inventory/production: scale-diseconomies/lot-sizing", "Discrete lot-sizing", "Production/scheduling: planning", "Integer programming: convex integer programming", "Facets", "Extended formulations"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.566.16106", "e:abstract": "Although the supply chain management literature is extensive, the benefits and challenges of coordinated decision making within supply chain <i>scheduling</i> models have not been studied. We consider a variety of scheduling, batching, and delivery problems that arise in an arborescent supply chain where a supplier makes deliveries to several manufacturers, who also make deliveries to customers. The objective is to minimize the overall scheduling and delivery cost, using several classical scheduling objectives. This is achieved by scheduling the jobs and forming them into batches, each of which is delivered to the next downstream stage as a single shipment. For each problem, we either derive an efficient dynamic programming algorithm that minimizes the total cost of the supplier or that of the manufacturer, or we demonstrate that this problem is intractable. The total system cost minimization problem of a supplier and manufacturer who make cooperative decisions is also considered. We demonstrate that cooperation between a supplier and a manufacturer may reduce the total system cost by at least 20%, or 25%, or by up to 100%, depending upon the scheduling objective. Finally, we identify incentives and mechanisms for this cooperation, thereby demonstrating that our work has practical implications for improving the efficiency of supply chains.", "e:keyword": ["Production/scheduling: sequencing", "Deterministic", "Single machine", "Multiple machine", "Manufacturing: performance/productivity", "Games/group decisions: cooperative", "Noncooperative"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.585.16095", "e:abstract": "We consider processing and shipment scheduling of a batch of size <i>M</i> jobs on a flexible (multifunctional) machine. All jobs in the batch require the same sequence of<i>N</i>operations on the machine. Costs are incurred in the forms of holding costs of jobs waiting for the next operation, setup costs whenever the machine is set up for a new operation, and shipment cost whenever the whole batch or a part of it is shipped to the customer. Using a dynamic programming formulation of the problem, we first show how the problem size increases in<i>M</i>and<i>N</i>. Then, by focusing on the properties of some classes of batch-splitting policies, a heuristic algorithm is presented that generates suboptimal policies. Some numerical results are provided which show that the algorithm performs very well.", "e:keyword": ["Production/scheduling", "Approximation/heuristic: flexible machine scheduling", "Transportation", "Scheduling: integrated production and transportation model"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.602.16097", "e:abstract": "This paper considers the problem of offering electricity produced by a series of hydroelectric reservoirs to a pool-type central market. The market model is a simplified version of the New Zealand wholesale electricity market, with prices modelled by a first-order Markov process. The demand for electricity is not explicitly modelled. The hydroelectric generator is assumed to be unable to influence market prices (i.e., to be a price-taker). We discuss the resulting stochastic dynamic program, methods for its solution, and the explicit optimal offer curves that it produces. It is shown that the utility function is monotone increasing with respect to both reservoir level and current price; however, the optimal offer curves need not be monotone. This is shown by example. Numerical results are provided.", "e:keyword": ["Dynamic programming: finite state", "Markov", "Probability: Markov processes", "Natural resources: energy", "Water resources"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.613.16107", "e:abstract": "We consider a problem of forest planning on pine plantations over a two to five year horizon. Basic decisions concern the areas to harvest in each period, the amount of timber to produce to satisfy aggregate demands for log exports, sawmills and pulp plants, and the roads to build for access and storage of timber. A linear programming model with 0--1 variables describes the decision process. Solution strategies involve strengthening of the model, lifting some of the constraints, and applying Lagrangean relaxation. Results on real planning problems show that even as these problems become more complex, the proposed solution strategies lead to very good solutions, reducing the residual gap for the most difficult data set from 162% to 1.6%, and for all data sets to 2.6% or less.", "e:keyword": ["Programming", "Integer: integer programming model for forest harvesting", "Programming", "Integer: relaxation: use of Lagrangean relaxation in forestry model", "Transportation", "Models", "Network: design road network for forest harvesting"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.629.16096", "e:abstract": "This paper presents an improved mixed-integer programming (MIP) model and effective solution strategies for the facility layout problem and is motivated by the work of Meller et al. (1999). This class of problems seeks to determine a least-cost layout of departments having various size and area requirements within a rectangular building, and it is challenging even for small instances. The difficulty arises from the disjunctive constraints that prevent departmental overlaps and the nonlinear area constraints for each department, which existing models have failed to approximate with adequate accuracy. We develop several modeling and algorithmic enhancements that are demonstrated to produce more accurate solutions while also decreasing the solution effort required. We begin by deriving a novel polyhedral outer approximation scheme that can provide as accurate a representation of the area requirements as desired. We also design alternative methods for reducing problem symmetry, evaluate the performance of several classes of valid inequalities, explore the construction of partial convex hull representations for the disjunctive constraints, and investigate judicious branching variable selection priority schemes. The results indicate a <i>substantial</i> increase in the accuracy of the layout produced, while at the same time providing a <i>dramatic</i> reduction in computational effort. In particular, three previously unsolved test problems from the literature for which Meller et al.'s algorithm terminated prematurely after 24 cpu hours of computation (on a SUN Ultra 2 workstation with 390 MB RAM) with respective optimality gaps of 10.14%, 26.45%, and 40%, have been solved to exact optimality with reasonable effort using our proposed approach.", "e:keyword": ["Facilities/equipment planning", "Layout: MIP model for the rectangular facility layout problem", "Programming", "Integer", "Cutting plane/facet: valid inequalities and disjunctive representations"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.645.16103", "e:abstract": "We consider a multiple-server loss model where customers arrive at a gatekeeper according to a Poisson process. A cost<i>c</i>is incurred if a new arrival is blocked from entering the system by the gatekeeper, while a larger cost <i>K</i> is incurred if an admitted customer finds all servers busy and therefore has to leave the system. The key assumption is that the gatekeeper is informed when an admitted customer finds all servers busy, but is not informed when served customers depart. Assuming an exponential service distribution, we show that, in the case of a single server, a threshold-type policy that blocks for a certain amount of time after a new arrival is admitted is optimal. When there are multiple servers, we propose two types of heuristic policies. We analytically compute the best policy of the first type, and use simulation to estimate that of the other.", "e:keyword": ["Dynamic programming/optimal control: applications in queueing systems", "Queues: admission control", "Optimal and heuristic policies"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.655.16098", "e:abstract": "We propose new cycle-based neighbourhood structures for metaheuristics aimed at the fixed-charge capacitated multicommodity network design formulation. The neighbourhood defines moves that explicitly take into account the impact on the total design cost of potential modifications to the flow distribution of several commodities simultaneously. Moves are identified through a shortest-pathlike network optimization procedure and proceed by redirecting flow around cycles and closing and opening design arcs accordingly. These neighbourhoods are evaluated and tested within a simple tabu search algorithm. Experimental results show that the proposed approach is quite powerful and outperforms existing methods reported in the literature.", "e:keyword": ["Networks/graphs", "Multicommodity: fixed-charge capacitated multicommodity network design", "Networks/graphs", "Heuristics: cycle-based neighbourhoods for metaheuristics", "Transportation models", "Network: static and dynamic applications in infrastructure and service design"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.4.668.16092", "e:abstract": "Consolidation of warehouses is a new trend in global logistics management, and the reduction in order processing and inventory costs is often cited as one of the main motivations. In this note we show that when retailers face constant demand rates and their ordering costs are independent of the warehouse that services them, consolidated systems are <i>rarely</i> suboptimal and <i>always</i> lead to close-to-optimal inventory replenishment costs. In particular, we prove that using two (one) properly selected warehouses, the systemwide inventory replenishment cost is in the worst case at most 2% (14.75%) more than the optimal.", "e:keyword": ["Inventory/production: multi-item/echelon/stage", "Deterministic", "Facilities/equipment planning: location", "Discrete"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.681.16755", "e:abstract": "Certain aspects of traffic flow measurements imply the existence of a phase transition. Models known from chaos and fractals, such as nonlinear analysis of coupled differential equations, cellular automata, or coupled maps, can generate behavior which indeed resembles a phase transition in the flow behavior. Other measurements point out that the same behavior could be generated by geometrical constraints of the scenario. This paper looks at some of the empirical evidence, but mostly focuses on different modeling approaches. The theory of traffic jam dynamics is reviewed in some detail, starting from the well-established theory of kinematic waves and then veering into the area of phase transitions. One aspect of the theory of phase transitions is that, by changing one single parameter, a system can be moved from displaying a phase transition to not displaying a phase transition. This implies that models for traffic can be tuned so that they display a phase transition or not.This paper focuses on microscopic modeling, i.e., coupled differential equations, cellular automata, and coupled maps. The phase transition behavior of these models, as far as it is known, is discussed. Similarly, fluid-dynamical models for the same questions are considered. A large portion of this paper is given to the discussion of extensions and open questions, which makes clear that the question of traffic jam dynamics is, albeit important, only a small part of an interesting and vibrant field. As our outlook shows, the whole field is moving away from a rather static view of traffic toward a dynamic view, which uses simulation as an important tool.", "e:keyword": ["Transportation", "Models", "Traffic", "Simulation: traffic flow models"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.711.16750", "e:abstract": "With the aim of contributing to the debate around OR/MS as a discipline, this study provides a historical comparative investigation of publicly available knowledge production in the field. The empirical investigation is based on a content analysis of 300 randomly selected articles from six major journals in the field. We have found: (1) since the late 1950s to the present day there has been no significant change in the types of published research in OR/MS in North America; (2) from the late 1950s to the present day, there have been significant differences in types of published research in OR/MS internationally. The imputed imbalance between theory and applications in published work had already occurred in the early stages of the development of OR/MS in North America and has since remained very much the same. Furthermore, research in the United Kingdom has been distinctly different from that dominant in North America and elsewhere. There are also indications that outside North America and the United Kingdom there is an emerging turn towards applications-oriented research. Over the last two or three decades there has been a significant increase overall in the share of articles published by academic authors.", "e:keyword": ["Professional"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.721.16758", "e:abstract": "Models in marketing with asymmetric reference effects lead to nonsmooth optimization problems and differential games which cannot be solved using standard methods. In this study, we introduce a new method for calculating explicitly optimal strategies, open-loop equilibria, and closed-loop equilibria of such nonsmooth problems. Application of this method to the case of asymmetric reference-price effects with loss-aversive consumers leads to the following conclusions: (1) When the planning horizon is infinite, after an introductory stage the optimal price stabilizes at a steady-state price, which is slightly below the optimal price in the absence of reference-price effects. (2) The optimal strategy is the same as in the symmetric case, but with the loss parameter determined by the initial reference-price. (3) Competition does not change the qualitative behavior of the optimal strategy. (4) Adopting an appropriate constant-price strategy results in a minute decline in profits.", "e:keyword": ["Marketing", "Pricing:optimal pricing with reference effects", "Games", "Differential:nonsmooth differential games", "Optimal control", "Applications: nonsmooth problems"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.735.16752", "e:abstract": "This paper presents a model for computing the parameters of an integrated inventory replenishment and outbound dispatch scheduling policy under dynamic demand considerations. The optimal policy parameters specify (i) how often and in what quantities to replenish the stock at an upstream supply chain member (e.g., a warehouse), and (ii) how often to release an outbound shipment to a downstream supply-chain member (e.g., a distribution center). The problem is represented using a two-echelon dynamic lot-sizing model with pre-shipping and late-shipping considerations, where outbound cargo capacity constraints are considered via a stepwise cargo cost function. Although the paper is motivated by a third-party warehousing application, the underlying model is applicable in the general context of coordinating inventory and outbound transportation decisions. The problem is challenging due to the stepwise cargo cost structure modeled. The paper presents several structural properties of the problem and develops a polynomial time algorithm for computing the optimal solution.", "e:keyword": ["Inventory/production", "Scale-diseconomies/lot-sizing", "Transportation", "Model assignment"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.748.16759", "e:abstract": "In this paper we establish the weakest known sufficient conditions for the existence of stationary delay moments in FIFO <i>GI/GI/s</i> queues, for <i>s</i>(ge)2. These conditions involve not only the service time distribution, as in the classic Kiefer and Wolfowitz conditions, but also the interplay of the traffic intensity and the number of servers in the queue. We then prove the necessity of our conditions for a large class of service times having finite first, but infinite (alpha)th, moment for some finite (alpha). Such service time distributions include many, but not all of, the class of heavy-tailed distributions: The Pareto and Cauchy are members; the Weibull is not.Our results are then applied to provide one answer to the classic question: When are <i>s</i> slow servers (operating at rate 1<i>/s</i>) better than one fast server (operating at rate 1)? We consider this question with respect to the rate of decay of the tail of stationary customer delay. In a system characterized by service times that have finite mean but lack some higher moments, such as are often used to model telecommunications traffic, for <i>s</i> greater than a traffic-related constant, the answer is <i>always</i>. Our results help to quantify the benefits of extra servers, while also pointing the way towards the derivation of bounds and asymptotics for the stationary delay distribution of multiserver queues having these types of service times. Such queues are attracting a great deal of academic research, motivated by their practical use modeling telecommunications systems.", "e:keyword": ["Probability", "Random walk", "Stochastic model application", "Queues: limit theorems", "Multichannel", "Computers: system design and operation"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.759.16753", "e:abstract": "We study a variant of the classical bin-packing problem, the ordered open-end bin-packing problem, where first a bin can be filled to a level above 1 as long as the removal of the last piece brings the bin's level back to below 1 and second, the last piece is the largest-indexed piece among all pieces in the bin. We conduct both worst-case and average-case analyses for the problem. In the worst-case analysis, pieces of size 1 play distinct roles and render the analysis more difficult with their presence. We give lower bounds for the performance ratio of any online algorithm for cases both with and without the 1-pieces, and in the case without the 1-pieces, identify an online algorithm whose worst-case performance ratio is less than 2 and an offline algorithm with good worst-case performance. In the average-case analysis, assuming that pieces are independently and uniformly drawn from [0, 1], we find the optimal asymptotic average ratio of the number of occupied bins over the number of pieces. We also introduce other online algorithms and conduct simulation study on the average-case performances of all the proposed algorithms.", "e:keyword": ["Mathematics: combinatorics", "Probability: stochastic model applications", "Transportation: costs"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.771.16749", "e:abstract": "We present and study a three-stage model of a decentralized distribution system consisting of <i>n</i> retailers, each of whom faces a stochastic demand for an identical product. In the first stage, before the demand is realized, each retailer independently orders her initial inventory. In the second stage, after the demand is realized, each retailer decides how much of her residual supply/demand she wants to share with the other retailers. In the third stage, residual inventories are transshipped to meet residual demands, and an additional profit is allocated. Our model is an extension of the two-stage model of Anupindi et al. (ABZ) (2001), which implicitly assumes that all residuals enter the transshipment stage. We show, however, that allocation rules in the third stage based on dual solutions, which were used in the ABZ model, may induce the retailers to hold back some of their residual supply/demand. In general, we study the effect of implementing various allocations rules in the third stage on the values of the residual supply/demand the retailers are willing to share with others in the second stage, and the trade-off involved in achieving an optimal solution for the corresponding centralized system.", "e:keyword": ["Games: cooperative", "Noncooperative", "Inventory/production: multistage"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.785.16756", "e:abstract": "The inverse spanning-tree problem is to modify edge weights in a graph so that a given tree <i>T</i> is a minimum spanning tree. The objective is to minimize the cost of the deviation. The cost of deviation is typically a convex function. We propose algorithms here that are faster than all known algorithms for the problem. Our algorithm's run time for any convex inverse spanning-tree problem is <i>O</i>(<i>nm</i> log<sup>2</sup><i>n</i>) for a graph on <i>n</i> nodes and <i>m</i> edges plus the time required to find the minima of the <i>n</i> convex deviation functions. This not only improves on the complexity of previously known algorithms for the general problem, but also for algorithms devised for special cases. For the case when the objective is weighted absolute deviation, Sokkalingam et al. (1999) devised an algorithm with run time <i>O</i>(<i>n</i><sup>2</sup><i>m</i> log(<i>nC</i>)) for <i>C</i> the maximum edge weight. For the sum of absolute deviations our algorithm runs in time <i>O</i>(<i>n</i><sup>2</sup> log <i>n</i>), matching the run time of a recent (Ahuja and Orlin 2000) improvement for this case. A new algorithm for bipartite matching in path graphs is reported here with complexity of <i>O</i>(<i>n</i><sup>1.5</sup> log <i>n</i>). This leads to a second algorithm for the sum of absolute deviations running in <i>O</i>(<i>n</i><sup>1.5</sup> log <i>n</i> log <i>C</i>) steps.", "e:keyword": ["Programming: integer", "Algorithms", "Optimization in integers", "Network/graphs: flow algorithms", "Parametric minimum cut", "Mathematics: convexity", "Convex optimization problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.798.16748", "e:abstract": "We design an algorithm for the high-multiplicity job-shop scheduling problem with the objective of minimizing the total holding cost by appropriately rounding an optimal solution to a fluid relaxation in which we replace discrete jobs with the flow of a continuous fluid. The algorithm solves the fluid relaxation optimally and then aims to keep the schedule in the discrete network close to the schedule given by the fluid relaxation. If the number of jobs from each type grow linearly with <i>N</i>, then the algorithm is within an additive factor <i>O</i>(<i>N</i>) from the optimal (which scales as <i>O</i>(<i>N</i><sup>2</sup>)); thus, it is asymptotically optimal. We report computational results on benchmark instances chosen from the OR library comparing the performance of the proposed algorithm and several commonly used heuristic methods. These results suggest that for problems of moderate to high multiplicity, the proposed algorithm outperforms these methods, and for very high multiplicity the overperformance is dramatic. For problems of low to moderate multiplicity, however, the relative errors of the heuristic methods are comparable to those of the proposed algorithm, and the best of these methods performs better overall than the proposed method.", "e:keyword": ["Production/scheduling", "Deterministic: approximation algorithms for deterministic job shops", "Queues optimization: asymptotically optimal solutions to queueing networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.814.16751", "e:abstract": "In this paper we address the problem of finding the simulated system with the best (maximum or minimum) expected performance when the number of systems is large and initial samples from each system have already been taken. This problem may be encountered when a heuristic search procedure---perhaps one originally designed for use in a deterministic environment---has been applied in a simulation-optimization context. Because of stochastic variation, the system with the best sample mean at the end of the search procedure may not coincide with the true best system encountered during the search. This paper develops statistical procedures that return the best system encountered by the search (or one near the best) with a prespecified probability. We approach this problem using combinations of statistical subset selection and indifference-zone ranking procedures. The subset-selection procedures, which use only the data already collected, screen out the obviously inferior systems, while the indifference-zone procedures, which require additional simulation effort, distinguish the best from the less obviously inferior systems.", "e:keyword": ["Simulation", "Statistical analysis: selecting the best system", "Simulation", "Efficiency: large-scale screening", "Programming/stochastic: terminal inference"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.5.826.16757", "e:abstract": "We consider a knapsack problem in which each item has two types of weight and the container has two types of capacity. We discuss the surrogate, Lagrangian, and continuous relaxations, and give an effective method to determine the optimal Lagrangian and surrogate multipliers associated with the continuous relaxation of the problem. These results are used to obtain an exact branch-and-bound algorithm, which also includes heuristic procedures and a reduction technique. The performance of bounds and algorithms is evaluated through extensive computational experiments.", "e:keyword": ["Programming", "Integer: algorithms", "Branch-and-bound", "Relaxation: two-constraint 0--1 knapsack problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.839.24917", "e:abstract": "We study strategic capacity planning in the semiconductor industry. Working with a major US semiconductor manufacturer on the configuration of their worldwide production facilities, we identify two unique characteristics of this problem as follows: (1) wafer demands and manufacturing capacity are both main sources of uncertainty, and (2) capacity planning must consider the distinct viewpoints from marketing and manufacturing. We formulate a multi-stage stochastic program with demand and capacity uncertainties. To reconcile the marketing and manufacturing perspectives, we consider a decomposition of the planning problem resembling decentralized decision-making. We develop recourse approximation schemes representing different decentralization schemes, which vary in information requirements and complexity. We show that it is possible to arrive at near optimal solutions (within 6.5%) with information decentralization while using a fraction (16.2%) of the computer time.", "e:keyword": ["Facilities/equipment planning", "Capacity expansion: strategic capacity planning", "Programming/stochastic: scenario analysis", "Recourse approximation", "Production", "Planning: decentralized coordination"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.850.24925", "e:abstract": "The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such problems. The approach “fits” a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function. We develop error bounds that offer performance guarantees and also guide the selection of both basis functions and “state-relevance weights” that influence quality of the approximation. Experimental results in the domain of queueing network control provide empirical support for the methodology.", "e:keyword": ["Dynamic programming/optimal control: approximations/large-scale problems", "Queues", "Algorithms: control of queueing networks"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.866.24918", "e:abstract": "In this paper we present a canonical setting that illustrates the need for explicitly modeling interactions between manufacturing and marketing/sales decisions in a firm. We consider a firm that sells an innovative product with a given market potential. The firm may not be able to meet demand due to capacity constraints. For such firms, we present a new model of demand, modified from the original model of Bass, to capture the effect of unmet past demand on future demand. We use this model to find production and sales plans that maximize profit during the lifetime of the product in a firm with a fixed production capacity. We conduct an extensive numerical study to establish that the trivial, myopic sales plan that sells the maximal amount possible at each time instant is not necessarily optimal. We show that a heuristic “build-up” policy, in which the firm does not sell at all for a period of time and builds up enough inventory to never lose sales once it begins selling, is a robust approximation to the optimal policy. Specializing to a lost-sales setting, we prove that the optimal sales plan is indeed of the “build-up” type. We explicitly characterize the optimal build-up period and analytically derive the optimal initial inventory and roll-out delay. Finally, we show that the insights obtained in the fixed capacity case continue to hold when the firm is able to dynamically change capacity.", "e:keyword": ["Production/scheduling", "Planning: optimal sales plans", "Marketing", "New products: modifying Bass model for supply constraints", "Dynamic programming/optimal control", "Application: application of maximum principle"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.880.24924", "e:abstract": "A formulation for the commitment of electric power generators under a deregulated electricity market is proposed. The problem is expressed as a stochastic optimization problem in which expected profits are maximized while meeting demand and standard operating constraints. Under an assumption of perfect competition, when an electric power producer has the option of trading electricity at market prices, a unit commitment schedule can be obtained by optimizing the self-commitment of each unit separately subject to stochastic prices. Three certainty-equivalent formulations of the stochastic self-commitment problem are provided. The procedures involve application of dynamic programming, statistical analysis, and asymptotic probability computations. The price of electricity is represented by a stochastic model depending on demand, generating unit reliabilities, and temperature fluctuations. We use several approximation methods (normal, Edgeworth series expansion, and Monte Carlo simulation) for computing the required probability distributions. Numerical examples are provided for a market consisting of 150 generating units.", "e:keyword": ["Decision analysis: unit commitment decisions under uncertainty", "Production/scheduling: electric power generation under deregulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.894.24919", "e:abstract": "When considering a decision tree for the purpose of classification, accuracy is usually the sole performance measure used in the construction process. In this paper, we introduce the idea of combining a decision tree's expected value and variance in a new probabilistic measure for assessing the performance of a tree. We develop a genetic algorithm for constructing a tree using our new measure and conduct computational experiments that show the advantages of our approach. Further, we investigate the effect of introducing diversity into the population used by our genetic algorithm. We allow the genetic algorithm to simultaneously focus on two distinct probabilistic measures---one that is risk averse and one that is risk seeking. Our bivariate genetic algorithm for constructing a decision tree performs very well, scales up quite nicely to handle data sets with hundreds of thousands of points, and requires only a small percent of the data to generate a high-quality decision tree. We demonstrate the effectiveness of our algorithm on three large data sets.", "e:keyword": ["Statistics", "Data analysis: data mining", "Marketing", "Estimation/statistical techniques: decision trees", "Computers/computer science", "Artificial intelligence: genetic algorithms"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.908.24922", "e:abstract": "Motivated by the problem of efficient estimation of expected cumulative rewards or cashflows, this paper proposes and analyzes a variance reduction technique for estimating the expectation of the sum of sequentially simulated random variables. In some applications, simulation effort is of greater value when applied to early time steps rather than shared equally among all time steps; this occurs, for example, when discounting renders immediate rewards or cashflows more important than those in the future. This suggests that deliberately stopping some paths early may improve efficiency. We formulate and solve the problem of optimal allocation of resources to time horizons with the objective of minimizing variance subject to a cost constraint. The solution has a simple characterization in terms of the convex hull of points defined by the covariance matrix of the cashflows. We also develop two ways to enhance variance reduction through early stopping. One takes advantage of the statistical theory of missing data. The other redistributes the cumulative sum to make optimal use of early stopping.", "e:keyword": ["Simulation", "Efficiency: variance reduction", "Statistics", "Design of experiments: data missing by design", "Finance", "Asset pricing: computational methods"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.922.24914", "e:abstract": "In this paper, we propose a multiperiod single-sourcing problem (MPSSP), which takes both transportation and inventory into consideration, suitable for evaluating the performance of a logistics distribution network in a dynamic environment. We reformulate the MPSSP as a Generalized Assignment Problem (GAP) with a convex objective function. We then extend a branch-and-price algorithm that was developed for the GAP to this problem. The pricing problem is a so-called Penalized Knapsack Problem (PKP), which is a knapsack problem where the objective function includes an additional convex term penalizing the total use of capacity of the knapsack. The optimal solution of the relaxation of the integrality constraints in the PKP shows a similar structure to the optimal solution of the knapsack problem, that allows for an efficient solution procedure for the pricing problem. We perform an extensive numerical study of the branch-and-price algorithm.", "e:keyword": ["Production/distribution: integrating production distribution and inventory", "Integer programming: branch-and-price algorithm and penalized knapsack problem"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.940.24921", "e:abstract": "The purpose of this paper is to present a branch-and-cut algorithm for the undirected <i>Traveling Purchaser Problem</i> which consists of determining a minimum-cost route through a subset of markets, where the cost is the sum of travel and purchase costs. The problem is formulated as an integer linear program, and several families of valid inequalities are derived to strengthen the linear relaxation. The polyhedral structure of the formulation is analyzed and several classes of valid inequalities are proved to be facet defining. A branch-and-cut procedure is developed and tested over four classes of randomly generated instances. Results show that the proposed algorithm outperforms all previous approaches and can optimally solve instances containing up to 200 markets.", "e:keyword": ["Transportation: vehicle routing", "Networks/graphs: traveling salesman"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.952.24913", "e:abstract": "This paper is concerned with the design of dynamic server assignment policies that maximize the capacity of queueing networks with flexible servers. Flexibility here means that each server may be capable of performing service at several different classes in the network. We assume that the interarrival times and the service times are independent and identically distributed, and that routing is probabilistic. We also allow for server switching times, which we assume to be independent and identically distributed. We deduce the value of a tight upper bound on the achievable capacity by equating the capacity of the queueing network model with that of a limiting deterministic fluid model. The maximal capacity of the deterministic model is given by the solution to a linear programming problem that also provides optimal allocations of servers to classes. We construct particular server assignment policies, called generalized round-robin policies, that guarantee that the capacity of the queueing network will be arbitrarily close to the computed upper bound. The performance of such policies is studied using numerical examples.", "e:keyword": ["Queues: networks", "Optimization", "Manufacturing: performance", "Productivity", "Production/scheduling: flexible manufacturing/line balancing"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.969.24920", "e:abstract": "Since Clark and Scarf's pioneering work, most advances in multiechelon inventory systems have been based on demand processes that are time independent. This paper revisits the serial multiechelon inventory system of Clark and Scarf and develops three key results. First, we provide a simple lower-bound approximation to the optimal echelon inventory levels and an upper bound to the total system cost for the basic model of Clark and Scarf. Second, we show that the structure of the optimal stocking policy of Clark and Scarf holds under time-correlated demand processes using a Martingale model of forecast evolution. Third, we extend the approximation to the time-correlated demand process and study, in particular for an autoregressive demand model, the impact of lead times and autocorrelation on the performance of the serial inventory system.", "e:keyword": ["Inventory/production: approximations", "Multiechelon", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.981.24912", "e:abstract": "We present an in-depth theoretical, algorithmic, and computational study of a linear programming (LP) relaxation to the precedence constrained single-machine scheduling problem 1|prec|(Sigma)<i><sub>j</sub>w<sub>j</sub>C<sub>j</sub></i> to minimize a weighted sum of job completion times. On the theoretical side, we study the structure of tight parallel inequalities in the <b>LP</b> relaxation and show that <i>every</i> permutation schedule that is consistent with Sidney's decomposition has total cost no more than twice the optimum. On the algorithmic side, we provide a parametric extension to Sidney's decomposition and show that a finest decomposition can be obtained by essentially solving a parametric minimum-cut problem. Finally, we report results obtained by an algorithm based on these developments on randomly generated instances with up to 2,000 jobs.", "e:keyword": ["Network/graphs", "Flow algorithms: parametric flows and Sidney decompositions", "Production/scheduling", "Approximations: 2-approximation algorithm", "Programming", "Integer", "Algorithms", "Relaxation/subgradient: integer formulation"]}, {"@id": "http://dx.doi.org/10.1287/opre.51.6.993.24923", "e:abstract": "We consider a periodic review inventory system with two priority demand classes, one deterministic and the other stochastic. The deterministic demand must be met immediately in each period. However, the units of stochastic demand that are not satisfied during the period when demand occurs are treated as lost sales. At each decision epoch, one has to decide not only whether an order should be placed and how much to order, but also how much demand to fill from the stochastic source. The firm has the option to ration inventory to the stochastic source (i.e., not satisfy all customer demand even though there is inventory in the system).We first characterize the structure of the optimal policy. We show that, in general, the optimal order quantity and rationing policy are state dependent and do not have a simple structure. We then propose a simple policy, called (<i>s, k, S</i>) policy, where <i>s</i> and <i>S</i> (ordering policy) determine when and how much to order, while <i>k</i> (rationing policy) specifies how much of the stochastic demand to satisfy. We report the results of a numerical study, which shows that this simple policy works extremely well and is very easy to compute.", "e:keyword": ["Inventory/production: policies", "Inventory/production: deterministic/stochastic", "Inventory/production: approximations/heuristics"]}, {"@id": "http://dx.doi.org/10.1287/opre.53.6.1027", "e:keyword": []}]