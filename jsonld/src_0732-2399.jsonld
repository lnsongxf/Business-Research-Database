[{"@id": "http://dx.doi.org/10.1287/mksc.1070.0335", "e:abstract": "This paper investigates the effect of product substitutability on Nash equilibrium distribution structures in a duopoly where each manufacturer distributes its goods through a single exclusive retailer, which may be either a franchised outlet or a factory store. Static linear demand and cost functions are assumed, and a number of rules about players' expectations of competitors' behavior are examined. It is found that for most specifications product substitutability does influence the equilibrium distribution structure. For low degrees of substitutability, each manufacturer will distribute its product through a company store; for more highly competitive goods, manufacturers will be more likely to use a decentralized distribution system.", "e:keyword": ["Channel management", "Distribution", "Vertical integration", "Industry analysis", "Game", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0331", "e:abstract": "A multinomial logit model of brand choice, calibrated on 32 weeks of purchases of regular ground coffee by 100 households, shows high statistical signficance for the explanatory variables of brand loyalty, size loyalty, presence/absence of store promotion, regular shelf price and promotional price cut. The model is parsimonious in that the coefficients of these variables are modeled to be the same for all coffee brand-sizes. The calibrated model predicts remarkably well the share of purchases by brand-size in a hold-out sample of 100 households over the 32-week calibration period and a subsequent 20-week forecast period. The success of the model is attributed in part to the level of detail and completeness of the household panel data employed, which has been collected through optical scanning of the Universal Product Code in supermarkets. Three short-term market response measures are calculated from the model: regular (depromoted) price elasticity of share, percent increase in share for a promotion with a median price cut, and promotional price cut elasticity of share. Response varies across brand-sizes in a systematic way with large share brand-sizes showing less response in percentage terms but greater in absolute terms. On the basis of the model a quantitative picture emerges of groups of loyal customers who are relatively insensitive to marketing actions and a pool of switchers who are quite sensitive.", "e:keyword": ["Choice", "Logit", "Marketing-mix", "Scanners"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0332", "e:abstract": "A channel of distribution consists of different channel members each having his own decision variables. However, each channel member's decisions do affect the other channel members' profits and, as a consequence, actions. A lack of coordination of these decisions can lead to undesirable consequences. For example, in the simple manufacturer-retailer-consumer channel, uncoordinated and independent channel members' decisions over margins result in a higher price paid by the consumer than if those decisions were coordinated. In addition, the ensuing suboptimal volume leads to lower profits for both the manufacturer and the retailer. This paper explores the problems inherent in channel coordination. We address the following questions. What is the effect of channel coordination? What causes a lack of coordination in the channel? How difficult is it to achieve channel coordination? What mechanisms exist which can achieve channel coordination? What are the strengths and weaknesses of these mechanism? What is the role of nonprice variables (e.g., manufacturer advertising, retailer shelf-space) in coordination? Does the lack of coordination affect normative implications from in-store experimentation? Can quantity discounts be a coordination mechanism? Are some marketing practices actually disguised quantity discounts? We review the literature and present a simple formulation illustrating the roots of the coordination problem. We then derive the form of the quantity discount schedule that results in optimum channel profits.", "e:keyword": ["Distribution", "Profitability", "Margins", "Coordination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0334", "e:abstract": "This paper analyzes how a firm should adjust its marketing expenditures and its price to defend its position in an existing market from attack by a competitive new product. Our focus is to provide usable managerial recommendations on the strategy of response. In particular we show that if products can be represented by their position in a multiattribute space, consumers are heterogeneous and maximize utility, and awareness advertising and distribution can be summarized by response functions, then for the profit maximizing firm: it is optimal to decrease awareness advertising; it is optimal to decrease the distribution budget unless the new product can be kept out of the market; a price increase may be optimal; and, even under the optimal strategy, profits decrease as a result of the competitive new product. Furthermore, if the consumer tastes are uniformly distributed across the spectrum: a price decrease increases defensive profits, it is optimal (at the margin) to improve product quality in the direction of the defending product's strength, and, it is optimal (at the margin) to reposition by advertising in the same direction. In addition we provide practical procedures to estimate (1) the distribution of consumer tastes and (2) the position of the new product in perceptual space from sales data and knowledge of the percent of consumers who are aware of the new product and find it available. Competitive diagnostics, such as the angle of attack, are introduced to help the defending manager.", "e:keyword": ["Competition", "Pricing", "Product entry", "Defensive marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0336", "e:abstract": "This paper considers the pricing decision faced by a producer of a commodity with a short shelf or demand life. A hierarchical model is developed, and the results of the single period inventory model are used to examine possible pricing and return policies. The paper shows that several such policies currently in effect are suboptimal. These include those where the manufacturer offers retailers full credit for all unsold goods or where no returns of unsold goods are permitted. The paper also demonstrates that a policy whereby a manufacturer offers retailers full credit for a partial return of goods may achieve channel coordination, but that the optimal return allowance will be a function of retailer demand. Therefore, such a policy cannot be optimal in a multi-retailer environment. It is proven, however, that a pricing and return policy in which a manufacturer offers retailers a partial credit for all unsold goods can achieve channel coordination in a multi-retailer environment.", "e:keyword": ["Distribution", "Coordination", "Channel", "Newsboy problem", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0330", "e:abstract": "A new model of consumer behavior is developed using a hybrid of cognitive psychology and microeconomics. The development of the model starts with the mental coding of combinations of gains and losses using the prospect theory value function. Then the evaluation of purchases is modeled using the new concept of transaction utility. The household budgeting process is also incorporated to complete the characterization of mental accounting. Several implications to marketing, particularly in the area of pricing, are developed.", "e:keyword": ["Mental accounting", "Consumer choice", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0333", "e:abstract": "This descriptive study explores the reasons for integration of the personal selling function, i.e., the use of employee (direct) salespeople rather than manufacturers' representatives (reps). A hypothesized model is developed based on both transaction cost analysis and the sales force management literature. Data from 13 electronic component manufacturers covering 159 U.S. sales districts are used to estimate a logistic model of the probability of going direct in a given district. Results are shown to be stable across specification and estimation methods and to fit the data well. The transaction cost model is generally supported. The principal finding is that the greater the difficulty of evaluating a salesperson's performance, the more likely the firm to substitute surveillance for commission as a control mechanism, i.e., to use a direct sales force. Among other findings, direct sales forces are also associated with complex, hard-to-learn product lines and with districts that demand considerable nonselling activities. Several factors prove unrelated, including company size, the amount of travel a district requires, and the importance of key accounts.", "e:keyword": ["Organizational design", "Organization control", "Sales force management", "Vertical integration"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.1", "e:abstract": "This study is to present a new method for constructing a perceptual map based on logit analysis. This is an extension of the explosion logit model of an individual choice to a problem of perceptual mapping giving rise to advantages over existing methods in various aspects. Firstly, input data format is flexible and user-friendly. Unlike traditional methods which typically requires a total dissimilarity ordering on all the pairs of the objects, it allows a partial ordering to any rank depth and a pivot ordering which gives a dissimilarity ordering of objects with respect to a specific pivot or anchored object. Secondly, it introduces the concept of an evoked set into perceptual mapping and allows a respondent to make judgments only within his/her evoked set. This, together with the first point, contributes to the reduction of the respondents' burden and the data noise. Thirdly, Monte Carlo experiments have revealed that our ML estimates give a considerably better fit to data than ALSCAL, one of the most widely used algorithms of MDS. Particularly worth noting is that our method using the data ranked only up to the top two thirds gives as good a fit as ALSCAL using totally ordered data. Although not exploited here, opening up a way for statistical inference in nonmetric MDS would be yet another advantage. An empirical application is made to a Canadian car market for illustration.", "e:keyword": ["Product positioning", "Perceptual mapping", "Nonmetric MDS", "Logit model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.18", "e:abstract": "The ability of consumers to make rational sequential purchase quantity decisions under imperfect knowledge about future prices in a product category is explored. Normatively, a consumer should make such decisions by defining a series of reservation prices which define how many buying-periods' supply should be held given an observed price. An experiment is reported in which consumers make sequential purchase quantity decisions under variations in the shape of the distribution of prices and its trend over time. Results suggest a number of systematic deviations from optimality. When facing stationary uniform and bimodal price distributions, for example, subjects tend to systematically overbuy when small purchases are called for and underbuy when large purchases are called for. Likewise, when facing ascending or descending series, behavior is the of that predicted by the normative analysis: there is an increasing tendency to defer purchases given increasing prices and accelerate purchases given descending prices. An explanation for the findings in terms of a Prospect Theory-type loss function in expenditures is offered.", "e:keyword": ["Stockpiling", "Price expectations", "Decision making under uncertainty", "Sequential decisions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.42", "e:abstract": "A method for obtaining early forecasts for the sales of new durable products based on Hierarchical Bayes procedures is presented. The Bass model is implemented within this framework by using a nonlinear regression approach. The linear regression model has been shown to have numerous shortcomings. Two stages of prior distributions use sales data from a variety of dissimilar new products. The first prior distribution describes the variation among the parameters of the products, and the second prior distribution expresses the uncertainty about the hyperparameters of the first prior. Before observing sales data for a new product launch, the forecasts are the expectation of the first stage prior distribution. As sales data become available, the forecasts adapt to the unique features of the product. Early forecasting and the adaptive capability are the two major payoffs from using Hierarchical Bayes procedures. This contrasts with other estimation approaches which either use a linear model or provide reasonable forecasts only after the inflection point of the time series of the sales. The paper also indicates how the Hierarchical Bayes procedure can be extended to include exogenous variables.", "e:keyword": ["New product diffusion", "Bayes methods", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.54", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.56", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.58", "e:abstract": "Because new products are critical to success and survival, the evaluation of new product ideas receives much attention. In the early stages of product development, concept evaluation models are used to predict market share and/or sales of hypothetical product concepts. These models have focused on consumer preferences, brand choice, and market share. Individual demand or usage rates have been considered exogenous. This paper presents a concept evaluation model which considers the effects of a new product on both market share and demand for the product class. A conceptual framework for the relationship between new product entries and product class demand is presented and then formulated into a mathematical model. An empirical application of the model illustrates some of the possible methods for model estimation and testing.", "e:keyword": ["New product models", "Forecasting", "Market growth", "Product class attraction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.74", "e:abstract": "While unobservable factors such as corporate culture, access to scarce resources, management skill, and luck can be postulated to be principal determinants of business success, their effects are all but ignored in studies of business performance. This study, making use of the PIMS data base, reports empirical evidence indicating that failure to control for unobservable factors influencing profitability both biases and exaggerates the effect of strategic factors. Indeed, the influence of unobservable factors is so pervasive as to invalidate many of the conclusions drawn from studies failing to control for their effects.", "e:keyword": ["Business performance", "Marketing strategy", "Unobservable effects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.86", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.88", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.9.1.92", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.9.2.97", "e:abstract": "The problem of optimizing advertising across a broad product range, where significant cross elasticities are likely, is explored. A linear-in-logs model is proposed and allocation rules for budgeting across products and media are derived. The model is estimated for a large European variety store chain. The results suggest profits could be increased by nearly 40 percent with no change in the advertising budget, if management shifted from the conventional bottom-up approach to the more systematic method outlined. The final section describes how management implemented the model.", "e:keyword": ["Advertising", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.2.114", "e:abstract": "We present a model that leads to an equilibrium with characteristics similar to the following stylized facts observed in retail markets: (a) Retailers advertise only selected brands; (b) Often low priced advertised brands are understocked; (c) In-store promotions are biased towards more expensive substitute brands. Together these practices constitute illegal bait and switch. Is this phenomenon necessarily harmful to consumers and to the economy? We show that bait and switch can benefit consumers because utility is created through in-store promotions and price competition is enhanced. This suggests that the FTC investigate further its ban on bait and switch.", "e:keyword": ["Pricing", "Promotion", "Regulation", "Bait and switch"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.2.125", "e:abstract": "An econometric market response model for measuring the effect of coupon promotions upon market share is developed and estimated. In addition to the brand's own couponing efforts, the model takes into account retailer promotions for the brand as well as competitive couponing activity. The model is multi-equation, simultaneous, and is estimated using scanner panel data. Results indicate that coupons have a pronounced effect upon market share, although the effect varies from brand to brand and may not be strong enough for some brands for couponing to be profitable in the short term.", "e:keyword": ["Market response models", "Sales promotion", "Couponing", "Coupon profitability"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.2.146", "e:abstract": "This paper presents a formal model of retail demand, based upon a household production framework, that permits rigorous analysis of retail assortments. The model captures the essential shifting of distribution costs between retailers and consumers that manifests itself in retailers' provision of distribution services for consumers. We apply the model to define rigorously conventional retailing concepts, such as assortment breadth and depth, and to explain the appearance of certain well-known phenomena such as market-basket pricing, nonprice retail competition, and the incentives for retail agglomerations to form. Throughout, distribution services play a critical role in binding together the analytical model and the diverse reality giving rise to retail assortments.", "e:keyword": ["Retail assortments", "Distribution services", "Consumption activities", "Demand complementarities"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.2.162", "e:abstract": "Two types of purchase timing modelsthose which model purchase incidence (whether or not) and those which model interpurchase time (when)are examined. We show that modelling purchase incidence is almost always preferable to modelling interpurchase time. We also derive upper bound <sup>2</sup>'s for purchase incidence models. These <sup>2</sup>'s serve as useful benchmarks for assessing the goodness-of-fit of empirical choice models.", "e:keyword": ["Consumer behavior", "Probabilistic models", "Interpurchasing times", "R2 upper bounds", "Continuous models/discrete prediction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.2.171", "e:abstract": "The dynamic evolvement of market share of a brand in a frequently purchased product category can be driven by two effects, purchase reinforcement, and advertising carryover. The first depends on the actual experience with the brand while the second on the retention of its producer's messages. We present a model to estimate simultaneously the relative magnitude of these two forces. The model also formally treats the issue of temporal aggregation. Our empirical results with several product categories indicate that for monthly and bi-monthly measurement periods, purchase reinforcement dominates carryover of advertising in affecting the evolution of market share.", "e:keyword": ["Advertising", "Lagged effect", "Purchase feedback"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.3.189", "e:abstract": "Demand for variety may arise from a taste for diversity in individual consumption and/or from diversity in tastes when each consumer chooses a single variant. The full degree of variety potentially demanded will not, in general, be supplied because scale economies (even to a small degree) mean that the potential welfare or revenue gain from greater variety must be balanced against the lower unit production costs with fewer variants. The economics of product variety consists essentially in analysing the effect of this balance in different situations, and comparing the degree of product variety for different market structures with each other and with the optimum. The survey commences with the work on market structures with single product firms (generalized monopolistic competition), tracing modern developments in both the Chamberlinian and Hotelling traditions. The latter has been particularly fruitful, due to the expansion of the original locational ideas into virtual spaces in product characteristics. The emphasis in recent work on product variety has been on multiproduct firms in both monopolistic and oligopolistic structures, including strategic market preemption. Although most work has been in a full information context, there have been advances in product variety under imperfect information (either by consumers to properties of the firms' products or firms as to consumers' demands).", "e:keyword": ["Product differentiation", "Product variety", "Monopolistic competition", "Oligopoly"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.3.207", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.9.3.212", "e:abstract": "In recent years, artificial intelligence research has provided new tools and techniques for marketing model builders. These tools, when combined with problem-solving knowledge from a specific domain, can be used to create expert systems. This methodology is most applicable in semistructured problem domains where the key relationships are logical rather than numerical and problem-solving knowledge is incomplete. One such problem in marketing is advertising design. In this paper, we describe the application of expert system techniques to the development of ADCAD, a system designed to assist advertisers of consumer products with the formulation of advertising objectives, copy strategy, and the selection of communication approaches. The paper highlights the assembly of ADCAD's knowledge base of ad design heuristics from multiple sources of published research and practitioner expertise and describes its structure and content. We discuss the procedures and obstacles associated with building, implementing, and validating such an expert system. Our experiences with ADCAD suggest that knowledge-based systems have significant potential for the consolidation and integration of marketing knowledge as interactive input for decision makers. We conclude by noting several avenues for the future development of advertising expert systems.", "e:keyword": ["Artificial intelligence", "Expert systems", "Advertising", "Creativity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.3.230", "e:abstract": "In this paper we examine the theoretical and empirical issues that arise for a decision maker who wishes to use the target population's preferences as an input for designing new products and services. The marketing literature suggests using essentially the same approach as that for the problem, with one exception. Instead of maximizing profits, share or volume, the to society is maximized. However, the social welfare literature has suggested several other measures of welfare and shows that no single welfare function dominates all others. Therefore, to choose an appropriate welfare function to maximize the decision maker must first examine the theoretical and empirical similarities between the alternatives. For six prominent welfare functions, we examine the theoretical differences in terms of the notion of promoted and the kind of . Empirically, we examine the similarities in social rankings and first choices induced by these welfare functions under various preference configurations. We also examine their sensitivity to measurement error and preference normalization.", "e:keyword": ["New products", "Social welfare"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.3.247", "e:abstract": "In this paper, we explore equilibrium pricing strategies in an infinite horizon repeated game for an oligopoly. We model the interactions between three firms in a market of switchers and loyals. Our analysis shows that if, there are sufficiently large number of switchers in the market, the demand parameter is within an acceptable range, and the firms have a sufficiently low discount rate, alternating promotions between national firms can be an outcome of a perfect Nash equilibrium between competing firms. This equilibrium results in an implicit collusion between national firms and therefore suggests that price promotions can be interpreted as a long run strategy pursued by national firms to defend their market shares from possible encroachments of a third firm. In this way we attempt to provide an explanation for the patterns of promotion observed in some industries and in particular the beverage industry.", "e:keyword": ["Price promotions", "Competition", "Cooperation", "Repeated games"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.3.263", "e:abstract": "Theories of exploratory behavior suggest that inertia and variety-seeking tendencies may coexist within the individual, implying that the same individual may exhibit inertia and variety-seeking at different times depending on his/her choice history. Past research has not allowed for such -consumer variability in these tendencies. The purpose of this study is to present a choice model that allows us to identify such hybrid behavior (i.e., a mixture of inertia and variety-seeking), and to distinguish hybrid behavior from simpler types of behavior such as pure inertia, pure variety-seeking and zero-order behavior. The model is estimated at the household level using panel data for three product classes, covering a total of 1069 households. The empirical analysis shows that more than half the households analyzed exhibited hybrid behavior rather than simpler types of behavior.", "e:keyword": ["Brand switching", "Variety-seeking", "Inertia", "Purchase event feedback"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.4.279", "e:abstract": "This paper analyzes the problem of designing the structure of multiproduct sales forces selling in an environment where purchase decisions are made on a straight rebuy or modified rebuy basis. It addresses two key issues pertaining to structure: How many sales forces should the firm have and which products should be assigned to each sales force? First, the key determinants of sales force structure are identified and described. Next, a mathematical programming model is formulated that can aid firms in designing the structure of their multiproduct sales force. An application is then described in detail including sales response specification, parameter estimation, solution procedure, and model implementation. Insights from several of the numerous implementations of the model in the pharmaceutical industry are also summarized.", "e:keyword": ["Sales force structure", "Product specialization", "Organizational design", "Sales resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.4.299", "e:abstract": "In this paper, we explore the role of franchising arrangements in improving coordination between channel members. In particular we focus on two elements of the franchising contract, namely, the royalty structure and the monitoring technology. We begin with a simple analysis where a manufacturer distributes its product through a retailer and the retail demand is affected by the retail price and the service provided by the retailer. In this context we show that neither royalty payments nor monitoring are needed for full coordination. We then extend the model to allow for free riding by franchisees and show that although monitoring helps affect the behavior of the franchisees, royalty payments are still unnecessary to coordinate the activities of the franchisor and the franchisees. Finally, we show that in environments where factors affecting retail sales are controlled by both the franchisor and the franchisee, royalty payments provide the necessary incentives for the franchisor while monitoring systems are used to ensure that franchisees also behave in the best interest of the channel. Our general results are found to be consistent with those in Brickley and Dark (1987) and Norton (1988), who conclude that franchising is affected by principal-agent problems. Our theory finds special support in the work of Lafontaine (1988) where it is concluded that the incidence of franchising is related to cost of monitoring and the importance of franchisor investments.", "e:keyword": ["Franchising", "Brand name", "Service", "Monitoring", "Moral hazard"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.4.319", "e:abstract": "This paper considers the question of how a sales manager should design the optimal compensation scheme for his salesforce when it consists of salespersons of varying selling skills, i.e., when the salesforce is heterogeneous. The manager's problem is to reward the salespersons based on observable, uncertain sales achieved by the salespersons. Under the assumption that both the manager and the salespersons are risk neutral, the optimal compensation scheme is derived. It consists of the manager offering a menu of plans, consisting of a quota, a payment for meeting quota, and a constant commission rate for sales above or below quota. Such schemes using constant commission rates are also called menus of linear plans. Salespersons choose the quota which best suits them, achieve sales, and are then rewarded based on their actual performance. This scheme, variants of which are often observed in practice, is shown to be optimal for sales environments characterized by commonly encountered sales response functions, and a large class of frequency distributions of selling skills in the salesforce. The problem is solved using the methods of principal-agent models. The key differences in managing homogeneous and heterogeneous salesforces are highlighted. Finally, the paper discusses the issues involved in practically implementing the optimal compensation scheme.", "e:keyword": ["Salesforce compensation", "Selling skill", "Heterogeneous salespersons", "Menu of linear plans", "Principal-agent models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.9.4.342", "e:abstract": "We start by assuming that a major benefit of many new durable products such as dishwashers and microwave ovens is time savings. Others, such as VCRs, also enhance the value of our leisure time. Using a household production framework we demonstrate that a utility maximizing individual will have a reservation price for the product which is a function of the product benefits and his wage rate. By assuming that the wage rate has an extreme value distribution across the population, we are able to derive, for the aggregate process, an income-price dependent logistic adoption equation. We then allow for the possibility that certain eligible consumers may delay their purchase of the product because they are unaware that it exists, are suspicious of its quality, or expect its price to fall. Unawareness, uncertainty and hope for further price declines are assumed to decrease with the increase in the number of previous adopters. The resulting diffusion model has the property that the product life cycle phenomenon can be explained jointly or separately by the income-price process, and the awareness-uncertainty-expectations process. Using data on household income, on durable penetration within different income classes, and on first-purchase sales the aggregate diffusion model and its premises are tested and supported. It is found that both the income-price and awareness-uncertainty-expectations processes are at work. However, the dependence of the awareness-uncertainty-expectations delay process on the number of previous adopters (e.g., word-of-mouth type effect) exists only in certain product categories and is usually relatively weak. It is demonstrated that if the word-of-mouth type effects are weak, a price skimming strategy is optimal for a monopolist and also is likely to be implemented by oligopolists. The sales forecasting implications of the model also are pursued.", "e:keyword": ["Durable products", "Diffusion of innovations", "Life cycle pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.1.1", "e:abstract": "The purchase timing decision is an important component of the dynamics of a household's purchase behavior. This decision is influenced by marketing and other variables, and the modeling of this dependence has recently received attention in the literature. In this paper, we build on previous studies and develop a comprehensive stochastic model that incorporates the major factors influencing interpurchase times. Specifically, we use a generalized version of Cox's proportional hazard model to test among competing probability distributions for the interpurchase times while incorporating effects due to marketing variables, observed household characteristics, and unobserved heterogeneity across households. The empirical finding from analyzing the IRI coffee data, suggests that the interpurchase times cannot be adequately described by probability distributions such as exponential, Erlang-2 or Weibull. The effects of unobserved heterogeneity are significant, and they impact the estimates of the effects of the covariates. We also find that a nonparametric procedure for estimating the effects of unobserved heterogeneity provides the best overall fit to the data and yields covariate estimates that are more consistent with prior expectations. Our model is validated by replicating the substantive empirical findings on an additional product category.", "e:keyword": ["Hazard function", "Purchase timing", "Unobserved heterogeneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.1.24", "e:abstract": "The authors develop and test a probabilistic model of purchase incidence and brand choice for frequently purchased consumer products. The model incorporates two ways of shopping in a category. Shoppers who have planned their purchasing (made a decision before entering the store) do not process in-store information and show no response to point-of-purchase promotions. Consumers who have not planned their purchasing in a category (deciding at the point of purchase) may process in-store information and may be strongly influenced by promotions. The two modes of information processing are called decision states and are labelled , respectively. The two-state model is calibrated on IRI scanner purchase records for saltine crackers. The model yields a significantly better fit than a one-state nested logit model and provides new insights into the relationship between shopping behavior and consumer purchase response.", "e:keyword": ["Brand choice", "Purchase incidence", "Nested logit", "Promotion", "Shopping behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.1.40", "e:abstract": "This paper considers sealed bidding in which bidders may submit two or more bids and after the bids are opened may, perhaps at a cost, withdraw bids that are more aggressive than would be necessary to win. Such withdrawal strategies are sometimes followed, but currently are surreptitious. However, legitimization of them would create potentially useful market mechanisms of potential interest to government agencies. These market mechanisms are also of theoretical interest since they are intermediate between first-price and second-price auctions. This paper presents models of such auctions. Both decision-theoretic models (applicable to surreptitious use of withdrawal strategies) and game theoretic models appropriate for openly withdrawable bid situations are developed. We describe a particular auction in which a winning bid was withdrawn and fit one of our decision theoretic models to data from it.", "e:keyword": ["Negotiations", "Bidding"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.1.58", "e:abstract": "A structured market for a product category is a pair (, ) consisting of a set of consumers who have the same decision tree, , which reflects the process by which the consumers in evaluate product features when making choices. Using a nested logit approach: (1) we present methods to test for the existence of hypothesized multiple structured markets, and (2) we present empirically determined insights on how brands compete within each structured market through the inclusion of marketing mix variables in our model. A successful empirical application of our methods using a panel data set of coffee purchases obtained from Selling Area Marketing Inc. confirms the existence of at least two distinct structured markets in the ground coffee market. The type-primary market is a segment of switchers, highly responsive to changes in market mix variables whereas the brand-primary market is a loyal segment, relatively unresponsive to marketing programs.", "e:keyword": ["Market structure", "Nested logit models", "Competitive analysis", "Market segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.1.83", "e:abstract": "Innovation diffusion models are developed to represent the spread of a new product from its manufacturer(s) to its ultimate users. In the marketplace, however, the growth of a new product can be retarded by supply restrictions such as the unavailability of the product due to limitations on the production capacity or difficulties encountered in setting up distribution systems. In the presence of supply restrictions, diffusion models must be developed to capture the dynamics of supply restrictions and to allow management to evaluate the impact of such supply restrictions on the growth of a new product in the market place. Based on the Bass innovation diffusion model, the objective of this paper is to suggest a parsimonious diffusion model that integrates the demand side dynamics with the supply side restrictions. The sensitivity of innovation diffusion patterns in the presence of supply restrictions is examined. An application examining the diffusion of new telephones in Israel is documented to illustrate the usefulness of the proposed model.", "e:keyword": ["New products", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.2.91", "e:abstract": "This paper proposes a taxonomy of consumer purchase strategies based on household decisions about which brand to purchase, how much, and when to purchase in a promotion intensive environment. We infer decision rules at the household level from supermarket scanner panel data and then cluster the inferred choice routes in order to discover the major purchasing strategies used in our population of interest. Subsequently, we test for the distribution of these purchase strategies in our customer population. Results add to our knowledge of consumer behavior in response to promotions.", "e:keyword": ["Promotion", "Consumer choice models", "Artificial intelligence", "Concept learning system"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.2.111", "e:abstract": "We propose a model to analyze the impact of advertising media plans and point-of-purchase marketing variables on a brand's market performance. Our model integrates brand choice, purchase incidence, and exposure behavior within a nonstationary stochastic framework. Moreover, it considers various aspects of consumer heterogeneity including individual differences in loyalty levels, purchase rates, and exposure probabilities for a population of consumers. The integrated model provides a relationship of advertising exposures from media plans, and other marketing variables, to measures of a brand's performance that include market share, penetration and depth of repeat patterns over time. In this paper, we focus on a multi-brand model formulation and stress its application to the analysis of advertising media plans. Based on single-source UPC scanner panel data for a frequently purchased product category, we provide an empirical test of the model. In this context, our analyses show that the model provides a good fit to the empirical data as well as accurate forecasts in hold-out predictive tests.", "e:keyword": ["Advertising reach/frequency", "Logit choice model", "Stochastic purchase incidence", "Consumer heterogeneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.2.131", "e:abstract": "This paper develops a modeling framework for making promotions decisions. In contrast to some of the prior research, the framework explicitly models promotions. Its central feature is the view of promotions competition as a multistage game in which regular prices are chosen first, followed by the choice of promotion depths and frequencies. It is used to illustrate the nature of competition between a national brand and private label. In equilibrium, the national brand promotes to ensure that the private label does not try to attract consumers away from the national brand. Moreover, the private label does not promote. This equilibrium is also contrasted with Varian's framework, used by other researchers, in which mixed strategy equilibrium prices are interpreted as promotions.", "e:keyword": ["Promotions", "Pricing", "Private labels", "Multi-stage game", "Nash equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.2.145", "e:abstract": "At the mathematical level, a factor or principal component of a factor analysis is simply a linear combination of variables under some constraints. Therefore, as in regression analysis, there are conditions under which individual or joint observations can be influential in the sense that their presence or absence significantly influences the obtained values of the estimated factor loadings. The nature of these effects as well as potential effects due to gross errors in the data set should be investigated in order to determine which observations, if any, need to be analyzed separately or excluded entirely. The purpose of this paper is (1) to propose a new technique for identifying influential observations and observations containing gross errors and (2) to discuss situations under which each is likely to significantly alter the results of a factor analysis.", "e:keyword": ["Factor analysis", "Influential observations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.2.161", "e:abstract": "This paper uses a case study and a simple mathematical model to study the link between the incumbency and incentives to innovate and introduce drastically new products. It identifies the conditions under which fears of self-cannibalization are particularly likely to lead incumbents to soft-pedal such innovations.", "e:keyword": ["New products", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.2.172", "e:abstract": "This empirical paper explores the relationship between consumer brand preference or loyalty and price elasticity in purchase behavior. This behavior is conceptualized as resulting from two distinct but related decisions, namely a brand choice decision and a purchase quantity decision. We argue that loyal consumers will be price sensitive in the choice decision than nonloyal consumers. However, this direction is expected to be reversed in the quantity decision with loyal consumers expected to be price sensitive than nonloyal consumers. We model the choice and quantity decisions jointly using the limited dependent variable framework described in Krishnamurthi and Raj. The data used are diary panel data on a frequently purchased product class from BURKE and caffeinated ground coffee scanner data from IRI. We show that loyals are less price sensitive than nonloyals in the choice decision but more price sensitive in the quantity decision. Managerial implications of the differing elasticities are discussed.", "e:keyword": ["Choice and quantity price elasticities", "Brand loyalty", "Panel data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.3.185", "e:abstract": "Rotating indifference curves are used to induce an income effect that favors superior brands at the expense of inferior brands in a discrete choice model. When calibrated on scanner panel data, the model yields an objective measure of brand quality which is related to the rate of rotation. The model also leads to asymmetric responses to price promotions where switching up to high quality brands is more likely than switching down. The model is capable of nesting the standard logit model, and is similar to a nested logit model when there exists clusters of brands of like quality. The model is used to explore a product line pricing decision where profits are maximized subject to the constraint that consumer utility is maintained.", "e:keyword": ["Quality", "Asymmetric switching", "Nonhomothetic utility", "Logit models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.3.205", "e:abstract": "This paper discusses the formal analysis of the underlying distance-based representations of consumer similarity judgments. Four models common in the marketing and psychological literature are examinedEuclidean and city-block spaces and ultrametric and additive trees. The analysis uses the distinction between algebraic and geometric structures as the basis for a unifying framework within which the four representations are compared and contrasted. The framework is then used to understand (1) the conditions under which model structure is theoretically revealing of the cognitive structure behind consumer behavior and (2) the degree to which similarity judgments and the resultant distance patterns are diagnostic of the appropriateness of a particular model. An important implication of the analysis is that there is a basic measurement indeterminacy associated with distance patterns, so that similarity data may not always reveal which is the true model in a given application. The consequences arising from this indeterminacy for the problem of model selection in marketing are illustrated with an empirical experiment designed to test the implications of the formal analysis in practical settings.", "e:keyword": ["Product positioning", "Multidimensional scaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.3.229", "e:abstract": "Two concepts of brand loyalty are defined, inertial brand loyalty resulting from time lags in awareness, and cost-based brand loyalty resulting from intertemporal utility effects. Their market level implications are formally derived in a continuous time model. It is found that inertial brand loyalty leads to equilibria with price dispersion, while cost-based brand loyalty also may allow single price equilibria. In all cases, as brand loyalty vanishes, so does the difference between the average trading price and the price which obtains with no brand loyalty. Consistent with empirical results, the theory predicts that the relationship between market share and performance is positive in cross-sectional studies, but flat in time-series studies. The theory is also consistent with the view that market share is an asset in itself. After developing the theory, several strategic implications are drawn. In the end, some questions for further theoretical and empirical research are raised.", "e:keyword": ["Brand loyalty", "Game theory", "Marketing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.3.246", "e:abstract": "Relationships between buying and selling organizations in business markets are varied and complex. One important relationship is procurement, or the type of alliance that buyers form with sellers to fulfill their purchasing needs. Multiple sourcing is often proposed to prevent a variety of procurement problems. We develop a bidding model to investigate the effect of multiple sourcing on competitive behavior prior to supplier selection. The model treats the number of bidders (and the decision to bid) endogenously. This distinctive feature allows us to show that the distribution of the number of bids is not independent of the competition type. Most previous models have not considered this question at all. Although multiple sourcing does increase participation in a bidding competition, we show that it can lead to strategic pre-award price increases that can mitigate the effects of the advantages stressed by most previous analyses of post-award supplier management.", "e:keyword": ["Business marketing", "Equilibrium", "Decision analysis", "Procurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.3.264", "e:abstract": "Consumers are different in their purchase rates and it is important to determine this heterogeneity. If a consumer's purchases follow a Poisson process (hence exponential interpurchase time), and purchase rates are distributed gamma across consumers, then a simple measure of heterogeneity is the shape parameter of the gamma distribution. Although we can use either the number of purchases or the interpurchase time data to estimate this heterogeneity, we suggest that number of purchases data are better and easier to use. It is also suggested that while method of moments (MOM) gives good parameter estimates for models using number of purchases data (e.g. NBD), it may be very misleading for models using interpurchase time data (e.g. Pareto). We also recommend caution when using maximum likelihood estimation procedure for the interpurchase time data if multiple observations are available for each consumer. This is to ensure that the model captures heterogeneity across consumers and not across observations.", "e:keyword": ["Stochastic models", "Interpurchase time", "Scanner data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.4.271", "e:abstract": "In recent studies of channel competition, it has been found that channel intermediaries reduce the intensity of direct competition between manufacturers. The underlying channel structure in most studies consists of two manufacturers and two retailers each of whom sells only manufacturer's product exclusively. This paper adds to this growing literature of channel competition by analyzing a channel structure with two competing manufacturers and one intermediary (a common retailer) that sells manufacturers' products. Unlike some exclusive dealers or retail outlets of a manufacturer, however, a common retailer is often a powerful player in the market. This paper studies three noncooperative games of different power structures between the two manufacturers and the retailer, i.e., two Stackelberg and one Nash games. It is shown that some of the results depend critically on the form of the demand function. With a linear demand function, a manufacturer is better off by maintaining exclusive dealers while a retailer has an incentive to deal with several producers. All channel members as well as consumers are better off when no one dominates the market. The common retailer benefits more than the manufacturers do from a symmetric decrease in the manufacturing cost. As products are less differentiated, all channel members' prices and profits increase: a counterintuitive result. When the demand function is nonlinear, however, an exclusive dealer channel provides higher profits to all than a common retailer channel given a power structure. As products are more differentiated, a manufacturer's profit decreases when a common retailer is used, but increases when an exclusive dealer is used. These results underscore the importance of choosing a correct demand function for a channel decision.", "e:keyword": ["Channels of distribution", "Competition", "Game theory", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.4.297", "e:abstract": "Consumer's purchase decisions of whether to buy, what to buy and how much to buy are examined simultaneously. Based on a consumer utility maximization problem, an unobservable threshold price, about which a consumer pivots from nonpurchase to purchase, can be deduced. With any model specification, the interrelationships between purchase decisions can be explicitly derived. The model is applied to coffee purchasing data. In addition, by suppressing the nonpurchase options the model is compared with an alternative approach appearing in the marketing literature.", "e:keyword": ["Single-source data", "Demand models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.4.316", "e:abstract": "A stochastic model is proposed to examine how changes in frequency of price discounts affect brand choice decisions of consumers who exhibit variety-seeking and reinforcement behavior. It is shown that the effect on choice depends on whether the brand offering discounts is a major or minor brand in the product category. This model extends the existing literature on stochastic models of variety-seeking behavior by explicitly incorporating switching due to promotions along with intrinsic switching due to variety-seeking. The model yields testable hypotheses which are supported in a laboratory experiment and on analyses of the IRI cracker data.", "e:keyword": ["Choice models", "Promotions", "Buyer behavior", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.4.338", "e:abstract": "A recent paper by Novak and Stangor (, Winter 1987) on testing competitive market structures involves testing a two-sided hypothesis to determine the nature of the competitive market structure. In this paper we provide an improvement to the Novak and Stangor procedure by considering a one-sided multivariate hypothesis and suggest methods for testing such hypotheses.", "e:keyword": ["Market structure"]}, {"@id": "http://dx.doi.org/10.1287/mksc.10.4.348", "e:abstract": "An early dismissal policy for unproductive recruiters is proposed. The policy is based on a bivariate stochastic model for productivity; this model considers both incidence of reporting a positive quantity of sales and the quantity per report. All recruiters are observed for a probational period; those who exceed minimum incidence and quantity requirements are allowed to continue to the end of a (fixed) maximum tenure while others are replaced with new recruiters after the probational period. We show that those who report less often must report a larger total quantity over the probational period in order to compensate for a larger chance variation. The univariate negative binomial distribution (NBD) model which considers quantity only is examined with respect to its robustness as an approximation in this context. It is found that the NBD leads to serious errors when the quantity reported is relatively homogeneous and reporting incidence is bimodal. Extensions to more conventional salesforces and to direct mail applications are indicated.", "e:keyword": ["Buyer behavior", "Salesforce"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.1.1", "e:abstract": "This paper presents a new multidimensional scaling (MDS) methodology which operationalizes the Krumhansl (1978) distance-density model for the analysis of asymmetric proximity data. In Krumhansl's conceptualization, the symmetric Euclidean interbrand distances typically associated with the traditional MDS model are augmented by the spatial density around the brands in the derived space. This modification allows the distance-density model to accommodate many of the empirically observed violations of the metric axioms (such as asymmetry). An operationalization of the distance-density model is particularly attractive to marketers who often work with asymmetric brand switching data to investigate market structure and competition. We describe this new MDS procedure which is sufficiently flexible to fit a number of competing hypotheses of proximity. The algorithm employed for estimation is technically presented together with various program options. An analysis of an asymmetric brand switching matrix for automobiles is presented to illustrate the methodology. The results of this analysis are also compared with the solutions obtained from several of the currently available procedures for handling asymmetric proximity data. Finally, technical extensions to three-way analyses, hybrid models, etc., are discussed.", "e:keyword": ["Similarity", "Krumhansl's distance-density model", "Multidimensional scaling", "Asymmetric proximity data", "Brand switching data", "Perceptual maps", "Product design and positioning", "Market structure"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.1.21", "e:abstract": "In the academic literature, the modelling of brand choice and switching behavior has a long history for frequently purchased packaged goods. Comparable efforts with consumer durable goods, however, are generally absent. This paper presents an application of the brand switching model proposed by Colombo and Morrison (1989) to a set of four major home appliances. Appliance brand loyalty, however, is shown to be a function of the timing of replacements, a factor that has not entered into the modelling of packed goods. As a result, the brand switching matrices are analyzed over the replacement cycle. This application illustrates how a brand switching analysis can be used to assess the relative competitive position of a firm in terms of the primary customer sources that a brand attracts.", "e:keyword": ["Consumer durable", "Replacement cycle", "Brand switching"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.1.39", "e:abstract": "Motivated by poor performance of standard estimation methods in our application, the problem of modeling the diffusion of a new service (or a product) is considered without the assumption of a homogeneous population. The model consists of a two-stage procedure where customers receive purchase occasions according to a conventional diffusion model and at each purchase occasion they buy according to a binary choice model. This approach permits explicit incorporation of individual customer demographics and product attributes, whence one can study changes in diffusion as a function of changes in product attributes, prices, advertising, and customer demographics. The model has provided excellent results in a number of applications including the one on fax penetration reported here. In our applications this approach has been useful in market segmentation, studying the effect of marketing strategies, and in evaluating the effects of new service features. A limited validation is carried out to judge the forecasting performance of the model used in the fax example. A simulation study is carried out to compare the proposed solution with the result obtained by fitting the Bass model to a set of market segments separately. The simulation indicates that the proposed approach substantially improves over the aggregate fitting.", "e:keyword": ["Diffusion of innovations", "Customer choice models", "Purchase incentives", "Product attributes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.1.54", "e:abstract": "Multiattribute market share models (MMSM) using supplier, product, and buyer attributes as explanatory variables generally use a logit model structure. The literature indicates two basic types of approaches for estimating coefficients, regression type approaches generally using log-share ratios and a nonregression approach which is based on maximum-likelihood principles. This article discusses a nonregression estimation approach, a minimum discrimination information (MDI), whose formulation differs from MDI procedures previously applied to the marketing literature. The MDI formulation enables researchers to estimate a logit model with proportions as the dependent variable and is computationally easy because it uses the same estimators as an MLE approach which assumes independence of trials. Equivalence between MDI and MLE estimates, under certain conditions, are discussed. The estimation methods are applied to a real world data set. The results indicate the MDI/MLE method does best in predicting the market shares of the suppliers according to some basic error criteria both at disaggregate and aggregate levels.", "e:keyword": ["Choice models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.1.64", "e:abstract": "Researchers often test consumers' abilities to discriminate between two product formulations. Such tests are used in new product development, quality control, and competitive assessment. This paper investigates three paired comparison taste tests, each requiring a different judgment. These are: preference (Which of these two do you prefer?), identification (Which one of these two is Brand ?) and detection (Are these two products the same or different?). Each task has biases that may influence the level of measured discrimination ability. To our knowledge, this is the first empirical study to compare subject discrimination ability as measured by all three tasks. Additionally, we test two scales of subject confidence ratings and discuss implications for product testing.", "e:keyword": ["Discrimination", "Taste tests", "Empirical Bayes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.1.76", "e:abstract": "Composite variables are those that may be mathematically decomposed into additive and/or multiplicative component variables. Several researchers have noted that the relationship between a composite variable and its components may be a mathematical artifact, but the effect of their inclusion as independent variables on the coefficients of the remaining variables in the model has not been recognized, nor has a formal expression for the resulting bias been presented. Structural analysis of composite dependent variables, as presented here, provides the key to understanding the nature and extent of this bias. It separates higher and lower level components from one another and also separates these components from other antecedent variables in the model. The advantages of this hierarchical decomposition are that (1) it reduces problems of misspecification and omitted variables, (2) by separately estimating antecedent effects on each component, it offers some insights into the underlying causal mechanisms that are not available from other techniques and (3) it ensures that regression coefficients can be interpreted in the standard way: the expected change in the dependent variable associated with a change in the independent variable, holding other independent variables in the equation constant. Moreover, hierarchical decomposition of the dependent variable can reproduce all information available from techniques that mix levels of analysis, but the converse is not true.", "e:keyword": ["Marketing", "Structural analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.1.95", "e:abstract": "Application of the two-step approach to structural equation modeling to the PARM data studied by Bagozzi and Silk (1983) results in quite different conclusions as to the psychometric properties of recall and recognition scores. The data they analyzed are more consistent with a simple, alternative measurement model, which has different implications for the dimensionality questions Bagozzi and Silk raised. After controlling for random error, recall and recognition scores for print ads are highly correlated and yet discriminable. So, while recall and recognition have considerable common variance, the unidimensionality hypothesis is rejected. Either recall or recognition (or both) contains a significant amount of specific variance. Taking reader interest scores into account does not change the conclusion; reader interest scores are best accounted for as a lower reliability indicator of recognition.", "e:keyword": ["Memory", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.2.117", "e:abstract": "This paper describes a case application of SIMOPT, a product positioning model and decision support system.", "e:keyword": ["Product positioning", "Conjoint analysis", "Market segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.2.133", "e:abstract": "The positioning and pricing of a new brand requires knowledge about the relationship of both demand and cost with potential attribute locations and prices. This paper addresses this problem and illustrates it in the context of the automobile market. Multi-attribute expected utility theory which allows for consumer uncertainty about the brands is used to model individuals' behavior. Attribute weights are estimated from market survey data on brands' attributes and preferences using LINMAP as an estimation procedure. Expected utility is then updated using the multinomial logit model and choice data to account for the observation that stated preferences do not perfectly reflect the eventual choice. It is hypothesized that when faced with an actual choice price may become more important to the consumer, while other attributes may become more or less important than is reflected in stated preferences. The resulting estimated choice probabilities are aggregated to form demand functions facing each brand which depend on all brands' prices and attribute space locations. Assuming there is a price equilibrium in the existing market and that firms have the same variable cost function, variable costs as a function of a brand's attribute levels are estimated. Given the demand and cost functions facing each firm including the potential new entrant, the profit maximizing positioning and pricing of a new brand is analyzed using a game theoretic approach. A solution is sought under the assumption that incumbents react to entry by changing their prices. Possible approaches to the translation of the perceptual attribute positioning of the new brand to physical and engineering attributes are reviewed. Improvements and future extensions of the study are discussed.", "e:keyword": ["Positioning", "New products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.2.154", "e:abstract": "Given that durable products are long-lived, there exists the possibility of secondary markets for used products as well as the potential for product obsolescence. This is an important issue in markets where technology changes rapidly, because the introduction of new versions of a product can make earlier versions obsolete. More generally, prices of older versions in the secondary market adjust in response to changes incorporated in new versions of the product. Thus, another method of evaluating consumers' response to a new product is by looking at the variation in market prices of the old product. This paper develops a general model to explore the relationship between primary markets for new cars and secondary markets for used cars. The results suggest that the depreciation of used cars is influenced strongly by the types of changes in new model cars.", "e:keyword": ["Automobile primary and secondary markets", "Product obsolescence", "Enhancement", "Depreciation", "Model discontinuation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.2.168", "e:abstract": "We develop a dynamic model for determining the equilibrium marketing effort levels for a manufacturer and a retailer in a two-member marketing channel. The existence of carry-over effects of marketing effort of channel members leads to an accumulation of goodwill for them over time. This necessitates modeling the relationship between the effort and channel sales in a dynamic framework as goodwill accumulation creates an incentive for channel members to invest in marketing effort in order to obtain future benefits. By explicitly recognizing that each member's decision is affected by the other's actions, we derive their equilibrium effort levels over time when they follow either a coordinated or an uncoordinated strategy and also determine the profit implications of these strategies for the channel members. We examine the effect of channel dynamics on the difference in profits resulting from following coordinated as opposed to uncoordinated strategies and identify situations in which this profit differential provides an incentive for channel members to coordinate their marketing efforts. Further, we derive empirically testable hypotheses that provide a basis for predicting when channel coordination would take place in a dynamic context.", "e:keyword": ["Channel dynamics", "Goodwill accumulation", "Channel coordination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.2.189", "e:abstract": "There are many products which are repeatedly purchased by consumers. In such cases it is likely that choice history, that is the sequence of choices made in the past, as well as marketing variables affect subsequent choice decisions. Attempts to model the effects of choice history have been generally based on the inclusion of variables that represent brand loyalty and/or variety seeking behavior. In this paper we present a model of dynamic choice behavior which is more general and incorporates four characteristics. The first characteristic labeled preference reinforcement and preference reduction represents loyalty and variety seeking. The second is the short-term reluctance of a consumer to move from the current brand (inertia) or the willingness to move to another brand (mobility). The third characteristic captures the effect of repetitive consumption (the long term effect) on inertia and mobility. The fourth characteristic incorporates the similarity or dissimilarity of choice alternatives. This is important in a dynamic model because choice on the current purchase occasion can be affected by whether a similar or dissimilar alternative was chosen on the previous occasion. Similarities of alternatives are represented in terms of distances. The effect of price on choice behavior is also modeled. Individual-level purchase data from a consumer panel are used to estimate a covariance probit and an independent probit specification of the model. From a substantive perspective the model gives interesting insights into the dynamics of choice behavior. The model predicts switches better than a benchmark model which incorporates only loyalty. In addition, it is superior to three benchmark models in overall predictive ability.", "e:keyword": ["Inertia/mobility", "Choice dynamics", "Covariance probit", "Similarity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.3.207", "e:abstract": "Our objective in this research is to relate variability in product category sales to promotional activity in the product category, and other category specific characteristics. The findings may be relevant from retailers' perspective as retailers' revenues are more closely related to the sales of the product category as opposed to the sales of any particular brand. We analyze data on about 2,000 brands from 25 different SAMI categories, obtained with the cooperation of a major grocery chain. Our data suggest that an increase in the magnitude of discounts increases the variability in category sales but an increase in the frequency of discounts has an opposite effect. Furthermore, categories which are bulky, or categories where the degree of competitiveness is high, exhibit lower variability in sales.", "e:keyword": ["Price discounts", "Product category sales", "Retailing", "Sales promotions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.3.221", "e:abstract": "Classical continuous-time models of advertising expenditure tend to fall into two categories, those that prescribe spending at a constant level and those that prescribe switching infinitely quickly between several different levels of spending. The latter practice, , though impossible literally, can be interpreted in practice to imply that the faster the switching the better. Empirical evidence, however, sometimes suggests the superiority of , alternating between different spending levels with finite frequency, for example in a periodic fashion. Furthermore, the actual behavior of marketing managers, who often advertise in flights or pulses, appears to differ from the optimal policies many current models prescribe. Showing how structural properties of common advertising models rule out finite-frequency pulsing , we develop a continuous-time model for which finite-frequency pulsing can be optimal. Relaxing these structural assumptions yields a new class of models which, for certain values of their parameters, lead to periodic pulsing optima; this is accomplished by inclusion of both S-shaped response and an exponential-smoothing filter. These theoretical results are illustrated through simulation of a variant of the Vidale-Wolfe model.", "e:keyword": ["Advertising", "Optimal control", "Pulsing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.3.235", "e:abstract": "A time series cross-sectional analysis of 18 successful later entrants in 8 categories of consumer packaged goods over the period from October 1983 to January 1988 confirms previous empirical findings that, after correcting for differences in marketing effort, later entrants suffer a long-term market share disadvantage. New evidence of the penalties associated with later entry are found in statistical estimation of models of cumulative trial, first repeat, and subsequent repeat purchasing. Significantly lower asymptotic levels are found in both trial and repeat behavior. However based on this data, the rate of approach of later entrants to their lower asymptotic performance measures is either equal to or faster than early entrants and provides evidence of a compensating partial effect accrued by later entrants.", "e:keyword": ["New products", "Order of entry", "Share", "Trial", "Repeat", "Pioneering"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.3.251", "e:abstract": "Learning curve effects, aspects of consumer demand models (e.g., reservation price distributions, intertemporal utility maximizing behavior), and competitive activity are reasons which have been offered to explain why prices of new durables decline over time. This paper presents an alternative rationale based on the buying behavior for products with overlapping replacement cycles (i.e., next generation products). A model for consumer sales of a new durable is developed by incorporating the replacement behavior of a previous generation product. Pricing strategies for two product generations are investigated analytically and with numerical methods. Results indicate that durable replacement behavior leads to a wider set of optimal pricing strategies than previously obtained. Several empirical illustrations of industry pricing practices for successive product generations are also shown to be consistent with the theoretical results. Finally, various areas for future research are outlined.", "e:keyword": ["Installed base", "Replacement behavior", "Technology substitution"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.3.266", "e:abstract": "Empirical research indicates that some consumers form price expectations which may impact their purchase behavior. While literature in operations research has built purchase policy models incorporating uncertain price expectations, these models have been built for commodities. Consumers face an environment with multiple brands. In this paper, we develop a model that incorporates consumer preferences and price expectations for multiple brands as determinants of normative consumer purchase behavior. The model demonstrates how commodity purchase policy models recognizing price uncertainty can be adapted to the study of multi-brand markets. The model is used to analyze the normative impact of changes in price promotion policies and holding costs on individual purchase behavior. It is also used in a Monte-Carlo market simulation that illustrates some scenarios where a post-promotion dip is more or less evident, and provides an explanation for the nonexistent post-promotion dip.", "e:keyword": ["Price expectations", "Stockpiling", "Inventory models", "Decision making under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.3.287", "e:abstract": "Although there has been a good deal of research on incorporating the effects of reference price formation into empirical models of consumer buying behavior, little formal theoretical work had been undertaken to date. This paper incorporates reference price effects into the traditional economic theory of consumer choice, and examines the effects of reference price formation on the results of the traditional theory, its marketing implications, and the implications for empirical models which examine the effects of reference price formation on actual consumer behavior. Several implications of the theoretical model are empirically tested using weekly retail egg sales data from Southern California. This analysis indicates that reference price formation does have significant effects on consumer behavior. Furthermore, these effects are asymmetric with consumers two and a half times more responsive to egg price increases that are in excess of the reference price than they are to comparable egg price decreases.", "e:keyword": ["Buyer behavior", "Choice models", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.3.310", "e:abstract": "This paper, guided by empirical findings in the literature, develops an advertising pulsing model for established frequently purchased products under the presence of advertising wearout effects. Using such a model, the superiority of Advertising Pulsing Policy over an equivalent Uniform Advertising Policy is explored analytically for both S-shaped and concave advertising response functions in relation to initial sales and the employed discount rate. The basic results of the research, summarized in four propositions, indicate that under the presence of advertising wearout and for both S-shaped and concave advertising response functions that: Pulsing is superior to the even policy for a zero discount rate. For all positive values of the discount rate, pulsing is superior to the even policy provided that initial sales are close enough to the equilibrium sales of the even policy. If initial sales are much different from the equilibrium sales of the even policy, pulsing would be superior for small values of the discount rate whereas the even policy would be superior for large values of the discount rate.", "e:keyword": ["Sales response", "Advertising policies", "Pulsing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.4.327", "e:abstract": "This paper examines two ways channel members at the manufacturing and retail ends deal with asymmetric information in the context of new product introduction. A manufacturer who has private information that demand for a new product will be high can differentiate itself from a manufacturer less confident of demand by undertaking high levels of pre-launch advertising and offering a high wholesale price. A retailer, for its part, can screen potentially high demand from potentially low demand products by stipulating a take-it-or-leave-it slotting allowance, the assumption being that the offer will be accepted only by manufacturers confident of sufficient demand to recover the high initial cost of slotting allowances. It is shown that manufacturers prefer to signal demand through advance advertising and wholesale price, retailers to screen demand through slotting allowances. It is shown that, unless advance advertising is sufficiently effective, slotting allowances yield higher total channel profits and higher social welfare.", "e:keyword": ["Distribution channels", "Signalling", "Screening", "Slotting allowances", "Asymmetric information"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.4.348", "e:abstract": "We determine the optimal response to competitive entry in a market characterized by a market share attraction model. The response of an incumbent is limited to changes in prices, advertising and distribution expenditures; brand positions are assumed to be fixed. We also assume that the entrant's position is chosen exogenously and that the sales potential of the market is constant. After proving the existence of a unique Nash equilibrium, we show that the optimal response depends on the relative market share of the incumbents. The response by nondominant brands (with market shares less than 50%) mirrors the prediction by decoupled response function modelsreduce price, advertising and distribution spending. For the dominant brand (having a market share of 50+%), the response is to reduce price and marketing spending. This finding explains previous empirical results that could not be addressed by the decoupled models.", "e:keyword": ["Defensive marketing strategy", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.4.359", "e:abstract": "Empirical research (as well as theoretical reasoning) has led to contradictory findings with regard to whether increases in brand advertising activities lead to increases or decreases in consumer price sensitivity. Lack of data to measure the exposure of individual households to advertisements and to capture competitive activities has been an important limitation to date. This study, using single source data for a midwestern US market, finds that increased advertising exposures are associated with increases in a household's brand choice price sensitivity for two frequently purchased consumer products. However, under high levels of advertising exposure, it is possible for a household's brand choice price sensitivity to decrease.", "e:keyword": ["Advertising", "Brand choice", "Econometric models", "Pricing research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.4.372", "e:abstract": "Multinomial logit models, especially those calibrated on scanner data, often use explanatory variables that are nonlinear functions of the parameters to be estimated. A common example is the smoothing constant in an exponentially weighted brand loyalty variable. Such parameters cannot be estimated directly using commercially available logit packages. We provide a simple iterative method for estimating nonlinear parameters at the same time as the usual linear coefficients. The procedure uses standard multinomial logit software and, in experience to date, converges rapidly. We prove that, under suitable conditions, the resulting parameter values are maximum likelihood estimates and show how to calculate asymptotic standard errors from normal computer output. Three applications illustrate the method in practice.", "e:keyword": ["Choice models", "Estimation and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.11.4.386", "e:abstract": "The multinomial probit model of brand choice is theoretically appealing for marketing applications as it is free from the independence of irrelevant alternatives property of the multinomial logit model. However, difficulties in estimation have restricted its widespread use in marketing. This paper presents an application of the method of simulated moments, a new methodology that enables easy estimation of probit models with a large number of alternatives in the choice set. We describe the theoretical development of the technique and using pseudo-simulated data, conduct numerical experiments to compare the method with existing techniques for estimating probit models. Using the scanner panel data on the purchases of catsup, we provide an empirical application of the method of simulated moments to the estimation of the parameters of a multinomial probit model. Estimating the covariance structure associated with the underlying latent variable probit model enables us to identify broad patterns of similarities across alternatives. It also enables us to derive a pairwise similarity matrix across choice alternatives which when input into a multi-dimensional scaling routine provides us with a graphical representation of competitive structure in the catsup market. For completeness, we compare the substantive implications for the effects of marketing variables obtained from the multinomial probit model with those obtained from models in the extant marketing literature.", "e:keyword": ["Brand choice", "Econometric models", "Estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.1.1", "e:abstract": "In recent years, many U.S. and Japanese firms have adopted Quality Function Deployment (QFD). QFD is a total-quality-management process in which the voice of the customer is deployed throughout the R&D, engineering, and manufacturing stages of product development. For example, in the first house of QFD, customer needs are linked to design attributes thus encouraging the joint consideration of marketing issues and engineering issues. This paper focuses on the Voice-of-the-Customer component of QFD, that is, the tasks of identifying customer needs, structuring customer needs, and providing priorities for customer needs. In the stage, we address the questions of (1) how many customers need be interviewed, (2) how many analysts need to read the transcripts, (3) how many customer needs do we miss, and (4) are focus groups or one-on-one interviews superior? In the stage the customer needs are arrayed into a hierarchy of primary, secondary, and tertiary needs. We compare group consensus (affinity) charts, a technique which accounts for most industry applications, with a technique based on customer-sort data. In the stage which we present new data in which product concepts were created by product-development experts such that each concept stressed the fulfillment of one primary customer need. Customer interest in and preference for these concepts are compared to measured and estimated importances. We also address the question of whether frequency of mention can be used as a surrogate for importance. Finally, we examine the stated goal of QFD, . Our data demonstrate a self-selection bias in satisfaction measures that are used commonly for QFD and for corporate incentive programs. We close with a brief application to illustrate how a product-development team used the voice of the customer to create a successful new product.", "e:keyword": ["New product research", "Product policy", "Measurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.1.28", "e:abstract": "This paper presents a technique for estimating a firm's brand equity that is based on the financial market value of the firm. Brand equity is defined as the incremental cash flows which accrue to branded products over unbranded products. The estimation technique extracts the value of brand equity from the value of the firm's other assets. This technique is useful for two purposes. First, the assigns an objective value to a company's brands and relates this value to the determinants of brand equity. Second, the isolates changes in brand equity at the individual brand level by measuring the response of brand equity to major marketing decisions. Empirically, we estimate brand equity using the macro approach for a sample of industries and companies. Then we use the micro approach to trace the brand equity of Coca-Cola and Pepsi over three major events in the soft drink industry from 1982 to 1986.", "e:keyword": ["Brand equity", "Finance-based estimation technique", "Brand management", "Advertising policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.1.53", "e:abstract": "Economies of scale are evident when a firm's average costs decline while its output expands, as when an advertising agency raises its gross income by serving more accounts and/or larger accounts. Economies of scope appear when cost savings can be realized by a single agency producing several products jointly, as compared to many agencies each producing them separately. How important are economies of scale and scope in advertising agency operations? In this paper cost models are formulated which represent how the principal component of agency costs, employment level, varies according to the mix of media and services an agency provides and the total volume of advertising it produces. These models are estimated and tested cross-sectionally utilizing data pertaining to the domestic operations of 401 U.S. agencies for 1987. The empirical evidence reported here indicates that both scale and particularly scope economies are highly significant in the operations of U.S. advertising agencies. We find that of the 12,000 establishments comprising the industry in 1987, approximately 200250 had domestic gross incomes of $34 million or more (or equivalently, billings of $2027 million) and therefore had service mixes and operating levels sufficiently large to take full advantage of all available size-related efficiencies. Furthermore, the overall structure of the industry is one where these large, fully efficient firms created and produced more than half of all the national advertising utilized in the U.S. during 1987. At the same time, vast numbers of very small agencies appear to operate with substantial cost disadvantages compared to large firms as a consequence of these scale and scope economies. These findings carry important implications concerning possible future changes in the industry structure. It seems highly doubtful that scale economies could motivate further mergers among the largest 200250 agencies. On the other hand, for small agencies, mergers and acquisitions might be attractive as means of mitigating their size-related cost disadvantages. Finally, our findings demonstrating the existence of scale and scope economies are consistent with the diminishing reliance on fixed rates of media commissions as the principal basis of agency compensation. They also cast strong doubts on size-related economies in operating costs as a viable explanation for the limited degree of vertical integration of agency services by large advertisers.", "e:keyword": ["Advertising agency", "Scale and scope economies"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.1.73", "e:abstract": "Using data on search and choice behavior from a local automobile market, we estimate monetary returns to search in terms of lower prices resulting from additional time invested in price search. For our analytical framework, we adapt a model developed in the job search literature to the problem of consumer search; this framework is especially useful for illuminating the relationship between time spent searching, the outcome of search, and demand and supply side variables. Our results indicate that, for this particular sample of buyers, marginal returns to search are broadly consistent with what one might expect if consumers balance costs and benefits of search, and that potential gains from additional search for lower car prices do not appear to be large for most consumers. Our study highlights many of the methodological difficulties involved in estimating returns to search, including isolating returns to different outcomes of search, and sensitivity of results to model specification and sampling error. We deal with these problems by trying to isolate time spent searching for price from other uses of search time, by deriving our model used in estimation from a specific conceptual framework, and by extensive specification testing.", "e:keyword": ["Information search", "Automobiles", "Economics of information", "Econometric models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.1.88", "e:abstract": "Given that firms pulse in advertising, should firms pulse in or out of phase? It is shown that out of phase maximizes the oligopoly profits and is also the Markov perfect equilibrium of the infinite horizon game. The basic intuition for this result comes from the following fact: it is more profitable to increase consideration when the competitor's consideration is lower. Evidence from several product categories seems to support this theoretical result.", "e:keyword": ["Competitive strategies", "Competitive advertising", "Pulsing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.1.103", "e:abstract": "The PIMS (Profit Impact of Marketing Strategies) data entail sparse time-series observations for a large number of strategic business units (SBUs), In order to estimate disaggregate marketing mix elasticities of demand, a natural solution is to pool different SBUs. The traditional, a priori approach is to pool together those SBUs which one believes in advance to be very similar with respect to their marketing mix elasticities. We propose an alternative maximum likelihood, latent-pooling method for simultaneously pooling, estimating, and testing linear regression models . This method enables the determination of a fuzzy pooling scheme, while directly estimating a set of marketing mix elasticities and intertemporal covariances for each pool of SBUs. Our analyses reveal different magnitudes and patterns of marketing mix elasticities for the derived pools. Pool membership is influenced by demand characteristics, business scope, and order of market entry.", "e:keyword": ["Econometric models", "Regression and other statistical techniques", "Marketing mix", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.2.125", "e:abstract": "This research investigates the antecedents and consequences of customer satisfaction. We develop a model to link explicitly the antecedents and consequences of satisfaction in a utility-oriented framework. We estimate and test the model against alternative hypotheses from the satisfaction literature. In the process, a unique database is analyzed: a nationally representative survey of 22,300 customers of a variety of major products and services in Sweden in 19891990. Several well-known experimental findings of satisfaction research are tested in a field setting of national scope. For example, we find that satisfaction is best specified as a function of perceived quality and disconfirmationthe extent to which perceived quality fails to match prepurchase expectations. Surprisingly, expectations do not directly affect satisfaction, as is often suggested in the satisfaction literature. In addition, we find quality which falls short of expectations has a greater impact on satisfaction and repurchase intentions than quality which exceeds expectations. Moreover, we find that disconfirmation is more likely to occur when quality is easy to evaluate. Finally, in terms of systematic variation across firms, we find the elasticity of repurchase intentions with respect to satisfaction to be lower for firms that provide high satisfaction. This implies a long-run reputation effect insulating firms which consistently provide high satisfaction.", "e:keyword": ["Choice models", "Defensive strategy", "Customer satisfaction", "Perceived quality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.2.144", "e:abstract": "In this paper we develop a model relating market share to average costs. We start with a theoretical model of the factors that affect the firm's average cost curve, partitioning these factors into (a) measurable firm and competitive environment characteristics, and (b) unobserved factors that are either fixed, random, or follow a first-order autoregressive process. We then link this theoretical model to an empirical model in which we specify three average cost equations for the organizational areas of purchasing, production, and marketing. Main effects for initial (lagged) market share position, as well as their interactions with factors characterizing the firm's competitive environment, represent the variables of key theoretical interest in our equations. We estimate these equations using PIMS data, and control for fixed, contemporaneous, and autoregressive unobservable factors. Our results suggest that market share can often lead to market power in the form of lower average costs. However, the firm's operating environment greatly moderates the effect of market share on average cost. In particular, we find that market share position only leads to lower average costs when the organizational unit operates in a competitive environment that gives it motivation and ability to realize power from its market share position.", "e:keyword": ["Market share", "Average costs", "Market power", "Organizational complacency", "PIMS data", "Panel data estimation", "Competitive strategy", "Econometric models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.2.167", "e:abstract": "Among the more prominent truisms in marketing are 80/20 type laws, e.g., 20 percent of the customers account for 80 percent of the purchases. These kinds of statistics indicate a certain degree of in customer purchases; i.e., the extent to which a large portion of the product's total purchases are made by a small fraction of all customers. Such concentration levels, suggesting that markets can be segmented in various ways, are often reported in basic marketing texts. We show that a of these concentration statistics is not nearly as easy or immediate as it is to compute them. The key factors influencing the degree of in purchases are reviewed, and we present a modeling approach for estimating the true level of among customers.", "e:keyword": ["Buyer behavior", "Estimation and other statistical techniques", "Market structure", "Measurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.2.184", "e:abstract": "We develop a comprehensive utility maximizing framework to study the impact of marketing variables on the category purchase, brand choice and purchase quantity decisions of households for frequently purchased packaged goods. The model allows for dependence among the three decisions while ensuring that these decisions provide, in combination, the greatest possible utility to the household. By accounting for variations in reservation prices and intrinsic brand preferences across households, the modeling framework explicitly captures the effects of unobserved heterogeneity on all three purchase decisions. The principal empirical finding from analyzing the A. C. Nielsen data for the yogurt product category is that the substantive implications for the effects of marketing variables are sensitive to whether these effects are determined conditional or unconditional on a product category purchase. Our results show that reservation prices and intrinsic brand preferences vary across households, and not accounting for these variations in the estimation could lead to biased estimates for the coefficients of the marketing variables. A comparison of our results to those obtained from a nested logit model of purchase incidence and brand choice reveals that our proposed model performs better using both a formal statistical test as well as the criterion of predictive validity in a holdout sample of panelists. Further, the purchase quantity model compares favorably with two alternative models of quantity choice in the validation sample.", "e:keyword": ["Buyer behavior", "Econometric models", "Choice models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.2.209", "e:abstract": "Farris, Parry and Ailawadi (1992; hereafter denoted FPA) demonstrate that bias can arise in a regression involving a composite dependent variable where a subset of components of the dependent variable are used as explanatory factors. They correctly observe that the Jacobson and Aaker (1985; hereafter denoted JA) model has explanatory factors that are also components of the ROI dependent variable and, as such, is subject to composite variable bias. FPA note that another way of viewing composite variable bias is that the coefficients in the model reflect not their impact on the dependent variable but rather their impact on the dependent variable less the elements of the components included as explanatory factors. As such, additional effects (analogous to indirect effects) may be present to the extent strategic factors influence the included components. FPA conclude that such bias explains the low estimate of the market share effect reported in JA. However, FPA's attempt to replicate our analysis and assess composite variable bias is flawed by a mistake in their analysis, i.e., their disaggregate models do not follow from the JA aggregate specification. The purpose of this note is to correctly assess the extent to which the JA estimate of the market share effect is affected by composite variable bias and to suggest approaches for modeling a composite dependent variable in the presence of unobservable factors. In particular, we (i) show that the disaggregate specifications of FPA do not follow from JA, (ii) look at specifications not subject to composite variable bias to investigate the magnitude of the composite variable bias in JA, and (iii) provide a disaggregate modeling framework that controls for unobservable effects.", "e:keyword": ["Competitive strategy", "Econometrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.3.213", "e:abstract": "We offer a framework to specify and estimate various sources of heterogeneity in multinomial logit brand choice models. We let each brand-specific intercept and each parameter of the explanatory variable vector vary randomly across households. In addition, we distinguish loyal households from the rest. Our results suggest that incorporating multiple sources of heterogeneity improves the model fit and suggests higher impact of marketing mix elements on brand choice. We highlight the importance of incorporating multiple sources of heterogeneity. The model we implement yields valuable managerial insights.", "e:keyword": ["Multinomial logit", "Heterogeneity", "Brand choice", "Random coefficients", "Loyalty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.3.230", "e:abstract": "This paper characterizes the manufacturer warranty policy and its effect on consumer behavior under the following conditions: consumers are heterogeneous in risk-preferences, consumer actions affecting the probability of warranty redemption are unobservable to the manufacturer, and the product reliability is known. We obtain the menu of warranty contracts, and then make connections with its institutional counterpart: the extended service contract. The model's implications for consumer behavior are examined using data obtained from a sample of recent buyers of new cars. The role of risk in consumer behavior with respect to choice of extended service contracts, and the allocation of effort for maintenance are found to be consistent with the model's predictions. The empirical analysis permits quantifying the demand for extended service contracts as a function of the extent of manufacturer warranty. The estimates show that for our sample of buyers a manufacturer warranty of three years is optimal in the sense of overcoming the role of risk-aversion in the choice of extended service contracts.", "e:keyword": ["Warranty", "Extended service contract", "Consumer heterogeneity", "Risk aversion", "Moral hazard"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.3.248", "e:abstract": "Single Source databases based on scanner data offer new opportunities for evaluating promotions and improving their effectiveness. Decision support needs vary depending on the decision maker's organizational vantage point. Some managers require the evaluation of promotion results in the short term. Others should take a medium- to long-term focus. An implemented model and automated system for measuring short-term incremental volume due to promotions by developing baselines of store-level normal sales is presented using store-level scanner data. Empirical validation results and real life applications are presented and discussed, including the use of the baselines as measures of brand health.", "e:keyword": ["Promotion evaluations", "Scanner data", "Baselines"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.3.270", "e:abstract": "Probabilistic discrete-choice models, such as multinomial logit models, are widely used to predict changes in market shares or total demand resulting from changes in policy variables under management control. These models often are evaluated in terms of their ability to predict choices in a holdout sample. This paper presents a new test for comparing predicted and observed choices. The results of a Monte Carlo experiment indicate that the new test has good finite-sample properties and high power in several circumstances likely to arise frequently in applications.", "e:keyword": ["Regression and other statistical techniques", "Brand choice", "Econometric models", "Choice models", "Estimation and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.3.280", "e:abstract": "Conventional questionnaire pretesting methods focus on directly identifying question defects, such as an ambiguous question. This paper proposes a new method that identities respondents' cognitive difficulties as they form answers to survey questions. It entails a content analysts of concurrent verbal protocols elicited during pretest interviews. The effectiveness of the methodology is illustrated with pretests of multiple versions of the same survey. The results are used to illustrate how this method yields diagnostic information about questionnaire problems and improvements. Then, the results are compared with the results of observational monitoring by managers. The findings indicate that a questionnaire pretesting methodology that quantifies respondents' cognitive difficulties is a useful enhancement for identifying and improving defective questions.", "e:keyword": ["Information processing", "Measurement", "Survey research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.3.304", "e:abstract": "When calibrating a brand choice model cross-sectionally, a measure of brand loyalty is often introduced into the utility function to account for differences in utility across households and over time. One of the most widely used measures of brand loyalty, proposed by Guadagni and Little (1983), is an exponential smoothing model of past choice behavior by the household. In this study, we argue that the exponential smoothing model of brand loyalty cannot properly distinguish between sources of variation in utility due to heterogeneity (across households) and sources of variation due to nonstationarity (within household over time). We introduce a new measure of brand loyalty, derived from a nonstationary Dirichlet-multinomial choice model, in which heterogeneity and nonstationarity are handled distinctly.", "e:keyword": ["Choice models", "Brand loyalty", "Heterogeneity", "Nonstationarity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.3.318", "e:abstract": "This paper illustrates the use of laboratory experimental auctions in a pretest market research program for new products. We review the experimental auctions literature, discuss the range of auction mechanisms available and present the advantages and disadvantages of using a particular mechanism for a laboratory pretest market. We then present a step-by-step example of how a theoretically incentive compatible auction mechanism (fifth-price, sealed-bid) was used in a laboratory pretest market for vacuum-packaged beef. Based on the illustration, we discuss the potential for using laboratory experimental auctions in pretest market research. We present the limitations that may be encountered in such applications and outline research aimed at improving the behavioral properties of the technique.", "e:keyword": ["Auctions", "Pretest market"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.4.339", "e:abstract": "Recent research suggests that the signal (e.g., sign or marker) with a point of purchase promotion will stimulate a significant sales increase, regardless of whether or not that signal is accompanied by a price cut. This paper develops a model of retailer profitability that incorporates this promotion signal sensitivity. In a field test, the profitability of the promotion policy prescribed by this model is compared to the profitability of two other promotion policy-setting paradigms: a model-based policy that does not consider promotion signal sensitivity and one prescribed by industry experts. The test results support the proposed model. Its policy generates 11% more category profit per unit than the model-based policy and 12% more than the industry experts. Implications for retailers and future research are discussed.", "e:keyword": ["Choice models", "Forecasting", "Promotion", "Retailing and wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.4.357", "e:abstract": "A set of alternatives under consideration is often divided into subsets (or local sets) by some external (e.g., product display format at the store) or internal (e.g., a decision rule) factor. We propose that the manner in which a global set of alternatives varying in price and quality is divided into local sets can have a systematic effect on consumer choice between a lower price alternative and a higher quality alternative. Three such effects are investigated: (1) Consumers who first choose from pairs of products ({A, B}, {B, C}, {A, C}) and then choose from the set of all three products ({A, B, C}) are more likely to prefer the cheapest alternative than consumers who only choose from the complete set; (2) The choice share of a low-price, low-quality brand is greater when alternatives varying in price, quality, and features are displayed by brand (i.e., each display presents different models of one brand) as compared to a display by feature level (i.e., each display presents comparable models of different brands); and (3) Consumers considering a pair of two-option local sets, each consisting of different brands and feature levels (e.g., a feature enhanced model of a low quality brand vs. a basic model of a high quality brand), are more likely to select a feature enhanced model from the global set (of four options) than those who consider the same alternatives in other local set configurations or consider only the global set. These predictions were supported in seven studies, which also provided insights into the boundaries of the effects (focusing on the paired-comparison effect). We discuss the theoretical and practical implications of the findings.", "e:keyword": ["Brand choice", "Buyer behavior", "Choice models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.4.378", "e:abstract": "Based upon a recently developed multiattribute generalization of prospect theory's value function (Tversky and Kahneman 1991), we argue that consumer choice is influenced by the position of brands relative to multiattribute reference points, and that consumers weigh losses from a reference point more than equivalent sized gains (loss aversion). We sketch implications of this model for understanding brand choice. We develop a multinomial logit formulation of a reference-dependent choice model, calibrating it using scanner data. In addition to providing better fit in both estimation and forecast periods than a standard multinomial logit model, the model's coefficients demonstrate significant loss aversion, as hypothesized. We also discuss the implications of a reference-dependent view of consumer choice for modeling brand choice, demonstrate that loss aversion can account for asymmetric responses to changes in product characteristics, and examine other implications for competitive strategy.", "e:keyword": ["Brand choice", "Buyer behavior", "Choice models", "Reference effects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.4.395", "e:abstract": "Some statistical methods developed recently in the biometrics and econometrics literature show great promise for improving the analysis of duration times in marketing. They incorporate the right censoring that is prevalent in duration times data, and can be used to make a wide variety of useful predictions. Both of these features make these methods preferable to the regression, logit, and discriminant analyses that marketers have typically used in analyzing durations. This paper is intended to fulfill three objectives. First, we demonstrate how decision situations that involve durations differ from other marketing phenomena. Second, we show how standard modeling approaches to handle duration times can break down because of the peculiarities inherent in durations. It has been suggested in recent marketing articles that an alternative to these conventional procedures, i.e., hazard rate models and proportional hazard regression, can more effectively handle duration type data. Third, to investigate whether these proposed benefits are in fact delivered for marketing durations data, we estimate and validate both conventional and hazard rate models for household interpurchase times of saltine crackers. Our findings indicate the superiority of proportional hazard regression methods vis--vis common procedures in terms of stability and face validity of the estimates and in predictive accuracy.", "e:keyword": ["Econometric modeling", "Estimation and other statistical techniques", "Pricing research", "Promotion", "Regression and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.12.4.415", "e:abstract": "In this paper, a formal test on prediction errors is developed for the cross-validation of regression models under the simple random splitting framework. Analytic as well as simulation results relate the statistical power of the test to the allocation of sample observations to estimation and validation subsets. The results indicate that splitting the data into halves is suboptimal. More observations should be used for estimation than validation. Furthermore, the proportion of the sample optimally devoted to validation is small for very limited samples ( < 20), increases to about 40% for medium-sized samples and decreases again for large samples ( > 60). However, although the 50/50 split is suboptimal, it is not tremendously so in a wide variety of circumstances.", "e:keyword": ["Econometric models", "Regression and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.1.3", "e:abstract": "The price competition between two firms can affect their decisions about product availability. With occasional stockouts a firm loses from foregone sales, yet it may indirectly benefit from the higher price a competitor is able to charge. The more prone customers are to search elsewhere for the product upon encountering a stockout, the greater is the gain from the reduced price competition between firms. This paper establishes conditions whereby the strategic interaction of reduced price competition between firms outweighs the loss from foregone sales. In such cases, a firm can increase its expected profits by choosing to stock less. These results provide a competitive rationale for the high stockout levels observed in some retail situations and have implications for the analysis of product availability in public policy settings.", "e:keyword": ["Competitive strategy", "Pricing research", "Product policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.1.23", "e:abstract": "Sales promotions and product enhancements are commonly expected to increase a brand's sales, when they do not negatively impact its utility and cost. That is, the purchase probability of consumers who find the promotion or additional feature attractive will increase, whereas the purchase likelihood of other consumers will not be affected. In contrast, we propose that consumers, who perceive a new feature or promotion as providing little or no value, will be less likely to purchase the enhanced brand even when the added feature clearly does not diminish the value of the brand. Thus, a new product feature or promotion may decrease a brand's overall choice probability when the segment of consumers who perceive it as providing little or no value is large compared to the segment that finds the feature attractive. This prediction was supported in three studies using actual promotions that have been employed in the marketplace (e.g., a Doughboy Collector's Plate that buyers of Pillsbury cake mix had the option to purchase for $6.19). We examined five alternative explanations for this effect. The results suggest that, when consumers are uncertain about the values of products and about their preferences, such features and premiums provide reasons against buying the promoted brands and are seen as susceptible to criticism. We discuss the theoretical and practical implications of the findings for segmentation, product, promotional, and pricing strategies.", "e:keyword": ["Brand choice", "Buyer behavior", "Product policy", "Promotion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.1.41", "e:abstract": "Customer base analysis is concerned with using the observed past purchase behavior of customers to understand their current and likely future purchase patterns. More specifically, as developed in Schmittlein et al. (1987), customer base analysis uses data on the frequency, timing, and dollar value of each customer's past purchases to infer  the number of customers currently active,  how that number has changed over time,  which individual customers are most likely still active,  how much longer each is likely to remain an active customer, and  how many purchases can be expected from each during any future time period of interest. In this paper we empirically validate the model proposed by Schmittlein et al. In doing so, we provide one of the few applications of stochastic models to industrial purchase processes and industrial marketing decisions. Besides showing that the model does capture key aspects of the purchase process, we also present a more effective parameter estimation method and some results regarding sampling properties of the parameter estimates. Finally, we extend the model to explicitly incorporate dollar volume of past purchases. Our results indicate that this kind of customer base analysis can be both effective in predicting purchase patterns and in generating insights into how key customer groups differ. The link of both these benefits to industrial marketing decision making is also discussed.", "e:keyword": ["Estimation and other statistical techniques", "Industrial marketing", "Measurement", "Promotion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.1.68", "e:abstract": "The paper provides a formal rationale for the common practice of using sales assistance. The argument is made in a model where a consumer's situation determines his needs. The products are differentiated and the map identifying the best match between a consumer's situation and a product is known only to the sales assistant. I look at procedure which involves the consumer revealing his needs and the sales assistant identifying the best matching product. This is compared to several other procedures and found to be best when the number of possible consumer situations is large. It is shown that reputational considerations may keep the sales assistant honest. I look at the impact of guarantees and other modifications to the model and discuss normative implications.", "e:keyword": ["Retailing", "Personal selling", "Comparative institutions", "Game theory", "Retailing and wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.1.83", "e:abstract": "A transaction cost approach is used to investigate a manufacturer's policy towards exclusive territory dealers who bootleg (sell across their assigned territories). We show that optimal enforcement policies will generally tolerate some level of bootlegging. This tolerance is a natural consequence of the transaction cost principle that institutional arrangements are not fully enforceable, and that self-enforcing implicit agreements will require some degree of tolerance. The actual degree of tolerance is determined by (a) the importance of reseller services, (b) the reduction in margin realized from surreptitious sales, and (c) the resellers' beliefs about the manufacturer's long-run commitment to the channel. We also show that deploying exclusive territories is beneficial to the manufacturer because it safeguards reseller services and permits resellers to capitalize on their superior local information. However, the latter effect is contingent on being able to convince resellers of the manufacturer's long run commitment to the channel.", "e:keyword": ["Channels of distribution", "Game theory", "Retailing and wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.1.100", "e:abstract": "We consider the case of a monopolist supplying an improving durable product to a population that is heterogeneous in its valuation of product quality. In a two-period framework, we show that if consumers expect the product to improve in present-value terms, then intertemporal discrimination might result in the first-period marginal consumer being left with zero surplus and some higher-end consumers postponing purchase. The resulting trajectories for quality and price do not constitute a subgame-perfect equilibrium. One of our conclusions is that the logic of profit maximization in the context of rational consumer choice imposes a demand-side constraint on the rate of product improvement. We also emphasize the disequilibrium consequences of improving a product so rapidly that high-end consumers are tempted to wait for a future new-and-improved version. Finally, the formulation adopted in the paper may be useful to understand observed differences in product improvement rates in different markets.", "e:keyword": ["Product policy", "Product improvement", "Pricing research", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.2.121", "e:abstract": "This paper presents an agency theoretic model-based approach that assists sales managers in determining the profit-maximizing structure of a common multiproduct sales quota-bonus plan for a geographically specialized heterogeneous sales force operating in a repetitive buying environment. This approach involves estimating each salesperson's utility function for income and effort and using these models to predict individual sales achievements and the associated aggregate profit for the firm under a specified plan. The utility function estimation is based on the salesperson's own preference rank-ordering of alternative sales quota-bonus plans. Once these functions have been estimated, they are incorporated in a mathematical programming model that the sales manager can use to determine the best plan. The authors demonstrate the approach in the context of a case involving the design of a two-product sales quota-bonus plan for a set of salespeople at a pharmaceutical products firm.", "e:keyword": ["Sales force research", "Sales quota-bonus plans", "Principal-agent model", "Heterogeneous sales-people", "Salesperson utility function", "Conjoint analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.2.145", "e:abstract": "We propose a stochastic choice model, and supporting empirical analyses, to understand the effect of package coupons on brand choice. Package coupons can be broadly classified into three types: peel off coupons, on-pack coupons, and in-pack coupons. Our model helps understand the relative impact of these three types of coupons on market share. We find that on-pack coupons may lead to a higher market share than peel-off coupons. What makes this result potentially interesting is that while the benefit of a peel-off coupon is realized immediately, the consumer has to wait until the next purchase occasion to cash in the benefit of an on-pack coupon. Though we primarily examine situations where package coupons are dropped frequently, a scenario that is quite representative of today's marketplace, we show that our results are also applicable when a manager is evaluating a particular coupon campaign. We compare the key predictions of our model with data collected from a series of in-store quasi-experiments. The data are consistent with the model's predictions.", "e:keyword": ["Promotions", "Brand choice", "Choice models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.2.165", "e:abstract": "This paper contains theoretical and empirical analysis of competition to retain customers. A formal game-theoretic model suggests that large firms are likely to exhibit greater customer retention rates than their smaller rivals in equilibrium even when their (common) customer retention technology does not exhibit increasing returns to scale. This hypothesis is corroborated by an empirical analysis of competition in ordinary life insurance.", "e:keyword": ["Competitive strategy", "Defensive strategy", "Buyer behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.2.177", "e:abstract": "This paper gives an example of renting the reputation of another agent to signal quality. We show that in a maximally separating equilibrium, manufacturers of high quality products distribute through retailers with strong reputation (reputable retailers), while manufacturers of low quality products distribute through retailers with no reputation (discounters). In this way, even if high quality manufacturers have no reputation of their own to post as bond, they can signal quality by posting the reputation of the retailers. In equilibrium, reputable retailers never default on their reputation. We also show that it pays the retailers to invest in reputation, as reputable retailers earn profits bounded away from zero under endogenous sequential entry, while the discounters' profits are zero.", "e:keyword": ["Channels of distribution", "Signaling game", "Retailing", "Quality perception", "Product quality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.2.190", "e:abstract": "The possibility of account conflicts is generally regarded as a very serious problem in the advertising industry. The problem is that once an advertising agency works for a firm it learns private information which it can use strategically, i.e., make available to the firm's competitors. But, in oligopolistic situations, knowing more about a competitor may not necessarily be beneficial, because the competitor may react to this knowledge. Similarly, allowing the competitor to have more information may not necessarily be detrimental. In fact, the decision of whether to share the same agency depends on three effects, which are identified here: (1) , (2) , and (3) . The first effect always favors sharing the same gency. The direction of the latter two effects is ambiguous. This ambiguity is resolved against the sharing of agencies when (1) the competitor's reaction to the firm's situation is especially harmful in that particular situation (strategic effect) and (2) the competitor's actions are increasingly harmful ().", "e:keyword": ["Competitive strategy", "Advertising policy", "Advertising agency"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.3.203", "e:abstract": "Over a large number of new products and technological innovations, the Bass diffusion model (Bass 1969) describes the empirical adoption curve quite well. In this study, we generalize the Bass model to include decision variables such as price and advertising. The generalized model reduces to the Bass model as a special case and explains why the Bass model works so well without including decision variables. We compare our generalized Bass model to other approaches from the literature for including decision variables into diffusion models, and our results provide both theoretical and empirical support for the generalized Bass model. We also show how our generalized Bass model can be used for product planning purposes.", "e:keyword": ["Diffusion", "Marketing mix", "New product research", "Pricing research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.3.224", "e:abstract": "A four segment trial-repeat model is developed to model sales of a frequently purchased product in the early stage of the product's life cycle. The model can be calibrated using aggregate data alone. Two versions of the model, one emphasizing the competitive aspect of marketing communications and another emphasizing the informative aspect, are estimated from data on 21 newly launched pharmaceutical products. The model provides valuable diagnostics such as an early estimate of the long-run market share of the new product, a decomposition of total sales (into trial sales due to marketing activities, trial sales due to word-of-mouth, and repeat sales), the changing composition of the trial market and repeat market over time, and the time when the trial market is expected to be saturated. In addition, the effectiveness of the entrant's marketing efforts, word-of-mouth communication, and buyers' trial experience are analyzed with the model. A cross-sectional analysis based on the empirical estimation results of our model reveals interesting insights into product introduction strategies. We find that: 1) the effectiveness of the firms' communication activities on trial is related mainly to product quality attributes and market growth whereas that of word-of-mouth is associated with product class characteristics and market competitiveness, and 2) the effect of product trial on repeat purchases is related mainly to product quality attributes and various market characteristics such as size, growth, competitiveness, and familiarity.", "e:keyword": ["Diffusion", "New product research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.3.248", "e:abstract": "We investigate the nature of competitive equilibrium for brands competing in a multi-attribute product space when consumer preferences for product attributes follow nonuniform distributions. We establish subgame-perfect equilibria in a two-stage game, where firms choose positions in the first stage and prices in the second stage. Two types of entry scenarios are investigated. In the first, the number of brands is given exogenously, and all of them choose positions simultaneously. In the second scenario, firms enter in sequence, and the early entrants can choose strategies to deter the entry of later entrants. We find the equilibria for two, three, and four brands for consumer preferences that follow a beta distribution. Nonuniform distributions result in equilibrium configurations that are substantially different from those obtained for uniform consumer distributions. In the two brands case, for sufficiently concentrated distributions we find asymmetric position equilibria with one firm at the corner of the market and one firm at an interior position. In the case of three brands, for sufficiently concentrated preferences we find symmetric equilibria that do not yield maximal differentiation. We also establish an equilibrium for the case of four competing brands. In addition we characterize entry deterring positioning strategies and examine how they vary with the distribution of consumer preferences.", "e:keyword": ["Competitive strategy", "Product policy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.3.274", "e:abstract": "In this paper, we propose and test a stochastic model of consumer choice that incorporates attribute-based variety seeking. Our stochastic variety-seeking model (SVS) has nested within it a fixed variety-seeking model, a zero-order model of choice, and a first-order (pure variety) model. We compare the SVS model to alternative models. Under stochastic variety seeking, we examine the nature of the variety sought and provide a test of the satiation hypothesis. Unlike fixed variety-seeking models, our model allows variety seeking to vary in intensity and consistency over individuals as well as over purchase occasions. We show that the model is equivalent to a random utility model with the features that the attributes of brands relative to those of the brand previously bought influence choice, and the extent of variety seeking is random over choice occasions for a given individual. The model permits a closed-form solution to the conditional switching probabilities parameterized by variety-seeking parameters, and dependent on brand attributes. We apply our model to individual choice data obtained from a field study specifically designed for the purpose at hand.", "e:keyword": ["Brand choice", "Choice models", "Stochastic choice", "Variety seeking"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.3.298", "e:abstract": "The paper offers a comparative analysis of different ways to sell products (selling formats) when buyers incur evaluation costs. Since these costs are sunk at the moment of trade, buyers may refrain from incurring them for fear of later opportunism on the part of sellers. It is found that the use of many common selling formats can be explained in terms of their ability to alleviate this problem. Specifically, I explain price advertising, seller colocation, and bargaining. The theory explains much of the divergence in retail trading institutions and leads to several testable predictions.", "e:keyword": ["Game theory", "Retailing", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.3.310", "e:abstract": "We investigate the firm's dynamic nonlinear pricing problem when facing consumers whose tastes vary according to a scalar index. We relax the standard assumption that the firm knows the distribution of this index. In general the firm should determine its marginal price schedule as if it were myopic, and produce information by lowering the price schedule; bunching consumers at positive purchase levels should be avoided. As a special case we also consider a market characterized by homogeneous consumers with a static, but unknown, demand curve. We show that when there are repeat purchases the forward-looking firm should tend towards penetration pricing; otherwise its strategy should tend towards skimming. We extend our insights to more general settings and discuss implications for pricing product lines.", "e:keyword": ["Pricing", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.4.327", "e:abstract": "Customer satisfaction incentive schemes are increasingly common in a variety of industries. We offer explanations as to how and when incenting employees on customer satisfaction is profitable and offer several recommendations for improving upon current practice. Faced with employee groups (including managers) who may have shorter time horizons than the firm, such systems enable a firm to use customer reaction to monitor implicitly how employees allocate effort between the short and long terms. These systems can be used to encourage employees to make tradeoffs that are in the best interests of the firm. We derive optimal reward systems for an equilibrium in which the firm maximizes profits, employees maximize their expected utility, and customers choose purchase quantities based on initial reputations, employee efforts (both ephemeral and enduring), and price. The formal model shows how the reliance placed on customer satisfaction in an incentive scheme should depend upon the precision with which customer satisfaction is measured and the extent to which employees focus on the short term. Recommendations for improving upon current practice include: measure customers, former customers, potential customers; measure satisfaction with competitors' products; disaggregate satisfaction to reflect better the performance of employee groups, and, when different customer segments have different switching costs or they vary in the precision with which their satisfaction can be measured, then measure the segments separately and assign different weights in the incentive plan. Throughout the paper we interpret the formal results based on our experience with actual firms and the current literature. We close with a brief discussion of on-going research at field sites.", "e:keyword": ["Competitive strategy", "Measurement", "Customer satisfaction", "Incentives"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.4.351", "e:abstract": "We explore the effect of dealing patterns on consumer purchase behavior by developing a normative purchase quantity model that can incorporate dealing pattern. The model adds to the stream of research on optimal purchasing policy by demonstrating how dealing patterns can be incorporated in a simple manner in dynamic programming models. Implications for purchase behavior are derived by employing the model in a numerical simulation in which time between deals is characterized by a Weibull distribution. The flexibility of the Weibull distribution enables us to establish how particular facets of the dealing distribution (e.g., certainty in deal timing, minimum time between deals) affect consumer behavior with respect to optimal purchase quantity, inventory, etc. One of the implications of the model is that the average quantity purchased on deal should be larger when there is greater certainty in deal timing. The model also shows that the average quantity purchased on deal should be larger when deals are spaced further apart. even though the buyer is presented with the same number of deals. We test certain model implications in a laboratory experiment and find actual behavior varying across dealing patterns in a manner consistent with model implications.", "e:keyword": ["Promotions", "Buyer behavior", "Price expectations", "Inventory models", "Decision making under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.4.374", "e:abstract": "In increasingly competitive and deregulated service environments, telecommunications service providers are interested in using advertising to stimulate usage. Measuring response in telephone usage to advertising presents difficulties because the effects are likely to be small, and the usage exhibits high variability. The substantive question of whether advertising has any effect on telephone usage is addressed through a systematic analytical process that combines exploratory data analysis with formal modeling. We find that telephone usage does respond to advertising, that this response can be quantified, and that households with heavy usage of telephone service respond proportionally more than light usage households.", "e:keyword": ["Advertising and media research", "Buyer behavior", "Estimation and other statistical techniques", "Regression and other statistical techniques", "Telecommunications"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.4.392", "e:abstract": "This paper develops a model to address an important problem facing a manufacturer of a durable product. That is, how can it plan its new product introductions to minimize the obsolescence of the old product, preserve its market for the new product, and keep copycat products at bay? We analyze these issues by developing a two-period model in which a firm sells an old product in period 1 and a new product in period 2. We consider various new product introduction strategies such as product replacement, line extension, and upgrading. We find that the optimal response to a threat of competitive clones is to increase the level of product innovation. This increase in innovation coupled with the entry of a competitive product implies that the incumbent firm has less of an incentive to leapfrog the old product or shelve the new product.", "e:keyword": ["Competitive strategy", "Game theory", "Product policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.13.4.412", "e:abstract": "We study franchise arrangements that allow franchisees with exclusive territories to own their customers. This permits franchisees to benefit from positive externalities in the franchise network through interfranchise transfers based on the purchases by their customers at other franchises on the network. Using the structure of a single franchisor and many franchisees, we show that, in general, interfranchise transfers between franchisees and incentives for franchisee investment in the expansion of their customer base are critical both to the size and to the benefits derived from the franchise network. Specifically, we find that when individual franchisees make investments in marketing effort to increase their customer base, the franchisor's setting of the interfranchise transfer trades off the positive effects on network size with the negative effects of removing franchisee incentive for investment. This result is due to the fact that interfranchise transfers encourage adoption, but discourage full investment in marketing effort. As compared to first-best franchisee investment, use of the royalty and the inter-franchise transfer directly dissipates franchisee profits, and indirectly dissipates franchisee profits through less than universal adoption, thereby causing franchisees to underinvest. As compared to traditional franchise systems, however, use of the interfranchise transfer results in franchises making greater investments than they otherwise would.", "e:keyword": ["Channels of distribution", "Pricing research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.1.1", "e:abstract": "Are marketing efforts able to affect long-term trends in sales or other performance measures? Answering this question is essential for the creation of marketing strategies that deliver a sustainable competitive advantage. This paper introduces persistence modeling to derive long-term marketing effectiveness from time-series observations on sales and marketing expenditures. First, we use unit-root tests to determine whether sales are stable or evolving (trending) over time. If they are evolving, we examine how strong this evolution is (univariate persistence) and to what extent it can be related to marketing activity (multivariate persistence). An empirical example on sales and media spending for a chain of home-improvement stores reveals that some, but not all, advertising has strong trend-setting effects on sales. We argue that traditional modeling approaches would not pick up these effects and, therefore, seriously underestimate the long-term effectiveness of advertising. The paper concludes with an agenda for future empirical research on long-run marketing effectiveness.", "e:keyword": ["Econometric models", "Marketing mix", "Advertising and media research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.1.22", "e:abstract": "Trade shows are an important but under-researched component of the promotion mix for most industrial products. In this paper, we develop a three-stage model of trade show performance, relying on different indices of performance at each stage: attraction, contact, and conversion efficiency. We model the impact of preshow promotion, booth space, use of attention-getting techniques, competition, number and training of booth salespeople on the extent of attraction, contact, and conversion. The results from an empirical application using data from 85 firms that participated in a major trade show in 1991 suggest significant and different impact of these variables. In addition, we illustrate how the model can be used to evaluate trade-offs among different decision variables. Finally, we develop some general results implied by our model concerning the optimal allocation of trade show resources.", "e:keyword": ["Industrial marketing", "Marketing mix", "Trade shows"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.1.43", "e:abstract": "This paper recommends that manufacturers consider a pull price promotion as a coordination device in an independent channel of distribution. Uncoordinated decisions of both manufacturer and retailer to charge high prices can break down the effort to expand the market, resulting in losses to the channel as a whole. We show that manufacturers can enhance channel price coordination by designing pull price discounts that target price-conscious consumers. The increased price coordination improves total channel profits and consumer surplus. Supporting pull with push increases the probability of coordination.", "e:keyword": ["Channels of distribution", "Promotions", "Pricing research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.1.61", "e:abstract": "In a product category based on dynamic technology, new products enter the market in rapid succession, and the competitive situation changes almost daily. Because technological features of available products tend to improve while prices tend to decline, customers develop expectations that may influence their purchase decisions. We model the impact of customer expectations regarding and on product market share in the personal computer industry, finding significant nonlinear effects of both. These effects are observed when actual product price and/or technology differ from expectations by a threshold amount. Our results suggest implementable implications for high-tech product managers: in particular, price and technology should meet, but not exceed, customer expectations. This does not mean that managers should strive for mediocrity; rather, continuous improvement should be implemented so that product development efforts lead customer expectations.", "e:keyword": ["Competitive strategy", "Pricing research", "Product policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.1.82", "e:abstract": "This paper investigates the impact of reference price effects on retailer price promotions and describes why these effects can make promoting profitable. First, we analyze the profit impact of reference price effects generated by a single period of promotion. The promotion can increase profit if the gain that these effects create in the promotion period outweighs the loss they create in future periods. We then describe how retailers can estimate the optimal strategy of recurring promotions that maximizes profits from reference price effects over a time horizon. Examples of such strategies are presented for a retailer selling a national brand of peanut butter. We obtain insights into how promotion prices, timing, and profits are affected by changes in costs, interest rates, consumers' reactions to reference price effects, and error in estimates used in the model. The retailer's optimal reaction to a trade deal is also examined. This strategy involves a phase of increased promotion activity sandwiched between phases of decreased activity. We explain these results using the effects described in the single-period model.", "e:keyword": ["Buyer behavior", "Pricing research", "Promotion", "Retailing and wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.1.105", "e:abstract": "Recent reports show that manufacturers in many consumer product industries continue to use coupons as one of their main promotional tools. The rising intensity of coupon competitions, however, makes the issue of having better measures for coupon effects ever more important. This paper proposes a method to assess the category expansion effect in an environment where rival brands distribute coupons repeatedly over a period of time. A consumer utility maximization model is presented in which the decision of coupon use is endogenized along with the demand decision. Using single-source detergent data as benchmark, the proposed model is an advance over the conventional regression model in that the latter tend to produce upward-biased estimations for the category impact of coupons.", "e:keyword": ["Coupon promotions", "Category sales"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.2.123", "e:abstract": "Influential forecasts occur when the forecast itself determines whether the forecast is tested. New product sales forecasts are often influential because a low forecast may cause a firm not to launch a new product so that actual sales are never observed. This paper considers a dilemma we face as influential forecasters. Our client requests an unbiased forecast but pressures sometimes exist to provide a bias forecast. From theoretical and empirical perspectives, we discuss the impact of these pressures on the quality of forecasts. We find that:  Noninfluential forecasts, generally, create pressure for statistically biased forecasts.  As influence increases, the pressures increase.  When our forecasts eliminate alternatives, (e.g., product designs, advertising campaigns), not all forecasts are tested.  Not validating all forecasts causes two effects: Survivor's Curse and Prophet's Fear.  Survivor's Curse makes statistically unbiased forecasts appear optimistic (i.e., overestimate actual sales) because, often, only optimistic forecasts are tested.  Forecasts appearing statistically unbiased or pessimistic might cause concern. Perhaps, some failures are justified.  Prophet's Fear encourages pessimistic forecasts because these forecasts cause hidden opportunity losses while optimistic forecasts cause observable actual losses.  Tested forecasts may appear completely unbiased despite a pessimistic pre-launch bias.  Although no perfect solution exists, clients may lessen bias with experimentation and by seeking more accurate forecasts. Forecasters may lessen bias with forecasts conditioned on launching and by seeking more accurate forecasts.", "e:keyword": ["Forecasting", "New product research", "Channel relationships", "Bias", "Brand management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.2.148", "e:abstract": "Consumers often simultaneously purchase multiple units in the same product category. Whether they are best off purchasing an assortment of alternatives or an equal number of one alternative is addressed in this paper. Models of an expected-utility-maximizing consumer are developed, and it is shown that an assortment may have greater expected utility than an equal number of any one alternative. The rationale is that the assortment allows the consumer to choose from inventory the alternative most appropriate for a consumption occasion using preference information not available at the purchase occasion. It is also shown that, when consuming alternatives from inventory, the consumer may forego the alternative offering the most utility in the expectation that it will yield even more utility in the future.", "e:keyword": ["Buyer behavior", "Choice models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.2.166", "e:abstract": "This paper addresses the issue of retail price image by offering an explanation for how and when stores can use their advertised prices to signal the prices of other products in the store. A model of a two-product retail market is presented in which stores advertise the price of one product and customers do not know the price of the other product before selecting which store to visit. In a model with full customer information, stores with different marginal costs charge different prices for each product. When customers do not know each store's marginal cost type, an opportunity arises for each store to signal its cost type using its advertised prices. In such a model, additional equilibria exist. In particular, stores with different costs may charge the same advertised price while continuing to charge different prices for the unadvertised product. Data from competing drycleaning stores is generally consistent with the model predictions. A number of additional properties of the equilibria are discussed and possible extensions to the model are proposed.", "e:keyword": ["Signalling", "Pricing", "Price image", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.2.189", "e:abstract": "Retailers have not gained profitability at the expense of manufacturers in the last two decades, according to accounting and stock market measures of profits, despite substantial industry restructuring and new forms of channel rivalry. Both manufacturers and retailers have experienced reductions in profitability. Consumers may be the net beneficiaries these and other changes. In this study we survey the recent evolution of the grocery channel, examine the connections between changes in channel structure, conduct, and performance, and assess whether profitability has shifted as a result. We relate these findings to the issue of whether power has shifted in the grocery channel.", "e:keyword": ["Channels of distribution", "Channel power", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.2.224", "e:abstract": "In this paper, the one-dimensional vertical differentiation model (Shaked and Sutton [Shaked, A., J. Sutton. 1982. Relaxing price competition through product differentiation. 313.], Moorthy [Moorthy, K. S. 1988. Product and price competition in a Duopoly. (Spring) 141168.]) is extended to two dimensions and an analysis of product and price competition is presented. A two-stage game theoretic analysis in which two firms compete first on product positions and then on price is conducted. Closed form equilibrium solutions are obtained for each stage in which competitors are unrestricted in their choices of price or product positions. A significant finding of this research is that unlike the one-dimensional vertical differentiation model, firms do not tend towards maximum differentiation, although this solution is possible under certain conditions. When the range of positioning options on each of the dimensions is equal, MaxMin product differentiation occurs. That is, in equilibrium, the two firms tend to choose positions which will represent maximum differentiation on one dimension and minimum differentiation on the other dimension.", "e:keyword": ["Competitive strategy", "Game theory", "Product policy", "Pricing research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.2.250", "e:abstract": "In this addendum, we present a measure of discrimination ability that is free of the downwardly biasing assumptions of previous discrimination measures. We discuss the magnitude of the bias in the previous measures, the reliability of all of the measures, intercorrelations among them, and their appropriate uses.", "e:keyword": ["New product research", "Buyer behavior", "Taste tests", "Discrimination ability", "Affect and cognition", "Bayesian true scores"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.253", "e:abstract": "Increasingly, researchers in marketing are recognizing the lability of attribute importance weights derived from measurement techniques, such as conjoint analysis. As has been suggested by Simonson and Tversky, attribute importance weights can be sensitive to competitive product context and to purchase situation. This paper describes and applies a procedure for adjusting conjoint importance weights to predict consumers' actual or potential product choices. We discuss the approach from both a descriptive and prescriptive viewpoint. In particular, the latter perspective provides strategic insights into how attribute importance modifications can increase brand share. An industry case, based on real data, is used to illustrate the approach.", "e:keyword": ["Competitive strategy", "Scaling methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.271", "e:abstract": "We propose a model that seeks the optimal timing and depth of retail discounts with the optimal timing and quantity of the retailer's order over multiple brands and time periods. The model is based on an integration of consumer decisions in purchase incidence, brand choice and quantity with the dynamics of household and retail inventory. The major contribution of the model is that it shows how the optimum depth and timing of discount varies with key demand characteristics such as consumer stockpiling, loyalty, response to the marketing mix, and segmentation. In addition, the optima also vary with key supply characteristics such as retail margins, depth and frequency of manufacturer deals, retail inventory, and retagging costs. The most valuable contribution of the model is that it can provide an optimal discount strategy for multiple brands over multiple time periods. The optimization model runs on a user-friendly personal computer program. An application based on UPC scanner data illustrates the model's uses. Sensitivity analyses of the optimization model under alternative scenarios reveal novel insights as to how optimal discounts vary as a function of the key demand and supply characteristics.", "e:keyword": ["Optimal promotions", "Retailing", "Consumer response", "Discount timing", "Mathematical programming"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.300", "e:abstract": "Nonparametric density estimation using a kernel method is proposed to model consumer brand choice. Recent availability of large scanner panel data allows the use of nonparametric approach, which has few or at least fewer underlying assumptions and affords greater structural flexibility. By removing as many assumptions as possible, the author constructs the ultimate nonparametric model, radically departing from the traditional approaches, to highlight the differences in implementation and performance. The proposed model does not involve either parameters that approximate certain distributions as in stochastic models or latent concepts such as utility as in utility maximization models. The performance criteria include prediction of market response and brand choice, share tracking, and robustness under violation of various assumptions involved in parametric choice models, such as correlated disturbance and misspecification. The method is compared with a popular parametric counterpart, the multinomial logit model, on simulated and actual scanner panel data. The paper emphasizes the conceptual importance of the nonparametric approach by discussing its advantages, limitations, and its complementary role in developing, refining, and diagnosing parametric models. This perspective affords insight to modeling philosophy and suggests the possibility of a hybrid approach.", "e:keyword": ["Choice models", "Promotion", "Estimation", "Other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.326", "e:abstract": "Consideration sets have been the recent focus of a large volume of research in marketing. The primary orientation of this stream of research has been toward consideration set composition, measurement, and the theoretical formation process itself. This paper proposes a new multidimensional scaling methodology (MDS) devised to spatially represent preference intensity collected over consumers' consideration sets. Predictions concerning the probability of consideration set membership, as well as the degree of preference intensity of these brands within a consideration set, are possible from such a model. In addition, consumer heterogeneity is accommodated vis á vis latent market segment level estimation. The technical details of the proposed MDS methodology are presented. Two actual commercial applications of the procedure are provided in the modeling of consideration sets and respective preference intensity for intenders for mid-size and luxury automobiles. Finally, limitations and directions for future research in this area are discussed.", "e:keyword": ["Buyer behavior", "Choice models", "Scaling methods", "Segmentation research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g1", "e:abstract": "It is our hope that some of the empirical generalizations presented in this volume would be of practical value to management. Some of the generalizations help codify our knowledgethey are discussed in several paperssome can serve as useful guidelines in the generation of strategic marketing options. In addition, we hope that this volume would stimulate additional research and actions by marketing scientists in industry and academia.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g6", "e:abstract": "Marketing has matured to the point where it seems desirable to take stock of where we are, what we have learned, and fruitful directions for extending the knowledge base that has developed. Science is a process involving the interaction between empirical generalizations and theory. An is a pattern or regularity that repeats over different circumstances and that can be described simply by mathematical, graphic, or symbolic methods. One of the purposes of the Empirical Generalizations Conference held at Wharton on February 1618, 1994 was to develop a list of examples of such empirical generalizations in marketing. Empirical generalization can precede a theory to explain it or it can be predicted by a theory. Science is the process of interaction between theory and data that leads to higher level theories. Examples are provided here of empirical generalizations in marketing and their theoretical counterparts. One example is provided of a higher level theory.", "e:keyword": ["Diffusion", "Brand choice", "Pricing research", "Empirical generalizations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g20", "e:abstract": "Should we judge empirical generalisations as mere regularities, separately from theory? The paper argues that this may bring out the links with theory better and gives illustrative examples. It also notes how in pursuing empirical generalisability more explicitly, we will have to shift how we design and analyse our studies.", "e:keyword": ["Repeat rates", "Price elasticities", "Many sets of data", "Differentiated replications"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g29", "e:abstract": "As well as being generalizations based on repeated empirical evidence, good empirical generalizations have five other characteristics: scope, precision, parsimony, usefulness, and a link with theory.", "e:keyword": ["Empirical generalizations", "Research methods", "Science", "Knowledge development", "Replication", "Scope", "Phenomena", "Meta-analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g36", "e:abstract": "A decade of work in marketing meta-analysis has produced empirical generalizations concerning parameters in models of advertising, price, diffusion, and consumer behavior. Results from these meta-analyses should replace the now discredited zero null hypotheses of such parameters in future work. Probably more important than nonzero grand mean average effects is an approach called , which provides estimated parameter values for specific conditions reflecting markets and research technologies. Systematic application of the methodology can also help guide research along productive routes and away from repetition of work which has little potential to add new knowledge.", "e:keyword": ["Generalization", "Meta-analysis", "Diffusion", "Advertising", "Buyer behavior", "Pricing", "International marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g47", "e:abstract": "In this paper we investigate how to make empirical generalizations in marketing. We argue that for substantive empirical generalizations to exist in an area, there should be a sufficient body of research about recurring phenomena. We outline criteria and a procedure to search for and identify such generalizations, and we apply the procedure to the area of business marketing negotiations. We find that, in spite of a sizable literature on business marketing negotiations, there appears to be little overlap between what researchers have studied to date and many characteristics of real-world bargaining situations. We do identify one significant generalization: that bargainers who are problem solvers settle disputes more efficiently than those who take adversarial positions. However, we note that a significant theorypractice gap exists that must be bridged before more substantive generalizations can be identified in the area of business marketing negotiations. More broadly, we suggest that issues such as the sampling or selection of research studies and the match of reported research with real phenomena are serious concerns in our search for empirical generalizations in marketing and that it is not apparent that such generalizations exist in marketing domains.", "e:keyword": ["Bargaining", "Business markets", "Empirical generalizations", "Negotiations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g61", "e:abstract": "All empirical data and the resulting parameters are subject to error. In this paper we explicitly use the standard, but hardly profound, model <disp-formula>$$\\hbox{Observed Value} = \\hbox{True Score} + \\hbox{Error}$$</disp-formula> as a lens for better viewing empirical studies in search of empirical generalizations in marketing. This lens is especially valuable when the unit of analysis is the individual consumer. However, even when a macro study contrasts price elasticities across cities, this framework can be very helpful.", "e:keyword": ["Unobservable constructs", "Summary statistics", "Appropriate comparisons", "Shrinkage estimators"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g71", "e:abstract": "Many empirical regularities in the buying behavior of consumers have been linked together into a comprehensive model, the Dirichlet. In this paper we list some of the well-established regularities, show how they are theoretically intertwined, and illustrate how this approach to modeling can assist the marketing analyst.", "e:keyword": ["Brand choice", "Store choice", "Replication studies", "Empirical generalisations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g79", "e:abstract": "The diffusion model developed by Bass (Bass, F. M. 1969. A new product growth model for consumer durables. (January) 215227.) constitutes an empirical generalization. It represents a pattern or regularity that has been shown to repeat over many new products and services in many countries and over a variety circumstances. Numerous and various applications of the model have lead to further generalizations. Modifications and extensions of the model have lead to further generalizations. In addition to the empirical generalizations that stem from the model, we discuss here some of the managerial applications of the model.", "e:keyword": ["Diffusion", "Forecasting", "New product research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g89", "e:abstract": "This paper offers the generalization that competitive promotions are mixed strategies. First an empirical regularity is established that promotions are independent across competitors. This regularity is then elaborated on in the context of a promotion game. The promotion game is linked to observable outcomes, and a classification of possible situations is developed. In particular, the classification includes the prisoners' dilemma, battle of the sexes, and marketing models of promotion competition. The evidence for the generalization comes from a variety of product markets, spanning trade promotions, retail price reductions, and retail promotions such as advertised specials. The product markets include coffee, baby diapers, toilet tissue, saltines, dishwashing fluid, ketchup, and detergents, among others. The data in some cases were from cooperating grocery chains and in others from IRI scanner panels. The evidence from extant research and from new analyses is presented. Each situation is identified as belonging to one or another element of the classification. Based on the entirety of the evidence, there is strong support for the proposition that competitive promotions are mixed strategies. A second generalization, based on more limited data, is that the depth of promotion has a bimodal distribution. Implications of the generalizations both for managerial practice and future research are discussed.", "e:keyword": ["Promotions", "Game theory", "Mixed strategies", "Trade deals", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g101", "e:abstract": "This paper investigates the relationship between market share and promotional expenditures the long run. Using data that span a decade and 91 product categories we find that market shares are stationary for a majority of the products in the data base. We find that relative promotional expenditures for the products are offsetting in the long run. Finally, for products whose market shares show a trend, it is difficult to discern the impact of relative promotional expenditure on the evolution of market share. This result generalizes the empirical finding of Bass et al. (Bass, F. M., M. M. Givon, M. U. Kalwani, D. Reibstein, G. P. Wright. 1984. An investigation into the order of the brand choice process. (4) 267287.) that ... offsetting competitive activity plays a role in the maintenance of what appears to be stationary and zero order behavior.", "e:keyword": ["Game theory", "Pricing research", "Promotions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g109", "e:abstract": "We present empirical generalizations about conditions under which marketing variables evolve or remain stationary. We first define evolution statistically and make the case why it is an important concept for increasing our understanding of long-run marketing effectiveness. We then briefly review ways in which evolution can be tested empirically from readily available data. We present a database of over 400 prior analyses and catalog the relative incidence of stationarity versus evolution in market performance and marketing spending. We find that evolution is the dominant characteristic for sales and marketing-mix spending, but that stationarity is the dominant characteristic for market share. Thus we find strong support for the conjecture that many markets are in a long-run equilibrium where the relative position of the players is only temporarily disturbed by their respective marketing activities. We assess the impact of a number of covariates on the likelihood of finding stationarity/evolution in sales and market share, and discuss the managerial implications of our findings.", "e:keyword": ["Econometric models", "Marketing mix", "Evolution", "Stationarity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g122", "e:abstract": "By synthesizing findings across the sales promotion literature, this article helps the reader understand how promotions work. We identify and explain empirical generalizations related to sales promotion; that is, effects that have been found consistently in multiple studies involving different researchers. We also identify issues which have generated conflicting findings in the research, as well as important sales promotion topics that have not yet been studied. This overview of the research and findings from the sales promotion literature is intended to offer direction for future research in the area.", "e:keyword": ["Sales promotion", "Retailer promotions", "Trade promotion", "Empirical generalizations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g133", "e:abstract": "This article reports on the first in-market experimentally based measurement of long-term TV advertising effects for a representative cross-section of 55 tests for established consumer packaged goods. The typical BehaviorScan<sup></sup> weight test has a test group exposed to a heavier TV advertising weight than a matched control group for a one-year period, after which the experiment stops. For this analysis, the two groups were tracked and analyzed for another two years, when the only difference between the two groups was the TV advertising treatment during the first year. The analysis shows that when TV weight increases had a significant impact during the year of the weight increases, during the following two years, on average, the sales impact of the first year is approximately double and, on average, it was from an increasing in buying rate in the test group. The analysis also shows that if TV weight increases had no significant impact during the first year, on average, they had no impact in the two following years.", "e:keyword": ["Advertising", "Carry over", "Long term"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g141", "e:abstract": "This paper provides a theoretical explanation for the inconsistent findings from previous econometric analyses of aggregated data concerning the duration of advertising carryover. Using this theoretical explanation and the knowledge of a data interval bias, the paper reviews and summarizes the empirical work in the area of sales response modeling, and then after adjusting for aggregation bias, presents support for an empirical generalization that the average advertising duration interval is of brief durationtypically between six and nine months.", "e:keyword": ["Advertising", "Advertising duration/carryover", "Regression-data aggregation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g151", "e:abstract": "Consumers' sensitivities to price changes are an important input to strategic and tactical decisions. It has been argued that price sensitivities depend on factors such as advertising. Prior studies on the effect of advertising on consumer price sensitivity have found seemingly conflicting results. We analyze the characteristics of previous studies in marketing and generate a set of three empirical generalizations. These are (1) an increase in price advertising leads to higher price sensitivity among consumers, (2) the use of price advertising leads to lower prices, and (3) an increase in nonprice advertising leads to lower price sensitivity among consumers. These generalizations have important implications for managers and researchers. Managers need to coordinate their advertising and pricing decisions to attain maximum profits. For researchers, our summary and discussion of empirical results provide directions for future.", "e:keyword": ["Advertising", "Pricing research", "Empirical generalization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g161", "e:abstract": "Considerable theoretical justification for consumers' use of psychological reference points exists from the research literature. From a managerial perspective, one of the most important applications of this concept is reference price, an internal standard against which observed prices are compared. In this paper, we propose three empirical generalizations that are well-supported in the marketing literature. First, there is ample evidence that consumers use reference prices in making brand choices. Second, the empirical results on reference pricing also support the generalization that consumers rely on past prices as part of the reference price formation process. Third, consistent with other research on loss aversion, consumers have been found to be more sensitive to losses, i.e. observed prices higher than reference prices, than gains. We also propose topics for further research on reference prices.", "e:keyword": ["Reference price", "Brand choice", "Decision making"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g170", "e:abstract": "There are three classical measures of brand awareness: aided, spontaneous, and top-of-mind. The relationships between these measures, across a set of brands in the same product category, are close, but highly nonlinear. We show that these relationships can be linearized, in all product classes, by performing a logistic transformation on each measure. This amounts to describing the process by which consumers answer awareness questions by a Rasch model, originally proposed to describe the success of students in answering exam questions. The brand's salience is equivalent to the students' competence, and the difficulty of the awareness question is equivalent to the test difficulty. We briefly summarize the research process that led to this empirical generalization. Managerial implications are described, mainly linked to the diagnosis of locked versus open product categories.", "e:keyword": ["Buyer behavior", "Measurement", "Scaling methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g180", "e:abstract": "Are there general algebraic laws which describe how consumers make choices from sets of alternatives? In this paper we review the verdict of research which has sought to answer this question. We focus on the functional forms which have been found to best characterize three component processes of consumer choice: those of attribute valuation, attribute integration, and choice. Our central conclusion is that there exists support for three major generalizations about the form of consumer decision processes: (1) subjective attribute valuations are a nonlinear, reference-point dependent, function of the corresponding objective measure of product attributes; (2) the integration rule which best describes how these attribute valuations are integrated to form overall valuations is multiplicative-multilinear, characterizing an overweighting of negative attribute information; and (3) the choice rule which links overall valuations of an option to the likelihood that it is chosen from a set is a member of a family of functions which recognize the attributewise proximity of a considered alternative to others in the set. The evidence supporting these generalizations is reviewed, as well as their implications for future theoretical and applied work in consumer choice modeling.", "e:keyword": ["Choice models", "Decision making", "Context effects", "Multiattribute models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g190", "e:abstract": "In this paper we review evidence of a generalized convex cross-sectional relationship between retail distribution and unit market share, i.e., large-share brands have more share points per percentage of distribution than small-share brands. The dynamics and structure of distribution and share can help explain many phenomena in marketing, including this convex shape: (1) market share is both a cause and an effect of distribution, and (2) in the typical convenience goods distribution system there are a few large outlets that stock many brands and numerous smaller outlets that stock the leading brands only. Generally, the observed cross-sectional curve relating distribution and share will reflect the retailers' stocking decisions, not the incremental effect of distribution on share. However, a logically consistent model of share based on (1) and (2), when combined with the assumption of low search loyalty, results in customers being willing to switch from preferred to available brands. A further consequence is that the marginal effect of weighted distribution on share is likely to be increasing, i.e., result in convex curves relating distribution and share for a given brand. In some cases, and for some measures of distribution, these convex curves have been observed in time-series data for brands that failed and lost distribution over a relatively short period of time. The implication is that marketers should monitor distribution carefully, as it is the result of combined effects of brand preference, loyalty, and push programs. With a better understanding of the market share/distribution relationship, managers should be in a better position to forecast marketplace results for a given level of distribution.", "e:keyword": ["Distribution", "Market share"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g203", "e:abstract": "As more nations add customer satisfactionas a measure of quality of economic outputto what they presently collect about the economy, it becomes increasingly important to understand the role of customer satisfaction and its relationship to other economic measures. In an attempt to contribute to such an understanding, this paper presents two empirical generalizations about customer satisfaction. First, the distribution of customer satisfaction is negatively skewed. It is suggested that negative skewness is a condition for a free market. Second, the association between market share and customer satisfaction is not positive (and often negative) in cross sectional analysis. While challenging to firms that pursue both market share goals and increased customer satisfaction, the finding is consistent with fundamental economic theory and the strategy literature.", "e:keyword": ["Quality", "Customer satisfaction", "Skewer distribution", "Market share"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g212", "e:abstract": "Three established and four emerging empirical generalizations are identified below. The first established generalization is that there is a negative relationship between order of market entry and market share. Second, for consumer packaged goods and prescription anti-ulcer drugs, the entrant's market share divided by the first entrant's share roughly equals one divided by the square root of order of market entry. Third, in mature markets, market pioneer share advantages slowly decline over time. While the emerging generalizations require additional research support, the initial findings suggest: (1) for consumer packaged goods, order of market entry has a stronger negative relationship with trial penetration than with repeat purchase; (2) market pioneers have broader product lines than late entrants; (3) skill and resource profiles differ across market pioneers, early followers, and late entrants; and (4) order of market entry is not related to long-term survival rates. Future research topics are also discussed.", "e:keyword": ["Order of market entry", "Early entry advantage", "And market pioneer"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.3.g222", "e:abstract": "This paper provides an approach for assessing generalizable effects of strategic actions on firm performance. We identify five key issues that need to be addressed before one can have confidence in the obtained strategic generalization. In addition, we suggest a methodology for attacking each of these five issues. We then illustrate this framework using the example of demand-side returns to R&D spending. We document two empirical generalizations in this research. First and foremost, we show that demand returns to R&D spending depend on whether the firm has ability and motivation to take advantage of the R&D investment. Thus, only firms with high ability motivation leverage the R&D investment into monopoly rents in the form of subsequent price increases. Second, we show that the concept of a firm needing both ability and motivation to sustain returns from a strategic action generalizes across strategic actions. We make two other conclusions about empirical approaches to strategy research. First, to produce valid empirical generalizations about the effects of strategic actions on firm performance requires a study to address adequately the five issues we establish herein. Second, one readily available strategy database, PIMS, enables the researcher to address each of these issues.", "e:keyword": ["Competitive strategy", "PIMS", "R&D", "Cross-sectional/time-series analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.4.343", "e:abstract": "We consider two broad categories of incentives by which a manufacturer can motivate its retailers to provide high customer satisfaction: (1) manufacturer assistance that reduces the retailer's cost of providing customer satisfaction (CS assistance); and (2) customer satisfaction index (CSI) bonus. We show that if a retailer has a long-term orientation, CS assistance is a more effective coordination mechanism that induces the retailer to expend more effort at customer satisfaction. However, if the retailer has a short-term orientation, CSI bonus is a more effective coordination mechanism. We then show that a long-term oriented retailer is more valuable to a manufacturer than a short-term oriented one. Finally, we show that the use of CS incentives results in greater profits for both the manufacturer and the retailer.", "e:keyword": ["Channels of distribution", "Customer satisfaction", "Incentives", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.4.360", "e:abstract": "This paper explores channel coordination by a manufacturer that sells through competing retailers and that treats these retailers equally, as required by the Robinson-Patman Act. The authors show that, in general, there exists no single two-part tariff with a constant per-unit charge that will duplicate the behavioral results (i.e., prices, quantities, and channel profits) that are obtained by a vertically integrated system; that is, the channel cannot be coordinated except in the trivial cases of identical or noncompeting retailers. However, an appropriately specified quantity-discount schedule will enable the channel to earn the same profits generated by a vertically integrated system. Conditions are derived under which a manufacturer will prefer to offer various two-part tariffs with constant per-unit charges instead of the channel-coordinating quantity-discount schedule. The authors also establish the existence of a menu of two-part tariffs that mimics all results of a vertically integrated system. However, only under stringent conditions will retailers select the appropriate tariff from the menu. When these conditions are not satisfied, the channel is worse off than in the case of a single, second-best tariff. It is also demonstrated that under a wide range of parametric values the manufacturer will prefer to offer the second-best two-part tariff rather than a menu of two-part tariffs that could maximize channel profits.", "e:keyword": ["Channels of distribution", "Pricing research", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.4.378", "e:abstract": "Direct marketing (mail) is a growing area of marketing practice, yet the academic journals contain very little research on this topic. The most important issue for direct marketers is how to sample targets from a population for a direct mail campaign. Although some selection methods are described in the literature, there seems to be not a single paper discussing the analytical and statistical aspects involved. The objective of this paper is to introduce a comprehensive methodology for the selection of targets from a mailing list for direct mail. At least theoretically, this methodology leads to more efficient selection procedures than the existing ones. The latter are not based on an optimal selection strategy, whereas we explicitly take the profit function into account. By equating marginal costs and marginal returns we determine which households should receive a mailing in order to maximize expected profit. In the empirical part we show that our methodology has great predictive accuracy and generates higher net returns than traditional approaches.", "e:keyword": ["Econometric models", "Estimation and other statistical techniques", "Direct marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.4.395", "e:abstract": "With the advent of panel data on household purchase behavior, and the development of statistical procedures to utilize this data, firms can now target coupons to selected households with considerable accuracy and cost effectiveness. In this article, we develop an analytical framework to examine the effect of such targeting on firm profits, prices, and coupon face values. We also derive comparative statics on firms' optimal mix of offensive and defensive couponing, the number of coupons distributed, redemption rates, face values, and incremental sales per redemption. Among our findings: when rival firms can target their coupon promotions at brand switchers, the outcome will be a prisoner's dilemma in which the net effect of targeting is simply the cost of distribution plus the discount given to redeemers.", "e:keyword": ["Competitive strategy", "Promotion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.4.417", "e:abstract": "Why is it that some products carry a minimal manufacturer base warranty even though all consumers are risk-averse? Conventional wisdom suggests that it is profitable for the manufacturer to offer a comprehensive warranty in this setting. We provide in this paper an explanation for the provision of minimal warranty in markets where all consumers are risk-averse. Minimal warranties are created by the impact of consumer moral hazard and competition in the insurance after-market for the product. We show that consumers who purchase optional extended warranties from an independent provider of insurance create a significant negative externality on the warranty redemption costs of the manufacturer. This, in turn, creates a significant erosion in the manufacturer's profits from warranty insurance. Consequently, it is in the best interest of the manufacturer to drop the level of warranty coverage provided with the product. This intuition holds for homogeneous as well as heterogeneous markets of consumers.", "e:keyword": ["Game theory", "Pricing research", "Warranties", "Risk aversion", "Service contracts", "Moral hazard"]}, {"@id": "http://dx.doi.org/10.1287/mksc.14.4.442", "e:abstract": "Direct marketing is witnessing explosive growth. As consumers increasingly purchase products from their homes, their ability to judge the quality of products they buy is significantly reduced. In this paper we study how money-back guarantees can signal product quality in such environments. We interpret product quality broadly to mean both the level of attributes promised as well as the firm's consistency in delivering on those promises. Key aspects of our formulation are the explicit consideration of transaction costs, and alternative signals of product quality. Transaction costs are the costs the seller or buyer faces when redeeming a money-back guarantee. We show that money-back guarantees signal quality by exploiting the higher probability of returns for a lower quality product, and the attendant higher transaction costs. However, if the seller's transaction costs are very large, then there are less costly ways to signal, namely charging a high price. We compare the signaling performance of (1) price, (2) price with uninformative advertising, and (3) price with a money-back guarantee. Whereas uninformative advertising does not work at all in our model, under certain conditions a money-back guarantee is necessary to signal, and under other conditions, a money-back guarantee is a useful supplement to price.", "e:keyword": ["Money back guarantee", "Warranty", "Signaling quality", "Transaction costs", "Separating equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.1.1", "e:abstract": "We construct two models of the behavior of consumers in an environment where there is uncertainty about brand attributes. In our models, both usage experience and advertising exposure give consumers noisy signals about brand attributes. Consumers use these signals to update their expectations of brand attributes in a Bayesian manner. The two models are (1) a dynamic model with immediate utility maximization, and (2) a dynamic forward-looking model in which consumers maximize the expected present value of utility over a planning horizon. Given this theoretical framework, we derive from the Bayesian learning framework how brand choice probabilities depend on past usage experience and advertising exposures. We then form likelihood functions for the models and estimate them on Nielsen scanner data for detergent. We find that the functional forms for experience and advertising effects that we derive from the Bayesian learning framework fit the data very well relative to flexible ad hoc functional forms such as exponential smoothing, and also perform better at out-of-sample prediction. Another finding is that in the context of consumer learning of product attributes, although the forward-looking model fits the data statistically better at conventional significance levels, both models produce similar parameter estimates and policy implications. Our estimates indicate that consumers are risk-averse with respect to variation in brand attributes, which discourages them from buying unfamiliar brands. Using the estimated behavioral models, we perform various scenario evaluations to find how changes in marketing strategy affect brand choice both in the short and long run. A key finding obtained from the policy experiments is that advertising intensity has only weak short run effects, but a strong cumulative effect in the long run. The substantive content of the paper is potentially of interest to academics in marketing, economics and decision sciences, as well as product managers, marketing research managers and analysts interested in studying the effectiveness of marketing mix strategies. Our paper will be of particular interest to those interested in the long run effects of advertising. Note that our estimation strategy requires us to specify explicit behavioral models of consumer choice behavior, derive the implied relationships among choice probabilities, past purchases and marketing mix variables, and then estimate the behavioral parameters of each model. Such an estimation strategy is referred to as structural estimation, and econometric models that are based explicitly on the consumer's maximization problem and whose parameters are parameters of the consumers' utility functions or of their constraints are referred to as structural models. A key benefit of the structural approach is its potential usefulness for policy evaluation. The parameters of structural models are invariant to policy, that is, they do not change due to a change in the policy. In contrast, the parameters of reduced form brand choice models are, in general, functions of marketing strategy variables (e.g., consumer response to price may depend on pricing policy). As a result, the predictions of reduced form models for the outcomes of policy experiments may be unreliable, because in making the prediction one must assume that the model parameters are unaffected by the policy change. Since the agents in our models choose among many alternative brands, their choice probabilities take the form of higher-order integrals. We employ Monte-Carlo methods to approximate these integrals and estimate our models using simulated maximum likelihood. Estimation of the dynamic forward-looking model also requires that a dynamic programming problem be solved in order to form the likelihood function. For this we use a new approximation method based on simulation and interpolation techniques. These estimation techniques may be of interest to researchers and policy makers in many fields where dynamic choice among discrete alternatives is important, such as marketing, decision sciences, labor and health economics, and industrial organization.", "e:keyword": ["Brand choice", "Buyer behavior", "Choice models", "Econometric modelling", "Information processing", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.1.21", "e:abstract": "Manufacturer-supported trade deals remain one of the major competitive tools in today's marketplace. This is true despite the fact that such trade deals are often claimed to be unprofitable for manufacturers. The unprofitability is attributed to the fact that retailers forward buy and do not pass the price discounts on to the consumers. The audience for this paper includes practitioners and academics who have been concerned with the ubiquitous practice of trade dealing in spite of its purported unprofitability. The paper attempts to understand the motivations for trade dealing by comparing the profitability of trade dealing in the presence of forward buying to a situation in which retailers carry no inventory. Moreover, we shed light on why manufacturers sometimes offer trade deals even though the retailers do not pass through these deals to the consumers. We study the problem of trade dealing through an economic model that includes the manufacturers, the retailer, and the consumers. In this way we include all three levels of the distribution system, and model the dynamic effects of forward buying by the retailer. The main features of the model can be summarized as follows. At the manufacturer level, each of two manufacturers is allowed to offer a regular price and a deal price, which, if accepted by the retailer, requires the retailer to display and merchandise the product. Moreover, the manufacturer is assumed to incur a selling cost with respect to offering a trade deal. The retailer, on the other hand, decides whether to accept the deal, determines how much to order, and sets the retail prices of the two brands. The retailer needs to set a price below the regular price to achieve maximum effectiveness from its display and merchandising activities. If the retailer accepts a trade deal, it incurs a fixed cost of display and merchandising the product reflecting the opportunity cost of display. If the retailer does not accept the trade deal, it can order at the non-deal price. Furthermore, the retailer can order more than what can be sold to the consumers in that period, i.e, forward buy. The retailer can carry inventory from either manufacturer at a cost but has an upper limit to the total amount of inventory carried in any period. The consumer population is assumed to consist of brand loyals and switchers. The brand loyals buy as long as the price is below the reservation price, while the switchers choose the brand on display if the retail price is lower than the regular price and the price of the competing brand. With these as the basic elements of the model, we characterize the equilibrium manufacturer and retailer pricing strategies with and without inventories in an infinite-horizon model with discounting. The central result of the paper is derived by comparing the profits to the manufacturers for the case when retailers are allowed to forward buy to the case when they are not allowed to forward buy. This comparison shows that although forward buying is profitable to the retailer through the availability of goods at lower prices, an important consequence of forward buying is the decreased intensity of competition between manufacturers. The decrease in the intensity of competition yields higher profits to the manufacturers as compared to the case where the retailer is not allowed to carry any inventory. The intuition behind our result is seen by taking a closer look at intensity of trade dealing in the presence and absence of forward buying. Consider the worst and the best trade deals offered by manufacturers, in equilibrium, when the retailer is not allowed to carry any inventory and explore their viability when the retailer is allowed to inventory. If the retailer is allowed to carry inventory and holds some inventory, the worst trade deals become unacceptable to the retailer. This is because the inventory allows the retailer to not buy all the units demanded in the subsequent period, and the retailer can therefore reject such trade deals. Similarly, the best trade deals offered by manufacturers when the retailer is not allowed to inventory do not remain as profitable because the retailer forward buys and the manufacturers lose future sales and profits. Thus trade deals at both extremes (the best and the worst) that were profitable to the manufacturers when the retailer is not allowed to forward buy are no longer viable when the retailer is allowed to forward buy. This increases the overall probability of not offering trade deals and leads to decreased intensity of competition.", "e:keyword": ["Competition", "Trade promotions", "Merchandising", "Forward buying"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.1.38", "e:abstract": "It is an everyday marketplace occurrence that brands lose and gain share. However, a brand's sales gain or loss can be attributable to very different factors, and thus understanding the sources of sales gain or loss would seem to be an important aspect of a brand manager's job. The primary purpose of this research is to develop a model that can answer the following questions: (i) What are the sources of gain or loss of a brand's sales due to category volume and brand switching? (ii) What consumer and marketing characteristics affect consumers' purchase frequency of the product category and that of different brands? (iii) Do all consumers behave similarly, or are there distinct segments which respond to marketing actions differently? (iv) If such segments exist, what is the size and composition of each segment, and what is the appropriate strategy for each group of consumers? Our modeling approach, which decomposes a brand's sales into category volume and brand choice components, has many similarities and also several differences with traditional approaches. Similar to the NBD and Dirichlet models, we assume that a consumer's category purchase rate follows a Poisson distribution, and the number of purchases per brand follows a multinomial distribution. Our model differs from traditional models by including marketing mix variables, by accounting for loyal or near-loyal consumers, by explicitly incorporating consideration sets, and by segmenting consumers on the basis of their brand perceptions and their responses to marketing mix variables. We account for consumer hetereogeneity by identifying homogeneous latent segments that capture differences in consumers' response to marketing variables in both brand choice and category volume behavior. Calibration of the category volume and brand choice models and the separation of loyalty and switching segments are done simultaneously so that there is no need to assume any specific hierarchy and, in contrast to the usual assumption of independence between choice and volume which may be unreasonable at the aggregate level, the requirement is that choice and volume decisions be independent only at the segment level. We investigate the properties of our model in the context of a national survey of supermarket purchases of jumbo paper towels. 2,500 households were surveyed who were asked to provide information on: rolls of paper towels bought in the last 4 weeks, rolls of each brand of paper towel bought in the last 4 weeks, brand usually bought, brands that a household bought or would consider buying, average price paid or expected for each brand bought or considered, ratings on 20 brand attributes (e.g., strength, absorbency, etc.), and demographic information (e.g., family size). We found that not only did the modeling framework outperform a variety of competing models, it also provided insights into the competitive structure in this market. The loyal segments could be distinguished on the basis of price sensitivity. Less than 10% of the loyal households were price insensitive and, in general, households showed increasing price sensitivity in their category volume decisions if they had more children and were heavier users of paper towels. This was consistent across all brands. The model estimates that about 71% of the households are switchers and five switching segments are needed to characterize household purchase patterns. Two of the largest switching segments (labeled as Price Sensitive and Value Segments) are very price sensitive in both their brand choice and category volume components. In both these segments private label brands are dominant with about 30% share. Segments also emerged on the basis of Strength, Absorbency, and Tearing Ease of paper towels. Interestingly, we found that brand shares within these segments are quite consistent with the objective quality ratings of brands as given by . Finally, price elasticity analysis for one of the brands, Bounty, reveals that a 5% drop in Bounty's price increases its sales by 13.6%. Almost half of this increase comes from brand switching, with the other half coming from increases in category volume (e.g., stockpiling). Bounty gains the most from the price sensitive segments (Price and Value Segments). Private labels are hurt least by Bounty's price cut.", "e:keyword": ["Buyer behavior", "Choice models", "Promotion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.1.60", "e:abstract": "We consider a group of frequently purchased consumer brands which are partial substitutes and examine two situations; the first where the group of brands is managed by a retailer, and second where the brands compete in an oligopoly. We assume that demand is a function of actual prices and reference prices, and develop optimal dynamic pricing policies for each situation. In addition to researchers studying pricing strategy, our results may interest retailers choosing between hi-lo pricing and an everyday low price, and manufacturers assessing whether to follow Procter & Gamble's lead and replace a policy of funding consumer price reductions through trade deals with a constant wholesale price. A reference price is an anchoring level formed by customers based on the pricing environment. The literature suggests that demand for a brand depends not only on the brand price, but also whether the brand price is greater than the reference price (a perceived loss) or is less than it (a perceived gain). The responses to gains and losses are asymmetric. Broadly speaking, we find that when enough consumers weigh gains more than losses, the optimal pricing policy is cyclical. Likewise, when they weigh losses more than gains, a constant price is optimal. Thus, we provide a rationale for dynamic pricing which is quite distinct from the three explanations previously offered: (1) decreasing unit variable costs due to learning effects, (2) the transfer of inventory to consumers who face lower inventory holding costs than do retailers, and (3) competitive effects. Our explanations apply even when the other explanations do not, i.e., in mature product categories where learning effects are minimal, when retailer inventories are minimized through the use of just-in-time policies and when competitive effects do not exist, as in a monopoly. Greenleaf (1995) has shown numerically that in the presence of reference price effects, the optimal pricing policy for a monopolist can be cyclical. We first analytically extend Greenleaf's result to a monopolist with a constant cost of goods, facing a homogeneous market where all customers either weigh gains more than losses or vice versa. Using this building block we examine a monopolist retailer managing multiple brands. We assume that demand is a linear function of prices of multiple brands, and together with an expression which reflects the reference price effect. Further, we assume that the retailer maximizes average profit per period. Next, we analyze a duopoly and extend the results to an oligopoly. We assume that the manufacturers are able to set the retail prices, as in an integrated channel. Here, we retain the same demand function as for the retailer and derive Markov Perfect Nash equilibria. We use two alternative processes of reference price formation: the exponential smoothed (ES) past price process which is frequently used in the literature, and for the multi-brand situations, the recently proposed reference brand (RB) process (Hardie, Johnson, and Fader 1993). In the latter, the reference price is the current price of the last brand boughtthe reference brand. We adapt the individual level RB formulation in Hardie et al., to an aggregate demand specification. For the ES process, we obtain most results analytically; for the RB process we use simulation. Finally, we extend our results to a population with two customer segments: Segment 1 which weighs gains more than losses, and Segment 2 which does the opposite, i.e., is loss averse. When the market consists exclusively of Segment 1 customers and ES is the reference price process, we find that prices are cyclical in all cases analyzed, i.e., for a monopoly, a monopolist retailer managing multiple brands, a duopoly, and an oligopoly. If the RB formulation is the underlying process, a monopolist retailer managing two brands uses cyclical prices, but in a duopoly, the equilibrium solution is for the brands to maintain constant prices. When all customers belong to Segment 2 (i.e., they are loss averse) constant prices are optimal in all cases for both reference price formulations. When the population consists of both Segment 1 and Segment 2 and the ES process applies, we develop a sufficient condition for cyclical pricing policies to be optimal. The condition is expressed in terms of the proportion of the two segment sizes, the absolute difference between the gain and loss parameters of each segment, and their respective exponential smoothing constants. Interestingly, for reasonable values of the latter two factors, cyclical policies are optimal even when the proportion of Segment 1 is quite small. Similar magnitudes are obtained numerically for the RB case.", "e:keyword": ["Reference prices", "Pricing research", "Promotion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.1.86", "e:abstract": "In this paper we examine the issue of balancing media advertising (pull strategy) and trade promotions (push strategy) for manufacturers of consumer packaged goods utilizing a three-stage game theoretic analysis and test model's implications with scanner panel data. We develop a model of two competing manufacturers who distribute their brand to consumers through a common retailer. In the model the manufacturers directly advertise their brand to consumers and also provide trade deals to the retailer. Each manufacturer's brand has a loyal segment of consumers who buy their favorite brand unless the competing brand is offered at a much lower price by the retailer. The number of loyal consumers is different for the two brands and so is the strength of their loyalty to their favorite brand. The loyal consumers of the brand with stronger loyalty require a larger price differential in favor of the rival brand before they will switch away from their favorite brand. The manufacturers first decide advertising spending level, and then the wholesale price of their respective brands. The two manufacturers do not observe each other's decisions while making these decisions, however they do take into account how the other firm is likely to react as a function of their own decisions. Advertising directly affects the strength of loyalty a consumer has for the favorite brand. If the favorite brand advertises, the loyalty strength increases but if the rival brand advertises, it decreases. The marginal effect of own versus competing brand advertising is different in magnitude. The two manufacturers provide trade deals to the retailer by discounting the brand from a regular wholesale price. The trade discounts are partially passed on to the consumers by the retailer who sets the retail prices of the two brands after observing the wholesale prices. The retail shelf price discounts make the promoted brand more attractive to the consumers due to the reduced price differential between their favorite brand and the promoted brand, thus affecting their switching behavior. The model and its analysis shed light on the role of brand loyalty in the optimal advertising and trade promotion policies for the two manufacturers. The analysis indicates that, if one brand is sufficiently stronger than the other and if advertising is cost effective, then the stronger brand loyalty requires less advertising than weaker brand loyalty, but a larger loyal segment requires more advertising than a smaller loyal segment. Moreover, stronger brand loyalty requires more trade promotion spending under these conditions. The analysis also indicates that the retailer promotes the stronger loyalty brand more often but provides a smaller price discount for it compared to the weaker loyalty brand. These analytical results can be understood better if we view advertising as a defensive strategy used to build brand loyalty which helps in retaining the loyal consumers, and price promotions as an offensive strategy used to attract the loyal consumers away from the rival brand. For example, the result that the stronger brand invests less in advertising than the weaker brand can be explained as follows. The stronger loyalty brand does not find use of advertising attractive because it faces little threat from the weaker brand due to its sufficiently stronger loyalty. Instead it spends more on promotions (provided advertising is cost effective) to attract away the weaker brand's loyal consumers. The weaker brand, on the other hand, finds it optimal to defend its loyal franchise by spending more on advertising, as promotions do not help much due to the difficulty in attracting away the stronger brand's loyal consumers. In this sense, the stronger brand plays offensive by using more trade promotions, and the weaker brand plays defensive by emphasizing advertising. We also conduct an empirical analysis of the model's propositions using scanner panel data on seven frequently purchased nondurable product categories. In a sample of 38 national brands from the seven categories we find that weaker loyalty brands spend more on advertising; brands with larger loyal segment spend more on advertising; and the retailer promotes stronger loyalty brands more often but provides a smaller price discount on average for them compared to weaker loyalty brands. These findings are consistent with the model.", "e:keyword": ["Advertising", "Brand loyalty", "Game theory", "Promotional mix"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.1.109", "e:abstract": "In our paper, Cross-Validating Regression Models in Marketing Research, which appeared in the Vol. 12, No. 4, Fall 1993 issue of Marketing Science, pp. 415427, we develop a formal statistical test for cross-validating regression models under the simple random splitting framework. The original publication neglected to present definitions for two of the parameters used. We present these in this Addendum.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.15.2.113", "e:abstract": "The primary objective of this paper is to develop a parsimonious model for forecasting the gross box-office revenues of new motion pictures based on early box office data. The paper also seeks to provide insights into the impact of distribution policies on the adoption of new products. The model is intended to assist motion picture exhibitor chains (retailers) in managing their exhibition capacity and in negotiating exhibition license agreements with distributors (studios), by allowing them to project the box-office potential of the movies they plan to or currently exhibit based on early box-office results. It is also of interest to practitioners in other software industries (e.g., music, books, CD-ROMs) where the distribution intensity is highly variable over the product life cycle and is an important determinant of new product adoption patterns. The model and its extensions are of interest to academic researchers interested in modeling distribution effects in new product adoption, as well as forecasters looking for ways to leverage historical data on related products to forecast the sales of new products. We draw upon a queuing theory framework to conceptualize stochastically the consumer's movie adoption process in two stepsthe to see the new movie, and the on the adoption decision. The parameter for the time-to-decide process captures the intensity of information intensity flowing from various information sources, while the parameter for the time-to-act process is related to the delay created by limited distribution intensity and other factors. Our conceptualization extends existing new product forecasting models, which assume that consumers act instantaneously on the motivating information they receive about the new product. The resulting model is parsimonious, yet it accommodates a wide range of adoption patterns. In addition, the stochastic formulation allows us to quantify the uncertainty surrounding the expected adoption pattern. In the empirical testing, we focus on the most parsimonious version of the modeling framework. BOXMOD-I, a model that assumes stationarity with respect to the two shape parameters that characterize the adoption process. The model produces fairly accurate early forecasts using at most the first three weeks of data for calibration, and the predictive performance of the model compares favorably with benchmark models. We propose extensions of the basic model that account for more realistic non-stationary distribution intensity patternsincluding a wide release pattern that relies on intensive distribution and promotion, and a platform release pattern that involves a gradual buildup of distribution intensity. Finally, we present an adaptive weighing scheme that combines initial parameter estimates obtained from a meta-analysis procedure with estimates obtained from early data to produce forecasts of box-office revenues for a new movie when little or no box-office data are available. An important finding from the empirical testing is that motion picture box-office revenue patterns display remarkable empirical regularity. We find that there are only three classes of adoption patterns, and these can all be represented within the basic model by using a two-parameter. Exponential or Erlang-2 probability distribution, or a three parameter Generalized Gamma distribution. We also find that cumulative box-office revenues can be predicted with reasonable accuracy (often within 10% of the actual) using as little as two or three data points. However, our attempts to predict revenue patterns without any sales data meet with limited success. While the scale parameter can be estimated reasonably well from a historical database of parameter values, we find that it is considerably more difficult to predict the shape parameters using historical data. The parsimony we seek in developing the model comes at the cost of several limiting assumptions. We assume that the time-to-decide subprocess and the time-to-act subprocess are independent, which may not be the case if decisions on continued exhibition by retailers are endogenously related to box-office revenues over the life cycle. In the basic model formulation, we also assume that the time-to-act process can be represented by an exponential distribution, which may not always be the case. While we provide some empirical evidence to support these assumptions, further research could relax these and other assumptions to enrich the basic model, although this would entail some loss in parsimony.", "e:keyword": ["Forecasting", "Motion pictures", "Distribution", "Stochastic models", "Consumer behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.2.132", "e:abstract": "Our objective in this paper is to explain the relationship between a manufacturer's brand advertising and its impact on wholesale and retail margins in consumer goods markets. We construct a model of re-tailers and manufacturers, and using tools from game theory explain why under some conditions a manufacturer's advertising can squeeze, i.e., lower, the retail margin while simultaneously increasing the wholesale margin. Our paper should be of interest to applied analytical and empirical researchers in marketing as well as managers interested in understanding the strategic impact of brand advertising on margins. The consumer goods retail market is characterized by intense rivalry among retailers competing for a share of the consumer dollar. Retailers carry many products, and on any given purchase occasion a typical consumer buys a subset of the vast number of items a retailer has on its shelf. In general consumers are ignorant about the prices of all the products they want to buy and consequently select a retailer to shop at based on the advertised prices of a subset of the products they intend to buy. Given this, retailers tend to compete more aggressively based on the prices of a selected set of items by advertising these prices to consumers. The items that the retailers select to compete on are those that most consumers desire and value highly. Since the profit from any customer is the sum of profits from advertised and unadvertised items, the intensity of retail competition, as evident from the prices of these items, increases with the amount the consumer will expend on the unadvertised items once at the store. This aggressiveness therefore translates into lower retail margins on these selected items since the retailers expect that consumers, once inside a store, will buy non-advertised products as well on which the retailers make money. Thus manufacturers who are more adept at using pull strategies to enhance the popularity of their product, obtain d significant competitive advantage vis à vis others. The positioning of the product and the image conveyed through advertising act as drivers in creating this advantage which results in higher wholesale prices that these manufacturers can charge the retailers. There are several key insights from our analysis. Our model explains why retail and wholesale margins can move in opposite directions and also suggests whenin those retail markets where consumers shop for a basket of goods. Our analysis also reveals that retailers make higher margins on unadvertised products and less on advertised products. Furthermore it shows the power of a popular brand where its popularity can be enhanced through brand advertising. From a managerial standpoint we also show that the effectiveness of advertising should not be narrowly interpreted in terms of increase in share or awareness but should include the ability to charge a higher wholesale price. Finally our analysis sheds light on extant, and provides guidance to future, empirical work in this area.", "e:keyword": ["Advertising", "Wholesale and retail margins", "Channel competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.2.152", "e:abstract": "Consumer heterogeneity is fundamental to the marketing concept, providing the basis for market segmentation, targeting and positioning, as well as micro-marketing. Substantial effort has already been devoted to incorporate heterogeneity in brand choice models. However, most of the research in this area has focused on differences in preferences or tastes across consumers. In contrast, limited attention has been given to the possibility that consumers might also differ in the process they follow when making choices. Failure to account for either form of consumer heterogeneity may lead to misinterpretations of market structure and market segments, as demonstrated in our simulations. Our main research objective in this paper is to account for these two forms of consumer heterogeneity with an integrated model. We develop a choice model that simultaneously identities consumer segments on the basis of their preferences, response to the marketing mix, and choice processes. This choice model is a finite mixture of nested logit models that incorporates the mixture of multinomial logits as a special case. One main limitation of the now popular multinomial logit model is that choices by each consumer are assumed to be . As a consequence, that model predicts that, within each segment, a brand has exactly the same cross-elasticities upon every competitor. In contrast, our mixture of nested logits allows for violations of the IIA property within each segment, leading to more realistic patterns of brand competition, where one brand draws differently from each competitor, depending on the choice process used by members of the segment. In addition to the flexible combination of choice processes and preference structures, our model also incorporates a latent class of hard-core loyal consumers who are expected to always buy the same brand, and are thus insensitive to price and promotions. Application of our model to household scanner data in the peanut butter category led to four types of segments: (a) , who do not respond to price and promotions. (b) , for which the brand choice precedes the choice of product form (creamy vs. crunchy). (c) . Members of these segments first choose the product form and then decide for the brand (Peter Pan, Jif, Skippy, or Store brand). (d) . Members of these segments make choices according to the IIA property. Our results showed a large (14%) hard-core loyal segment. We also identified three segments of the , one segment of the and three segments. All these segments also differ in their preferences for brands and product forms. One useful feature of our model is that it allows for cross-elasticity structures with non-proportional draw, even when computed within a homogeneous segment. Most importantly, the proposed model for more complex cross-elasticity structures within a segment, rather than the elasticities to any particular pattern a priori. These cross-elasticity patterns imply distinct competitive environments within each segment, leading to different price and promotion strategies in each segment. For example, a promotion by Jif-crunchy to a segment is more likely to cannibalize on shares of the creamy form of the same brand, than to draw from competitors. If offered to a segment, the same promotion is more likely to draw shares from competitors. Because of its finite-mixture formulation, our proposed model shares the same limitations of this category of models. For example, maximum likelihood estimation of the model may lead to local optima, thus requiring a multiple number of random starts, in order to increase the chances of finding the model that best fits to the data. Our model also requires specification of the number and type of segments in each particular application. Several criteria are available to select the number of segments. However, the specification of choice processes must be based on a heuristic procedure, or on the basis of prior knowledge.", "e:keyword": ["Market structure", "Finite mixture model", "Nested logit", "Household heterogeneity", "Brand choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.2.173", "e:abstract": "The drive to satisfy customers in narrowly defined market segments has led firms to offer wider arrays of products and services. Delivering products and services with the appropriate mix of features for these highly fragmented market segments requires understanding the value that customers place on these features. Conjoint analysis endeavors to unravel the value or partworths, that customers place on the product or service's attributes from experimental subjects' evaluation of profiles based on hypothetical products or services. When the goal is to estimate the heterogeneity in the customers' partworths, traditional estimation methods, such as least squares, require each subject to respond to more profiles than product attributes, resulting in lengthy questionnaires for complex, multiattributed products or services. Long questionnaires pose practical and theoretical problems. Response rates tend to decrease with increasing questionnaire length, and more importantly, academic evidence indicates that long questionnaires may induce response biases. The problems associated with long questionnaires call for experimental designs and estimation methods that recover the heterogeneity in the partworths with shorter questionnaires. Unlike more popular estimation methods, Hierarchical Bayes (HB) random effects models do not require that individual-level design matrices be of full rank, which leads to the possibility of using fewer profiles per subject than currently used. Can this theoretical possibility be practically implemented? This paper tests this conjecture with empirical studies and mathematical analysis. The random effects model in the paper describes the heterogeneity in subject-level partworths or regression coefficients with a linear model that can include subject-level covariates. In addition, the error variances are specific to the subjects, thus allowing for the differential use of the measurement scale by different subjects. In the empirical study, subjects' responses to a full profile design are randomly deleted to test the performance of HB methods with declining sample sizes. These simple experiments indicate that HB methods can recover heterogeneity and estimate individual-level partworths, even when individual-level least squares estimators do not exist due to insufficient degrees of freedom. Motivated by these empirical studies, the paper analytically investigates the trade-off between the number of profiles per subject and the number of subjects on the statistical accuracy of the estimators that describe the partworth heterogeneity. The paper considers two experimental designs: each subject receives the same set of profiles, and subjects receive different blocks of a fractional factorial design. In the first case, the optimal design, subject to a budget constraint, uses more subjects and fewer profiles per subject when the ratio of unexplained, partworth heterogeneity to unexplained response variance is large. In the second case, one can maintain a given level of estimation accuracy as the number of profiles per subject decreases by increasing the number of subjects assigned to each block. These results provide marketing researchers the option of using shorter questionnaires for complex products or services. The analysis assumes that response quality is independent of questionnaire length and does not address the impact of design factors on response quality. If response quality and questionnaire length were, in fact, unrelated, then marketing researchers would still find the paper's results useful in improving the efficiency of their conjoint designs. However, if response quality were to decline with questionnaire length, as the preponderance of academic research indicates, then the option to use shorter questionnaires would become even more valuable.", "e:keyword": ["Consumer preferences", "Multi-attributed models", "Consumer research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.2.192", "e:abstract": "In Raju, Dhar, and Morrison (1994), a paper that appeared earlier in this journal, we developed an analytical model and conducted empirical analyses to examine the effect of package coupons on market share. In this epilogue, we extend the analytical framework in our earlier paper to study the relative impact of package coupons on profits. We also report findings from five quasi-experiments (including two new quasi-experiments) that provide empirical validity to our model-based predictions. Our results have significance for brand managers in packaged goods firms. Our analysis suggest that while selecting among package coupons, brand managers should carefully define which performance criterion to use: market share impact, redemption rate, or profit impact. Package coupons that lead to the highest market share impact (or redemptions) may not lead to the highest profit impact. We compare the relative profit impact of the following package coupons: peel-off, on-pack, and in-pack. coupons must be redeemed on the same purchase occasion on which they are obtained. coupons are obtained at one purchase occasion but can only be redeemed for a discount on the couponed brand at a future purchase occasion. coupons are similar to on-pack coupons except that the consumer is aware of the presence of these coupons when the product is purchased (in-pack coupons are printed or placed inside the package). Consumers with an in-pack or on-pack coupon from a previous purchase occasion will have a higher probability of purchasing the couponed brand even if the brand was not currently offering coupons. Consequently, to understand choice behavior, it is not enough to take into consideration the current choice environment; our model therefore must also keep track of whether or not a consumer has a package coupon that was obtained on an earlier purchase occasion. In other words, since choice is dependent on the purchase environment as well as the state of the consumer, we use a Markov model to represent the choice process. Analytical results are derived based on the long run probabilities of the Markov transition matrix. Our analytical and empirical results suggest that by and large, of the various package coupons examined in our research, on-pack coupons lead to the highest impact on profits. Furthermore, while peel-offs lead to a higher market share than in-packs, because in-packs stimulate repurchase among previous buyers, they lead to higher profits than peel-offs; though only for stronger brands.", "e:keyword": ["Sales promotions", "Coupons", "Stochastic models", "Brand choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.3.207", "e:abstract": "The advent of optical scanning devices and decreases in the cost of computing power have made it possible to assemble databases with sales and marketing mix information in an accurate and timely manner. These databases enable the estimation of demand functions and pricing/promotion decisions in real time. Commercial suppliers of marketing research like A. C. Nielsen and IRI are embedding estimated demand functions in promotion planning and pricing tools for brand managers and retailers. This explosion in the estimation and use of demand functions makes it timely and appropriate to re-examine several fundamental issues. In particular, demand functions are latent theoretical constructs whose exact . Estimates of price elasticities, profit maximizing prices, inter-brand competition and other policy implications are on the parametric form assumed in estimation. In practice, many forms may be found that are not only theoretically plausible but also consistent with the data. The different forms could suggest different profit maximizing prices leaving it unclear as to what is the appropriate pricing action. Specification tests may lack the power to resolve this uncertainty, particularly for non-nested comparisons. Also, the structure of these tests does not permit seamless integration of estimation, specification analysis and optimal pricing into a unified framework. As an alternative to the existing approaches, I propose a Bayesian mixture model (BMM) that draws on Bayesian estimation, inference, and decision theory, thereby providing a unified framework. The BMM approach consists of input, estimation, diagnostic and optimal pricing modules. In the input module, alternate parametric models of demand are specified along with priors. Utility structures representing the decision maker's attitude towards risk can be explicitly specified. In the estimation module, the inputs are combined with data to compute parameter estimates and posterior probabilities for the models. The diagnostic module involves testing the statistical assumptions underlying the models. In the optimal pricing module the estimates and posterior probabilities are combined with the utility structure to arrive at optimal pricing decisions. Formalizing demand uncertainty in this manner has many important payoffs. While the classical approaches emphasize choosing a demand specification, the BMM approach emphasizes constructing an objective function that represents a mixture of the specifications. Hence, pricing decisions can be arrived at when there is no consensus among the different parametric specifications. The pricing decisions will reflect parametric demand uncertainty, and hence be more robust than those based on a single demand model. The BMM approach was empirically evaluated using store level scanner data. The decision context was the determination of equilibrium wholesale prices in a noncooperative game between several leading national brands. Retail demand was parametrized as semilog and doublelog with diffuse priors for the models and the parameters. Wholesale demand functions were derived by incorporating the retailers' pricing behavior in the retail demand function. Utility functions reflecting risk averse and risk neutral decision makers were specified. The diagnostic module confirms that face validity measures, residual analysis, classical tests or holdout predictions were unable to resolve the uncertainty about the parametric form and by implication the uncertainty with regard to pricing decisions. In contrast, the posterior probabilities were more conclusive and favored the specification that predicted better in a holdout analysis. However, across the brands, they lacked a systematic pattern of updating towards any one specification. Also, none of the priors updated to zero or one, and there was considerable residual uncertainty about the parametric specification. Despite the residual uncertainty, the BMM approach was able to determine the equilibrium wholesale prices. As expected, specifications influence the BMM pricing solutions in accordance with their posterior probabilities which act as weights. In addition, differing attitudes towards risk lead to considerable divergence in the pricing actions of the risk averse and the risk neutral decision maker. Finally, results from a Monte Carlo experiment suggest that the BMM approach performs well in terms of recovering potential improvements in profits.", "e:keyword": ["Pricing", "Bayesian econometrics", "Decision theory", "Regression", "Nonnested models", "Demand structures", "Scanner data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.3.222", "e:abstract": "Order of entry has been demonstrated to have a significant effect on market share. A number of explanations for this effect have been suggested in the marketing and strategy literatures. To date, the market share advantage gained by pioneers has typically been treated as a main effectan automatic regularity. Treating order-of-entry as a main effect implies that there is no penalty on the effectiveness of a brand's marketing instruments for late entry and that a late entrant can compensate for being late by dedicating sufficient marketing resources to their product. In this study, we investigate the influence of order-of-entry into a market on the effectiveness of a firm's marketing mix decisions by asking the question, Can followers compensate for not being first by their marketing mix decisions? Also, even if they can compensate for being late, does this effort become increasingly more difficult with later entry? That is, are there asymmetries in the effectiveness of a brand's marketing mix variables that relate to its order of entry into the market, or as has been typically assumed to date, is order of entry strictly a main effect? An asymmetry exists, for example, if the market response to advertising is different for the first entrant versus the second or third entrant. An asymmetry also exists if the effects of, say, a price change by the first entrant on the second entrant are different than the effects on the third entrant. We develop a market share attraction model where the parameters vary as a function of order-of-entry. Our main contribution is in modeling the sources of order-of-entry advantage as asymmetries in the effectiveness of a brand's marketing instruments. Hence, distinct from previous research we explain why there are inherent order-of-entry effects. This paper is potentially of interest to researchers developing market share models and studying the effectiveness of marketing-mix variables. The substantive implication of our results concern directly academics interested in marketing strategy as well as the practicing marketing strategists. We model asymmetries in the market response of early entrants versus late entrants using data from two durables and three nondurables categories. With one exception, all data sets are established from the inception of the category and hence do not suffer from the possible bias of excluding pioneers who have failed. Results show that asymmetries in the effectiveness of a brand's marketing mix variables are an essential source of order-of-entry effects; we find that the main effects of order of entry are minimal. Order-of-entry effects do not necessarily lead to lower shares, but overcoming these effects is not without substantial cost to the late entrant. Our results support previous research that has demonstrated advantages to early entry. In addition, we provide guidelines for how late entrants should compete. Later entry tends to reduce a competitor's price sensitivity, suggesting that they not instigate in a price war with earlier entrants in order to gain share. Order-of-entry tends to decrease response to quality and to promotion. To achieve the same impact on market share, later entrants need a bigger change in quality and need to spend more on promotion. Our data did not support an asymmetric effect on advertising.", "e:keyword": ["Marketing mix", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.3.243", "e:abstract": "The issue of consumer heterogeneity in discrete choice analysis has been attracting much attention recently. Research has suggested that heterogeneity can result in biased parameter estimates which, in turn, can lead to incorrect conclusions. Among the many methods proposed in the literature to handle heterogeneity, models seem to be the most attractive from a substantive point of view. However, in order to provide consistent estimates, these models typically require long purchase histories. This difficult constraint has prevented their widespread use. In this paper, we propose a new model which offers the benefits of fixed effects models without requiring long purchase histories. Our approach differs from the classic formulation in two ways. First, we calibrate the common and fixed-effects sequentially rather than simultaneously. This two-stage estimation permits us to obtain unbiased estimates of the common parameters even when the sample has households with very few observations. Second, we incorporate the fixed effects in a multiplicative rather than in an additive form. Our assumptions regarding the error term result in a probit model with heteroskedastic and temporally correlated random utilities. We develop a method of moments based estimator to calibrate parameters of models with this type of error structure. This procedure exploits the property that the method of moments yields estimates which are asymptotically as efficient as the maximum likelihood estimates given an appropriate starting point and a matrix of instruments. This is a very general procedure in that its mechanics are not tied to the proposed model. It can be readily applied to other problems where probit models are used to analyze data with serial correlation. This is a methodological contribution of the paper. The model and estimation procedure are illustrated on the A. C. Nielsen scanner data base for the liquid detergent category. Our results indicate that the proposed model can provide a better fit than random effects and loyalty based models and better predictive performance than random effects models. From a substantive perspective, our analysis provides the following findings. (a) thus, most households have very similar price sensitivity in this category; (b) suggesting that there are some households that are highly responsive to promotions while there are quite a few households that are less influenced by promotions; (c) which is an indication that a reduction in preference for a brand is associated with an increase in price sensitivity and ; (d) thus, as the preference for a brand increases, response to promotions by the brand also increases; and (e) implying that, as households become more price sensitive, purchases may be driven more by price than by promotions; in other words, rather than responding to promotions such as displays and features as signals of a reduced price, price sensitive households actively evaluate prices in arriving at a choice. The proposed approach can be used by theoretical as well as applied researchers of brand choice behavior. Specifically, estimates of fixed-effects provided by the model can be used to cluster households into groups with similar parameters. Profiling the resulting groups in terms of demographics would provide interesting insightsfrom a theoretical perspectiveregarding the household characteristics that are associated with different types of response behavior. It would also help managers to better target their brands. Using the proposed approach for cross-category analyses of fixed-effects would be an interesting area of future research. For instance, given a sample of households and their purchase histories in multiple categories, the proposed model can be used to explore the relationship between household behavior and category characteristics. In particular, issues such as the following can be investigated by comparing household-level estimates across categories: (a) are households that are highly price sensitive in one category also price sensitive in other categories? (b) what household characteristics are associated with high (low) price sensitivity or high (low) promotion response across categories? and (c) in what types of categories are households likely to exhibit high (low) price sensitivity or high (low) promotion response?", "e:keyword": ["Brand choice", "Choice models", "Heterogeneity", "Fixed-effects", "Method of moments"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.3.262", "e:abstract": "We examine the basic premise that consumers may anticipate future promotions and adjust their purchase behavior accordingly. We develop a structural model of households who make purchase decisions to minimize their expenditure over a finite period. The model allows for future expectations of promotions to enter the purchase decision. Households with adequate inventory of the product may face a trade-off of buying in the current period with a coupon or defer the purchase until next period, given their expectations of future promotions. Thus, we provide a framework for examining the impact of consumer expectations on choice behavior. The target audiences for our paper are (a) empirical researchers who intend to make structural models part of their applied research agenda; and (b) managers who value and seek to understand the impact of consumers' coupon expectations on current purchase behavior. Our research objective is to provide an empirical framework to examine whether and to what extent consumers anticipate future coupon promotions and adjust purchase behavior. The central premise of our approach is that a rational consumer minimizes the present discounted value of the cost of a purchase where cost in a single period consists of purchase price, inventory holding cost, gains from coupons, and potential stockout cost. We aim to test whether our hypotheses regarding the various elements of the cost structure are supported and that whether consumers take into account future discounted cost when making current purchase decisions. The research methodology we adopt is relatively new in econometrics and known as the estimable stochastic structural dynamic programming method. The methodology amounts to incorporating a maximum likelihood routine embedded in a dynamic programming problem. The dynamic programming problem is solved several times within a maximum likelihood iteration for each value of the state space elements and for each value of the parameters in the parameter set. The state space in our model consists of purchase and nonpurchase alternatives in each time period, coupon availability and no coupon availability in each time period, level of inventory in each time period for each household, and consumption rate of each household. We use scanner panel data on purchases in the disposable diaper product category and promotions. We estimate the inventory holding and stockout costs, brand-specific value of coupons, and consumers' expectations of future coupons. The key insights and lessons learned can be summarized as follows: (1) Our results are consistent with the notion that consumers hold beliefs about future coupons, and that such beliefs affect the purchase decision. We find that the dynamic optimization model performs significantly better than a single-period optimization model and a naive benchmark model. (2) We find a high and significant stockout cost, consistent with the essential nature of the product category. Our estimate of the holding cost yields a reasonable annualized percentage value when converted to the cost of capital. We find that consumer valuation of coupons differ markedly across brands. (3) Our empirical evidence supports the notion that consumers hold beliefs about future coupon availability. We also find that the expectations about future coupons, estimated endogenously, differ depending upon whether or not a coupon was available in the current period. Thus, the proposed model structure yields rich managerial insights and facilitates several what if scenarios. A possible limitation of our model, and estimable structural models in general, is the computational cost. While it is possible to conceptually extend the state space to accommodate variations across households and add a richer parameter structure, each addition multiplies the size of the state space and the computation time. For this reason, we have kept the state space as tight as possible and refrained from additions that would otherwise enable us to incorporate heterogeneity in consumer decisions. For example, we assumed that consumers are similar other than reflected by their purchase behavior. We built a category purchase incidence model rather than a brand choice model. We refrained from including unobserved heterogeneity in the parameters. We chose to opt out of modeling autocorrelation and other time-dependent error term patterns in the likelihood function. Thus, we have made an effort to build a structural model that reasonably reflects consumer purchase behavior without requiring expensive computation. Currently, there are developments in econometrics to approximate the computation of the valuation functions without sacrificing much accuracy. When these methods are well developed we expect that structural models will become more commonplace in marketing.", "e:keyword": ["Consumer expectations", "Econometrics", "Estimable stochastic dynamic programming models", "Promotions", "Structural models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.3.280", "e:abstract": "In this paper we develop a general class of dynamic brand choice models, called (LB) models, which are consistent with the theory of random utility maximization of consumer choice behavior. The underlying random utility process is Markov, and the inter-temporal evolution of the (utility-maximizing) brand choice process is also Markov. The models permit parsimonious parameterizations of the random utility process in brand choices with the resulting switching probabilities being functionally related to explanatory variables. The model allows for structural state dependence (feedback), habit persistence (inertia), and unobserved heterogeneity. The theoretical development shows that several well known stochastic brand choice models can be deduced from random utility maximization theory. From a managerial perspective, the usefulness of the proposed model stems from its ability to separate out the effects of habits, state dependence and heterogeneity. Strong state dependence effects imply incentives for inducing trial of a brand (e.g., product sampling). In contrast with state dependence, a strong habit persistence effect may be indicative of buyer behavior where inducement of trial of a different brand may not be sufficient to maintain a (sustained) defection of the consumer from the habitually purchased brand to the trial brand. Failure to distinguish between these two effects has important implications. For example, a model that only accounts for state dependence effects would, in the presence of only habit persistence, incorrectly attribute it to state dependence. Based on this the manager could decide to embark on an expensive sampling program that might prove ineffective due to the absence of state dependence. It is also important to distinguish between the effects of unobserved heterogeneity and state dependence. In the absence of true state dependence, failing to account for unobservable variations across households (such as differences in price sensitivities), results in the temporally persistent unobservable elements showing up as state dependence in the model. Hence, a manager may incorrectly opt for sampling as the appropriate marketing action, whereas, a couponing or price promotion strategy should have been preferred. We estimate the model parameters using the AC Nielsen household scanner panel data set on catsup purchases. Further, we investigate empirical techniques for overcoming the initial conditions problem that affects many dynamic models of brand choice. Through simulation analysis using the estimated parameters, we show that the calculated profitability of a promotion must take into account the multi-period impact due to state dependence. Further, we demonstrate how ignoring heterogeneity can result in spurious state dependence, thereby making marketing tactics such as product sampling appear far more attractive than they actually are. Specifically, the model that accounts only for habit persistence and state dependence effects predicts a share increase of 2.4% points through sampling for one of the brands in the empirical analysis. Once the effects of unobserved heterogeneity are accounted for, however, this number drops precipitously to 0.3%. While even this low number might fulfill the objectives of the brand manager, it ensures that expectations are not eight times that number. It is important to note that this paper is a first attempt at analyzing the three fundamental dimensions of dynamic brand choice behavior. Our formulation represents a reduced form approach as opposed to a structural approach. Other limitations of the model include our focus on time-invariant choice sets. Extensions of the model to situations where choice sets vary over time are possible, but are difficult. Another limitation is the dependence on multivariate extreme value distributions. An alternative would be to use multivariate normal distributions, i.e., a probit-like structure which has certain useful properties. A comparison of the two approaches would be useful. Empirical extensions of the model to allow for higher-order processes and nested logit type model structures could also be fruitful. Explicitly incorporating variety-seeking behavior in the model would further enrich the theoretical framework proposed in this paper.", "e:keyword": ["Brand choice", "Econometric models", "Estimation and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.4.301", "e:abstract": "The efficient operation of a salesforce is a critical element in the profitability of many firms. Three factors play key roles: the salesforce's size, its allocation and its productivity. This gives rise to the following questions: can salesforce performance be improved by (1) hiring more salespeople, (2) allocating them more effectively to the various sales districts and/or (3) improving salesperson productivity through better calling patterns in terms of consumers and product line items? The practice of most firms and the methodology used in most of the academic literature to address salesforce design and productivity questions is a Bottom Up approach. This approach starts with assessments by each salesperson of the sales and effort corresponding to each customer and prospect in their territory. These assessments are then aggregated to the territory, district and national levels. This paper takes an alternative Top Down approach. It is based on an estimated relationship between district level sales and salesforce size, effort and other variables. This more macro level decision tool can be used by management in parallel to, and as an objective check of, the more conventional and more subjective Bottom Up approach. We develop an efficient frontier methodology which allows us to estimate how total district sales respond to salesforce size, district potential and competitive activity in the firm's best performing districts. The methodology utilized is based on Data Envelopment Analysis (DEA) and yields a benchmark measure of each district's efficient frontier sales (sales assuming the district's salesforce allocates its effort as done in the best performing districts). Based on the estimated response function we discuss the three potential sources of increased profitability: closing the inefficiency gap of each of the lower performing districts, optimally reallocating the current salesforce to the various districts, and changing the current size of the salesforce to its optimal level. The inefficiency gap issue is addressed through comparison of the parameter estimates for the best districts obtained through our methodology with those of an average district sales response function obtained using regression analysis. This comparison points to an important methodological finding. The use of multiple estimation results may lead to an improved understanding of the phenomenon being studied (in our case, the identification of the likely causes of district productivity inefficiencies). The latter two sources of increased profitability, salesforce reallocation and changes in the current salesforce size, are addressed analytically given the district level efficient frontier sales response function. The proposed Top Down procedure using the efficient frontier methodology and the insights it provides are examined by evaluating the operations of two different salesforces, one selling manufacturing equipment and the other business equipment. In both cases, regression-based analysis would have resulted in a declaration that the status-quo was close to optimal, while the frontier-based analysis pointed out that strong gains were possible in certain districts. In particular, for both firms, the greatest increases in profit are obtained through improved salesforce efficiency in the lower performing districts, not through salesforce size or district allocation adjustments. At the more micro-level, a comparison of the frontier and regression parameters made it possible to identify which specific changes in the daily operations of the salesforces would allow the realization of these potential productivity gains. In our two cases this could be obtained through more emphasis on pursuing prospective accounts.", "e:keyword": ["Salesforce", "Benchmarking", "Frontier estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.4.321", "e:abstract": "An important aspect of marketing practice is the targeting of consumer segments for differential promotional activity. The premise of this activity is that there exist distinct segments of homogeneous consumers who can be identified by readily available demographic information. The increased availability of individual consumer panel data open the possibility of direct targeting of individual households. The goal of this paper is to assess the information content of various information sets available for direct marketing purposes. Information on the consumer is obtained from the current and past purchase history as well as demographic characteristics. We consider the situation in which the marketer may have access to a reasonably long purchase history which includes both the products purchased and information on the causal environment. Short of this complete purchase history, we also consider more limited information sets which consist of only the current purchase occasion or only information on past product choice without causal variables. Proper evaluation of this information requires a flexible model of heterogeneity which can accommodate observable and unobservable heterogeneity as well as produce household level inferences for targeting purposes. We develop new econometric methods to implement a random coefficient choice model in which the heterogeneity distribution is related to observable demographics. We couple this approach to modeling heterogeneity with a target couponing problem in which coupons are customized to specific households on the basis of various information sets. The couponing problem allows us to place a monetary value on the information sets. Our results indicate there exists a tremendous potential for improving the profitability of direct marketing efforts by more fully utilizing household purchase histories. Even rather short purchase histories can produce a net gain in revenue from target couponing which is 2.5 times the gain from blanket couponing. The most popular current electronic couponing trigger strategy uses only one observation to customize the delivery of coupons. Surprisingly, even the information contained in observing one purchase occasion boasts net couponing revenue by 50% more than that which would be gained by the blanket strategy. This result, coupled with increased competitive pressures, will force targeted marketing strategies to become much more prevalent in the future than they are today.", "e:keyword": ["Target marketing", "Coupons", "Heterogeneity", "Bayesian hierarchical models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.4.341", "e:abstract": "While the new product literature suggests that managerially controllable factors most strongly affect new product success, few studies have examined how these factors differ across countries. The objectives of this article are: (1) to develop a model of factors associated with new product success, (2) to directly compare the factors that managers perceive to be associated with new product success in the United States and China, and (3) to demonstrate the application of various statistical analyses for increasing the confidence that may be placed in empirical findings and outline methods for assessing whether significant estimation biases exist in cross-sectional data. Our paper should be of interest to new product researchers and international comparative marketing researchers. The implications of our results should also be of considerable value and interest to executives faced with the complex task of selecting and managing new product development projects as well as to those firms experiencing international competition. To accomplish our objectives, we develop a model of managerially controllable factors related to new product success, which includes variables related to the organization, the new product development process, and the product itself. The model is tested using data collected on 142 new products launched in the United States and 470 new products launched in China. We conduct case studies to examine the appropriateness of the data collection methods, to establish the content validity of the concepts, and to assess the usefulness of the measures and constructs in a Chinese context. The model was tested using EQS with covariance matrices as input. We tested the measurement model before assessing the structural relationships. Once the measurement issues were satisfactorily resolved, the structural model was tested for each country individually. Since the results of the individual models for the United States and China were satisfactory, we performed a two-group simultaneous path analysis in order to test for similarities and differences in the factors of new product success between the United States and China. We tested whether or not the path coefficients were invariant across the two countries using a Lagrangian Multiplier test. The challenge for all cross-sectional studies is to reject the hypothesis that many of the parameters associated with the dependent variable are biased by omitted firm level effects and other specification errors. We examine these possible biases within the structural equations framework by examining the robustness of the parameters with and without the measurement error interactions fixed and an examination of nominological validity for each country. In addition, since we collected two observations per firm in our Chinese sample, we also examine whether bias exists due to omitted firm effects by comparing within and between estimates. The stability of the within and between estimates also suggests that our findings are robust to omitted firm effects. Our findings provide important managerial guidelines concerning appropriate managerial actions to take in the new product development process for new product managers. First, firms are advised to build appropriate new product development resources and expertise. In both countries, our findings suggest that adequate marketing research, sales force, distribution, advertising, and promotional resources and skills are required for proficiently conducting market assessment studies, testing products, and introducing products. Furthermore, technical resources and skills are positively linked with proficiency in conducting technical activities. Sufficient R&D and engineering resources and skills are related to proficiency in performing technical assessments, designing products, and manufacturing products. Second, our results reveal that a higher proficiency in marketing and technical activities leads to a higher level of new product success in both countries. The level of proficiency of technical activities appears to relate more to the level of new product success than do marketing activities. Third, it is important to collect and assess market and competitive information in order to understand customers' needs, wants, and specifications for the product; to know customers' price sensitivity; to understand customers' purchase decisions; and to learn about competitors' strategies, strengths, and weaknesses. In China, product quality is the second most highly correlated factor of new product success. Therefore, firms competing in China should direct their marketing and technical efforts toward developing quality products.", "e:keyword": ["New product research", "New product management", "International marketing", "Omitted variable estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.4.359", "e:abstract": "Internal market structure analysis infers brand positions in an attribute space from preference and choice data, given a market in which consumers have heterogeneous tastes for attributes. Previous market structure models have adopted a static framework (e.g., Elrod 1988, Chintagunta 1994, Elrod and Keane 1995). Furthermore, they assumed that consumer perceptions of brand attributes do not vary across consumers. Yet, these approaches may render inaccurate representations of market structure if there is state dependence in consumer choice behavior. This paper attempts to incorporate consumer choice dynamics into market structure models by specifying the source of choice dynamics explicitly. In particular, the process by which past purchases affect current choices is modeled in a framework which captures both consumer habit persistence and variety seeking behavior. More specifically, consumer preferences for brand attributes are modeled to depend on the attributes of brands bought on the previous purchase occasion. Furthermore, the modeling approach adopted incorporates heterogeneity in both consumer preferences and perceptions of brand attributes. The audience of this paper includes practitioners and academics interested in understanding consumer choice processes and inferring market structure from consumer choice data. The proposed models are estimated on Nielsen scanner panel data for margarine, peanut butter, yogurt, and liquid detergent using simulated maximum likelihood techniques. The empirical results suggest that accounting for choice dynamics improves both in-sample and out-of-sample fit. The results indicate that the average consumer is habit persistent in all the product categories studied. This result is consistent with the findings of Kannan and Sanchez (1994), who conducted an aggregate analysis of consumer variety seeking behavior across product categories. However, the results obtained in this paper suggest that consumers are heterogeneous with respect to the processes by which past purchases affect current purchases. These results provide strong evidence for habit persistence and variety seeking in brand attributes to be the behavioral source of consumer choice dynamics in food categories. Thus, consumer tastes (utility weights) seem to be affected by the attributes of the brands consumed in the past. Given the empirical result that a large proportion of consumers are habit persistent, this suggests that tastes are reinforced by the brand attributes consumed in the past. The empirical results also show that not accounting for state dependence in market structure models for panel data may produce misleading results, that is, depending on consumer behavior patterns, models that do not account for state dependence may distort the true nature of competition among brands. More specifically, the results confirm the expectation that if there is habit persistence, that is, if consumer tastes are reinforced by attributes of brands consumed in the past, models that do not capture this choice dynamics will overestimate the distance between (similar) brands. Furthermore, the policy experiments conducted suggest that (1) static models overestimate the short-run impact of a price cut on the sales of the brand on promotion, (2) price cuts hurt the sales of the more similar brands more, and (3) free samples affect relatively less similar brands the most. Finally, this paper studies variety seeking and habit persistence across brands over purchase occasions. However, variety seeking behavior may also involve the purchase of a portfolio of brands or items at a purchase occasion. Consumers may buy multiple items knowing that prior to the next trip they may want to consume different items (Simanson 1990, Walsh 1995). This type of behavior can be modeled within the context of dynamic expected utility maximization with forward-looking consumers. The development and estimation of market structure models that include forward-looking consumers who maximize expected-utility over a planning horizon, incorporating their future tastes and needs, and shopping for an inventory of brands, remain an important future research issue.", "e:keyword": ["Brand choice", "Buyer behavior", "Econometric models", "Market structure"]}, {"@id": "http://dx.doi.org/10.1287/mksc.15.4.379", "e:abstract": "Several studies have shown that promotions of national brands yield more effect than those of store brands (e.g., Allenby and Rossi 1991, Blattberg and Wisniewski 1989). However, the evolution of price-quality data available from over the last 15 years seems to reveal a reduction of the quality gap between store brands and national brands, while price differences remain substantial. Simultaneously, the share of private label brands has increased ( 1994). In this context, we study whether we can maintain a view of the world where national brands may easily attract consumers from store brands through promotions, whereas store brands are relatively ineffective in attracting consumers from national brands by such means. We analyze consumer reactions to price discounts in a parsimonious preference model featuring loss aversion and reference-dependence along dimensions of price and quality (Hardie, Johnson, and Fader 1993, Tversky and Kahneman 1991). The key result of our analysis is that, given any two brands, there is an asymmetric promotion effect in favor of the higher quality/higher price brands if and only if the quality gap between the brands is sufficiently large in comparison with the price gap. Thus, the direction of promotion asymmetry is not unconditional. It depends uniquely on the value of the ratio of quality and price differences compared to a category specific criterion, which we call Φ. If the ratio of quality and price differences is larger than this criterion, the usual asymmetry prevails; if such is not the case, the lower quality/lower price brands promote more effectively. More precisely, our model predicts that cross promotion effects depend on two components of brand positioning in the price/quality quadrant. First, we define a variable termed positioning advantage that indicates whether, relative to the standards achieved by another brand, a given brand is underpriced (positive advantage) or overpriced (negative advantage). Promotion effectiveness is increasing in this variable. Second, cross promotion effects between two brands depend on their distance in the price/quality quadrant. This variable impacts promotion effectiveness negatively and symmetrically for any pair of brands. Positioning advantage and brand distance are orthogonal components of brand positioning, irrespective of the degree of correlation between available price and quality levels in the market. Empirically, we investigate the role of brand positioning in explaining cross promotion effects using panel data from the chilled orange juice and peanut butter categories. We compute the independent positioning variables, positioning advantage and brand distance, from readily available data on price and quality positioning after obtaining our estimates of Φ. We next measure promotion effectiveness by estimating choice share changes in response to a price discount, using a choice model that does not contain any information about quality/price ratios. Finally, we test the relation between the two positioning variables and the promotion effectiveness measures. The data reveal that in the orange juice category lower quality/lower price brands generally promote more effectively than higher quality/higher price brands. In the peanut butter data the opposite asymmetry holds. In both cases, inter-brand promotion patterns are well explained by the positioning variables. An attractive feature of our model is that, in addition to the direction of promotion asymmetries, it also explains the extent of those asymmetries. A further interesting aspect of this approach is that we go beyond a categorization of brands into price tiers. For instance, lower tier brands in our data may promote more effectively than one national brand but less effectively than another. Consistent with our theoretical predictions, the data presented here seem to confirm that such cases occur because the lower tier brand offers a favorable trade-off of price and quality differences compared with one national brand and a less favorable trade-off compared with the other. The content of this paper is potentially relevant for brand managers or retailers concerned with predicting the impact of their promotions. The paper is of particular interest to marketing scientists who study the performance of store brands versus national brands and may also appeal to those who wish to explore the marketing implications of behavioral decision theory. Finally, our investigation does not reject Blattberg and Wisniewski's (1989) finding, shared by Allenby and Rossi (1991) and Hardie, Johnson, and Fader (1993), that national brands have a advantage in promotion effectiveness. Rather, it formalizes when this principle advantage is overruled by positioning disadvantages of such brands.", "e:keyword": ["Promotion", "Brand choice", "Brand positioning", "Loss aversion", "Reference-dependence"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.1.1", "e:abstract": "Retailers in developed countries increasingly provide one-stop shopping. Why? Are they responding to growing demand for time-saving convenience? Or are they responding to economies of scale made possible by new retail technology? And what role, if any, is played by improvements in automotive, refrigeration, and other consumer technology? To examine these questions, we develop a model that can help explain the growth of one-stop shopping, estimate the model with aggregate grocery retail data, and compare the implications of our model with competing explanations. Our model includes several desirable features: (1) The extent of one-shop shopping is endogenously determined. (2) Consumer store choice is determined in an explicit household-production, utility-maximizing context. This choice is based on a tradeoff between time-saving shopping convenience and price. (3) The model is readily amenable to empirical testing. The key endogenous parameter of our model describes the number of categories carried by a store. In the context of grocery retailing, this parameter can distinguish between a single-category specialty store, a simple grocer, a supermarket, and a large superstore. This parameter is determined by the interplay of consumers economizing on shopping time and retail competition in the presence of technological constraints which relate operating cost with assortment size. Our approach may also be viewed as an example of how the boundary between retail service production and household service production can be modeled as a market outcome (in the spirit of Betancourt and Gautchi [Betancourt, Roger, David Gautschi. 1990. Demand complementarities, household production, and retail assortments. (2, Spring) 146161.] and Wernerfelt [Wernerfelt, Birger. 1994. An efficiency criterion for marketing design. (November) 462470.]). The nature of extant retail formats and other channel-related institutions circumscribes the boundary between retailers and households. With the notable early exception of Baumol and Ide (Baumol, William J., Edward A. Ide. 1956. Variety in retailing. (1, October) 93101.), few explicit models describe retail format determination as an equilibrating market mechanism. Our model describes the determination of one aspect of retail formats (namely, the extent of one-stop shopping). Our hope is that similar approaches can be used to describe other aspects and types of emergent retail formats. We estimate our model using U.S. aggregate annual data for a 26-year period, and later corroborate parts of the analysis using cross-sectional data for the 48 contiguous states of the U.S. The equilibrium conditions of our model translate into a system of two equations with two endogenous variables. Estimates of the reduced form parameters provide our central empirical findingthat per capita disposable income has had a significant positive effect on both supermarket assortment (as expected from our model) and store operating costs (perhaps somewhat less expected). This suggests that greater prevalence of one-stop shopping has been a response to growing demand for time-saving convenience. The estimates of the underlying structural parameters also provide a measure of the imputed net savings to consumers from supermarket shopping, taking into account reductions in shopping time made possible by supermarkets (savings equivalent to approximately 2.2% of expenditures on grocery products over the 19611986 period). We lastly consider competing hypotheses. While a number of explanations can account for increased store assortment, these explanations have divergent implications for retail margins, operating costs, and profits. These implications suggest that retail scale economies were not the drivers of one-stop shopping in the years of our data. Nor was the impetus to increase store size primarily a desire to bring in new higher-margin items. However, we acknowledge that transportation and inventory-holding technologies are prerequisites to one-stop shopping and that improvements in these technologies were particularly important in the early growth period of supermarkets.", "e:keyword": ["Store formats", "Retailing", "Channels of distribution"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.1.24", "e:abstract": "Selling information that is later used in decision making constitutes an increasingly important business in modern economies (Jensen [Jensen, Fred O. 1991. Information services. Congram, Friedman, eds. , Chapter 22. AMA-COM, New York, 423443.]). Information is sold under a large variety of forms: industry reports, consulting services, database access, and/or professional opinions given by medical, engineering, accounting/financial, and legal professionals, among others. This paper is the first attempt in marketing to study competition in the rapidly emerging information sector. Specifically, we are interested in answering the following questions: (1) Is competition fundamentally different when competing firms sell information rather than traditional goods and services, andif yeswhy? (2) What are the implications of such differences for decision makers (marketers and regulators)? (3) Can we explain some of the observed marketing strategies in the information industry? As such, the audience of the paper includes academics as well as professionals who are interested in understanding what is specific about competition in information markets. Familiarity with the practical implications of such differences and understanding of the mechanisms that drive them is essential for those who are faced with the problem of marketing information. To answer the above research questions we build a simple game-theoretic model that consists of two firms selling information to a population of consumers who are heterogeneous in their willingness to pay for the of information. The most important features of the model are the following. Information products sold by the two firms are modeled as random draws from two normal distributions having equal mean. The variances of these distributions and their correlatedness constitute the product-attribute space, which is assumed to be common knowledge. Consumers are interested in assessing the mean of the distributions and to do so they can buy the sample from any of the firms or they can buy both samples and combine them to obtain a more accurate estimate. of information is linked to the accuracy of consumers' estimate of the mean which in turn is influenced by the accuracy of each sample as well as by their correlatedness. Consumers' utility depends on the quality of information they purchased, on their inherent utility for quality (taste), and on the total price they paid to acquire information. Knowing consumer preferences, firms simultaneously price their information products. The main finding of the paper is that information markets face unique competitive structures. In particular, the qualitative nature of competition changes depending on basic product characteristics. While traditional products and services compete either as substitutes or as complements in the relevant product-attribute space, information may be one or the other, depending on its position within the product-attribute space. Said differently, the nature of competition changes with a continuous change in basic product-attribute levels. The intuition behind this finding is the following. When purchasing information, consumers facing important decisions may find it beneficial to purchase from several information sellers. This is more likely to happen when the reliability of information is low and the sources of information are independent. Under such conditions information products tend to be complements and, as a result, competition between sellers is relatively mild. In the opposite case, when information is reliable and/or sellers' sources are highly correlated, consumers are satisfied after consulting a single source. In this case, information products are substitutes and sellers tend to undercut one another's prices to induce consumers to choose their brand. Understanding this discontinuity in competitive structures has important implications for decision makers as very different strategies are optimal under different product characteristics. Under substitution, traditional strategies to avoid competition (e.g., differentiation) are recommended. When the competing products' reliability is generally low (they are complements) firms are better off accommodating competition. In fact, we find that a firm may benefit from inviting a competitor. Finally, our findings are also important for regulators of information markets. As the literature on complementarity suggests, price fixing agreements between firms offering complementary products may benefit firms as well as consumers.", "e:keyword": ["Information sales", "Competitive strategy", "Complements", "Substitutes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.1.39", "e:abstract": "Two endemic problems face researchers in the social sciences (e.g., Marketing, Economics, Psychology, and Finance): unobserved heterogeneity and measurement error in data. Structural equation modeling is a powerful tool for dealing with these difficulties using a simultaneous equation framework with unobserved constructs and manifest indicators which are error-prone. When estimating structural equation models, however, researchers frequently treat the data as if they were collected from a single population (Muthn [Muthn, Bengt O. 1989. Latent variable modeling in heterogeneous populations. 557585.]). This assumption of homogeneity is often unrealistic. For example, in multidimensional expectancy value models, consumers from different market segments can have different belief structures (Bagozzi [Bagozzi, Richard P. 1982. A field investigation of causal relations among cognitions, affect, intentions, and behavior. 562584.]). Research in satisfaction suggests that consumer decision processes vary across segments (Day [Day, Ralph L. 1977. Extending the concept of consumer satisfaction. W. D. Perreault, ed. , Vol. 4. Association for Consumer Research, Atlanta, 149154.]). This paper shows that aggregate analysis which ignores heterogeneity in structural equation models produces misleading results and that traditional fit statistics are not useful for detecting unobserved heterogeneity in the data. Furthermore, sequential analyses that first form groups using cluster analysis and then apply multigroup structural equation modeling are not satisfactory. We develop a general finite mixture structural equation model that simultaneously treats heterogeneity and forms market segments in the context of a specified model structure where all the observed variables are measured with error. The model is considerably more general than cluster analysis, multigroup confirmatory factor analysis, and multigroup structural equation modeling. In particular, the model subsumes several specialized models including finite mixture simultaneous equation models, finite mixture confirmatory factor analysis, and finite mixture second-order factor analysis. The finite mixture structural equation model should be of interest to academics in a wide range of disciplines (e.g., Consumer Behavior, Marketing, Economics, Finance, Psychology, and Sociology) where unobserved heterogeneity and measurement error are problematic. In addition, the model should be of interest to market researchers and product managers for two reasons. First, the model allows the manager to perform response-based segmentation using a consumer decision process model, while explicitly allowing for both measurement and structural error. Second, the model allows managers to detect unobserved moderating factors which account for heterogeneity. Once managers have identified the moderating factors, they can link segment membership to observable individual-level characteristics (e.g., socioeconomic and demographic variables) and improve marketing policy. We applied the finite mixture structural equation model to a direct marketing study of customer satisfaction and estimated a large model with 8 unobserved constructs and 23 manifest indicators. The results show that there are three consumer segments that vary considerably in terms of the importance they attach to the various dimensions of satisfaction. In contrast, aggregate analysis is misleading because it incorrectly suggests that except for price all dimensions of satisfaction are significant for all consumers. Methodologically, the finite mixture model is robust; that is, the parameter estimates are stable under double cross-validation and the method can be used to test large models. Furthermore, the double cross-validation results show that the finite mixture model is superior to sequential data analysis strategies in terms of goodness-of-fit and interpretability. We performed four simulation experiments to test the robustness of the algorithm using both recursive and nonrecursive model specifications. Specifically, we examined the robustness of different model selection criteria (e.g., CAIC, BIC, and GFI) in choosing the correct number of clusters for exactly identified and overidentified models assuming that the distributional form is correctly specified. We also examined the effect of distributional misspecification (i.e., departures from multivariate normality) on model performance. The results show that when the data are heterogeneous, the standard goodness-of-fit statistics for the aggregate model are not useful for detecting heterogeneity. Furthermore, parameter recovery is poor. For the finite mixture model, however, the BIC and CAIC criteria perform well in detecting heterogeneity and in identifying the true number of segments. In particular, parameter recovery for both the measurement and structural models is highly satisfactory. The finite mixture method is robust to distributional misspecification; in addition, the method significantly outperforms aggregate and sequential data analysis methods when the form of heterogeneity is misspecified (i.e., the true model has random coefficients). Researchers and practitioners should only use the mixture methodology when substantive theory supports the structural equation model, segmentation is infeasible, and theory suggests that the data are heterogeneous and belong to a finite number of unobserved groups. We expect these conditions to hold in many social science applications and, in particular, market segmentation studies. Future research should focus on large-scale simulation studies to test the structural equation mixture model using a wide range of models and statistical distributions. Theoretical research should extend the model by allowing the mixing proportions to depend on prior information and/or subject-specific variables. Finally, in order to provide a fuller treatment of heterogeneity, we need to develop a general random coefficient structural equation model. Such a model is presently unavailable in the statistical and psychometric literatures.", "e:keyword": ["Structural equation models", "Market segmentation", "Finite mixture models", "Confirmatory factor analysis", "Customer satisfaction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.1.60", "e:abstract": "Every Day Low Pricing (EDLP) strategy has proved to be a successful innovation resulting in higher profits to supermarkets adopting it in competition with Promotional Pricing (PROMO). Conventional wisdom attributes this success either to lower costs or to EDLP better serving time constrained consumers, while discouraging cherry pickers who seek promotions. However, it is unclear that such cost savings are being fully realized since EDLP stores also engage in price promotions. Also, continued existence of PROMO stores means that costs are not the only factor, and they compete effectively without relying just on the cherry pickers. Furthermore, experimental evidence suggests that a supermarket cannot obtain higher profits by merely setting constant low prices, leading to the question: exactly what makes EDLP successful? This question is of particular relevance to both academics and practitioners who have been intrigued by the success of this retailing strategy. More generally, the retailing issues addressed in this paper, the economic analysis of competition, and the empirical findings should be of interest to the broader community of researchers and managers. We investigate the factors contributing to EDLP's success by analyzing the competition between supermarkets through a game theoretic analysis of a market consisting of both time constrained consumers and cherry pickers. Key features of our model are: consumers shop and purchase a basket of goods based on price announcements by stores and rational expectations of unannounced prices; stores carry more than one good and compete through prices, service, convenience, and appropriate communication strategies; and no exogenous cost asymmetries. We derive the conditions under which retailers choosing different strategies (EDLP and PROMO) is a perfect Nash equilibrium. Our analysis shows that the EDLP store's offering of constant every day low prices is an equilibrium outcome, endogenously determined. Successful implementation of the EDLP strategy involves communication of relative basket prices, implying that merely setting constant low prices is not viable. We further demonstrate that while time constrained consumers find every day low prices at EDLP attractive and cherry pickers the promotions at PROMO, clientele effects are in fact more complicated. Specifically, in equilibrium the PROMO store offers a higher service level as desired by time constrained consumers and the EDLP store a lower service level in keeping with the needs of cherry pickers. This choice of service by the two stores results in a cleaner segmentation of the market. The higher relative basket price and service at the PROMO store results in a larger base of time constrained consumers to shop at the PROMO store and a larger base of cherry pickers to shop at the EDLP store, even though some cherry pickers continue to visit the PROMO store to avail of the price specials. In this way, our results contradict the conventional wisdom on EDLP strategy as being mainly geared towards time constrained consumers. Finally, industry profits are higher in an EDLP-PROMO equilibrium than when stores adopt identical strategies. Our analysis and results also offer a more complete characterization of the EDLP and PROMO strategies. Indeed, we show that EDLP and PROMO strategies are positioning strategies, rather than merely pricing strategies, with different elements: price/promotions, service, and communications. While the EDLP store uses basket prices to attract both segments, the PROMO store uses service and price specials to compete in the time constrained and cherry picking segment, respectively. Given these different approaches of the two stores, the communication strategies of the EDLP and the PROMO stores emphasize these differences as well. In this way we show, as suggested by Corstjens and Corstjens (1994), that positioning in a retail context involves developing multidimensional strategies appealing to all segments, while each element of the strategy may focus on a different consumer segment. This is in contrast to the traditional view of segmentation in which different products in a product line, for example, are designed to appeal to different segments. We complete the analysis by examining the data from the trade press and a survey conducted in a major metropolitan area. These data, while limited in scope, support our theoretical results.", "e:keyword": ["Retail competition", "Every day low pricing", "Segmentation and positioning", "Consumer expectations", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.1.81", "e:abstract": "Manufacturers' returns policies are a common feature in the distribution of many products. The obvious rationale for returns policies is insurance. Practitioners, not surprisingly, have a different perspective and view returns as a cost of doing business. In this paper, we study the strategic effect of returns policies on retail competition and highlight its profitability implications for a manufacturer. The setting for our research is the distribution of products with uncertain demand, limited shelf lives, and retail competition. Our objective is to provide a better understanding of when manufacturers should adopt returns policies. The insights are obtained with a model based on the economics of strategy and decision making under uncertainty. We show that when retailing is competitive and there is no uncertainty in demand, a returns policy subtly induces retailers to compete more intensely. The provision of a returns policy reduces retail prices without affecting wholesale prices, thereby reducing retailer margins and improving manufacturer profitability. The intuition is that with a returns policy, Cournot-like levels of retail stocks cannot be sustained. Each retailer will order stocks so that it will not be constrained by stocks, and so, intensifying retail competition. When, however, demand is uncertain and there is a single retailer, a returns policy encourages the retailer to over-stock, and so decreases (upstream) manufacturer profits. While the provision of a returns policy leads to lower retail margins, the optimality of returns policy depends on the overall uncertainty and marginal cost. A returns policy reduces the dispersion in retail prices between the high and low states of demand and the range of retail prices in the returns case is contained within the range of retail prices for the no-returns case. In the general setting, when there are competing retailers and demand is uncertain, there is a trade-off for the manufacturer between the benefits (more intense retail competition) and the costs (excessive stocking) of a returns policy. We find that a manufacturer should accept returns when the marginal production cost is sufficiently low and demand uncertainty is not too great. To validate the results of our theory, we conduct an empirical test with data from a major U.S. retailer. The tests confirm our prediction that a returns policy intensifies retail competition and reduces retailer margins. Our theory and empirical test should interest practitioners and researchers in distribution, especially those concerned with managing retail competition.", "e:keyword": ["Returns policies", "Retail competition", "Pricing", "Perishables", "Demand uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.2.97", "e:abstract": "In this paper we show that performance information about forgone alternatives (i.e., alternative that were considered but not chosen) can have a significant impact on post-choice valuation. Our approach introduces a new and parsimonious way of looking at satisfaction that combines the literature on post-choice valuation with research regarding generalized expected utility theory. While the post-valuation literature focuses on the selected brand as the valuation's basis (e.g., Anderson and Sullivan [Anderson, E. W., M. W. Sullivan. 1993. The antecedents and consequences of customer satisfaction for firms. (Spring) 125143.], Bolton and Drew [Bolton, R., J. Drew. 1991. A multistage model of customers' assessments of service quality and value. 375384.]), we draw on a stream of research in generalized expected utility theory that considers both chosen and forgone alternatives as the basis for valuation (e.g., Bell [Bell, D. 1983. Risk premiums for decision regret. 11561166 and Bell, D. 1985. Disappointment in decision making under uncertainty. 127.]; Loomes and Sugden [Loomes, G., R. Sugden. 1982. Regret theory: An alternative theory of rational choice under uncertainty. 805824 and Loomes, G., R. Sugden. 1986. Disappointment and dynamic consistency in choice under uncertainty. 271282.]). The result is a combined model of post-choice valuation that explicitly incorporates both concepts. Specifically, we extend the existing paradigm of post-choice valuation to include buyers' regret concerning forgone alternatives, proposing a generalized utility theory-based treatment of post-choice product assessment that uses the intuitively appealing concepts of disappointment and regret as the basis. We propose a model for conceptualizing post-choice valuation that is consistent with the existing literature, discuss how this model extends the construct to consider the influence of forgone alternatives, and report results of an empirical test that contrasts our model to important recent work in the area (e.g., Boulding et al. [Boulding, W., A. Kalra, R. Staelin, V. A. Zeithaml. 1993. A dynamic process model of service quality: From expectations to behavioral intentions. (February) 727.]). Our generalized model of post-choice valuation is based on the sum of three components that represent factors that may contribute to consumers' assessment of a chosen product or service. The first component is expected performance. The second component is disappointment, which captures the discrepancy between actual and expected performance (much as the disconfirmation construct in traditional satisfaction research). The third component is regret, which captures the difference between the performance of the chosen product/service and the performance of a forgone product/service. This perspective is useful in that risk is captured by the disappointment and regret terms, providing an intuitively appealing decomposition of post-choice valuation and offering several advances over previous representations of disappointment and regret. We test our model via a choice experiment. Participants in the empirical study were asked to make choices between successive lottery pairs. They were then given outcome feedback on the forgone alternative as well as on the chosen alternative in each lottery pair. Immediately following outcome feedback for each choice, subjects were asked to evaluate their decision. Our results clearly suggest an effect of regret on post-choice valuationinformation about the forgone alternative influenced subjects' valuation of the chosen alternative. We also find evidence that, as predicted, the effects of disappointment and regret on post-choice valuation are asymmetric. Specifically, the negative effect of disappointment on post-choice valuation was greater than the positive effect of elation. Similarly, the negative effect of regret was greater than the positive impact of rejoicing. Our research offers five contributions to the literature on post-choice valuation. First, our results illustrate the advantage of using generalized utility theory as the basis for conceptualizing and modeling post-choice valuation. We derive a model of post-choice valuation that formally captures the components of disappointment and regret and show that the outcome of both the chosen alternative (through disappointment) and a forgone alternative (through regret) can influence the chosen alternative's valuation. Second, we formally integrate the concepts of disappointment and regret, which have been examined separately for many years, into a single model based on a multiattribute preference structure. Third, we argue that the effects of disappointment and regret on post-choice valuation are asymmetric and present empirical evidence in this regard. Fourth, our results suggest that word-of-mouth regarding forgone alternatives may affect post-choice valuation, extending research that has not heretofore considered forgone outcomes' role in this process. Finally, our development provides a preconsumption measure of potential disappointment and regret in modeling choice. At the time of choice, consumers may visualize the feelings of disappointment and/or regret that will be derived at consumption, taking into account both the chosen brand and forgone brands. Our generalized utility theory-based approach to the post-choice valuation construct can be useful in examining the role of disappointment and regret as preconsumption constructs.", "e:keyword": ["Buyer behavior", "Utility theory", "Disappointment", "Regret", "Post-choice valuation", "Satisfaction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.2.112", "e:abstract": "Slotting allowanceslump sum transfers from manufacturers to retailers for carrying new productshave become an important part of promotional agreements over the past decade. Hardly known before the mid-1980s, they now represent a significant cost to launching a new entry in a wide range of product categories. Despite being commonplace, slotting allowances have remained extremely controversial both with manufacturers and retailers. The controversy, in part, follows from a poor understanding of the role that slotting allowances actually play in new product introductions. We attempt to clarify the purpose slotting allowances serve by relating the payment of a slotting allowance to the retailer's cost structure and informational asymmetries within a channel. We consider a manufacturer introducing a new product into a retail channel. The retailer is independent of the manufacturer and only accepts the product if he expects to recover a positive fixed cost at the terms of trade offered by the manufacturer. Following acceptance, the retailer exerts merchandising effort and sets the retail price. We show that if the manufacturer and the retailer are equally informed of the product's demand, the terms of trade never include a slotting allowance. High retail costs are compensated through a lower wholesale price. Similarly, if the manufacturer is better informed of the product's demand, she prefers to convey that information through the wholesale price alone. That is, a high wholesale price, not a slotting allowance, is the manufacturer's preferred signaling instrument. Signaling with the wholesale price alone fails, however, when the retailer has high fixed costs. To convey information and assure retailer participation, the terms of trade must include a positive slotting allowance. A slotting allowance thus serves two purposes in launching a product: passing information down to the retailer and shifting costs up to the manufacturer. We show that the manufacturer prefers paying a slotting allowance to undertaking purely wasteful advertising. A principal virtue of a slotting allowance, then, is keeping money within the channel. Our work is novel along two important dimensions. First, others (e.g., Chu [Chu, Wujin. 1992. Demand signaling and screening in channels of distribution. (4, Fall) 327347.]) have assumed that slotting fees arise as manufacturers respond to retailer demands. Here, the manufacturer willingly offers an allowance. As a consequence, slotting allowances do not represent a windfall for the retailer; he merely breaks even on a product for which a slotting allowance is paid. Second, we tie the payment of a slotting allowance to the retailer's fixed cost and the overall terms of trade. This allows us to consider a number of comparative statics with interesting implications. For example, a retailer may receive a slotting allowance for some categories and not for others if his costs differ across categories. A slotted product is offered at a lower wholesale price which results in greater retailer effort than for a product on which no allowance is paid. Over a range of fixed costs, greater retailer effort should be correlated with a higher slotting allowance. Finally, for a specific functional form, we show that slotting allowances become more common (in the sense that they are paid over a greater range of retailer costs) as the retailer has greater merchandising ability.", "e:keyword": ["Slotting allowances", "Channel management", "Asymmetric information", "New product introductions", "Signaling", "Performance guarantees"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.2.129", "e:abstract": "There is widespread belief that firms should pursue superiority in both customer satisfaction and productivity. However, there is reason to believe these two goals are not always compatible. If a firm improves productivity by downsizing, it may achieve an increase in productivity in the short-term, but future profitability may be threatened if customer satisfaction is highly dependent on the efforts of personnel. If so, there are potential tradeoffs between customer satisfaction and productivity for industries as diverse as airlines, banking, education, hotels, and restaurants. Managers in these types of service industries, as well as goods industries in which the service component is increasing, need to understand whether or not this is the case. For example, if efforts to improve productivity can actually harm customer satisfactionand vice-versathe downsizing of U.S. and European companies should be viewed with concern. It follows that developing a better understanding of how customer satisfaction and productivity relate to one another is of substantial and growing importance, especially in light of expected continued growth in services throughout the world economy. The objective of this paper is to investigate whether there are conditions under which there are tradeoffs between customer satisfaction and productivity. A review of the literature reveals two conflicting viewpoints. One school of thought argues that customer satisfaction and productivity are compatible, as improvements in customer satisfaction can decrease the time andeffort devoted to handling returns, rework, warranties, and complaint management, while at the same time lowering the cost of making future transactions. The second argues that increasing customer satisfaction should increase costs, as doing so often requires efforts to improve product attributes or overall product design. A conceptual framework useful in resolving these contradictory viewpoints is developed. The framework serves, in turn, as a basis for developing a theoretical model relating customer satisfaction and productivity. The model predicts that customer satisfaction and productivity are less likely to be compatible when: 1) customer satisfaction is relatively more dependent on customizationthe degree to which the firm's offering is customized to meet heterogeneous customers' needsas opposed to standardizationthe degree to which the firm's offering is reliable, standardized, and free from deficiencies; and 2) when it is difficult (costly) to provide high levels of both customization and standardization simultaneously. To move forward from the model's propositions to the development of testable hypotheses, we argue that services are more likely than goods to have the preceding characteristics. Hence, tradeoffs between customer satisfaction and productivity should be more prevalent for services than for goods. Although this classification is not precisemany services are standardizable and many goods have a service componentit has the advantage of allowing an initial test of the propositions. The empirical work employs a database matching customer-based measures of firm performance with traditional measures of business performance, such as productivity and Return on Investment (ROI). The central feature of this database is the set of customer satisfaction indices provided by the Swedish Customer Satisfaction Barometer (SCSB). The SCSB provides a uniform set of comparable customer-based firm performance measures and offers a unique opportunity to test the study's hypotheses. The findings indicate that the association between changes in customer satisfaction and changes in productivity is positive for goods, but negative for services. In addition, while both customer satisfaction and productivity are positively associated with ROI for goods and services, the interaction between the two is positive for goods but significantly less so for services. Taken together, the findings suggest support for the contention that tradeoffs are more likely for services. Hence, simultaneous attempts to increase both customer satisfaction and productivity are likely to be more challenging in such industries. Of course, this does not imply that such firms should not seek improvements in both productivity and customer satisfaction. For example, appropriate applications of information technology may improve both customer satisfaction and productivity simultaneously. The findings should provide motivation for future research concerning the nature of customer satisfaction and productivity, as well as appropriate strategy and tactics for each one. It is worth emphasizing that this is an issue that is not only important today, but certainly will become even more important in the future. As the growth of services continues and world markets become increasingly competitive, the importance of customer satisfaction will also increase. To compete in such a world, firms must strike the right balance between their efforts to compete and their efforts to compete .", "e:keyword": ["Customer satisfaction", "Productivity", "Services versus goods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.2.146", "e:abstract": "An increasing number of products are being sold with components that themselves are brand names. Examples are personal computers with Intel microprocessors and diet soft drinks that use the NutraSweet formulation. Branded components may alter consumers' valuation of the bundle, necessitating changes in the ways firms identify and price such bundled products. Surprisingly, the existing marketing literature is silent on this issue. The purpose of our paper is to propose an analytical approach that helps marketers of products with branded components make optimal pricing and partner selection decisions. Our managerial objectives are twofold: (a) To the seller of the bundled product, we suggest the optimal bundled product offering, optimal selling prices of alternative products, revenues and profits; and (b) to the branded component manufacturers, we indicate the most favorable alliance partner(s), and payoff gains/losses of aligning with other branded components instead of unbranded alternatives. Our analytical approach is also likely to be helpful to academics researching bundling and ingredient branding issues. The specific problem modeled assumes the bundled product has two components that are consumed jointly. Furthermore, each component can be either a brand name or unbranded. In this model, the consumer has no control over the choice of the components in the bundle; the seller decides what form of bundle components (i.e., branded or unbranded) to offer. We assume that the product that is marketed eventually enjoys a monopoly. Drawing from research in signaling and brand alliance, we posit each branded component may either enrich or suppress the value of the partnering component in the bundle as perceived by the consumers. Our approach has three methodological stages. First, we build an individual level model to assess the valuation of alternative products and their principal components at the level of a randomly drawn consumer. To do so, we rely primarily on the theory of reservation prices and Weber's theory on price/value changes. Second, we aggregate such valuation across consumers to assess the market's overall valuation. For this purpose, we invoke parametric distributions based on integral transform theory. Third, we develop payoff functions based on market valuation of alternative products and supply-side costs. The managerial objectives are met based on results from this stage. Our approach was successfully applied to the context of a university bookstore that intends to sell Windows-based laptop computers. The two principal components of the laptop computer bundle are the microprocessor chip and the personal computer platform on which the chip is implanted. The seller may choose either Compaq with Intel Inside or a simpler bundle featuring one of these with an unbranded, complementary component, or an entirely unbranded bundle. We used data collected from 192 potential consumers via a survey. Included in the survey were measures of consumers' perceptions of functional aspects of the components, and their reservation prices for alternative bundled products. The approach identified the most profitable bundle for the seller and its optimal price. It also provided estimates of the revenue impact of the gain or loss for each branded component by partnering the other branded component instead of the unbranded alternative. As such, the model facilitates the determination of the optimal price, price premium, and profits for products with branded components, as well as the identification of the ideal partner for each branded component manufacturer. It is apt to caution that products with branded components need not always lead to price premiums or lead to win-win outcomes. Reasons such as incongruity between the branded components or domination of one of the components over the other may drive potential consumers away, thus hurting profits. Implications of the results to negotiate component prices are discussed in the paper. The study limitations and directions for future research are also presented.", "e:keyword": ["Branded components", "Bundling", "Pricing", "H-function distribution"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.2.166", "e:abstract": "Incorporating demographic variables in brand choice models is conceptually appealing and has numerous managerial benefits. Retailers and brand managers can assess geodemographic variations in demand and marketing mix response in order to implement micromarketing strategies. For example, a retailer planning to locate a new outlet can get some sense of the differences in demand patterns and price and promotion sensitivities in the new trading area in order to make initial stocking, inventory, pricing, and promotion decisions. For existing outlets, retailers can fine tune the assortment and merchandising activities in a category to match local market conditions. Similarly, a packaged goods manufacturers would benefit from getting a sense of whether they should stress promotional activity for one part of a brand's product line in some retail trading areas, and other parts of the brand's line in other retail trading areas. Unfortunately, a general finding across existing studies is that the impact of demographic variables on brand choice is neither strong nor consistent. These findings are puzzling given that one would expect certain demographic variables, such as income, to have some influence on brand choice behavior. Moreover, Hoch et al. (Hoch, S. J., B. D. Kim, A. J. Montgomery, P. E. Rossi. 1995. Determinants of store-level price elasticity. (February) 1729.), using store level data, find that a relatively small set of demographic variables is much more influential than competitive variables in explaining differences in price sensitivity across retail trading areas. In light of such contradictory findings, there is a need for a better conceptualization of the role of income and other demographic variables in household purchasing, and this is the primary objective of this study. This paper presents a microeconomic-based framework called the indivisible alternatives (IDA) framework to model household brand choice. A key aspect of the IDA framework is that it explicitly models alternatives in the market as being indivisible, while past work has either explicitly or implicitly assumed perfect divisibility. Indivisibilities force a household's selection of a brand size and its level of category expenditure into a single joint decision. Hence, demographic variables that influence category expenditure (e.g., income) also influence the choice of a brand size. In addition, since alternatives in the category come in several discrete sizes, indivisibilities introduce differences in holding costs in the choice of a brand-size combination. Consequently, demographic factors that influence holding costs through consumption rate differences (e.g., household size) impact the choice of a brand size in a product category. In this manner, indivisibilities in market offerings naturally lead to differences in choice behavior across households that can be linked to demographic factors in a logical fashion. In empirical applications to two different scanner panel data sets (ketchup and ground coffee), the proposed framework compares favorably with two benchmark specifications in terms of both goodness of fit and predictive validity. The results from these product categories indicate that a household's price sensitivity is inversely related to its income level, and that factors such as household size and seasonality, which are likely to influence consumption rates, make households more or less willing to buy larger package sizes. Income elasticity estimates from the model confirm that, ceteris paribus, households with lower incomes will have a higher propensity to purchase private labels and generic brands, and a lower propensity to purchase national brands, compared with households with higher incomes. The micromarketing potential of the IDA framework is explored in market simulations conducted for two different zip codes within a metropolitan area that vary with respect to income levels and other demographic factors. The results indicate that for the product categories studied, there are substantial differences across market areas in their response to the retail promotion of very large and very small package sizes within the same brand's product line. The differences in response suggest that it may be beneficial to customize promotional programs to market areas based on their underlying demographic composition. Finally, our findings suggest a link between household income and deal proneness that is conditional on other demographic variables and the expenditure required for the brand size in question. Specifically, we find that smaller households with lower income levels are more likely to respond to promotions for smaller, less expensive items within a category, while larger households with higher incomes are more likely to respond to promotions for larger, more expensive items within a category.", "e:keyword": ["Brand choice", "Demographic variables", "Geodemographics", "Micromarketing", "Deal proneness", "Private labels"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.3.185", "e:abstract": "This paper examines two strategic pricing decisions within channels: using foresight (i.e., price leadership) and considering category implications (i.e., product line pricing). Are price leadership and product line pricing always the best pricing strategies for a channel member? If not, when does this occur and why? By investigating these questions, we address some major concerns of both marketing practitioners and scholars interested in channel management issues. In addition, this study provides an indepth discussion on why previous analytic studies produced answers to these questions that depend upon the choice of the form of demand functions. As such, this study should significantly resolve the debate among analytic marketing modelers about the right demand specification. At the core of our discussion lies the concept of , which is defined in terms of the direction of a channel member's reaction to the actions of its channel partner within a given demand structure. Specifically, if a channel member's best reaction is to reduce its margin when its channel partner increases its margin, the type of vertical strategic interaction is referred to as (VSS). If the best reaction is to increase the margin, the environment is referred to as (VSC). If the best reaction is no margin change, it is referred to as (VSI). Using a game theoretic approach, we demonstrate that these three types of vertical strategic interactions represent a key driving force for optimal decisions on channel price leadership and product line pricing. Our investigation involves mathematical analyses of an industry model composed of two manufacturers selling competing products, both carried by two competing retailers. As such, the model allows for retailer product line pricing as well as manufacturer and retailer level competition. In addition, this general model can be used to analyze three more restrictive industry settings often found in the channels literature, i.e., a bilateral monopoly (Jeuland and Shugan [Jeuland, Abel, Steven M. Shugan. 1983. Managing channel profits. (Summer) 23972.]), two competing manufacturers selling through competing franchised retailers (McGuire and Staelin [McGuire, Timothy W., Richard Staelin. 1983. An industry equilibrium analysis of downstream vertical integration. (Spring) 16192.]), and two competing manufacturers selling through one common retailer using product line pricing (Choi [Choi, S. Chan. 1991. Price competition in a channel structure with a common retailer. (Fall) 27196.]). Unlike many other channel studies, most of our analyses are performed without assuming particular functional forms of demand curves. Thus, this paper provides greater assurance that the insights from this stream of research are broadly applicable, not only across industry structures but also across demand conditions. The paper starts out by defining three different rules for how prices are set: The manufacturer uses foresight, the retailer uses foresight, and neither channel member uses foresight. We then show a one-to-one mapping between the type of vertical strategic interaction and the optimality of channel price leadership. Specifically, a channel member finds it profitable to be a price leader for VSS but prefers to be a follower for VSC. For VSI, channel members are indifferent to the channel price leadership issue, as it has no effect on channel member profits. We also show that there exist conditions under which a retailer might see a in profits when it changes its policy from non-product line pricing to product line pricing. Such conditions arise when the retailer is not a price leader and the environment is characterized by VSS or VSC. At a more general level, this study suggests not only the value but also the cost to a firm for using superior knowledge (i.e., foresight and/or product line pricing) in making strategic marketing decisions. In this way, ignorance can be bliss. We also explore the link between demand characteristics and the three types of vertical strategic interaction. We show that the type of vertical strategic interaction present in a given environment is closely related with the convexity of the demand curve and the level of demand for a given price. Interestingly, we find that linearity of demand is not a necessary condition for any of the three types of vertical strategic demand function. Consequently, in evaluating the robustness of analytic analyses, it may be more important to determine the type of vertical strategic interaction assumed instead of whether the demand is linear or nonlinear. Finally, our results are limited to situations where the channels are not coordinated and the retailer's precommitment to particular pricing policy and decision is credible. Although such situations still capture a significant portion of reality, we acknowledge that the insights from this study might not be applicable in all situations.", "e:keyword": ["Game theory", "Channel management", "Price leadership", "Product line pricing", "Strategy", "Vertical strategic interaction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.3.208", "e:abstract": "Our objective in this paper is to explain across-retailer variation in private label performance. Although retailers have lots to gain by better understanding the determinants of successful store brand programs, this knowledge also is very valuable to manufacturers. Lessons learned from competing with other national brands may not transfer one-to-one to the store brand case because, quite simply, a popular private label program changes the status of the retailer from being solely a customer to also a competitor. When customers are competitors, standard predatory tactics may not be appropriate; instead there is a premium on creating a successful basis for coexistence. Our findings from this study are therefore expected to have a broad based appeal both to practitioners and academics working in the evolving area of store brands. Store brands are the only brand for which the retailer must take on all responsibilityfrom development, sourcing, and warehousing to merchandising and marketing. Unlike decisions retailers take about national brands, which in large measure are driven by the manufacturer's actions, the retailer plays a more determinant role in the success or failure of its own label. Based on data from 34 food categories for 106 major supermarket chains operating in the largest 50 retail markets in the U.S., we use regression-based analyses to show that variation in store brand performance across retailers is systematically related to underlying consumer, retailer, and manufacturer factors. The key insights provided by our analysis are as follows: (1) Overall chain strategy in terms of commitment to quality, breadth of private label offerings, use of own name for private label, a premium brand offering, and number of stores consistently enhance the retailer's store brand performance in all categories. Also, the extent to which the retailer serves a customer base containing less wealthy and more elderly households and operates in less competitive markets improves the performance of the store brand. (2) The everyday low price (EDLP) positioning benefits the store brand but only in lower quality categories where the value positioning of the store may be better aligned with the price advantage of the store brand. (3) Supporting recent statements in the popular press, our analysis suggests that retailer promotional support can significantly enhance private label performance. (4) Retailers often use national brands to draw customers to their stores. Retailers who pursue this traffic building strategy usually carry more national brands, deeper assortments, and offer better everyday (lower price gap) and promotional prices on national brands. Each of these actions works against the retailer's own brands, highlighting the important balancing act the retailer must perform to profitably manage the sales revenue and margin mix in each of their categories. At the same time, adding a higher quality premium store brand program may mitigate this tradeoff. (5) Unlike cross-category studies, our within-category across-retailer analysis shows that the national brandprivate label price differential exerts an important positive influence on store brand performance. (6) When retailers obtain more than their fair share of a category (high category development index), they also do much better with private labels. (7) From the national brand's perspective, encouraging the retailer to carry more brands and deeper assortments may be the most effective way to keep store brands in check. The importance of these variables, however, may depend on the national brand's market position. For example, a category leader may be glad to see a rise in store brand share if it comes at the expense of one of its secondary national brand competitors. (8) The exact impact of most of the variables depends on the underlying quality of store brands in a category. When store brand quality is high, competition at the retail and brand level is more important, as are variables capturing economies of scale and scope enjoyed by the retailer. In contrast, demographics associated with consumer price sensitivity and EDLP pricing matter more in low quality categories. (9) Finally, premium store brands offer the retailer an avenue for responding to the national brand's ability to cater to heterogeneous preferences. This appears more likely in categories where store brands already offer high quality comparable to the national brands. We argue that private labels threaten national brands most in categories when there is high variance in share across categories (as opposed to high average share per se). In high variance categories, store brand share could increase dramatically if the poor performing retailers imitate best practices. Future research can extend this work in several ways both on the substantive and methodological fronts.", "e:keyword": ["Private labels", "Store brands", "Retailing", "Distribution channels"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.3.228", "e:abstract": "Managerial decisions involving marketing channels are among the most critical that an organization must make. Part of the reason for this importance is that relationships between manufacturers and their intermediaries usually involve long-term commitments that are difficult to change. On the other hand, in order to respond to the realities of the market place, an organization must be ready to adapt its distribution practicessometimes under considerable uncertainty about the long-term consequences. Such a problem faces the U.S. automobile industry, which has led manufacturers to experiment with various channel structures. When manufacturers first changed their distribution policies, they were clear about the short-term effect on sales, but were unsure about its longer term impact on profitability. In this article, we develop a model to analyze the marketing of durable products through multiple channels. Our analysis suggests that, even though it was not apparent at the time, manufacturers were indeed behaving optimally when they changed their policies. Our model provides insights not only to automobile manufacturers but also to practitioners and academics who are interested in understanding the unique problems associated with marketing durable products through multiple channels. We develop a two-period model by assuming that a single manufacturer markets a durable product through two retailersa rental agency and a dealer. The rental agency focuses mainly on renting the product in a daily rental market while the dealer focuses on selling the product to a different set of customers in the sales market. To model the development of channels in the U.S. automobile industry, we analyze three different channel structures. The first structure, a , reflects the state of the industry through most of the 1980s, when rental agencies were franchised solely to rent and dealers solely to sell the cars. In response to a decrease in overall sales, manufacturers encouraged rental agencies to sell their slightly used rental cars in the consumer market, resulting in the second structure, an . Dealers did not like this arrangement, however, and in the next experiment, a , some manufacturers began buying back used rental cars and selling them through dealers. In terms of the consumer side of the model, we assume that consumers are heterogeneous and have product valuations that are distributed uniformly between a low and a high value. In addition, they recognize that as the durable depreciates with use, its secondhand market value decreases. While both sold and rented goods depreciate with use, we assume, based on an analysis of market prices, that sold goods depreciate at a higher rate than rented goods. Given these different depreciation rates and consumers' underlying utility functions, we develop the market demand functions in the dealer's and rental agency's markets. Then for each of the channel structures, we solve the intermediaries' and manufacturer's problems. The main contribution of this article is that it allows us to evaluate the profitability associated with various channel structures for all the players in our analysisthe dealer, the rental agency, and the manufacturer. In terms of the intermediaries, we find that the overlapping channel is the most profitable structure for the rental agency; on the other hand, it is the least profitable for the dealer. In terms of manufacturer profitability, our model suggests that the separate channel is the least profitable, and the overlapping channel is the most profitable. It is interesting to note that the distribution structure in existence today is more akin to a buyback channel. This strikes us as a compromise channel, which alleviates dealer concerns with the overlapping channel, and yet does not harm rental agencies as much as a separate channel. These are surprising results because conventional wisdom has been that the overlapping channel was competing away profits for all players. This suggests to us that automobile manufacturers were indeed on the right track when they began experimenting with the structure of their distribution channels.", "e:keyword": ["Marketing channels", "Durable goods", "Channel conflict", "Automobile industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.3.246", "e:abstract": "Side payments, known politely as gainsharing and pejoratively as bribery, are prevalent in marketing. Indeed, many management schools have added ethics modules to their basic marketing courses to discuss these issues and there is much discussion of side payments in the literature (e.g., Adams 1995, Borrus [Borrus, Amy. 1995. A world of greased palms. (November 6) 3638.], Mauro [Mauro, Paolo. 1997. Why worry about corruption? . Washington, D.C.], Mohl [Mohl, Bruce. 1996. Auto dealers color surveys to polish image: Buyer's say salesmen tamper with makers' questionnaires. (August 11) B1, B6.], Murphy [Murphy, Kate. 1995. Corporate gifts: What's naughty or nice. (December 11) 122.], Peterson [Peterson, Barbara S. 1996. Taxing question: Will the government make you pay for your next travel benefits? (February) 2730.], and Rose-Ackerman [Rose-Ackerman, Susan. 1996. Bribes and gifts. , April 1920. Yale University, New Haven, CT.]). We seek to provide insight with respect to one class of marketing side payments. We hope that our analyses clarify some of the issues and suggest how these side payments affect marketing activities. We begin by focusing on one common example of potential side paymentssalesforce ratings of internal sales support. We derive two formal results and speculate on how these results generalize. The two results are (1) that having one group of employees rate another implies that there are almost always incentives for side payments, but (2) the side payments need not reduce the firm's profit. At least in theory, the firm is always able to revise the reward system to factor out these side payments. The first result, based on a straightforward proof, has important practical implications for managers who may wish to preclude side payments. They may be unable to design a ratings-based reward system that does not have inherent incentives for side payments. The second result, in our opinion, is quite surprising. It suggests that marketing managers might be advised to invest more time into understanding how side payments affect employee reactions to reward systems. They might want to reconsider costly efforts to monitor, police, or preclude such side payments. While our results do not substitute for a moral discussion of side payments, we hope that the formal structure for one common marketing situation provides valuable insight. The system we analyze is based on a practical managerial problem we have observed. The salesforce evaluates a sales support group with a real-valued rating. The sales support group is rewarded based on that rating, whereas the sales-force is rewarded based on outcomes, such as sales or customer satisfaction, that indicate incremental profits to the firm. The reward to the salesforce might also depend upon how it rates sales support. For example, the salesforce might be held to a higher standard whenever it rates sales support as excellent. (We argue in the paper that the firm will want this to happen.) In addition, the salesforce might ask for a side payment from the sales support group as compensation for high ratings. We cast the practical problem as a formal game and incorporate the following issues: (1) incremental actions taken by the salesforce and by sales support are perceived to be onerous, (2) the measure of incremental profit is a noisy measure, (3) both the salesforce and sales support are risk averse, (4) given the reward system imposed by the firm, both the salesforce and sales support will maximize their well-being, and (5) given the structure of the reward system, the firm will seek to maximize expected profits. We first show that there are almost always incentives for side payments. Specifically, we demonstrate that sales support is better off with a side payment, while the salesforce is no worse off. This is not surprising because the reward to sales support is increasing in the rating, while in the absence of a side payment, the salesforce will select a rating such that its net marginal returns to increasing the rating are zero. The exception occurs when the rating is constrained by the firm to be less than this optimal rating, but even then there might be incentives for side payments. We next show that the firm can anticipate these side payments and design a reward system to factor them out at no loss of profit. The intuition is straightforward. The firm first adjusts the marginal returns in the reward functions for sales support and for the salesforce such that they will each take the optimal actions even though they engage in side payments. Then the firm adjusts their fixed compensation so that the firm extracts its full profit. The proof is difficult because we must show that adjusted reward systems exist and we must show that they allow the full profit to be extracted. Throughout the paper we discuss the practical implications of our results. We close by highlighting future research opportunities.", "e:keyword": ["Incentive systems", "Salesforce", "Agency theory", "Side payments"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.3.256", "e:abstract": "A consistent pattern observed for really new household consumer durables is a takeoff or dramatic increase in sales early in their history. The takeoff tends to appear as an elbow-shaped discontinuity in the sales curve showing an average sales increase of over 400%. In contrast, most marketing textbooks as well as diffusion models generally depict the growth of new consumer durables as a smooth sales curve. Our discussions with managers indicate that they have little idea about the takeoff and its associated characteristics. Many managers did not even know that most successful new consumer durables had a distinct takeoff. Their sales forecasts tend to show linear growth. Yet, knowledge about the takeoff is crucial for managers to decide whether to maintain, increase, or withdraw support of new products. It is equally important for industry analysts who advise investors and manufacturers of complementary and substitute products. Although previous studies have urged researchers to examine the takeoff, no research has addressed this critical event. While diffusion models are commonly used to study new product sales growth, they do not explicitly consider a new product's takeoff in sales. Indeed, diffusion researchers frequently use data only from the point of takeoff. Therefore, nothing is known about the takeoff or models appropriate for this event. Our study provides the first analysis of the takeoff. In particular, we address three key questions: (i) How much time does a newly introduced product need to takeoff? (ii) Does the takeoff have any systematic patterns? (iii) Can we predict the takeoff? We begin our study by developing an operational measure to determine when the takeoff occurs. We found that when the base level of sales is small, a relatively large percentage increase could occur without signaling the takeoff. Conversely, when the base level of sales is large, the takeoff sometimes occurs with a relatively small percentage increase in sales. Therefore, we developed a threshold for takeoff. This is a plot of percentage sales growth relative to a base level of sales, common across all categories. We define the takeoff as the first year in which an individual category's growth rate relative to base sales crosses this threshold. The threshold measure correctly identifies the takeoff in over 90% of our categories. We model the takeoff with a hazard model because of its advantages for analyzing time-based events. We consider three primary independent variables: price, year of introduction, and market penetration, as well as several control variables. The hazard model fits the pattern of takeoffs very well, with price and market penetration being strong correlates of takeoff. Our results provide potential generalizations about the time to takeoff and the price reduction, nominal price, and penetration at takeoff. In particular, we found that:  On average for 16 post-World War II categories:  the price at takeoff is 63% of the introductory price;  the time to takeoff from introduction is six years;  the penetration at takeoff is 1.7%.  The time to takeoff is decreasing for more recent categories. For example, the time to takeoff is 18 years for categories introduced before World War II, but only six years for those introduced after World War II.  Many of the products in our sample had a takeoff near three specific price points (in nominal dollars): $1000, $500 and $100. In addition, we show how the hazard model can be used to predict the takeoff. The model predicts takeoff one year ahead with an expected average error of 1.2 years. It predicts takeoff at a product's introduction with an expected average error of 1.9 years. Even against the simple mean time to takeoff of six years for recent categories, the model's performance represents a tremendous improvement in prediction. It represents an immeasurable improvement in prediction for managers who currently have no idea about how long it takes for a new product to takeoff. The threshold rule for determining takeoff can be used to distinguish between a large increase in sales and a real takeoff. Some limitations of this study could provide fruitful areas for future research. Our independent variables suffer from endogeneity bias, so alternative variables or methods could address this limitation. Also, the takeoff may be related to additional variables such as relative advantage over substitutes and the presence of complementary products. Finally, examination of sales from takeoff to their leveling off could be done with an integrated model of takeoff and sales growth or with the hazard model we propose. Generalizations about this period of sales growth could also be of tremendous importance to managers of new products.", "e:keyword": ["New product research", "Product policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.3.271", "e:abstract": "Pioneers' marketing mix reactions to new entries are recognized as important determinants of the outcome of pioneerlate mover competition, particularly in price-inelastic markets such as those for pharmaceuticals, cigarettes, and luxury goods. Managers in such markets are interested in better understanding when to accommodate (i.e., decrease marketing spending) or retaliate (i.e., increase spending) in nonprice marketing variables such as advertising and salesforce. In addition, the reallocation of marketing resources toward advertising (indicated by a pull strategy) or salesforce (indicated by a push strategy) upon entry is strategically important to managers. Previous theoretical research shows that pioneers should retaliate in both static and growing markets. Results from empirical research are mixed in that they support both accommodation and retaliation in growing markets. Empirical research also shows that a pioneer accommodates (retaliates) with its low (high) elasticity marketing mix variable. Contrary to prior research, however, some pioneers have successfully accommodated late movers in growing markets, and in some cases, have accommodated with their stronger marketing mix variables and also retaliated with their weaker marketing mix variables. For example, Bristol Myers Squibb's Capoten accommodated the entry of Merck's Vasotec in the growing ace-inhibitors market with its more powerful variable, salesforce, but also retaliated with its less potent variable, advertising. Moreover, not much is known about how the pioneer's marketing mix allocation should change (i.e., toward pull vs. push strategies) in response to new entries. We seek to better explain the pioneer's reactions and predict its shift in marketing mix allocation upon new entry. We note that prior research's predictions on the pioneer's reactions are based on a limited number of key factors such as product-market characteristics and the pioneer's elasticities prior to a new entry. In this paper, we extend previous research by adding two other critical factors, namely, the impact of new entry on the pioneer's elasticities and margin, and different competitive game structures to better predict and explain the pioneer's reactions. We develop analytical results on the pioneer's reactions in price, advertising, and salesforce in different competitive games (both Nash and different leader-follower games). In these results we identify the conditions under which the pioneer should accommodate, or retaliate, or not react to a late mover's entry, and shift its marketing mix allocation toward pull versus push strategies. We empirically illustrate some of the analytical results using data from a pharmaceutical category. We show that a pioneer who adopts a follower (leader) role with respect to a marketing mix variable in a static (growing) market, and witnesses a decrease (an increase) in own elasticity and margin upon a new entry, should accommodate (retaliate) in that variable. However, we are also able to show that there are cases where these general reactions don't hold. Thus, for example, it is possible for a pioneer to find it optimal to accommodate in a growing market or to retaliate even when its elasticity decreases upon entry, depending on the combination of competitive game, the impact of entry on elasticities and margin, and market growth. In this way, our results point to the fact that it is necessary to look not only at one factor at a time, but instead examine the combination of all the factors. We explain the empirical support for both accommodation and retaliation in growing markets by showing that the pioneer should accommodate (retaliate) a late mover with its low (high) elasticity marketing mix variable. Competitively high (low) elasticity variables are not (are) likely to be significantly reduced by a new entry in the anticipated competitive game. With regard to reallocation of the pioneer's marketing mix, we show that the change in the pioneer's marketing mix allocation should follow the change in the relative marketing mix effectiveness after new entry. This, in turn, depends on the structure of competition, the impact of the late mover on its elasticities and margins, and the competitor's marketing mix elasticities, in addition to own elasticities. The results can guide managers on how factors such as competitive structure, changes in elasticities and margin, and market growth impact the pioneer's marketing mix decisions, and on when to accommodate, retaliate, or not react to a late mover's entry, and shift marketing mix allocation toward pull versus push strategies.", "e:keyword": ["Competitive strategy", "Defensive strategy", "Econometric models", "Game theory", "Marketing mix", "Resource allocation policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.4.295", "e:abstract": "Customers often have to wait during the process of acquiring and consuming many products and services. These waiting experiences are typically negative and have been known to affect customers' overall satisfaction with the product or service. To better manage these waiting experiences, many firms have instituted a variety of programs not only to reduce the actual duration of the wait but also to improve customers' perceptions of it. In this paper, we examine the impact of one such initiative, namely, the institution of a waiting time guarantee, on customers' waiting experiences. A waiting time guarantee is a commitment from a firm to serve its customers within a specified period of time. If the firm fails to meet this commitment for some customers then it compensates them for the delay. Today, a large number of firms in a variety of industries such as fast food, banking, industrial distribution, and healthcare offer such time guarantees to their customers. We develop a utility theory-based model of customers' satisfaction with waiting in line. The model is based upon the assumption that when a customer joins a queue he or she has some prior beliefs about the distribution of service times at the firm. The customer estimates the likely duration of the waiting time on the basis of these beliefs about the service times and the observed queue length. We further assume that as the customer observes the service times for other customers who are ahead in the queue, he or she successively updates these beliefs about the distribution of service times in a Bayesian manner. We then posit that the customer's satisfaction both during as well as the end of the wait is determined by the difference between the customer's updated and the prior estimates of the total waiting time. We apply the model to derive select hypotheses pertaining to the impact of a waiting time guarantee on customers' waiting experiences. These hypotheses are based upon the assumption that an offer of a time guarantee is a signal of reliability from the firm and reduces customers' perceived variance around the expected service times. We empirically test these hypotheses using data from a series of interactive, computer-based laboratory experiments. In these experiments, we used the computer to create animations of reallife waiting experiences. The computer display consisted of a queue of customers waiting for service at a counter. One of the customers represented the participant in the experiment. During the course of the experiment, each participant joined the queue, waited in line for service, and then exited the system. At several points during the wait, each participant reported his or her level of satisfaction with the waiting experience. Our results suggest that if customers observe the service times to be less than expected, their satisfaction increases monotonically during the wait. Further, under such circumstances, the explicit provision of a waiting time guarantee enhances satisfaction both during as well as at the end of the wait. However, if customers observe the service times to be more than expected, then their satisfaction typically declines at the beginning of the wait but increases toward the end of the wait. Further, under these circumstances, the initial positive impact of the provision of a waiting time guarantee declines over time. Moreover, at the end of the wait, customers in guaranteed environments are actually less satisfied than those in unguaranteed environments. Overall, we find that a time guarantee, if met, increases satisfaction at the end of a wait; however, if violated, then it decreases satisfaction at the end of the wait. We discuss the implications of these and other empirical findings for the management of customers' waiting experiences.", "e:keyword": ["Waiting time", "Time guarantees", "Services", "Customer satisfaction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.4.315", "e:abstract": "Micro-marketing refers to the customization of marketing mix variables to the store-level. This paper shows how prices can be profitably customized at the store-level, rather than adopting a uniform pricing policy across all stores. Historically, there has been a trend by retailers to consolidate independent stores into large national and regional chains. This move toward consolidation has been driven by the economies of scale associated with these larger operations. However, some of these large chains have lost the adaptability of independent neighborhood stores. Micro-marketing represents an interest on the part of managers to combine the advantages of these large operations with the flexibility of independent neighborhood stores. A basis for these customized pricing strategies is the result of differences in interbrand competition across stores. These changes in interbrand competition are measured using weekly store-level scanner data at the product level. Obviously, this presents a huge estimation problem, since we wish to measure substitution between each product at a store-level. For a chain with 100 stores and 10 products in a category we would need to estimate over 100,000 parameters. To reliably estimate these individual store differences we phrase our problem in a hierarchical Bayesian framework. Essentially, each store-level parameter can be thought of as a combination of chain-level and random store-specific effects. The improvement in estimating this model comes from exploiting the common chain-level component. In addition, we relate these store-specific changes to demographic and competitive characteristics of the store's trading area, which helps explain why these differences are present. These estimated differences in price response are in turn used to set store-level prices. To simplify and focus the problem we limit our attention to everyday price changes (i.e., the prices of products that are not advertised). There are many marketing variables that can be adjusted at a storelevel (e.g., promotions and product assortments); the reason we concentrate upon everyday pricing is driven by its importance in the marketing mix, that most profits are earned on products sold at their everyday price, and the amenability of everyday prices to store-level customizations. A limitation of this approach is that it yields only a partial solution to the retailer's global optimization problem. A challenge for the retailer in implementing micro-marketing pricing strategies is to retain a consistent image while altering prices that adapt to neighborhood differences in demand. Our approach is to search for price changes that leave image unchanged. We argue that a sufficient condition for holding the input to store image constant from everyday pricing is to hold average price and revenues at their current levels. We implement this condition by introducing constraints into the profit maximization problem. Future research into store choice may yield more efficient conditions. A benefit of holding the retailer's image constant is that it does not require costly new information about competitors and promotional activity. Instead, retailers are able to derive these store-level customizations based largely upon scanner data. This is very advantageous since this information is already being collected and is readily available. Our results indicate that micro-marketing pricing strategies would be profitable and could increase gross profit margins by 4 percent to 10 percent. When these gross profit gains are considered after administrative and operating costs are taken into account, they could increase operating profit margins by 33 percent to 83 percent. These gains come from encouraging consumers through everyday price changes to switch to more profitable bundles of products, and not through overall price changes at the chain-level. These results show that the information contained in the retailer's store-level scanner data is an under-utilized resource. By exploiting this information using newer and more powerful computational techniques managers can better appreciate its value. The implication is that profits could be increased and gains can be made by using this information as the basis for micro-marketing.", "e:keyword": ["Pricing", "Micro-marketing", "Segmentation research", "Retailing", "Scanner data", "Estimation", "Bayesian hierarchical models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.4.338", "e:abstract": "Studies estimating the Bass model and other macro-level diffusion models with an unknown ceiling feature three curious empirical regularities: (i) the estimated ceiling is often close to the cumulative number of adopters in the last observation period, (ii) the estimated coefficient of social contagion or imitation tends to decrease as one adds later observations to the data set, and (iii) the estimated coefficient of social contagion or imitation tends to decrease systematically as the estimated ceiling increases. We analyze these patterns in detail, focusing on the Bass model and the nonlinear least squares (NLS) estimation method. Using both empirical and simulated diffusion data, we show that NLS estimates of the Bass model coefficients are biased and that they change systematically as one extends the number of observations used in the estimation. We also identify the lack of richness in the data compared to the complexity of the model (known as ill-conditioning) as the cause of these estimation problems. In an empirical analysis of twelve innovations, we assess how the model parameter estimates change as one adds later observations to the data set. Our analysis shows that, on average, a 10% increase in the observed cumulative market penetration is associated with, roughly, a 5% increase in estimated market size , a 10% decrease in the estimated co-efficient of imitation , and a 15% increase the estimated co-efficient of innovation . A simulation study shows that the NLS parameter estimates of the Bass model change systematically as one adds later observations to the data set, even in the absence of model misspecification. We find about the same effect sizes as in the empirical analysis. The simulation also shows that the estimates are biased and that the amount of bias is a function of (i) the amount of noise in the data, (ii) the number of data points, and (iii) the difference between the cumulative penetration in the last observation period and the true penetration ceiling (i.e., the extent of right censoring). All three conditions affect the level of ill-conditioning in the estimation, which, in turn, affects bias in NLS regression. In situations consistent with marketing applications, can be underestimated by 20%, underestimated by the same amount, and overestimated by 30%. The existence of a downward bias in the estimate of and an upward bias in the estimate of , and the fact that these biases become smaller as the number of data points increases and the censoring decreases, can explain why systematic changes in the parameter estimates are observed in many applications. A reduced bias, though, is not the only possible explanation for the systematic change in parameter estimates observed in empirical studies. Not accounting for the growth in the population, for the effect of economic and marketing variables, or for population heterogeneity is likely to result in increasing and decreasing as well. In an analysis of six innovations, however, we find that attempts to address possible model misspecification problems by making the model more flexible and adding free parameters result in larger rather than smaller systematic changes in the estimates. The bias and systematic change problems we identify are sufficiently large to make long-term predictive, prescriptive and descriptive applications of Bass-type models problematic. Hence, our results should be of interest to diffusion researchers as well as to users of diffusion models, including market forecasters and strategic market planners.", "e:keyword": ["Diffusion", "Estimation and other statistical techniques", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.4.354", "e:abstract": "Diffusion models represent one of the key successes in marketing science. We understand a great deal about what influences and shapes within-country diffusion patterns. However, we understand far less about what influences and shapes cross-country (or inter-country) diffusion patterns. Clearly, for firms operating in a truly global environment, understanding to what extent prior adoption in one country affects current (and consequently future) adoption in another country can be an important consideration. Further, understanding how the of interaction across countries impacts the diffusion process can have profound implications for product launch strategies for new durable products. In this paper, we address the extent to which prior adoption of a new product in one country affects adoption in other countries. In particular, we investigate the importance of the pattern of interaction (what we call mixing) across countries in the context of a new product diffusion model. More specifically, mixing refers to the pattern of communication within and across countries. For example, do Italians communicate only with other Italians or do they influence consumers in other countries? Understanding and empirically estimating how these mixing patterns across countries influence the subsequent diffusion process is the central research objective of this paper. While previous research in marketing addressing cross-country diffusion has assumed very specific forms of cross-country interaction, the present study develops and estimates a flexible form of mixing that allows for the simultaneous estimation of mixing patterns across multiple countries. In particular, we examine the effect of mixing behavior across populations on new product adoption, viewing mixing as occurring across a continuum with segregation (no mixing) at one end and random mixing at the other. Intermediate forms of mixing lie along this continuum and are called Bernoulli mixing. In order to produce generalizable results, we obtained sales data on four product categories in the European community (EC) nations: VCRs from 19771990, microwave ovens 19751990, compact disc players 19841993, and home computers 19811991. For each product, data were collected for 10 EC nations: Great Britain, Germany, France, Italy, Spain, Belgium, Denmark, Netherlands, Sweden, and Austria. A diffusion model that incorporates cross-country prior adoption, cross-country mixing patterns, and individual country covariates was estimated simultaneously across the 10 countries for each of the four products. On the basis of the empirical results, we conclude that mixing behavior across segments is an important consideration in new product diffusion. Specifically, the observed pattern of mixing and the strength of cross-segment influences are important considerations in allocating the marketing mix during product introduction stages. The practical implication of mixing behavior is that if one wishes to understand diffusion in Belgium (for example) it is important to not only know the diffusion parameters in Belgium, but also those in other EC nations. Accordingly, these results should be of interest to managers responsible for coordinating new product launches across countries and to researchers interested in investigating cross-country diffusion issues. As an example of the strategic implications of the results, we note that under Bernoulli mixing future country launches will benefit not only from prior adoption in countries with high contact rates, but in particular from countries with high rates of contact with . Although there is some variance across products, in general Germany, France, Italy, and Spain are the most gregarious, as measured by the rate of contact. The results suggest that a strategy of first focusing on Germany, France, Italy, and Spain will maximize adoption in subsequent countries. Each of these countries tends to have a higher percentage of its contacts with individuals internal to its borders as well as with other countries. This suggests that these countries would have relatively quick adoption internally, while having a strong influence externally. Further, for countries such as Denmark, Netherlands, Austria, and Sweden, a higher percentage of their contacts are derived from outside their borders. Thus, seeding the diffusion process by introducing first in Germany, France, Italy, and Spain would benefit later adoption in countries such as Denmark, Netherlands, Austria, and Sweden. Although the strength of the individual country influence varies from product to product, this result is generally consistent across the four products.", "e:keyword": ["Cross-country diffusion", "International marketing", "New product research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.4.370", "e:abstract": "This paper presents a definitive description of neural network methodology and provides an evaluation of its advantages and disadvantages relative to statistical procedures. The development of this rich class of models was inspired by the neural architecture of the human brain. These models mathematically emulate the neurophysical structure and decision making of the human brain, and, from a statistical perspective, are closely related to generalized linear models. Artificial neural networks are, however, nonlinear and use a different estimation procedure (feed forward and back propagation) than is used in traditional statistical models (least squares or maximum likelihood). Additionally, neural network models do not require the same restrictive assumptions about the relationship between the independent variables and dependent variable(s). Consequently, these models have already been very successfully applied in many diverse disciplines, including biology, psychology, statistics, mathematics, business, insurance, and computer science. We propose that neural networks will prove to be a valuable tool for marketers concerned with predicting consumer choice. We will demonstrate that neural networks provide superior predictions regarding consumer decision processes. In the context of modeling consumer judgment and decision making, for example, neural network models can offer significant improvement over traditional statistical methods because of their ability to capture nonlinear relationships associated with the use of noncompensatory decision rules. Our analysis reveals that neural networks have great potential for improving model predictions in nonlinear decision contexts without sacrificing performance in linear decision contexts. This paper provides a detailed introduction to neural networks that is understandable to both the academic researcher and the practitioner. This exposition is intended to provide both the intuition and the rigorous mathematical models needed for successful applications. In particular, a step-by-step outline of how to use the models is provided along with a discussion of the strengths and weaknesses of the model. We also address the robustness of the neural network models and discuss how far wrong you might go using neural network models versus traditional statistical methods. Herein we report the results of two studies. The first is a numerical simulation comparing the ability of neural networks with discriminant analysis and logistic regression at predicting choices made by decision rules that vary in complexity. This includes simulations involving two noncompensatory decision rules and one compensatory decision rule that involves attribute thresholds. In particular, we test a variant of the satisficing rule used by Johnson et al. (Johnson, Eric J., Robert J. Meyer, Sanjoy Ghose. 1989. When choice models fail: Compensatory models in negatively correlated environments. (August) 255270.) that sets a lower bound threshold on all attribute values and a latitude of acceptance model that sets both a lower threshold and an upper threshold on attribute values, mimicking an ideal point model (Coombs and Avrunin [Coombs, Clyde H., George S. Avrunin. 1977. Single peaked functions and the theory of preference. 216230.]). We also test a compensatory rule that equally weights attributes and judges the acceptability of an alternative based on the sum of its attribute values. Thus, the simulations include both a linear environment, in which traditional statistical models might be deemed appropriate, as well as a nonlinear environment where statistical models might not be appropriate. The complexity of the decision rules was varied to test for any potential degradation in model performance. For these simulated data it is shown that, in general, the neural network model outperforms the commonly used statistical procedures in terms of explained variance and out-of-sample predictive accuracy. An empirical study bridging the behavioral and statistical lines of research was also conducted. Here we examine the predictive relationship between retail store image variables and consumer patronage behavior. A direct comparison between a neural network model and the more commonly encountered techniques of discriminant analysis and factor analysis followed by logistic regression is presented. Again the results reveal that the neural network model outperformed the statistical procedures in terms of explained variance and out-of-sample predictive accuracy. We conclude that neural network models offer superior predictive capabilities over traditional statistical methods in predicting consumer choice in nonlinear and linear settings.", "e:keyword": ["Consumer decision making", "Neural networks", "Statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.1.4", "e:abstract": "In the standard economic account of consumer behavior the cost of a purchase takes the form of a reduction in future utility when expenditures that otherwise could have been made are forgone. The reality of consumer hedonics is different. When people make purchases, they often experience an immediate , which can undermine the pleasure derived from consumption. The ticking of the taxi meter, for example, reduces one's pleasure from the ride. We propose a double-entry mental accounting theory that describes the nature of these reciprocal interactions between the pleasure of consumption and the pain of paying and draws out their implications for consumer behavior and hedonics. A central assumption of the model, which we call , is that consumption that has already been paid for can be enjoyed as if it were free and that the pain associated with payments made prior to consumption (but not after) is buffered by thoughts of the benefits that the payments will finance. Another important concept is , which refers to the degree to which consumption calls to mind thoughts of payment, and vice versa. Some financing methods, such as credit cards, tend to weaken coupling, whereas others, such as cash payment, produce tight coupling. Our model makes a variety of predictions that are at variance with economic formulations. Contrary to the standard prediction that people will finance purchases to minimize the present value of payments, our model predicts strong that they should prefer to prepay for consumption or to get paid for work after it is performed. Such pay-before sequences confer hedonic benefits because consumption can be enjoyed without thinking about the need to pay for it in the future. Likewise, when paying beforehand, the pain of paying is mitigated by thoughts of future consumption benefits. Contrary to the economic prediction that consumers should prefer to pay, at the margin, for what they consume, our model predicts that consumers will find it less painful to pay for, and hence will prefer, flat-rate pricing schemes such as unlimited Internet access at a fixed monthly price, even if it involves paying more for the same usage. Other predictions concern spending patterns with cash, charge, or credit cards, and preferences for the earmarking of purchases. We test these predictions in a series of surveys and in a conjoint-like analysis that pitted our double-entry mental accounting model against a standard discounting formulation and another benchmark that did not incorporate hedonic interactions between consumption and payments. Our model provides a better fit of the data for 60% of the subjects; the discounting formulation provides a better fit for only 29% of the subjects (even when allowing for positive and negative discount rates). The pain of paying, we argue, plays an important role in consumer self-regulation, but is hedonically costly. From a hedonic perspective the ideal situation is one in which payments are tightly coupled to consumption (so that paying evokes thoughts about the benefits being financed) but consumption is decoupled from payments (so that consumption does not evoke thoughts about payment). From an efficiency perspective, however, it is important for consumers to be aware of what they are paying for consumption. This creates a tension between hedonic efficiency and what we call . Various institutional arrangements, such as financing of public parks through taxes or usage fees, play into this tradeoff. A producer developing a pricing structure for their product or service should be aware of these two conflicting objectives, and should try to devise a structure that reconciles them.", "e:keyword": ["Mental Accounting", "Framing", "Consumer Choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.1.29", "e:abstract": "Product design, pricing policies, and promotional activities influence the primary and secondary demand for goods and services. Brand managers need to develop an understanding of the relationships between marketing mix decisions and consumer decisions of whether to purchase in the product category, which brand to buy, and how much to consume. Knowledge about factors most effective in influencing primary and secondary demand of a product allows firms to grow by enhancing their market share as well their market size. The purpose of this paper is to develop an individual level model that allows an investigation of both the primary and secondary aspects of consumer demand. Unlike models of only primary demand or only secondary demand, this more comprehensive model offers the opportunity to identify changes in product features that will result in the greatest increase in demand. It also offers the opportunity to differentially target consumer segments depending upon whether consumers are most likely to enter the market, increase their consumption level, or switch brands. In the proposed hierarchical Bayes model, an integrative framework that jointly models the discrete choice and continuous quantity components of consumer decision is employed instead of treating the two as independent. The model includes parameters that capture individual specific reservation value, attribute preference, and expenditure sensitivity. The model development is based upon the microeconomic theory of utility maximization. Heterogeneity in model parameters across the sample is captured by using a random effects specification guided by the underlying microeconomic model. This requires that some of the effects are strictly positive. This is accommodated through the use of a gamma distribution of heterogeneity for some of the parameters. A normal distribution of heterogeneity is used for the remaining parameters. Gibbs sampling is used to estimate the model. The key methodological contribution of this paper is that we show how to specify a hierarchical Bayes continuous random effects model that integrates consumer choice and quantity decisions such that individual-level parameters can be estimated. Individual level estimates are desirable because insights into primary demand involve nonlinear functions of model parameters. For example, consumers not in the market are those whose utilities for the choice alternatives fall below some reservation value. The proposed methodology yields individual specific estimates of reservation values and expenditure sensitivity, which allow assessment of the origins of demand other than the switching behavior of consumers. The methodology can also be used to help identify changes in product features most likely to bring new customers into a market. Our work differs from previous research in this area as we lay the framework needed to obtain individual-level parameter estimates in a continuous random effects model that integrates choice and quantity. The methodology is demonstrated with survey data collected about consumer preferences and consumption for a food item. For the data available, a large response heterogeneity was observed across all model parameters. In spite of limited data available at the individual level, a majority of the individual level estimates were found to be significant. Predictive tests demonstrated the superiority of the proposed model over existing latent class and aggregate models. Particularly, significant gains in predictive accuracy were observed for the no-buy behavior of the respondents. These gains demonstrate that by structurally linking the choice and quantity models results in a more accurate characterization of the market than existing finite mixture approaches that model choice and quantity independently. We show that our joint model makes more efficient use of the available data and results in better parameter estimates than those that assume independence. Finally, the individual level demand analysis is illustrated through a simple example involving a $1.00 price cut. We demonstrate practical usefulness of the model for targeting by developing the demographic, attitudinal, and behavioral profiles of consumer groups most likely to increase consumption, enter the market, or switch brands because of a price cut decision.", "e:keyword": ["Demand Analysis", "Choice Models", "Product Design", "Product Repositioning", "Discrete Continuous Models", "Gibbs Sampling", "Metropolis-Hastings Algorithm"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.1.45", "e:abstract": "Many service organizations have embraced relationship marketing with its focus on maximizing customer lifetime value. Recently, there has been considerable controversy about whether there is a link between customer satisfaction and retention. This research question is important to researchers who are attempting to understand how customers' assessments of services influence their subsequent behavior. However, it is equally vital to managers who require a better understanding of the relationship between satisfaction and the duration of the provider-customer relationship to identify specific actions that can increase retention and profitability in the long run. Since there is very little empirical evidence regarding this research question, this study develops and estimates a dynamic model of the duration of provider-customer relationship that focuses on the role of customer satisfaction. This article models the duration of the customer's relationship with an organization that delivers a continuously provided service, such as utilities, financial services, and telecommunications. In the model, the duration of the provider-customer relationship is postulated to depend on the customer's subjective expected value of the relationship, which he/she updates according to an anchoring and adjustment process. It is hypothesized that cumulative satisfaction serves as an anchor that is updated with new information obtained during service experiences. The model is estimated as a left-truncated, proportional hazards regression with cross-sectional and time series data describing cellular customers perceptions and behavior over a 22-month period. The results indicate that customer satisfaction ratings elicited prior to any decision to cancel or stay loyal to the provider are positively related to the duration of the relationship. The strength of the relationship between duration times and satisfaction levels depends on the length of customers' prior experience with the organization. Customers who have many months' experience with the organization weigh prior cumulative satisfaction more heavily and new information (relatively) less heavily. The duration of the service provider-customer relationship also depends on whether customers experienced service transactions or failures. The effects of perceived losses arising from transactions or service failures on duration times are directly weighed by prior satisfaction, creating contrast and assimilation effects. How can service organizations develop longer relationships with customers? Since customers weigh prior cumulative satisfaction heavily, organizations should focus on customers in the early stages of the relationshipif customers' experiences are not satisfactory, the relationship is likely to be very short. There is considerable heterogeneity across customers because some customers have a higher utility for the service than others. However, certain types of service encounters are potential relationship landmines because customers are highly sensitive to the costs/losses arising from interactions with service organizations and insensitive to the benefits/gains. Thus, incidence and quality of service encounters can be early indicators of whether an organization's relationship with a customer is flourishing or in jeopardy. Unfortunately, organizations with good prior service levels will suffer more when customers perceive that they have suffered a loss arising from a service encounterdue to the existence of contrast effects. However, experienced customers are less sensitive to such losses because they tend to weigh prior satisfaction levels heavily. By modeling the duration of the provider-customer relationship, it is possible to predict the revenue impact of service improvements in the same manner as other resource allocation decisions. The calculations in this article show that changes in customer satisfaction can have important financial implications for the organization because lifetime revenues from an individual customer depend on the duration of his/her relationship, as well as the dollar amount of his/her purchases across billing cycles. Satisfaction levels explain a substantial portion of explained variance in the durations of service provider-customer relationships across customers, comparable to the effect of price. Consequently, it is a popular misconception that organizations that focus on customer satisfaction are failing to manage customer retention. Rather, this article suggests that service organizations should be proactive and learn from customers they defect by understanding their current satisfaction levels. Managers and researchers may have underestimated the importance of the link between customer satisfaction and retention because the relationship between satisfaction and duration times is very complex and difficult to detect without advanced statistical techniques.", "e:keyword": ["Customer Satisfaction", "Durations", "Retention", "Defensive Strategy", "Proportional Hazards Model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.1.66", "e:abstract": "In recent years, the supermarket industry has become increasingly competitive. One outcome has been the proliferation of a variety of pricing formats, and considerable debate among academics and practitioners about how these formats affect consumers' store choice behavior. This paper advances the idea that consumer shopping behavior (as defined by average size of the shopping basket and the frequency of store visits) is an important determinant of the store choice decision when stores offer different price formats. A recent article that summarized the result of Bruno's management switching the chain from EDLP to HILO illustrates the importance of this issue: The company's price-conscious customers, used to shopping for a fixed basket of goods, stayed away in droves. Thus, the audience for this paper includes practitioners and academics who wish to understand store choices or predict how a change in price format might affect store profitability and the mix of clientele that shop there. This paper attempts to understand the relationship between grocery shopping behavior, retail price format, and store choice by posing and answering the following questions. First, after controlling for other factors (e.g., distance to the store, prior experience in the store, advertised specials), do consumer expectations about prices for a basket of grocery products (expected basket attractiveness) influence the store choice decision? This is a fairly straightforward test of the effect of price expectations on store choice. Second, are different pricing formats (EDLP or HILO) more or less attractive to different types of shoppers? To adequately answer the second question, we must link consumers' category purchase decisions, which collectively define the market basket, and the store choice decision. We study the research questions using two complementary approaches. First, we develop a stylized theory of consumer shopping behavior under price uncertainty. The principal features and results from the stylized model can be summarized as follows. Shoppers are defined (in a relative sense) as either large or small basket shoppers. Thus, we abstract from the vicissitudes of individual shopping trips and focus on meaningful differences across shoppers in terms of the expected basket size per trip. The shoppers make category purchase incidence decisions and can choose to shop in either an EDLP or a HILO store. Large basket shoppers are shoppers who have a relatively high probability of purchase for any given category, and as such they are more captive to prices across many different categories. The first two propositions summarize the price responsiveness of shoppers. In particular, the large basket shoppers are responsive to price in their individual category purchase incidence decisions; this makes them responsive to the expected basket price in their store choice decisions. This key structural implication of the model highlights an asymmetry between response at the category level and response at the store level. The result is quite intuitive; a (large basket) shopper with less ability to respond to prices in individual product categories will be more sensitive to the expected cost of the overall portfolio (the market basket) when choosing a store. The final proposition derives the price at which a given shopper will be indifferent between an EDLP and a HILO store. The key insight is that as a shopper increases his or her tendency to become a large basket shopper, the EDLP store can increase its (constant) price closer and closer to the price in the HILO store. Conversely, as the shopper becomes more of small basket shopper, the EDLP store must lower its price closer to the deal price in the HILO store. Thus, we have the interesting result that small basket shoppers prefer HILO stores, . The empirical testing mirrors the development of the consumer theory. We test the implications of the propositions using a market basket scanner panel database. The database includes two years of shopping data for 1,042 households in two separate market areas. We first use household-level grocery expenditures to model the probability that a household is a large or small basket shopper. Subsequently, we estimate purchase incidence and store choice models. We find that after controlling for important factors such as household distance to the store, previous experience in the store, and advertised specials, price expectations for the basket influence store choice. Furthermore, EDLP stores get a greater than expected share of business from large basket shoppers; HILO stores get a greater than expected share from small basket shoppers. Consistent with the implications of the propositions, large basket shoppers are relatively price inelastic in their category purchase incidence decisions and price elastic in their store choice decisions.", "e:keyword": ["Shopping Behavior", "Choice Models", "Pricing", "Market Basket"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.2.91", "e:abstract": "Differences between consumers in sensitivity to marketing mix variables have been extensively documented in the scanner panel data. All studies of consumer heterogeneity focus on a specific category of products and ignore the fact that the purchase behavior of panel households is often observed simultaneously in multiple categories. If sensitivity to marketing mix variables is a common consumer trait, then one should expect to see similarities in sensitivity across multiple categories. The goal in this paper is to measure the covariance of both observed (linked to measured characteristics of households) and unobserved heterogeneity in marketing mix sensitivity across multiple categories. Measurement of correlation in sensitivities across categories will serve to guide the interpretation of the literature on household heterogeneity. If there is a large correlation, one can be more confident that sensitivity to marketing variables is a fundamental household property and not simply a category-specific anomaly. Detection of correlation in sensitivities across categories requires an appropriate methodology that can handle the high dimensional covariance structures and properly account for uncertainty in estimation. For example, a simple approach might be to fit a brand choice model to each of the available categories in turn, ignoring the data in the other categories. For each category, household parameter estimates could be obtained for the parameters corresponding to price, display, and feature sensitivity. These parameter estimates could be viewed as data and the correlations across categories could be computed. Such a procedure could induce a downward bias in the estimation of correlation due to the independent sampling errors, which are present in each parameter estimate. We develop a hierarchical model structure that introduces an explicit correlation structure across categories and utilizes the data in multiple categories at the same time. To reduce the size of the covariance matrix, we use a variance components approach. We introduce household-specific demographic variables to decompose the correlation across categories into that which can be ascribed to observable and unobservable sources. Shopping behavior variables such as shopping frequency and market basket size as well as intensity of shopping in a category are also included in the model. Using data on five categories, we find substantial and statistically important correlations ranging from .32 for price sensitivities to .58 for feature sensitivity. These correlations are much larger than the correlations obtained with the state-of-the-art techniques available prior to our work. We attribute our ability to detect substantial correlations to our method, which involves the joint use of multiple category data in a parsimonious and efficient manner. Unlike previous studies with panel data, household demographic variables are found to be strongly related to price sensitivity. Higher income households are less price sensitive and large families are more price sensitive. Shopping behavior variables are also important in explaining price sensitivity. Households that visit the store often are more price sensitive. Households with larger market baskets are less price sensitive, confirming the view of Bell and Lattin (1998). Heavy user households tend to be both less price sensitive and less display sensitive. The evidence presented here of substantial correlations validates, in part, the notion that sensitivity to marketing mix variables is a consumer trait and is not unique to specific product categories. It also opens the possibility of using information across categories in making inferences about consumer brand preference and marketing mix sensitivity, providing a richer source of information for target marketing.", "e:keyword": ["Multiple categories", "Choice models", "Parameter correlations", "Bayesian methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.2.107", "e:abstract": "Our primary objective in this paper is to analyze a framework that simultaneously examines the role of monitoring and incentives in the design of sales force control systems. Previous research has focused exclusively on the role of incentives in directing salesforce effort. We build on the structure provided by the past work and analyze an agency-thoeretic model in which a salesperson generates wealth for the firm by expending effort across two dimensions, namely, internal and external. We assume that effort in the internal dimension can be monitored relatively cheaply whereas effort in the external dimension can be monitored only at infinite cost. We then analyze the following two scenarios: (i) a pure incentives world wherein both effort dimensions are governed through the use of incentive pay, and (ii) a monitoring and incentives world wherein the internal dimension is monitored and the external dimension is governed through the use of incentive pay. In addition to modeling the notion of partial monitoring in this manner, we also explicitly allow the firm to choose the level of risk aversion desired in its salesperson. Of course, salespeople who are relatively risk-tolerant command higher reservation wages; consequently, such salespeople are likely to be valuable only to those firms that emphasize incentive pay in their control systems. Our analysis across the two scenarios helps us to demonstrate the implications and value of introducing monitoring into the control structure. Specifically, we find that monitoring allows the firm to decrease the weight placed on incentives and hire a relatively risk-averse salesperson from the salesforce labor market. These actions, in turn, permit the firm to reduce the risk premium and the reservation wage offered to the salesperson. In direct contrast to these monetary savings, however, we find that an adverse side effect of monitoring is that it induces salespeople to overemphasize the effort devoted to the monitored dimension while underemphasizing the effort devoted to the nonmonitored dimension. This adverse effect of monitoring notwithstanding, we find that the overall benefit of increased monitoring is that it allows the firm to the amount of total compensation paid to the salesperson. These analytical findings are consistent with the prescriptions found in the popular business press where it is often stated that compensation plans that emphasize incentive pay are characterized by independence in managing activities (lack of monitoring) as well as high income potential. These findings are also consistent with the popular wisdom that incentive-laden compensation plans are generally more appropriate for individuals who are risk takers and entrepreneurial in nature. We also delineate the conditions where monitoring can improve on the profits obtained in a pure incentives world. Specifically, we find that monitoring can prove to be most valuable when the importance of internal activities is high and the level of incentives is low. Finally, we conclude by conducting a sensitivity analysis to examine the robustness of our results to the specifications we utilize in our modeling efforts. Overall, we view the main contribution of our research efforts as one of explicitly delineating the tradeoffs associated with the use of monitoring and incentives in the design of salesforce control systems. As such, our paper should be of interest to academics and practitioners interested in the design of salesforce control systems.", "e:keyword": ["Monitoring", "Salesforce compensation", "Salesforce control", "Agency-theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.2.124", "e:abstract": "We present a conceptual framework to describe the commercial zapping phenomenon and use it to identify factors that influence channel switching during commercials. Drawing on previous research, published reports of practitioner gut feel, interventions used by advertisers to reduce channel switching, and proprietary studies reported in the published literature, we describe how these variables might potentially affect the decision to zap a commercial. We use a latent class approach to model the impact of the identified factors on two aspects of the switching decisionwhether or not a commercial is zapped (modeled with a binary logit model) and, conditional on a zap having taken place, the number of seconds that the commercial was watched before being zapped (modeled within the proportional hazards framework). The model is estimated on telemeter data on commercial viewing in two categories (spaghetti sauce and window cleaners) obtained from a single-source, household scanner panel. The results from the empirical analysis show that households can be grouped into two segments. The first, which consists of about 35% of households in the sample, is more zap-prone than the second. For this zapping segment, the probability of zapping a commercial is lower for households who make more purchases in the product category. Also, zapping shows a J-shaped response to previous exposures to the commercial, with the associated zapping elasticity reaching its minimum value at around 14 exposures and increasing rapidly thereafter. This finding suggests that advertisers should be cautious not to use media schedules that have excessive media weight or that emphasize frequency over reach. We found that zapping probabilities for ads aired around the hour and half-hour marks to be significantly higher than for other pod locations. Based on these results, we argue that prices for advertising pods located around the hour/half-hour marks should be between 5% to 33% lower than those in the remaining portion of the program. We explore the impact of advertising content on zapping and find that the presence of a brand differentiating message in a commercial causes a statistically significant decrease in zapping probabilities. While the magnitude of this effect is small, the finding suggests that it may be helpful to include qualitative variables in future models of advertising response. We propose the expected proportion of time that an ad is watched as a benchmark to compare 15-second and 30-second ad formats from a zapping standpoint. We found no significant differences between the two formats on this dimension. Our analysis also shows that, due to the impact of previous exposures on zapping, the use of 15-second ads runs a greater risk of reaching the threshold exposure level beyond which zapping probabilities start to increase. This implies that while managers must be cognizant of the risks of overexposure for any ad, it is especially important in the case of the shorter, 15-second ad format.", "e:keyword": ["Advertising and Media", "Econometric Modeling", "Zapping", "Single-source data analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.2.139", "e:abstract": "Sale signs increase demand. The apparent effectiveness of this simple strategy is surprising; sale signs are inexpensive to produce and stores generally make no commitment when using them. As a result, they can be placed on any products, and as many products, as stores prefer. If stores can place sale signs on any or all of their products, why are they effective? We offer an explanation for the effectiveness of sale signs by arguing that they inform customers about which products have relatively low prices, thus helping customers decide whether to purchase now, visit another store, or perhaps return to the same store in the future. This explanation raises two additional issues. First, why do stores prefer to place sale signs on products that are truly low priced (stores could use sale signs to increase demand for any of their products)? Second, how many sale signs should a store use; should they limit sale signs to just their relatively low priced products or should they also place them on some of their higher priced products? The paper addresses each of these questions and in doing so investigates how much information sale signs reveal. Our arguments are illustrated using a formal game-theoretic model in which competing stores sell imperfect substitutes in two-period overlapping seasons. Stores choose price and sale sign strategies and new customers arrive each period and decide whether to purchase immediately or delay and return in the future (to the same store or a different store). Customers who delay purchasing risk that the product will not be available in the following period and incur an additional transportation cost when they return. Two factors balance these costs. First, customers correctly anticipate that the price will be lower if the product is available in the next period. Second, customers who return to a different store may find a product that better suits their needs. In deciding how to respond, customers use price and sale sign cues to update their expectations about which products will be available in the next period. Stores' sale sign and price strategies are entirely endogenous in the model, as is the impact of sale signs on demand. In our discussion we highlight the information revealed by sale signs, including the source of its credibility, its sensitivity to the number of sale signs that are used, and the resulting shift in customer demand. We point to two key results. First, we show that the underlying signal is self-fulfilling: if customers believe that products with sale signs are more likely to be relatively low priced, then firms prefer to place sale signs on lower priced products. Second, we demonstrate that sale signs are self-regulating. Stores may introduce noise by placing sale signs on some more expensive products. However, if customers' price expectations are sensitive to the number of products that have sale signs, this strategy is not without cost. Using additional sale signs may reduce demand for other products that already have sale signs. Our model is unique in several respects. First, we describe how stores use multiple signals to communicate with customers and recognize that customers vary in how much they learn from each signal. Price alone resolves uncertainty for some customers, but other customers use both prices and sale signs to resolve their uncertainty. Second, although previous signaling models have recognized that signals may be noisy (not always accurate), noise in these signals is typically exogenous, resulting from uncontrolled environmental distortions. In our model, stores endogenously choose to introduce noise so that sale signs only partially reveal which products are discounted. Our explanations are supported by several examples. Although we focus on fashion products, our findings have application to any market in which customers are uncertain about relative price levels.", "e:keyword": ["Sale Signs", "Retailing", "Pricing", "Promotion Signals", "Signal Jamming"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.2.156", "e:abstract": "When designing a product line, a manufacturer is often aware that it does not control the ultimate targeting of the products in the line to the different consumer segments. While the manufacturers can attempt to influence the target customers through communications in appropriate media, product design, and the choice of channels of distribution, the ultimate targeting is made by a retailer, which might only care about its own interests, and is fully in control of interactions with customers, including how the product is sold and displayed. This occurrence is widespread in numerous markets, for example, frequently purchased consumer products, home appliances, personal computers, automobiles, etc. The audience for this paper includes practitioners and academics who want to better understand how a manufacturer selling through an intermediary can better induce this intermediary to have a targeting strategy consistent with the manufacturer's intentions and be willing to carry the full product line. The paper attempts to find what are the main issues a manufacturer selling through a distribution channel has to worry about when designing the product line. The problem of the product line design for a distribution channel is modeled with the manufacturer, the retailer or several competing retailers, and the consumers. In this way all the three levels of the distribution system are included. The model can be summarized as follows. The manufacturer decides how many products to have in the line and the physical characteristics of each product, quality. Each product may or may not be targeted at a different market segment. The manufacturer decides as well how many market segments to try to target and the prices to charge the retailer for each type of product. Given the product line being offered by the manufacturer, the retailer (or competing retailers) decides which products to carry, the market segments that are going to be targeted, which product to target to each segment, and the prices being charged the consumers for each product. The consumer market is composed of different market segments that value quality differently: Some market segments are willing to pay more for quality than other market segments. The paper presents the results for two market segments, but a greater number of market segments can also be accounted for. We characterize the equilibrium targeting strategies of the manufacturer and retailer (or competing retailers) in terms of number of products in the line, the physical characteristics of each product, the prices charged by the manufacturer for each product, the consumer prices charged by the retailer for each product, and the product bought by each market segment. We compare the results with the coordinated channel outcome, where the manufacturer and the retailer work together to maximize the overall channel profits. The results are related to the other coordination problems previously studied in the literature (for example, the standard double marginalization effect of higher prices reducing demand) in the sense that the retailer makes decisions caring only about its own profits and not the overall channel profits. The paper shows that, if possible, the best strategy for the manufacturer is to increase the differences in the products being supplied (in comparison to the direct selling/coordinated channel case). If the manufacturer is not able to increase these differences, it then elects to price the product line such that some of the consumer segments end up not being served. The intuition for this result is that the manufacturer, by increasing the differences among the different products, is still making major profits on the high end segments, while getting some positive profits from the low end segments and guaranteeing that the retailer actually targets the different products to the consumer segments intended by the manufacturer. Were the manufacturer not to increase the differences among the different products being offered, the retailer would only target the higher end consumer segments, because also targeting the lower end segments would involve losing too many rents on the higher end segments. Another way of seeing the problem is that the channel pricing distortions increase the cannibalization forces across the product line. The manufacturer tries to compensate for this by increasing the product differentiation across the line. If increasing the differences among the different products being offered is not possible, the manufacturer then drops the low end consumer segments and concentrates on the high end of the market (which is more profitable). The unit margins of both the retailer and manufacturer are also shown to be increasing with the quality level of the product.", "e:keyword": ["Product Line", "Distribution Channels", "Product Policy", "Pricing", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.2.170", "e:abstract": "Batsell and Polking (1985) developed one of the important choice models that address the problem of independence from irrelevant alternatives. In this note, we propose an estimation method that directly estimates Batsell and Polking's model. Compared to the indirect estimation method suggested by Batsell and Polking, the direct method is simpler, making the BP model more accessible to potential users.", "e:keyword": ["Estimation", "Choice model", "Competitive Structure"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.181", "e:abstract": "Consumers now purchase several offerings from direct sellers, including catalog and Internet marketers. These direct channels exist in parallel with the conventional retail stores. The availability of multiple channels has significant implications for the performance of consumer markets. The literature in marketing and economics has, however, been dominated by a focus on the conventional retail sector. This paper is an effort toward modeling competition in the multiple-channel environment from a strategic viewpoint. At the outset, a parsimonious model that accommodates the following consumer and market characteristics is introduced. First, the relative attractiveness of retail shopping varies across consumers. Second, the fit with the direct channel varies across product categories. Third, the strength of existing retail presence in local markets moderates competition. Fourth, in contrast with the fixed location of the retail store that anchors its localized market power, the location of the direct marketer is irrelevant to the competitive outcome. The model is first applied in a setting where consumers have complete knowledge of product availability and prices in all channels. In the resulting equilibrium, the direct marketer acts as a competitive wedge between retail stores. The direct presence is so strong that each retailer competes against the remotely located direct marketer, rather than against neighboring retailers. This outcome has implications for the marketing mix of retailers, which has traditionally been tuned to attract consumers choosing between retail stores. In the context of market entry, conditions under which a direct channel can access a local market in retail entry equilibrium are derived. Our analysis suggests that the traditional focus on retail entry equilibria may not yield informative or relevant findings when direct channels are a strong presence. Next, the role of information in multiple-channel markets is modeled. This issue is particularly relevant in the context of direct marketing where the seller can typically control the level of information in the marketplace, sometimes on a customer-by-customer basis (e.g., by deciding on the mailing list for a catalog campaign). When a certain fraction of consumers does not receive information from the direct marketer, the retailers compete with each other for that fraction of the market. The retailer's marketing mix has to be tuned, in this case, to jointly address direct and neighboring retail competition. The level of information disseminated by the direct marketer is shown to have strategic implications, and the use of market coverage as a lever to control competition is described. Even with zero information costs, providing information to all consumers may not be optimal under some circumstances. In particular, when the product is not well adapted to the direct channel, the level of market information about the direct option should ideally be lowered. The only way to compete with retailers on a larger scale with a poorly adapted product is by lowering direct prices, which lowers profits. Lowering market information levels and allowing retailers to compete more with each other facilitates a higher equilibrium retail price. In turn, this allows a higher direct price to be charged and improves overall direct profit. On the other hand, when the product is well adapted, increasing direct market presence and engaging in greater competition with the retail sector yields higher returns. The finding that high market coverage may depress profits raises some issues for further exploration. First, implementing the optimal coverage is straightforward when the seller controls the information mechanism, as in the case of catalog marketing. The Internet, in contrast, is an efficient mechanism to transmit information, but does not provide the sellers with such control over the level of market information. A key reason is that the initiative to gather information on the Internet lies largely with consumers. The design and implementation of mechanisms to control aggregate information levels in electronic markets can, therefore, be an important theme for research and managerial interest. Second, direct marketers have traditionally relied on the statistical analysis of customer records to decide on contact policies. The analysis in this paper reveals that these policies can have significant strategic implications as well. Research that integrates the statistical and strategic aspects could make a valuable contribution. The paper concludes with a discussion of issues for future research in multiple-channel markets, including avenues to model competition in settings with multiple direct marketers.", "e:keyword": ["Channels of distribution", "Catalog marketing", "Direct marketing", "Electronic marketing", "Internet marketing", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.196", "e:abstract": "The alignment of sales territories has a considerable impact on profit and represents a major problem in salesforce management. Practitioners usually apply the balancing approach. This approach balances territories as well as possible with respect to one or more attributes such as potential or workload. Unfortunately, this approach does not necessarily guarantee maximizing profit contribution. Thus, it does not provide an evaluation of the profit implications of an alignment proposal in comparison with the existing one. In consequence, several authors proposed nonlinear integer optimization models in the 1970s. These models attempted to maximize profit directly by considering the problems of allocating selling time (calling plus travel time) across accounts as well as of assigning accounts to territories simultaneously. However, these models turned out to be too complex to be solvable. Therefore, the authors have either approximated the problem or proposed the application of heuristic solution procedures on the basis of the suboptimal principle of equating marginal profit of selling time across territories. To overcome these limitations, we propose a new approach, COSTA, an acronym for contribution optimizing sales territory alignment. In contrast to previously suggested profit maximizing approaches, COSTA operates with sales response functions of any given concave form at the level of sales coverage units (SCUs) that cover a group of geographically demarcated individual accounts. Thus, COSTA works with sales response functions at a more aggregated level that requires less data than other profit maximization approaches. COSTA models sales as a function of selling time, which includes calling time as well as travel time, assuming a constant ratio of travel to calling time. In addition, the formulation of the model shows that an optimal solution requires only equal marginal profits of selling time across sales coverage units per territory, but not across SCUs of different territories. Basically, COSTA consists of an allocation model and an assignment model, both of which are considered simultaneously. The allocation model optimally allocates the available selling time of a salesperson across the SCUs of his or her territory, whereas the assignment model assigns the SCUs to territories. Thus, COSTA predicts the corresponding profit contribution of every possible alignment solution, which enables one to perform what-if-analyses. The applicability of the model is supported by the development of a powerful heuristic solution procedure. A simulation study showed that COSTA provided solutions that are on average as close as 0.195% to an upper bound on the optimal solution. The proposed heuristic solution procedure enables one to solve large territory alignment problems because the computing time increases only quadratically with the number of SCUs and proportionally to the square root of the number of salespersons. In principle, we also show how COSTA might be expanded to solve the salesforce sizing as well as the salespersons' location problem. The usefulness of COSTA is illustrated by an application. The results of this application indicated substantial profit improvements and also outlined the weaknesses of almost balanced territories. It is quite apparent that balancing is only possible at the expense of profit improvements and also does not lead to equal income opportunities for the salespersons. This aspect should be dealt with separately from territory considerations by using territory-specific quotas and linking variable payment to the achievement of these quotas. Furthermore, the superiority of COSTA turned out to be stable in a simulation study on the effect of misspecified sales response functions. COSTA is of interest to researchers as well as practitioners in the salesforce area. It aims to revive the stream of research in the 1970s that already proposed sales territory alignment models aimed at maximizing profit. Such profit maximizing models are theoretically more appealing than approaches that strive to balance one or several attributes, such as potential or workload. COSTA's main advantage over previous profit maximizing approaches is that it is less complex. Consequently, COSTA demands less data so that even large problems can be solved close to optimality within reasonable computing times.", "e:keyword": ["Salesforce research", "Industrial marketing", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.214", "e:abstract": "A key task of advertising media planners is to determine the best media schedule of advertising exposures for a certain budget. Conceptually, the planner could choose to do continuous advertising (i.e., schedule ad exposures evenly over all weeks) or follow a strategy of pulsing (i.e., advertise in some weeks of the year and not at other times). Previous theoretical analyses have shown that continuous advertising is optimal for nearly all situations. However, pulsing schedules are very common in practice. Either the practice of pulsing is inappropriate or extant models have not adequately conceptualized the effects of advertising spending over time. This paper offers a model that shows pulsing strategies can generate greater total awareness than the continuous advertising when the effectiveness of advertisement (i.e., ad quality) varies over time. Specifically, ad quality declines because of advertising wearout during periods of continuous advertising and it restores, due to forgetting effects, during periods of no advertising. Such dynamics make it worth-while for advertisers to stop advertising when ad quality becomes very low and wait for ad quality to restore before starting the next burst again, as is common in practice. Based on the extensive behavioral research on advertising repetition and advertising wearout, we extend the classical Nerlove and Arrow (1962) model by incorporating the notions of repetition wearout, copy wearout, and ad quality restoration. is a result of excessive frequency because ad viewers perceive that there is nothing new to be gained from processing the ad, they withdraw their attention, or they become unmotivated to react to advertising information. refers to the decline in ad quality due to passage of time independent of the level of frequency. is the enhancement of ad quality during media hiatus as a consequence of viewers forgetting the details of the advertised messages, thus making ads appear like new when reintroduced later. The proposed model has the property that, when wearout effects are present, a strategy of pulsing is superior to continuous advertising even when the advertising response function is concave. This is illustrated by a numerical example that compares the total awareness generated by a single concentrated pulse of varying duration (blitz schedules) and continuous advertising (the even schedule). This property can be explained by the tension between the pressure to spend the fixed media budget quickly to avoid copy wearout and the opposing pressure to spread out the media spending over time to mitigate repetition wearout. The proposed model is empirically tested by using brand-level data from two advertising awareness tracking studies that also include the actual spending schedules. The first data set is for a major cereal brand, while the other is for a brand of milk chocolate. Such advertising tracking studies are now a common and popular means for evaluating advertising effectiveness in many markets (e.g., Millward Brown, MarketMind). In the empirical tests, the model parameters are estimated by using the Kalman filter procedure, which is eminently suited for dynamic models because it attends to the intertemporal dependencies in awareness build-up and decay via the use of conditional densities. The estimated parameters are statistically significant, have the expected signs, and are meaningful from both theoretical and managerial viewpoints. The proposed model fits both the data sets rather well and better than several well-known advertising models, namely, the Vidale-Wolfe, Brandaid, Litmus, and Tracker models, but not decisively better than the Nerlove-Arrow model. However, unlike the Nerlove-Arrow model, the proposed model yields different total awareness for different strategies of spending the same fixed budget, thus allowing media planners to discriminate among several media schedules. Given the empirical support for the model, the paper presents an implementable approach for utilizing it to evaluate large numbers of alternative media schedules and determine the best set of media schedules for consideration in media planning. This approach is based on an algorithm that combines a genetic algorithm with the Kalman filter procedure. The paper presents the results of applying this approach in the case studies of the cereal and milk chocolate brands. The form of the best advertising spending strategies in each case was a pulsing strategy, and there were many schedules that were an improvement over the media schedule actually used in each campaign.", "e:keyword": ["Advertising strategy", "Advertising wearout", "Aggregate response models", "Pulsing schedules", "Kalman filter estimation", "Genetic algorithm"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.236", "e:abstract": "Experimental conjoint choice analysis is among the most frequently used methods for measuring and analyzing consumer preferences. The data from such experiments have been typically analyzed with the Multinomial Logit (MNL) model. However, there are several problems associated with the standard MNL model because it is based on the assumption that the error terms of the underlying random utilities are independent across alternatives, choice sets, and subjects. The Multinomial Probit model (MNP) is well known to alleviate this assumption of independence of the error terms. Accounting for covariances in utilities in modeling choice experiments with the MNP is important because variation of the coefficients in the choice model may occur due to context effects. Previous research has shown that subjects' utilities for alternatives depend on the choice context, that is, the particular set of alternatives evaluated. Simonson and Tversky's tradeoff contrast principle describes the effect of the choice context on attribute importance and patterns of choice. They distinguish , which are caused by the alternatives in the offered set only, and , which are due to the influence of alternatives previously considered in choice experiments. These effects are hypothesized to cause correlations in the utilities of alternatives within and across choice sets, respectively. The purpose of this study is to develop an MNP model for conjoint choice experiments. This model is important for a more detailed study of choice patterns in those experiments. In developing the MNP model for conjoint choice experiments, several hurdles need to be taken related to the identification of the model and to the prediction of holdout profiles. To overcome those problems, we propose a random coefficients (RC) model that assumes a multivariate normal distribution of the regression coefficients with a rank one factor structure on the covariance matrix of these regression coefficients. The parameters in this covariance matrix can be used to identify which attributes and levels of attributes are potentional sources of dependencies between the alternatives and choice sets in a conjoint choice experiment. We present several versions of this model. Moreover, for each of these models we allow utilities to be either correlated or independent across choice sets. The Independent Probit (IP) model is used as a benchmark. Given the dimensionality of the integrations involved in computing the choice probabilities, the models are estimated with simulated likelihood, where simulations are used to approximate the integrals involved in the choice probabilities. We apply and compare the models in two conjoint choice experiments. In both applications, the random coefficients MNP model that allows choices in different choice sets to be correlated (RC) displays superior fit and predictive validity compared with all other models. We hypothesize that the difference in fit occurs because the RC model accommodates correlations among choice sets that are caused by background contrast effects, whereas the model that treats choice sets as independent (iRC) accounts for local contrast effects only. The iRC model shows superior model fit compared with the IP model, but its predictions are worse than those of the IP model. We find differences in the importance of local and background contrast effects for choice sets containing different numbers of alternatives: The background contrast effect may be stronger for smaller choice sets, whereas the local contrast effect may be stronger for bigger choice sets. We illustrate the differences in simulated market shares that are obtained from the RC, iRC, and IP models in three hypothetical situations: product modification, product line extension, and the introduction of a me-too brand. In all of those situations, substantially different market shares are predicted by the three models, which illustrates the extent to which erroneous predictions may be obtained from the misspecified iRC and IP models.", "e:keyword": ["Conjoint choice experiments", "Multinomial probit", "Random taste variation", "Random utility"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.253", "e:abstract": "Previous research on state dependence indicates that a brand's purchase probabilities vary over time and depend on the levels of inertia and variety seeking and on the identity of the previously purchased brand. Brand-choice probabilities obtained from models such as the logit and the probit are, however, fixed over time, conditional on the previous brand purchased and on the levels of marketing variables. Consequently, state dependence has largely been studied as a time-invariant phenomenon in brand-choice models, with the levels of inertia and variety seeking assumed to be constant over time. To account for the time-varying nature of state dependence would require a model in which brand-switching probabilities depend upon interpurchase times. One modeling framework that can account for this dependence is based on the hazard function approach. The proposed approach works as follows. All other factors being equal, an inertial household purchasing a brand on a particular occasion is most likely to repurchase that brand on the next occasion. If the household switches, it will be to a brand located perceptually close, in attribute space, to the previously purchased brand. In other words, an inertial household has the highest switching hazard for the same origin and destination brands, with a progressively lower hazard rate for brands perceptually located farther and farther away from the origin brand. The amount by which the hazard is lowered depends upon the perceptual distance and the inertia level of the household. On the other hand, if the household is variety seeking, the most likely brand purchased would be a brand located farthest away from the previously purchased brand in attribute space. In other words, the hazard rate of repurchase is the lowest, with the rate increasing with the distance of the destination brand from the origin brand and the level of that household's variety-seeking tendency. The effects of inertia and variety seeking are, therefore, incorporated at the attribute level into a brand-purchase timing model. In doing so, we attempt to provide greater insight into the nature of state dependence in models of purchase timing. Our model and estimation procedure will enable us to distinguish between households that are inertial and those that are variety prone. In addition to accounting for state dependence, the model also accounts for the effects of unobserved heterogeneity among households in their brand preferences and in their sensitivities to marketing activities. A majority of studies in marketing using the hazard function approach to investigate purchase timing have not accounted for heterogeneity in marketing-mix effects. The study integrates recent methods that incorporate the effects of inertia and variety seeking in brand-choice models with a semi-Markov model of purchase timing and brand switching. The proposed model enables us to (1) infer market structure via a perceptual map for the sample households, and (2) investigate implications for the introduction of a line extension. We provide empirical applications of the proposed method using three different household-level scanner panel data sets. We find that differing levels of inertia and variety seeking characterize the three data sets. The findings are consistent with prior beliefs regarding these categories. In addition, our results indicate that the nature of interbrand purchase timing behavior depends upon the extent of inertia or variety seeking in the data. We are also able to characterize the structure of the three product markets studied. This provides implications for interbrand rivalry in the market. Further, we demonstrate how the model and results can be used to predict the location of a line extension in the perceptual space of households. Finally, we obtain implications for the timing of brand promotions.", "e:keyword": ["Hazard models", "Purchase timing", "Brand choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.271", "e:abstract": "One of the major responsibilities of the Editor and the Area Editor is to decide on the appropriateness of a submitted paper. As stated in prior editorials (Staelin 1996, 1997), four major criteria are used in making this determination: (1) Will the paper be of interest to our readers? (2) Is it readable? (3) Is it not wrong? (4) Does it make a contribution to the field? Assuming the determination is positive, it is the job of the Editor, with the help of the review team, to specify a course of action that he believes will lead to a publishable paper. What then leads the review team to encourage the publishing of three papers that directly comment on each other's work and, in the process, alter the existing practice of this journal?", "e:keyword": ["Modeling", "Editorial Guidelines"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.273", "e:abstract": "While the field of marketing science has long been interested in the effects of promotional efforts, public policy issues involving illegal marketer fraud and deception have generally not been addressed in this body of work. One key exception to this generalization is a article suggesting that the practice of bait and switch may be beneficial to consumers and, furthermore, that the Federal Trade Commission should investigate revising its standards to legitimize this practice (Gerstner and Hess 1990). This finding and recommendation seemed so significant that it is surprising that the recommendation has not, to date, been explored in greater detail. In this paper we further explore the impact of the two components of bait and switch: out of stock and upselling. We do this by using Moorthy's (1993) theoretical modeling framework to systematically extend and assess the Gerstner and Hess model. We find that the previously reported increase in consumer welfare that arises from allowing out-of-stock conditions at retailers is actually due to the utility created by salespersons' explaining product features and benefits, not by the out of stock. Thus, the ramifications of both our legal and modeling analyses are that deceptive bait-and-switch practices result in harm to consumers and be legalized. Our paper concludes by proposing worthwhile modeling issues for further exploration. In addition, we suggest that our procedure for analyzing public policy issues (by exploring the confluence of law, consumer behavior, and marketing models) can serve as a useful exemplar for further contributions to public policy by marketing scientists.", "e:keyword": ["Pricing", "Promotion", "Public Policy", "Bait and Switch"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.283", "e:abstract": "In our 1990 paper we demonstrated that a law prohibiting bait and switch may have the surprising consequence of hurting the consumers it was designed to protect. Wilkie, Mela, and Gundlach (1998) postulate that this may be false if upselling is equally effective when the bait brand is available and when it is out of stock. We show here that our earlier conclusion is correct in a more general setting: A law prohibiting bait and switch in a competitive market can reduce consumer well-being but never improve it. When bait and switch occurs, it creates welfare gains, and when it would create welfare losses, it does not occur, regardless of a law prohibiting the practice.", "e:keyword": ["Pricing", "Promotion", "Public Policy", "Bait and Switch"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.3.290", "e:abstract": "We applaud the advances in this colloquy and the areas of convergence that are emerging. However, this reply points out that the purported benefits of bait and switch found in Hess and Gerstner (1998) are predicated upon (i) only a single component (availability) within the broader domain of bait and switch; (ii) the assumption that one of the parameters in the consumer utility function differs with the availability of advertised brands; and (iii) a further assumption that no other parameters in the model will change when the availability condition changes. After assessing these developments, we conclude that (i) the legal status of bait-and-switch schemes is fine as it stands; (ii) when understood in their true complexity, parameters in the consumer utility functions likely will not differ with regard to availability, thus obviating the finding of increased consumer welfare; and (iii) even if it is believed that utility functions would differ, effects on other model parameters clearly suggest that consumers will be worse off with bait and switch. Despite these differences, however, we are pleased with the developments the dialogue has produced.", "e:keyword": ["Pricing", "Promotion", "Public Policy", "Bait and Switch"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.4.297", "e:abstract": "Reflection of the editor-in-chief of about the complexity of the editorial and reviewing process.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.17.4.301", "e:abstract": "In this paper we analyze the joint implications of two effects: (a) inserting independent profit-maximizing retailers into the channel system provides buffering to the manufacturers from price competition when their products are highly substitutable and intrachannel contracts are observable (as shown by McGuire and Staelin 1983 under the assumption of constant marginal production costs), and, (b) lack of channel coordination results in a reduction in manufacturer's incentives to invest in efforts to reduce production costs (as shown by Jeuland and Shugan 1983 for the case of bilateral monopoly). We show that both these results are robust in the sense that the first holds even in the presence of the vertical externality of manufacturer's effort reduction in a noncoordinated channel, and the second holds regardless of the degree of substitutability between the competing channel's products. Specifically, we analyze a four-stage game with two manufacturers and two retailers, where the intrachannel contracts are linear and observable and manufacturers make investments in process improvements to reduce their production costs. We find that the optimal channel structure decision depends on interactions between two parameters: the degree of substitutability between products and the level of investments required to achieve production cost reduction. These parameters represent what have been widely interpreted in the management literature as the two primary generic strategies that most organizations follow in order to gain competitive advantage: cost leadership and product differentiation (Porter 1980). Thus, our analysis brings out the strategic and interdisciplinary nature of the channel structure decision that can significantly affect firm profitability. Our main results are as follows. First, we find that decentralized, noncoordinated channels appear as more profitable equilibrium than integration (or perfectly coordinated channels) at high product substitutability even when process innovation dimension is accounted for, in agreement with the literature. However, the range of substitutability over which decentralization is an equilibrium strategy is smaller the easier it is to reduce production costs. Intuitively, the easier the cost reduction, the larger the cost penalty that the channel incurs as a result of not coordinating investment and pricing decisions between channel members, and thus smaller the range over which decentralization is an equilibrium. This implies that there is an explicit tradeoff between efficiency and strategic incentives in distribution channel design. Second, we show that decentralized manufacturers invest less in process innovation than integrated manufacturers do, regardless of the structure of the competing channel and the degree of substitutability between products. Consequently, a decentralized channel has higher costs, charges higher prices, and produces lower quantities than an integrated channel does. Moreover, these differences get larger the easier the cost reduction. The effect on manufacturer profits, however, is not that clear. Manufacturers make higher profits by decentralizing if products are highly substitutable, in agreement with McGuire and Staelin (1983) and Coughlan and Wernerfelt (1989). However, we also find that the relative profitability of decentralization at high substitutability (and of integration at low substitutability) increases the easier the cost reduction. Moreover, the range of substitutability over which decentralization is more profitable than integration is itself larger the easier the cost reduction (though decentralization is an equilibrium strategy over a smaller range). Thus, process innovation accentuates the profit difference between integrated and decentralized channels and makes the Prisoner's Dilemma situation worse in the choice of distribution channel structure. Finally, we analyze two examples of coordinated decision making in a channel: a divisional integrated system and franchising. In the first case, we find that decentralization can emerge as a unique (and more profitable) equilibrium at high product substitutability, in contrast to McGuire and Staelin (1983). In the second case, we find that decentralization is not always a unique equilibrium and it is not always more profitable than integration, in sharp contrast to the results by Coughlan and Wernerfelt (1989). Thus, franchising does not provide a sure way of achieving channel coordination when marginal production costs are not constant. In sum, this paper highlights the importance of simultaneously considering both the horizontal and the vertical dimensions of interorganizational relations on one hand and, on the other, paying attention to cross-functional interactions across marketing and operational decisions to better understand the underlying incentives that shape firm and market structures; conventional focus of marketing on demand side effects and of operations on cost side effects can lead to suboptimal decisions.", "e:keyword": ["Channels of distribution", "Game theory", "Cost reduction", "Pricing research", "Marketing/manufacturing interface"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.4.317", "e:abstract": "Consumers' attempts to control their unwanted consumption impulses influence many everyday purchases with broad implications for marketers' pricing policies. Addressing theoreticians and practitioners alike, this paper uses multiple empirical methods to show that consumers voluntarily and strategically ration their purchase quantities of goods that are likely to be consumed on impulse and that therefore may pose self-control problems. For example, many regular smokers buy their cigarettes by the pack, although they could easily afford to buy 10-pack cartons. These smokers knowingly forgo sizable per-unit savings from quantity discounts, which they could realize if they bought cartons; by rationing their purchase quantities, they also self-impose additional transactions costs on marginal consumption, which makes excessive smoking overly difficult and costly. Such strategic self-imposition of constraints is intuitively appealing yet theoretically problematic. The marketing literature lacks operationalizations and empirical tests of such consumption self-control strategies and of their managerial implications. This paper provides experimental evidence of the operation of consumer self-control and empirically illustrates its direct implications for the pricing of consumer goods. Moreover, the paper develops a conceptual framework for the design of empirical tests of such self-imposed constraints on consumption in consumer goods markets. Within matched pairs of products, we distinguish relative virtue and vice goods whose preference ordering changes with whether consumers evaluate immediate or delayed consumption consequences. For example, ignoring long-term health effects, many smokers prefer regular (relative vice) to light (relative virtue) cigarettes, because they prefer the taste of the former. However, ignoring these short-term taste differences, the same smokers prefer light to regular cigarettes when they consider the long-term health effects of smoking. These preference orders can lead to dynamically inconsistent consumption choices by consumers whose tradeoffs between the immediate and delayed consequences of consumption depend on the time lag between purchase and consumption. This creates a potential self-control problem, because these consumers will be tempted to overconsume the vices they have in stock at home. Purchase quantity rationing helps them solve the self-control problem by limiting their stock and hence their consumption opportunities. Such rationing implies that, per purchase occasion, vice consumers will be less likely than virtue consumers to buy larger quantities in response to unit price reductions such as quantity discounts. We first test this prediction in two laboratory experiments. We then examine the external validity of the results at the retail level with a field survey of quantity discounts and with a scanner data analysis of chain-wide store-level demand across a variety of different pairs of matched vice (regular) and virtue (reduced fat, calorie, or caffeine, etc.) product categories. The analyses of these experimental, field, and scanner data provide strong convergent evidence of a characteristic crossover in demand schedules for relative vices and virtues for categories as diverse as, among others, potato chips, chocolate chip cookies, cream cheese, beer, soft drinks, ice cream and frozen yogurt, chewing gum, coffee, and beef and turkey bologna. Vice consumers' demand increases less in response to price reductions than virtue consumers' demand, although their preferences are not generally weaker for vices than for virtues. Constraints on vice purchases are self-imposed and strategic rather than driven by simple preferences. We suggest that rationing their vice inventories at the point of purchase allows consumers to limit subsequent consumption. As a result of purchase quantity rationing, however, vice buyers forgo savings from price reductions through quantity discounts, effectively paying price premiums for the opportunity to engage in self-control. Thus, purchase quantity rationing vice consumers are relatively price insensitive. From a managerial and public policy perspective, our findings should offer marketing practitioners in many consumer goods industries new opportunities to increase profits through segmentation and price discrimination based on consumer self-control. They can charge premium prices for small sizes of vices, relative to the corresponding quantity discounts for virtues. Virtue consumers, on the other hand, will buy larger amounts even when quantity discounts are relatively shallow. A key conceptual contribution of this paper lies in showing how marketing researchers can investigate a whole class of strategic self-constraining consumer behaviors empirically. Moreover, this research is the first to extend previous, theoretical work on impulse control by empirically demonstrating its broader implications for marketing decision making.", "e:keyword": ["Intertemporal choice", "Pricing policy", "Product policy", "Segmentation", "Self-control"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.4.338", "e:abstract": "This paper analyzes how manufacturers should coordinate distribution channels when retailers compete in price as well as important nonprice factors such as the provision of product information, free repair, faster check-out, or after-sales service. Differentiation among retailers in price and nonprice service factors is a central feature of markets ranging from automobiles and appliances to gasoline and is especially observed in the coexistence of high-service retailers and lower price discount retailers. Therefore, how a manufacturer should manage retail differentiation is an important channel management question. Yet, the approach in the existing literature has been to examine channel coordination under the standard symmetric contracting assumption that offering a uniform contract to all the retailers in a market will be sufficient for coordination. I bring this assumption into question and ask when is it optimal for the manufacturer to use the channel contract to deliberately induce retail differentiation (even if the retailers were ex-ante identical in their cost and other characteristics). The paper identifies the type of channel contracts that can endogenously induce symmetry as opposed to differentiation among retailers. Next, the paper highlights a type of channel conflict that arises from the very nature of retail price-service competition. A manufacturer might find the retailers to be excessively biased towards price competition at the cost of service provision or vice-versa. The paper establishes when a manufacturer is likely to stimulate greater price as opposed to greater service competition among the retailers. The framework that I develop to address these issues highlights the role of two basic types of consumer heterogeneity. Consumers are heterogeneous in their locations (as in the spatial models of horizontal differentiation) and in their willingness to pay for retail services (as in the models of vertical differentiation). The model also uses a natural relationship in retail markets between the travel/time cost incurred by a consumer and her willingness to pay: The more affluent consumers who have a higher willingness to pay for retail services also have a higher cost for their personal time. Given these market features, the paper analyzes the problem faced by a manufacturer who sells to competing retailers. The paper shows that the standard notion in the literature of offering similar contracts to all the retailers is sufficient only in markets with substantial locational differentiation relative to the differences in the willingness to pay. Effective channel management in these markets simply requires mechanisms that ensure that retailer interests are aligned so that they compete by offering a mix of price and service that is desirable from the manufacturer's point of view. However, in markets with small locational differentiation and substantial diversity in consumer willingness to pay, the manufacturer's problem is not just to align retailer interests, but to also use the channel contract to induce the correct level of retail differentiation. This helps the manufacturer to better cater to the diversity in consumer willingness to pay and to prevent the cut-throat competition that the retailers would otherwise have indulged in. The manufacturer can achieve this through the use of menu-based contracts. Menu-based contracts induce differentiated retailer behavior despite the fact that the retailers are not forced into accepting different terms of trade. This aspect can be useful in shielding manufacturers from litigation under the Robinson-Patman act. The paper also shows that for relatively high-ticket items retailers tend to be excessively biased towards competing in the provision of retail services. The correlation between consumer willingness to pay for service and travel costs implies that for high-ticket products, the competing retailers will focus on the more service-sensitive customers at the cost of ignoring the price-sensitive consumers in the market. The manufacturer is therefore likely to encourage greater price competition among the retailers. In contrast, for low-ticket items the manufacturer prefers to reduce price competition and encourage greater provision of services. This provides an endogenous rationale for the use of price ceilings versus floors. The basic model is also extended to consider the effect of upstream competition between manufacturers. Under upstream competition, coordinating retail price and service decisions is not always optimal for an individual manufacturer. This extension to manufacturer competition provides a basis for understanding the role of retail price-service differentiation in the context of a channel duopoly. It also shows that a mixed distribution channel (a channel in which one manufacturer chooses to be coordinated while the other chooses to be noncoordinated) can be an equilibrium in markets with weak brand loyalty.", "e:keyword": ["Channel coordination", "Retail differentiation", "Retail services", "Menu-based contracts", "Consumer heterogeneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.4.356", "e:abstract": "This paper addresses the question of how the vertical structure of a product line relates to brand equity. Does the presence of premium or high-quality products in a product line enhance brand equity? Conversely, does the presence of economy or low-quality products in a product line diminish brand equity? Economists and marketing researchers refer to variation in quality levels of products within a category as vertical differentiation, whereas variation in the function or category of the products is referred to as horizontal differentiation. Much of the existing research on the relationship between product line structure and brand equity has focused on the horizontal structure of the product line and has been primarily concerned with what happens when the product line of a brand is extended horizontally into new categories? Researchers have been concerned primarily with how the extension fares, but the effect of the extension on the products is also important. There is an analogous question of what happens when the product line of a brand is extended vertically, either up market or down market. This question of vertical extensions is part of the more general issue of how the vertical structure of a product line relates to brand equity. The specific research questions addressed in this paper are: (1) do premium or high-quality products enhance the brand equity associated with the other products in the line? (2) Conversely, do economy or low-quality products diminish the brand equity associated with the other products in the line? These research questions are relevant to three managerial issues in product-line strategy. First, what are the costs and benefits of including down market products within a brand? Second, what are the implications of including high-end models within a brand? Third, when should high-end and low-end products be offered under an existing brand umbrella and when should these products be offered under separate brands? We address these research questions empirically through an analysis of the models and brands within the U.S. mountain bicycle industry. We use price premium above that which can be explained by the physical characteristics of the bicycle as a metric for brand equity. We then test several hypotheses related to the relationship between extension of the product line upward and downward and the price premium commanded by the brand. We further support this analysis with a simple laboratory experiment. The analysis reveals that price premium, in the lower quality segments of the market, is significantly positively correlated with the quality of the lowest-quality model in the brand's product line; and, that for the upper quality segments of the market, price premium is also significantly positively correlated with the quality of the highest-quality model in the brand's product line. The results of the analysis are supported by the outcome of an experiment in which 63 percent of the subjects preferred a product offered by a high-end brand to the equivalent product offered by a low-end competitor. These results imply that managers wishing only to maximize the equity of their brands would offer only high-quality products and avoid offering low-quality products. However, this result must be moderated by the overall objective of maximizing profits. Maximizing profits is likely to involve a tradeoff between preserving high brand equity (and therefore high margins) and pursuing the volume typically located in the lower end of the market. One of the most significant implications of this research is that product line managers need to be mindful not just of the incremental cannibalization or stimulation of sales of products that are immediate neighbors of an extension to the product line, but also the effect of such an extension on the brand equity in other, possibly quite different, parts of the product line.", "e:keyword": ["Brand equity", "Price premium", "Product line extent", "Product line breadth", "Product variety", "Brand strategy", "Bicycle industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.4.380", "e:abstract": "Empirical studies examining responses to new product entries come to the puzzling conclusion that, in general, an incumbent reacts to a new entrant after a significant delay. Even easy-to-implement price cuts are observed after significant lag following entry. These findings seem to contradict the existing literature that either implicitly assumes or strongly advocates immediate defensive responses to limit competitive encroachment. When a competing firm enters the market, consumers may be uncertain about the entering firm's product quality. The incumbent firm (through rigorous tests) may fully know the entrant's quality. Suppose the incumbent aggressively lowers price. This may cause the consumers to wonder if indeed the entrant's quality is high. In other words, an incumbent's reaction may cause the consumers to make inferences about the entrant's quality. Such strategic implications of the incumbent's reactions have to be carefully analyzed before determining the optimal response by the incumbent. In this paper, we propose a conceptual framework for understanding differences in the magnitude and timing of incumbents' responses to competitive entries. We consider a model in which a monopolist incumbent firm faces competitive entry. The incumbent firm knows the true quality of the entrant with certainty. Although consumers are aware of the incumbent's product quality through their prior experience, they are initially uncertain of the entrant's product quality. In such a situation, a high-quality entrant has the incentive to signal her true quality through her strategic price choice. However, the uncertainty about the entrant's quality is favorable to the incumbent in the sense that consumers believe with a high probability that the entrant's quality is low. As a result, the strategic incentives facing the incumbent and the entrant oppose each other. While the entrant wants to signal her high quality, the incumbent wants to prevent her from doing so. We demonstrate that one way the incumbent can prevent the quality signaling is to select a higher than his optimal competitive (duopoly) price. In other words, the incumbent can prevent or jam the entrant's quality signaling by choosing a price higher than his optimal competitive price when consumers are fully informed about the entrant's true quality. Though the signal-jamming price is lower than the monopoly price, the price is substantially higher than the competitive price. This marginal reduction in the incumbent's price from the pre-entry monopolistic price represents a muted or lack of response by the incumbent to the competitive entry. However, once the entrant's quality gets revealed in subsequent periods through consumer usage and word of mouth, the entrant has no incentive to engage in quality signaling and the incumbent has no incentive to jam it. Therefore, the market reverts to the complete-information competitive prices, and the incumbent lowers his price considerably. This temporal pattern of muted price reduction in the first period followed by a sharp price reduction in the second period corresponds to a delayed defensive reaction in our model. Although the empirical studies suggest that the delayed reaction may arise due to factors such as managerial inertia or indecision, we demonstrate that such a behavior is indeed an optimal strategy for a profit-maximizing firm. Thus, our model reconciles empirical results with the equilibrium outcome of a strategic analytical framework. Furthermore, in an experimental setting, we test the predictive power of our framework and establish that consumers indeed form conjectures about the entrant's quality based on the incumbent's reactions. In the first experimental study, we find strong support for the notion that the incumbent's price reaction may indicate entrant's quality. In a follow-up study, we observe that whenever the incumbent lowers prices, respondents judge the quality of the entrant to be higher as compared to the case when prices are the same or increased. The managerial implication of this paper is that well-established incumbent firms should be cautious in the implementation of their defensive responses to product introductions of uncertain quality by competitors. Of particular concern are situations where the reactions are easily observable by consumers. A strong reaction may suggest that the incumbent takes the competitive threat seriously, leading consumers to believe in the quality of the competitor's product.", "e:keyword": ["New Product Entry", "Defensive Reaction", "Quality Signaling", "Price-quality Relationship", "Signal Jamming", "Asymmetric Information"]}, {"@id": "http://dx.doi.org/10.1287/mksc.17.4.406", "e:abstract": "The occurrence of temporary stock-outs at retail is common in frequently purchased product categories. Available empirical evidence suggests that when faced with stock-outs, consumers are often willing to buy substitute items. An important implication of this consumer behavior is that observed sales of an item no longer provide a good measure of its core demand rate. Sales of items that stock-out are right-censored, while sales of other items are inflated because of substitutions. Knowledge of the true demand rates and substitution rates is important for the retailer for a variety of category management decisions such as the ideal assortment to carry, how much to stock of each item, and how often to replenish the stock. The estimated substitution rates can also be used to infer patterns of competition between items in the category. In this paper we propose methods to estimate demand rates and substitution rates in such contexts. We develop a model of customer arrivals and choice between goods that explicitly allows for possible product substitution and lost sales when a customer faces a stock-out. The model is developed in the context of retail vending, an industry that accounts for a sizable part of the retail sales of many consumer products. We consider the information set available from two kinds of inventory tracking systems. In the best case scenario of a perpetual inventory system in which times of stock-out occurrence and cumulative sales of all goods up to these times are observed, we derive Maximum Likelihood Estimates (MLEs) of the demand parameters and show that they are especially simple and intuitive. However, state-of-the-art inventory systems in retail vending provide only periodic data, i.e., data in which times of stock-out occurrence are unobserved or missing. For these data we show how the Expectation-Maximization (EM) algorithm can be employed to obtain the MLEs of the demand parameters by treating the stock-out times as missing data. We show an application of the model to daily sales and stocking data pooled across multiple beverage vending machines in a midwestern U.S. city. The vending machines in the application carry identical assortments of six brands. Since the number of parameters to be estimated is too large given the available data, we discuss possible restrictions of the consumer choice model to accomplish the estimation. Our results indicate that demand rates estimated naively by using observed sales rates are biased, even for items that have very few occurrences of stock-outs. We also find significant differences among the substitution rates of the six brands. The methods proposed in our paper can be modified to apply to many nonvending retail settings in which consumer choices are observed, not their preferences, and choices are constrained because of unavailability of items in the choice set. One such context is in-store grocery retailing, where similar issues of information availability arise. In this context an important issue that would need to be dealt with is changes in the retail environment caused by retail promotions.", "e:keyword": ["Demand Estimation", "Maximum Likelihood", "Vending", "Poisson", "Stock-outs", "Substitution", "EM Algorithm", "Inventory Management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.1.1", "e:abstract": "In recent years, manufacturers have become increasingly disposed toward the use of sales promotions, often at the cost of advertising. Yet the long-term implications of these changes for brand profitability remain unclear. In this paper, we seek to offer insights into this important issue. We consider the questions of i) whether it is more desirable to advertise or promote, ii) whether it is better to use frequent, shallow promotions or infrequent, deep promotions, and iii) how changes in regular prices affect sales relative to increases in price promotions. Additional insights regarding brand equity, the relative magnitude of short- and long-term effects, and the decomposition of advertising and promotion elasticities across choice and quantity decisions are obtained. To address these points, we develop a heteroscedastic, varying-parameter joint probit choice and regression quantity model. Our approach allows consumers' responses to short-term marketing activities to change in response to changes in marketing actions over the long term. We also accommodate the possibility of competitive reactions to policy changes of a brand. The model is estimated for a consumer packaged good category by using over eight years of panel data. The resulting parameters enable us to assess the effects of changes in advertising and promotion policies on sales and profits. Our results show that, in the long term, advertising has a positive effect on brand equity while promotions have a negative effect. Furthermore, we find price promotion elasticities to be larger than regular price elasticities in the short term, but smaller than regular price elasticities when long-term effects are considered. Consistent with previous research, we also find that most of the effect of a price cut is manifested in consumers' brand choice decisions in the short term, but when long-term effects are again considered, this result no longer holds. Last, we estimate that the long-term effects of promotions on sales are negative overall, and about two-fifths the magnitude of the positive short-term effects. Finally, making reasonable cost and margin assumptions, we conduct simulations to assess the relative profit impact of long-term changes in pricing, advertising, or promotion policies. Our results show regular price decreases to have a generally negative effect on the long-term profits of brands, advertising to be profitable for two of the brands, and increases in price promotions to be uniformly unprofitable.", "e:keyword": ["Long-Term Effects", "Promotions", "Advertising", "Price Sensitivity", "Scanner Data", "Choice", "Purchase Quantity", "Switching Regression"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.1.23", "e:abstract": "This paper provides some empirical generalizations regarding how the relative prices of competing brands affect the cross-price effects among them. Particular focus is on the asymmetric price effect and the neighborhood price effect. The asymmetric price effect states that a price promotion by a higher-priced brand affects the market share of a lower-priced brand more so than the reverse. The neighborhood price effect states that brands that are closer to each other in price have larger cross-price effects than brands that are priced farther apart. The main objective of this paper is to test if these two effects are generalizable across product categories, and to assess which of these two effects is stronger. While the neighborhood price effect has not been rigorously tested in past research, the asymmetric price effect has been validated by several researchers. However, these tests of asymmetric price effect have predominantly used elasticity as the measure of cross-price effect. The cross-price elasticity measures the percentage change in market share (or sales) of a brand for 1% change in price of a competing brand. We show that asymmetries in cross-price elasticities tend to favor the higher-priced brand simply because of scaling effects due to considering percentage changes. Furthermore, several researchers have used logit models to infer asymmetric patterns. We also show that inferring asymmetries from conventional logit models is incorrect. To account for potential scaling effects, we consider the absolute cross-price effect defined as the change in market share (percentage) points of a target brand when a competing brand's price changes by one percent of the product category price. The advantage of this measure is that it is dimensionless (hence comparable across categories) and it avoids scaling effects. We show that in the logit model with arbitrary heterogeneity in brand preferences and price sensitivities, the absolute cross-price effect is symmetric. We develop an econometric model for simultaneously estimating the asymmetric and neighborhood price effects and assess their relative strengths. We also estimate two alternate models that address the following questions: (i) If I were managing the th highest priced brand, which brand do I impact the most by discounting and which brand hurts me the most through price discounts? (ii) Who hurts whom in National Brand vs. Store Brand competition? Based on a meta-analysis of 1,060 cross-price effects on 280 brands from 19 different grocery product categories, we provide the following empirical generalizations: 1. The asymmetric price effect holds with cross-price elasticities, but tends to disappear with absolute cross-price effects. 2. The neighborhood price effect holds with both cross-price elasticities and absolute cross-price effects, and is significantly stronger than the asymmetric price effect on both measures of cross-price effects. 3. A brand is affected the most by discounts of its immediately higher-priced brand, followed closely by discounts of its immediately lower-priced brand. 4. National brands impact store brands more so than the reverse when the cross-effect is measured in elasticities, but the asymmetric effect does not hold with absolute effects. Store brands hurt and are, in turn, hurt the most by the lower-priced national brands that are adjacent in price to the store brands. 5. Cross-price effects are greater when there are fewer competing brands in the product category, and among brands in nonfood household products than among brands in food products. The implications of these findings are discussed.", "e:keyword": ["Cross-Price Elasticities", "Packaged Goods", "Price Competition", "Promotions", "Private Labels"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.1.42", "e:abstract": "In marketing durable goods, manufacturers use varying degrees of leasing and selling to consumers, e.g., cars, photo-copiers, personal computers, airplanes, etc. The question that this raises is whether the distinction between leases and sales is simply one of price, or whether the proportion of leases and sales effects a firm's ability to compete in the market. In this paper we use two approaches to argue that leasing and selling create strategic consequences that extend beyond prices. First, we develop a stylized theoretical model that shows that the optimal proportion of leases and sales depends on the competitiveness of the market and on the inherent reliability of the firm's product. And second, we find support for the implications of our theoretical model with data from the automobile industry. The U.S. automobile industry has seen a large increase in leasing over the last five years. However, the extent to which leasing has been embraced varies widely across manufacturers. For example, in 1993 the sport utility segment had the following lease percentages: Ford Explorer, 29%; Jeep Grand Cherokee, 24%; Toyota 4-Runner, 11%; and Chevrolet Blazer, 9%. In addition, manufacturers often vary lease percentages across models. For example, in 1993 Ford leased 22% of its Crown Victoria model, 35% of its Taurus model, and 42% of its Probe model. A popular argument for why we see these differences is that higher priced cars are leased more often because leasing makes them more affordable. However, this rationale is not compelling in the face of our data. For example, the Ford Probe was priced significantly lower than the Crown Victoria and yet it was leased almost twice as often. To develop a better understanding of why we observe differences in the proportion of leasing, we develop a two-period model of a duopoly in which each manufacturer chooses its optimal quantity and the fraction of units it wants to lease. We find that in equilibrium neither firm leases all its unitseither they use a mix of leasing and selling or they use only selling. Our analysis suggests that the fraction of leased cars decreases as the manufacturers' products become more similar and the competition between them increases. The intuition for this result is that a higher fraction of leases puts the firm at a competitive disadvantage in the future. This occurs because, unlike firms that sell their product, firms that lease are at a price disadvantage. Another important finding in this paper is that the extent of leasing chosen by a manufacturer depends on the reliability of its product. In particular, all else being equal, the lower a product's reliability, the lower its proportion of leases. Within the context of the automobile industry, this suggests that more expensive cars may be leased more often because they are of higher quality and not necessarily because they are more expensive. Finally, we test the implications of our theoretical model with data from the U.S. automobile market. In particular, for 1993 model year cars, we develop a measure of reliability using data from . In addition, we develop a measure of the extent of competition in each segment of the automobile market. We support our hypotheses by finding that the extent to which a car model is leased depends strongly on its predicted reliability and on the competitive intensity within the segment.", "e:keyword": ["Competition", "Automobile", "Lease", "Durable Goods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.1.59", "e:abstract": "A commonly held belief has grocery and mass merchandise retailers gaining power relative to the upstream consumer package goods manufacturers. One of the major justifications for this belief is that manufacturers are now giving retailers more side payments such as trade allowances, slotting allowances, etc. However, a number of researchers have shown that these concessions have not translated into increased profit for the retailer relative to the manufacturer. This paper explores, via an analytic model, why one might see retailers getting concessions from the manufacturer without being able to translate them into high profits. We do this by representing the interaction between the channel members as a one-period profit maximizing game where manufacturers decide on how large a side payment (i.e., a concession) to give to each retailer and retailers decide on how much of this side payment to use to promote the manufacturer's product. At the heart of our model is a demand function for each product offering (i.e., a specific brand sold by a specific retailer). This demand function is linear in own and other's prices, but based on empirical evidence is assumed to be nonlinear in the effects of merchandising activities (e.g., short term price discounts, better shelf space, advertising, etc.). Specifically, merchandising effects are modeled with a square root function that also acknowledges most short-term promotional effects result in brand or store switching versus increase category volume. The model is composed of six parameters. These are the inherent popularity of the brand at a particular outlet, own price sensitivity, cross (but within the store) price sensitivity, customer's sensitivity to within store promotion activities, customer's sensitivity to between store promotional activities, and the degree to which promotional activities yield incremental product category sales. We use our underlying demand formulation to find the equilibrium solution to the full information, Stackelberg leader game for different store and brand loyalty environments as captured by our six parameters. These parameters are chosen based on empirical evidence to span possible market conditions that each channel member might have. Our findings and modeling efforts should be of interest to analytic channel modelers and scholars interested in the mass merchandising and grocery store industries. For a theoretical point of view we build upon the Case 4 model of Lee and Staelin (1997) to allow for promotional activities. From a substantive point of view we show that manufacturers will freely give retailers side payments even though they know these retailers will pocket a substantial portion of this concession. Moreover, we identify conditions within our model that lead to larger allowances, lower pass through rates, and lower retail profits; outcomes that are compatible with recent industry trends. In addition, we highlight the difference in effect on profits, prices, etc. of changes in consumers sensitivity to inter-store differences in storewide merchandising activities. One of the more counter-intuitive results of these analyses is that it is in the manufacturer's best interest to help retailers increase their spatial monopoly by decreasing consumers' tendency to cross-store shop because of merchandising activities.", "e:keyword": ["Channel Management", "Trade Promotions", "Nash Equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.1.77", "e:abstract": "We show that some of the most common beliefs about customer-perceived quality are wrong. For example, 1) it is not necessary to exceed customer expectations to increase preference, 2) receiving an expected level of bad service does not reduce preference, 3) rational customers may rationally choose an option with lower expected quality, even if all non-quality attributes are equal, and 4) paying more attention to loyal, experienced customers can sometimes be counter-productive. These surprising findings make sense in retrospect, once customer expectations are viewed as distributions, rather than simple point expectations. That is, each customer has a probability density function that describes the relative likelihood that a particular quality outcome will be experienced. Customers form these expectation distributions based on their cumulative experience with the good or service. A customer's cumulative expectation distribution may be conceptualized as being a predictive density for the next transaction. When combined with a diminishing returns (i.e., concave) utility function, this Bayesian theoretical framework results in predictions of: (a) how consumers will behave over time, and (b) how their perceptions and evaluations will change. In managerial terms, we conclude that customers consider not only expected quality, but also risk. This may help explain why current measures of customer satisfaction (which is highly related to expected quality) only partially predict future behavior. We find that most of the predictions of our theoretical model are borne out by empirical evidence from two experiments. Thus, we conclude that our approach provides a useful simplification of reality that successfully predicts many aspects of the dynamics of consumer response to quality. These findings are relevant to both academics and managers. Academics in the area of customer satisfaction and service quality need to be aware that it may be insufficient to measure only the point expectation, as has always been the standard practice. Instead it may be necessary to measure the uncertainty that the customer has with respect to the level of service that will be received. Due to questionnaire length constraints, it may not be practical for managers to include uncertainty questions on customer satisfaction surveys. Nevertheless it is possible to build a proxy for uncertainty by measuring the extent of experience with the service/good, and this proxy can be used to partially control for uncertainty effects. The findings of the study were obtained using 1) an analytical model of customer expectation updating, based on a set of assumptions that are well-supported in the academic literature, and 2) two behavioral experiments using human subjects: a cross-sectional experiment, and a longitudinal experiment. Both the analytical model and the behavioral experiments were designed to investigate the effects that of expectations might have, and especially the effects that might deviate from the predictions that would arise from a traditional point expectation model. The behavioral experiments largely confirmed the predictions of the analytical model. As it turned out, the analytical model correctly (in most cases) predicted behavioral effects that contradict some of the best-accepted truisms of customer satisfaction. It is now clear that a more sophisticated view of customer expectations is requiredone that considers not only the point expectation but also the likelihood across the entire distribution of possible outcomes. This distinction is not just academic, because it results in predictable behavior that deviates significantly from that which was traditionally expected based on simpler models.", "e:keyword": ["Quality", "Customer Satisfaction Measurement", "Customer Expectations", "Customer Retention", "Bayesian Updating", "Customer Life-time Value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.2.95", "e:abstract": "Consumers make multicategory decisions in a variety of contexts such as choice of multiple categories during a shopping trip or mail-order purchasing. The choice of one category may affect the selection of another category due to the complementary nature (e.g., cake mix and cake frosting) of the two categories. Alternatively, two categories may co-occur in a shopping basket not because they are complementary but because of similar purchase cycles (e.g., beer and diapers) or because of a host of other unobserved factors. While complementarity gives managers some control over consumers' buying behavior (e.g., a change in the price of cake mix could change the purchase probability of cake frosting), co-occurrence or co-incidence is less controllable. Other factors that may affect multi-category choice may be (unobserved) household preferences or (observed) household demographics. We also argue that not accounting for these three factors simultaneously could lead to erroneous inferences. We then develop a conceptual framework that incorporates complementarity, co-incidence and heterogeneity (both observed and unobserved) as the factors that could lead to multi-category choice. We then translate this framework into a model of multi-category choice. Our model is based on random utility theory and allows for simultaneous, interdependent choice of many items. This model, the multi probit model, is implemented in a Hierarchical Bayes framework. The hierarchy consists of three levels. The first level captures the choice of items for the shopping basket during a shopping trip. The second level captures differences across households and the third level specifies the priors for the unknown parameters. We generalize some recent advances in Markov chain Monte Carlo methods in order to estimate the model. Specifically, we use a substitution sampler which incorporates techniques such as the Metropolis Hit-and-Run algorithm and the Gibbs Sampler. The model is estimated on four categories (cake mix, cake frosting, fabric detergent and fabric softener) using multicategory panel data. The results disentangle the complementarity and co-incidence effects. The complementarity results show that pricing and promotional changes in one category affect purchase incidence in related product categories. In general, the cross-price and cross-promotion effects are smaller than the own-price and own-promotions effects. The cross-effects are also asymmetric across pairs of categories, i.e., related category pairs may be characterized as having a primary and a secondary category. Thus these results provide a more complete description of the effects of promotional changes by examining them both within and across categories. The co-incidence results show the extent of the relationship between categories that arises from uncontrollable and unobserved factors. These results are useful since they provide insights into a general structure of dependence relationships across categories. The heterogeneity results show that observed demographic factors such as family size influence the intrinsic category preference of households. Larger family sizes also tend to make households more price sensitive for both the primary and secondary categories. We find that price sensitivities across categories are not highly correlated at the household level. We also find some evidence that intrinsic preferences for cake mix and cake frosting are more closely related than preferences for fabric detergent and fabric softener. We compare our model with a series of null models using both estimation and holdout samples. We show that both complementarity and co-incidence play a significant role in predicting multicategory choice. We also show how many single-category models used in conjunction may not be good predictors of joint choice. Our results are likely to be of interest to retailers and manufacturers trying to optimize pricing and promotion strategies across many categories as well as in designing micromarketing strategies. We illustrate some of these benefits by carrying out an analysis which shows that the true impact of complementarity and co-incidence on profitability is significant in a retail setting. Our model can also be applied to other domains. The combination of item interdependence and individual household level estimates may be of particular interest to database marketers in building customized cross-selling strategies in the direct mail and financial service industries.", "e:keyword": ["Multicategory Models", "Shopping Baskets", "Retailing", "Micromarketing", "Multivariate Probit Model", "Hierarchical Bayes Models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.2.115", "e:abstract": "This paper attempts to shed light on the following research questions: When a firm introduces a new product (or service) how can it effectively use the different information sources available to generate reliable new product performance forecasts? How can the firm account for varying information availability at different stages of the new product launch and generate forecasts at each stage? We address these questions in the context of the sequential launches of motion pictures in international markets. Players in the motion picture industry require forecasts at different stages of the movie launch process to aid decision-making, and the information sets available to generate such forecasts vary at different stages. Despite the importance of such forecasts, the industry struggles to understand and predict sales of new movies in domestic and overseas markets. We develop a Bayesian modeling framework that predicts first-week viewership for new movies in both domestic and several international markets. We focus on the first week because industry players involved in international markets (studios, distributors, and exhibitors) are most interested in these predictions. We draw on existing literature on forecasting performance of new movies to formulate our model. Specifically, we model the number of viewers of a movie in a given week using a Poisson count data model. The number of screens, distribution strategy, movie attributes such as genre, and presence/absence of stars are among the factors modeled to influence viewership. We employ a hierarchical Bayes formulation of the Poisson model that allows the determinants of viewership to vary across countries. We adopt the Bayesian approach for two reasons: First, it provides a convenient framework to model varying assumptions of information availability; specifically, it allows us to make forecasts by combining different sources of information such as domestic and international market-specific data. Second, this methodology provides us with the entire distribution of the new movie's performance forecast. Such a predictive distribution is more informative than a point estimate and provides a measure of the uncertainty in the forecasts. We propose a Bayesian prediction procedure that provides viewership forecasts at different stages of the new movie release process. The methodology provides forecasts under a number of information availability scenarios. Thus, forecasts can be obtained with just information from a historical database containing data on previous new product launches in several international markets. As more information becomes available, the forecasting methodology allows us to combine historical information with data on the performance of the new product in the domestic market and thereby to make forecasts with less uncertainty and greater accuracy. Our results indicate that for all the countries in the data set the number of screens on which a movie is released is the most important influence on viewership. Furthermore, we find that local distribution improves movie sales internationally in contrast to the domestic market. We also find evidence of similar genre preferences in geographically disparate countries. We find that the proposed model provides accurate forecasts at the movie-country level. Further, the model outperforms all the extant models in the marketing literature that could potentially be used for making these forecasts. A comparison of root mean square and mean absolute errors for movies in a hold out sample shows that the model that combines information available from the different sources generates the lowest errors. A Bayesian predictive model selection criterion corroborates the superior performance of this model. We demonstrate that the Bayesian model can be combined with industry rules of thumb to generate cumulative box office forecasts. In summary, this research demonstrates a Bayesian modeling framework that allows the use of different information sources to make new product forecasts in domestic and international markets. Our results underscore the theme that each movie is unique as is each countryand viewership results from an interaction of the product and the market. Hence, the motion picture industry should use both product-specific and market-specific information to make new movie performance forecasts.", "e:keyword": ["Hierarchical Bayes", "New Products", "Motion Pictures", "International Markets", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.2.137", "e:abstract": "Considering the number of new product introductions and available product varieties today, the practice of product proliferation is visibly evident in many diverse industries. Given its prevalence in practice, understanding the determinants and implications of firm proliferation strategies clearly has important managerial relevance. Previous theoretical research has identified three primary effects of a proliferation strategy: (1) a broad product line can increase the overall demand faced by the firm, (2) a broad product line can affect supply by increasing costs, and (3) broad product lines can have strategic consequences (e.g., long product lines can deter entry, thereby allowing an incumbent firm to raise prices). However, despite the theoretical interest in this common business practice, there has been very little empirical research on this topic. Moreover, no empirical study has simultaneously considered all three of the possible effects associated with a proliferation strategy. Consequently, in this paper we propose a three-equation simultaneous system that captures both the determinants and market outcomes of a firm's product line decisions. In particular, we specify market share, price, and product line length equations, which are estimated by three stage least squares. Using this structure, we empirically study the personal computer industry over the period 19811992. Our empirical results demonstrate that proliferation strategies do not have a uni-dimensional explanation. We find that product proliferation decisions have both demand (market share) and supply (price) implications. Our empirical results also suggest that the firm-level net market share impact of product proliferation in the personal computer industry is negative (i.e., the cost increases associated with a broader product line dominate any potential demand increases). As expected, we find that structural competitive factors play an important role in the determinants and market outcomes of a firm's product line decisions. However, we do not find evidence of firms using proliferation strategies to deter entry in this industry. Finally, we also demonstrate that some of the empirical conclusions from previous research are reversed once product line length is specified as endogenous in the share and price specifications.", "e:keyword": ["Product Proliferation", "Product Line Pricing", "Entry Deterrence", "Personal Computer Industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.2.154", "e:abstract": "Department store chains use advertised price reductions as a major promotional tool to attract consumers to their stores. In advertising discounts, retailers typically use price claims that vary on two key dimensions. First, discounts may be specified either precisely (e.g., 60% off) or with nonspecific () information as in a range of discounts (e.g., 5070% off). Second, discounts may be offered on an entire group (e.g., Sale on All items) or on a subset of an advertised group of items (e.g., Save 5070% on items marked with a yellow dotonly items marked with a yellow dot qualify for the discount). Our objective in this paper is to develop a conceptual framework to understand how consumers respond to tensile versus precise claims on a group of advertised items for different price image stores. Using a series of three experimental studies, we identify key variables that consumers use informing an overall valuation of an advertised sale offer using a tensile versus a precise price claim. The studies also help us to link characteristics of the advertisement and the advertising store to the variables that affect a consumer's valuation of a sale offer using such claims. Consequently, we are not only able to obtain an understanding of a consumer's judgment process, but are also able to provide insights on how to design effective price claims by using variables that are under the retailer's control. We propose that consumers' valuation of an advertised sale offer depends on their subjective assessments about the probability with which they will find a desirable item at a discounted price (called ), the size of that discount (called ), and the probability of liking the sale item. In Experiment 1, we hold the probability of liking the sale item (all sale items are identical) and collect data from consumer responses to price advertisements to determine how consumer assessments of subjective probability and subjective discount depend on the type of price claim (precise versus tensile) used, the advertised level of discount, and the fraction of stock to be on sale. Our results show that when the fraction of stock specified to be on sale is low (high), consumers responding to a tensile claim are optimistic (pessimistic) about the discount they believe they will get, expecting a subjective discount greater (smaller) than the midpoint of the tensile range. Correspondingly, in responding to a precise claim, consumers expect a subjective discount equal to the advertised discount. There is also no difference in the subjective probability assessed for tensile and precise claims. Consequently, when the fraction of stock on sale is low (high) advertised deals with tensile claims are perceived to be more (less) attractive than with precise claims. In Experiment 2 we examine the real-world case of consumer responses to price advertisements from two stores (that differ in price image) in which the fraction of items on sale is not specified but needs to be and advertised sale items are comprised of three brands differing in quality. Our results show that the inferred fraction of stock is positively related to the store price image and negatively related to the advertised discount level. We find that the inferred fraction of stock on sale produces effects similar to the specified fraction of stock on sale. In addition, because we measure the perceived distribution of quality for sale items, we are able to examine its effects on consumers' overall valuation of an advertised sale offer. We predict and find that there is a threshold discount level for each store above which tensile claims are more effective and below which precise claims are more effective. We also find that the threshold discount is greater for a store with a higher price image. Finally, in Experiment 3 we apply our framework to sale offers where the entire advertised stock of items is on sale and show that consumer responses to precise versus tensile claims is a of our general analysis of consumer responses to advertisements in Experiment 2 in which a subset of the stock is on sale. Our work differs from previous research on tensile versus precise claims in both focus and substantive contexts. Our study also contributes to the literature studying the role of ambiguity in behavioral decision theory by examining the role of ambiguity in payoffs instead of probabilities.", "e:keyword": ["Sales Promotions", "Retailing", "Retail Management", "Pricing", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.2.178", "e:abstract": "The notion that individuals have an internal reference price against which they compare observed prices is well supported by several psychological theories. Empirically, several papers in the marketing literature, employing scanner panel data, have modeled the impact of reference prices on brand choice via the sticker shock formulation, in which consumers evaluate choice alternatives using differences between shelf prices and reference prices. Most of the studies reported thus far have not accounted for heterogeneity in price response among consumers and have typically imputed reference prices from the shelf prices of brands that a consumer is supposed to have observed on previous purchases in the category. Since category marketing activity can differentially affect the purchase timing of households, we argue that this measure of reference price may follow certain systematic patterns across consumers and, when combined with unaccounted for price response heterogeneity, may result in a spurious sticker shock effect. Specifically, we show that estimates of sticker shock are biased upward if households that are price-sensitive in the brand choice decision are also more responsive to category promotion activity in their purchase timing decision. We discuss some general conditions under which the bias occurs and conduct a simulation experiment to confirm our specific hypotheses. Our simulation results show that changes in purchase timing are a critical determinant of the bias in the sticker shock effect. We also show that unaccounted for price response heterogeneity can in itself result in a biased sticker shock parameter; however, this requires very large differences in price sensitivities across consumers, far greater than what is normally observed. We develop a hierarchical Bayes version of the nested logit model, which models heterogeneity via individual-level parameters in a continuous random effects framework. We estimate the model on scanner panel data from the yogurt and ketchup categories. We find, in both categories, that the 95% probability interval of the posterior distribution of the mean sticker shock coefficient contains the value zero. Therefore, at least for the data used in this study, there is no evidence for the sticker shock effect at the aggregate level. In contrast, the corresponding coefficient from a standard model (which ignores this heterogeneity) is highly significant and supports the existence of a (possibly spurious) sticker shock effect. Consistent with our explanation of the underlying cause of the bias, households that are more price-sensitive in the choice decision are also found to be more responsive to category promotion activity in their decision to purchase in the category. The results highlight the measurement problems associated with imputing reference prices from past prices. Since the frequency, duration, and price level of a retailer's promotional program depend on its size and prevalence, accurate estimates of the sticker shock effect are essential for formulating optimal promotion strategies. An adequate accounting of consumer heterogeneity is critical to this effort.", "e:keyword": ["Hierarchical Bayes Approach", "Choice Models", "Buyer Behavior", "Reference Price"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.195", "e:abstract": "This issue is all about managerial decision making, i.e., how managers go about making decisions and how these decisions can be improved via good marketing analyses. As such it differs somewhat from the regular issues of in that it contains no analytic models or papers that focus only on methodology. Yet it still is true to the overall mission of the journal: it addresses areas of inquiry relevant to marketing practitioners and academics.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.196", "e:abstract": "This paper provides an introduction to this Special Issue by a) providing a framework for evaluating the potential and actual success of marketing management support systems (MMSS), and b) briefly discussing how each paper in this Special Issue addresses the general topic of managerial decision making. The paper concludes by outlining some key questions that still need to be addressed.", "e:keyword": ["Measures of Success", "Decision Aids", "Managerial Decision Making"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.208", "e:abstract": "Retailers have long recognized that some categories are more important than others in consumers' store choice decisions. The overall profitability of a store requires careful category-level merchandising decisions to draw the most desirable consumers into the store. However, the traditional accounting measure of category profits offers imperfect help making these decisions since it does not take into account the effect of merchandising one category on the profits of other categories in the store. A profit measure which takes into account these important cross-effects is the most relevant performance metric for category management. We call this new construct , as it focuses on consumers and their store choice behavior, and is particularly pertinent to the calculus of marketing decision making. Despite its practical importance, the total impact of merchandising a specific product category on a store's profitability is difficult to measure, and in practice managers can only rely on intuitive calibration of marketing profits in making many retailing decisions. The difficulty arises from the fact that to directly observe the marketing profits of a category, one has to know how consumer store shopping behavior would change and hence what a store's profit would be if the category were to disappear from the consumers' store choice decision. Furthermore, it is difficult to devise a demand structure that is rich enough to capture bundled purchases on the part of consumers in a reasonable manner, but is simple enough to allow estimation on the basis of commonly observed variables. These two technical difficulties explain the conspicuous lack of research that systematically examines how to quantify what we call marketing profits. This paper builds a formal model of marketing profits. We start by formalizing shopper types, and then establish the implied relationship between accounting profits and marketing profits by examining shelf space allocations by a retailer. On the consumer side, we assume that some consumers pay attention to the assortments offered by different retailers when making their store choice decisions. This assumption allows us to establish the demand-side linkage between accounting profits and marketing profits. Consumer store choice decisions put pressure on the retailer to carry wide assortments in categories which are particularly critical to the store choice decisions of the most desirable consumers. Thus, the allocation of shelf space gives rise to the supply-side linkage between accounting profits and marketing profits. By examining the outcome of the supermarket's shelf space decision, we can merge these two linkages and determine the exact relationship between the accounting and marketing profits. Central to our theoretical structure is our assumption on retailers' shelf space allocation decisions. Because of the well-documented pressure that retailers face in making shelf space allocation decisions, we assume that they are acting in a reasonably close-to-optimal fashion by using either an automated planogram or simply by trial-and-error. Optimization requires that returns on shelf space allocated to any category in the store must be identical on the margin and equate to the shadow price of shelf space. It is this outcome of shelf-space allocation that allows us to uncover the implied relationship between accounting and marketing profits. This theoretical structure allows us to construct a measure of marketing profits which can be estimated with data commonly available to retailers. We demonstrate this measurement technique by using publicly available data, provided by Marsh Supermarkets, and show how marketing profits can improve merchandising decisions. In our particular application, we find many categories where the marketing profits of a category are very different from the traditional accounting profits. Further, we find that using this new marketing profits metric to make category-level feature advertising space decisions significantly improves the profitability of the retailer. The paper concludes by discussing how our measure of marketing profits might be improved by additional research, particularly if the researcher has data across many stores.", "e:keyword": ["Category Management", "Retailing", "Management Decision Models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.230", "e:abstract": "The need to justify one's decisions is a signal characteristic of decision making in a managerial environment. Even chief executives must communicate reasons for their actions. Yet, despite a significant amount of laboratory research on the effects of accountability on decision making, few studies have attempted to assess what affects accountability might have outside the lab for actual managers. In this paper, we use as subjects actual members of the professional account, research, and creative staffs of several advertising agencies in an experimental simulation of an advertising copy meeting. We demonstrate that accountability effects in complex, managerial decision contexts diverge considerably from those found in the lab. In particular, we find that accountability does not always lead managers to emphasize the most objectively accurate information, but instead can also encourage emphasis on information which is socially acceptable to the manager's key constituencies (those to whom they feel accountable, see Tetlock 1991). The social constituencies of accountable managers can differ significantly. For example, members of an advertising agency's account service or research departments are likely to favor the quantitative, analytical decision styles of their supervisors, while members of the same agency's creative department are likely to favor empathy and personal judgment. Requiring managers with different constituencies to justify their responses can cause policy bolstering, the tendency to favor a process of decision making particular to one's own constituency. Thus accountability may sometimes make it more difficult for a diverse group to reach a consensus, and may not have the desired positive impact on decision quality. In an experiment simulating an advertising copy meeting, we first solicited managers' private judgments of 12 ads, then presented them with research data and asked each of them to make numerical predictions of how much consumers would like each ad. The experiment manipulated the diagnosticity of the available research and the degree to which managers expected to be held accountable for their predictions. We used hierarchical linear regression to analyze the results. Hierarchical linear regression allows us to estimate simultaneously a within-subjects model capturing individual weighting coefficients for private judgment and research, and a between-subjects model of the effects of department membership and experimental conditions on these judgment-policy weights. This analysis also controls for the varying degrees of precision in the within-subjects estimates, as well as for correlations between the within-subjects weighting coefficients. . 1) Accountability had different impacts on the weighting schemes of members of account services and creative departments. Accountable research/account staff weighed research more in making their predictions than nonaccountable research/account staff. Accountable creatives, on the other hand, actually weighed research less than did nonaccountable creatives. 2) Because accountability caused subjects to attend assiduously to the research, the weight given private judgment reflected whether the data was diagnostic of the problem at hand. Accountable subjects presented with low-diagnosticity research reacted by emphasizing private judgment more heavily. 3) Accountability increased confidence in predictions; this effect was not dependent on the accuracy of subjects' predictions.  . We provide several suggestions and observations regarding current managerial practices, which may improve the impact of individual accountability on decision making.", "e:keyword": ["Accountability", "Advertising", "Decision Making", "Empirical Bayes", "Hierarchical Linear Regression"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.247", "e:abstract": "The authors report the findings from an exploratory investigation of the use of UPC scanner data in the consumer packaged goods industry in the U.S. The study examines the practitioner community's view of the use of scanner data and compares these views with academic research. Forty-one executives from ten data suppliers, packaged goods manufacturers, and consulting firms participated in wide-ranging, in-person, interviews conducted by the authors. The interviews sought to uncover key questions practitioners would like to answer with scanner data, how scanner data is applied to these questions, and the industry's perspective regarding the success that the use of scanner data has had in each area. The authors then compare and contrast practitioners' views regarding the resolution of each issue with academic research. This produces a 2  2 classification of each question as resolved or unresolved from the perspectives of industry and academia. Along the diagonal of the 2  2, issues viewed as unresolved by both groups are important topics for future research. Issues deemed resolved by both groups are, correspondingly, of lower priority. In the off-diagonal cells, industry and academics disagree. These topics should be given priority for discussion, information exchange, and possible further research. Practitioners reported that scanner data analysis has had the most success and been most widely adopted for decision making in consumer promotions (i.e., coupons), trade promotions, and pricing. For example, logit and regression models applied to scanner data have revealed very low average consumer response to coupons which has directly led to reduced couponing activity. Managers also reported high levels of comfort with and impact from analyses of trade promotions and price elasticities. While industry views most of the issues in these areas to be resolved, academic research raises concerns about a number of practices in common commercial use. These include price threshold analysis and trade promotion evaluation using baseline and incremental sales. In product strategy, advertising, and distribution management, practitioners reported that the use of scanner data has had more limited development, success, and impact. In the case of new product decisions, scanner data use has been slow to develop due to the inherent limitations of historical data for these decisions and a heavy reliance on traditional primary research methods. In advertising, scanner data is widely analyzed with models, but confusion among practitioners is very high due to controversies about methods (e.g., what level of data aggregation is best) and conflicting results. In distribution and retail management, scanner data use has tremendous potential but a mixed track record to date. Thus, practitioners view the use of scanner data as unresolved for most issues in product strategy, advertising, and distribution. This view is largely, though not entirely, consistent with academic research, which has only begun to address many of the key questions raised by practitioners. In light of the large number of unresolved issues and mixed record of scanner data use to date, the authors offer a series of specific recommendations for immediate and long-term research priorities that are likely to have the greatest impact on commercial utilization of UPC scanner data. Topics of immediate priority include price thresholds and gaps, baseline and incremental sales, base price elasticity, competitive reactions, measurement of advertising effects, management of brand equity, rationalization of product assortments, and category management. Long-term priorities include a greater emphasis on profitability versus sales or market share, developing prescriptive models versus descriptive models, and the need for industry standards.", "e:keyword": ["Scanner Data", "Marketing Research", "Marketing Models", "Research Priorities"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.274", "e:abstract": "A common event in the consumer packaged goods industry is the negotiation between a manufacturer and a retailer of the sales promotion calendar. Determining the promotion calendar involves a large number of decisions regarding levels of temporary price reductions, feature ads, and in-store displays, each executed at the level of individual retail accounts and brand SKUs over several months or a year. Though manufacturers spend much of their marketing budget on trade promotions, they lack decision support systems to address the complexity and dynamics of promotion planning. Previous research has produced insights into how to evaluate the effectiveness of promotional events, but has not addressed the planning problem in a dynamic environment. This paper develops a disaggregate-level econometric model to capture the dynamics and heterogeneity of consumer response. By modeling the purchase incidence (timing), choice and quantity decisions of consumers we decompose total sales into incremental and nonincremental (baseline plus borrowed). The response model forms the basis of a market simulator that permits us to search for the manufacturer's optimal promotion calendar (subject to a set of constraints, some of them imposed by the retailer) via the simulated annealing algorithm. Calendar profits are the net result of the contribution from incremental sales minus the opportunity cost from giving away discounts to nonincremental sales and the fixed costs associated with implementing promotional events (e.g., retagging, features, displays). Incremental sales result from promotion-induced switching, the acceleration and quantity promotion effects on those switchers, increased consumption and the carryover effect from purchase event feedback. We applied our approach to the promotion-planning problem of a large consumer-packaged goods company in a nonperishable, staple product category suggested by company executives (canned tomato sauce). Subject to a retailer pass-through constant rate of 80%, provided to us by the collaborating firm, the optimal promotion calendar produced by the modeling system followed a pattern of frequent and shallow temporary price reductions with no feature or display activity. We also analyze how that result would change under different retailer pass-through scenarios. Our findings indicated that the manufacturer could substantially improve the profitability of its sales promotion activity and that there would be a concurrent positive effect on retailer profit and volume levels. Management reported to us that the insights from the use of the system were implemented in their promotion-planning process and produced positive results. A validation analysis on follow-up data for one market showed that promotion activity could be significantly reduced, as recommended, with no adverse effect on the brand's market share, as predicted. To generalize the model beyond the specific category where it was implemented, we conducted a sensitivity analysis on the profile of the calendar (i.e., frequency, depth, and duration of deals) with respect to changes in market response, competitive activity, and retailer pass-through. First, we found that the optimal depth, frequency, and timing of discounts is stable for price elasticities ranging from near zero to around four (in absolute magnitude). We also found no systematic impact of competitive promotions on the profile of the optimal calendar. For example, variation in competitive activity did not affect the optimal depth or frequency of discounts. Lastly, we found changes in retailer pass-through to have a significant effect on the optimal depth and number of weeks of trade promotion that a manufacturer should offer. This emphasizes the importance to manufacturers of having accurate estimates of pass-through for purposes of promotion budgeting and planning.", "e:keyword": ["Trade Promotion", "Brand Management", "Decision Support Systems", "Scanner Data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.301", "e:abstract": "This article describes the implementation of a promotion-event forecasting system, PromoCast, and its performance in several pilot applications and validity studies. Pilot studies involved retail grocery chains with 95 to 185 stores per trading area. The goal was to provide short-term, tactical forecasts useful for planning promotions from a retailer's perspective. Thus, the forecast system must be able to handle any of the over 150,000 UPCs in each store's item master file, and must be scalable to produce approximately 800,000,000 forecasts per year across all the retailers served by (). This is a much different task than one that confronts a manufacturer, even one with a broad product line. Manufacturers can benefit from custom modeling in a product line or category. Retailers need a production system that generates forecasts that help promotion planning. Marketing scientists have typically approached promotion analysis from the manufacturer's perspective. One objective of this article is to encourage marketing scientists to rethink promotion analysis from a different perspective. From the retailer's point of view the planning unit is the promotion event. Neither weekly store-tracking data nor shopping-trip data from consumer panels are easily aggregated to reflect total sales during a promotion event. We describe the promotion-event databases and the statistical model developed using these databases. The data are the strategic asset. Our goal is to help retailers use their data to increase the profitability of promotions. We have data on the performance of each UPC in each store under a variety of promotion conditions, on each store's adeptness at executing various styles of promotions, as well as on chain-wide historical performance for each UPC. We use many historical averages from these databases to build a 67-variable, regression-style model. The forecast incorporates a simple bias correction needed when using a log-transformed dependent variable (the natural log of total unit sales). We argue that the historical averages matching the planned ad and display conditions provide a benchmark superior to the widely used base-times-lift method. When aggregated into case units (the natural unit for product ordering), 69% of the forecasts in our first validation study were within  one case compared to 39% within  one case using the appropriate historical averages. We report the results of two over-time validity studies that reflect the value of our model for retailers. The limitations and implications of this planning tool for managerial decision making concerning stocking levels are discussed. Whenever historical data are the strategic asset we face inherent limitations. Our model does not forecast new products. The forecast error increases when an existing product is promoted in a new way. Over 99.5% of the time, we have full data from which to create a forecast. However, with a database for a typical chain market containing over 20 million promotion events in the 30-month time frame we use, 100,000 events have less than ideal data. The breadth of the database (typically 150,000 UPS) makes it impractical to incorporate data on competitive offerings. We find that regression-style modeling is not adept at incorporating information on the 1,200 subcommodities managed in our pilot stores or the 1,000 manufacturers who supply those stores. Despite these limitations we show the value of using promotion-event data, how tactical forecasts based on these data can directly impact the bottom line of grocery retailers, and how store-by-store forecasts can help retailers with problems of running out of stock or overstocking.", "e:keyword": ["Retailing", "Promotion Planning", "Forecasting", "Promotion Event Data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.317", "e:abstract": "Baseline sales measure what retail sales would be in the absence of a promotion (Abraham and Lodish 1993), and models that measure baseline sales are widely used by managers to assess the profitability of promotions (Bucklin and Gupta 1999this issue). Estimates of baseline sales and promotional response are typically independent of past promotional activity, even though there is evidence to suggest that increased discounting reduces off-promotion sales and increases the percentage of purchases made on deal (e.g., Krishna 1994). As a result, models that do not consider dynamic promotional effects can mislead managers to overpromote. Given the widespread use of static models to evaluate the efficacy of promotions, it is particularly desirable to calibrate a dynamic brand sales model and use it to establish an optimal course of action. Accordingly, we develop a descriptive dynamic brand sales model and use it to determine normative price promotion strategies. Our descriptive approach consists of estimating a varying-parameter sales response model. Letting model parameters vary with past discounting activity accommodates the possibility that market response changes with firms' discounting policies. In the normative model, we use the estimates obtained in the descriptive model to determine optimal retailer and manufacturer prices over time. The results of the descriptive model indicate that promotions have positive contemporaneous effects on sales accompanied by negative future effects on baseline sales. The results of the normative model suggest that the higher-share brands in our data tend to overpromote while the lower-share brands do not promote frequently enough. We project that the use of our model could improve manufacturers' profits by as much as 7% to 31%. More generally, the normative results indicate that i) if deals become more effective in the current period, i.e., if consumers are more price sensitive, promotions should be used more frequently; and ii) as the negative dynamic effect of discounts on sales increases, the optimal level of discounting should go down. Without our approach, it would be difficult to make this trade-off exact. Finally, we demonstrate that these dynamic effects provide another perspective to the marketing literature regarding the existence of promotions.", "e:keyword": ["Price Promotions", "Baseline Sales", "Price Sensitivity", "Scanner Data", "Channel Dynamics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.333", "e:abstract": "Creative ideation is a highly complex process, which is difficult to formalize and control. Evidently, even in a complex thinking context certain patterns of creativity may emerge. Relying on such observed patterns may help in organizing the creative process by promoting routes that have been proven to lead to productive ideas and avoiding those that do not. The present research suggests that successful advertisements share and are characterized by such abstract patterns termed . The theoretical rationale for the emergence of such templates and the empirical studies that detect the key creativity templates underlying quality ads indicate that the templates are identifiable, objectively verifiable, and generalizable across multiple categories. Studies 1 and 2 were designed to identify and describe the templates. Six major creativity templates were derived by inference from a sample of 200 highly evaluated print ads drawn from award-winning ad contests such as The One Show (Study 1). Judges found that 89% of the ads could be explained by the six creativity templates. Following a formal description of the templates and their versions, a study comparing 200 award-winning and 200 nonwinning ads (Study 2) is reported. It was found that the two groups differed systematically in the number and distribution of creativity templates: 50% of the award-winning ads as opposed to only 2.5% of the nonwinning ads could be explained by the templates. Further validation of the template approach was obtained by manipulating presence or absence of templates in an experimental setting. In Study 3 groups of individuals were trained in template-based idea generation, an association technique, or not trained at all, prior to an ad-ideation task. Another group subsequently rated the ideas. Findings indicate that a priori knowledge of the templates was associated with the generation of higher quality ads in terms of creativity, brand attitude judgments, and recall (Study 4), with some variation in terms of feeling responses which included humor, emotion, and annoyance. The findings of the reported studies and several real-life applications conducted in leading ad agencies, indicate that the template taxonomy is a trainable, resource-saving, and effective tool. It simplifies and improves the decision-making process involved in designing advertising strategies. It can be applied either by hiring trained personnel employed by consulting firms, or by training the agency's own personnel to routinely evaluate past and current ads, and engage in creative activity. The template approach represents a step forward in defining a comprehensive model of the antecedents of outcome reactions to advertising stimuli. Improved understanding of the wide spectrum of reactions connecting the basic templates with end-user reactions is likely to be beneficial both for academicians and for practitioners. Such a framework can serve as a basis for a synthesis between the activity of creative professionals whose focal interest is the generation of ads, managers, whose main responsibility is strategy formulation, and academic activity, which focuses mainly on the consumer reaction-end of the advertising process. Hence, in addition to academicians, the relevant target audience is likely to include a wide array of communication-related personnel such as creative professionals and planners in advertising agencies, consultants, and brand managers. In addition, it is postulated that the template taxonomy provides the means to achieve creativity expertise. Unlike the divergent thinking approaches, in which the required expertise is not necessarily related to the creativity process itself (e.g. individuals can be trained to be better moderators in brainstorming), the creativity template approach is trainable and has the capacity to measure and directly improve creativity outcomes. The template taxonomy facilitates the focused cognitive effort involved in generating new ideas, the capacity to access relevant information, and enables high memorability of the reduced set of information needed to perform the tasks. The fact that templates are less transient than the ideas produced does not mean that templates are permanent or that they are insensitive to changes over long term frameworks. Indeed, advertising reflects social norms and trends, and as such, long term social trends are expected to reshape the templates and provide conditions for the evolution of new templates. Nonetheless, the dynamics of template changes are expected to be much slower than the dynamics of changes in ad hoc idea generation.", "e:keyword": ["Advertising Creativity", "Advertising Strategy", "Creativity in Marketing", "Marketing Ideation Processes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.352", "e:abstract": "Managing the allocation of shelf space for new products is a problem of significant importance for retailers. The problem is particularly complex for exhibitorsthe retailers in the motion picture supply chainbecause they face dynamic challenges, given the short life cycles of movies, the changing level of demand over time, the scarcity of shelf space, and the complex revenue sharing contract between the exhibitor and the distributor. In the face of this complexity, the aim of current research is to provide a structure for analyzing management problems of exhibitors in the movie industry. Using a mathematical programming approach and a fast, but readily accessible algorithm, we propose a decision support model, SilverScreener, whose aim is to help exhibitors make effective and timely decisions regarding theater screens management. The major objective is to help select and schedule movies for a multiple-screens theater over a fixed planning horizon in such a way that the exhibitor's cumulative profit is maximized. By treating the multiple screens as and the movies as , we provide an analogy of the current problem to the parallel machine scheduling problem. We formulate the resulting problem as an integer program. We depart from the typical parallel machine scheduling problems by introducing the that is particularly useful for solving the current problem. An important distinction between the current problem and typical machine scheduling problems is that the present approach allows for the choice of which movies to play; typically, in machine scheduling, all jobs have to be scheduled. We provide various analyses of normative versus actual decision making, based on publicly available data. The developed model is readily implementable and appears to lead to improved profitability in different comparative cases. Through sensitivity analysis, we demonstrate that the above results are robust to variations in various parameters of the problem. The main findings and insights from the normative policy suggest the following:  Based on SilverScreener's recommendations, the exhibitor can achieve substantially higher cumulative profit.  The improvement over actual decisions in terms of profitability appears to result from a combination of better selection and scheduling of the movies.  The general structure of the exhibitor's normative decision is: . We propose a two-tier integrated application of the model to show how the model can be applied to realistic decision making. The first tier involves development of a to help the manager plan an entire season and bid for movies before the start of that season. An ex ante revenue prediction scheme is developed, based intuitively on a of the forthcoming movies with similar movies played in this theater previously. If the forthcoming season's scheduling plan can be visualized as a two-dimensional (week-by-screen) matrix, then that matrix contains only empty cells before the first tier. After a bid plan is developed, the exhibitor can fill some of those empty cells. The remaining empty cells represent slots, which can be decided during the season by either extending movies the exhibitor booked before the season or by scheduling other movies which may become available later in the season. This motivates the second tierof the integrated approach. The second tier helps the exhibitor in weekly decision making during the season. This application involves rolling, and updating data, from one time window to another. The approaches followed in the two tiers of the integrated application are quite general in that they can incorporate a sophisticated demand prediction model, managerial judgments, or a combination of both. We also propose an alternative behavioral decision rule (heuristic), which exemplifies relationship dilemmas in the movie industry. This heuristic shows that the exhibitors need to be selective in their choice of movies and may suffer a substantial loss in profitability if they place too much emphasis on accommodating distributors.", "e:keyword": ["Movies", "Decision Support Systems", "Retailing", "Scheduling", "Integer Programming"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.373", "e:abstract": "The purpose of this paper is to develop and illustrate a systematic segment selection procedure that models the tradeoffs among evaluation criteria and organizational resource constraints. The target audience for this paper is both managers and academic researchers. For managers, our segment selection procedure provides assistance in formulating new product strategy by creating a common structured framework for understanding and resolving tradeoffs among segment evaluation criteria. For researchers, the procedure addresses an important gap in past segmentation research. The state-of-the-art in segmentation methods provides sophisticated analytic techniques for identifying homogenous groups of customers based on their preferences and optimally allocating resources to any subset of these segments. However, the intermediate decisions, involving how to evaluate the attractiveness of each segment and how to select the appropriate segments to serve such that long-run profitability is maximized subject to firm constraints, continue to be treated in an ad hoc manner. Segment selection is the critical line between the segment formation and resource allocation processes. Organizations are made up of people who represent different functional areas that are measured against different goals and performance requirements. Thus, the real challenge in segment selection is developing a model that coordinates resource competitive goals across the functional areas of an organization. Effective segment selection requires integration of the various decision criteria that play interrelated roles in determining product profitability, marketability, and manufacturability. Our research objective is to develop and illustrate a structured segment selection procedure that balances multiple decision criteria, thereby managing organizational diversity of views among key decision makers. The segment selection procedure provides a more structured approach to eliciting and explicitly modeling the tradeoffs among the multiple decision criteria. The segment selection procedure is a system of methodologies that identifies and selects market segments and product portfolios such that customer preferences, organizational objectives, and resource constraints are simultaneously satisfied. We employ a multistage research methodology incorporating conjoint analysis, cluster analysis, a product design optimization simulation, and a multiobjective integer programming (MOIP) model. Development of the MOIP model requires balancing the science of mathematical optimization against the art of problem definition and the reality of the implementation context. Our procedure facilitates management involvement in model development and blends managerial intuition with the model solution so that the final solution is optimal for the organization's situation. The segment selection procedure provides a structured method for balancing divergent performance metrics and normatively allocating resources to serve selected segments. Overall, the segment selection procedure combines mathematical modeling methods and managerial wisdom to design a total marketing plan for segmentation. Through an illustration in the automotive supply industry, we show that our procedure is an effective approach for integrating marketing, manufacturing, and financial performance information in the segment selection decision process. The segment selection procedure provides a framework for extensive sensitivity analysis of tradeoffs among alternative decision criteria so management can resolve how best to balance its short and long-term goals. This procedure is a generalizable process for systematic planning and winnowing down market opportunities according to carefully defined criteria. Successful implementation of the procedure requires managerial involvement and a blend of science and art. In our illustration, the final solution was a blend that leveraged the structure of the modeling process against the subtlety of the implementation context.", "e:keyword": ["Segment Selection", "Multiobjective Integer Programming Model", "Industrial Product Planning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.396", "e:abstract": "The usefulness of a technology product for an end-user often depends on the availability of complementary software products and services. Computers require software, cameras require film, and DVD players require movie programming in order for customers to value the whole product. This phenomenon, where the demand for hardware products is mediated by the supply of complementary software products, is called an network externality. Indirect network externalities create a two-way contingency between the demand for the hardware product and the supply of software products, and result in a strategic interdependence between the actions of hardware manufacturers and the actions of software providers. Indirect network externalities are gaining economic significance in technology markets, because hardware and software are typically provided by independent firms, and both sets of firms have an incentive to free-ride on each others' demand creation efforts. Despite the ubiquity of this phenomenon, it has largely been ignored in the marketing science literature. We present a conceptual and operational model for the evolution of markets with indirect network externalities. The key feature of our framework is to model the between the actions of hardware manufacturers and software complementors, created by the of consumer demand for the whole product on the actions of manufacturers as well as complementors. In addition, we incorporate marketing-mix effects on consumer response, as well as heterogeneity in consumer preferences for hardware and software attributes. We model consumer response using a latent-class choice model. To estimate the complementor response functions, we use a modified Delphi technique that allows us to convert qualitative response data into quantitative response functions. We integrate the consumer and complementor response models to create a simulation model that generates forecasts of market shares and sales volumes for competing technologies, as a function of marketing-mix effects and exogenously specified regulatory scenarios. The modeling framework is of interest to new product modelers interested in creating empirical models and decision-support systems for forecasting demand in technology markets characterized by indirect network externalities. The decision-support aspects of the modeling framework should appeal to managers interested in understanding and quantifying the complex interplay between hardware manufacturers and software complementors in the evolution of markets with indirect network externalities. We present an application of the modeling framework to the U.S. digital television industry, and use the framework to characterize the competition among analog and digital TV technologies. Our results suggest that complementor actions play an important role in the acceptance of digital TV technologies in general, and high definition television (HDTV) in particular. We find that forecasts that ignore the influence of indirect network externalities would be seriously biased in favor of HDTV. We illustrate how the modeling framework can be used to identify and profile customer segments in the digital TV market based on their utility for hardware-related features as well as programming-related features. We also illustrate the decision-support capabilities of the modeling framework by evaluating the sensitivity of the forecasts to varying marketing, regulatory, and complementor response scenarios. We derive implications for marketing and public affairs policies of the hardware manufacturers. The developments in the digital TV industry generally support our finding that HDTV will be a niche product, and will diffuse slower than originally expected due in part to the lack of programming. The delays in the introduction of digital TV to the marketplace also suggest that most forecasts for infrastructure-intensive technologies like digital TV may be too optimistic simply because they underestimate the delays in agreeing upon technology standards and resolving regulatory debates.", "e:keyword": ["Indirect Network Externalities", "Demand Forecasting", "New Products", "Chicken-and-Egg", "HDTV", "Endogeneity", "Heterogeneity", "Conjoint Analysis", "Technology"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.417", "e:abstract": "Much of the salesforce compensation literature has focused on developing incentive schemes to maximize effort levels on the part of the salesforce. The amount of effort to expend in the selling task is considered to be the sole decision variable for a sales representative. In this paper, we introduce another key decision variable for a sales representative which is how much risk to undertake in the selling task. In other words, we consider the fact that for a sales representative riskiness of performance (e.g., the dispersion of the probability distribution for sales) is often a choice and not a given fate. For example, a sales representative, trying to increase sales, may have the choice of allocating a given amount of effort on the low-risk approach of pursuing a small set of existing customers or incur the same effort on the high-risk option of getting new larger customers to switch from competitors. This paper examines how decisions on risk behavior on the part of the sales representatives are influenced by compensation schemes. We show that such decisions are sensitive to the payoff structure when a quota-based or a rank-order contest-based compensation scheme is used. More specifically, we argue that a high quota level or a rank-order contest where only the top few win induce sales representatives to opt for high-risk prospects, whereas a low quota level or a rank-order contest where a high proportion win induce sales representatives to opt for low-risk prospects. This does not stem from any kind of violation of standard expected utility theory but arises from the specific structure of jumps in payoffs. It is not that the inherent risk attitudes of the sales representatives are being altered. Rather, under some quota and contest conditions, a more risky prospect may yield higher expected utility for an inherently risk-averse sales representative while under some other quota and contest conditions, a less risky prospect may lead to a higher expected utility for an inherently risk-seeking sales representative. The theoretical propositions are tested in a series of five experiments. The first two experiments test the theoretical results of quota-based compensation. The quota levels are manipulated. Subjects select between segment types where the mean expected sales are the same but the variance varies. The next two experiments test the risk behavior of subjects in contest-based incentive schemes when the proportion of winners in the contest is manipulated. The results provide strong support for our models, with only a few subjects departing from the theoretical predictions. A fifth experiment shows some cognitive response data to explain the behavior that is inconsistent with the theoretical predictions. This paper provides implications that are useful for managers who design compensation schemes. A common assumption in most normative models on salesforce compensation is that all sales representatives are either risk averse or risk neutral. This might often lead to the conclusion that sales representatives cannot be expected to engage in high-risk activities in the absence of a risk premium over and above the compensation scheme. While this may be true if sales representatives are facing only a piece-rate compensation plan, it need not be the case when quota-based or contest-based compensation schemes are used. Our results suggest that when the sales quotas are set high or if the proportion of winners in a sales contest is low, sales representatives may engage in high-risk behavior. Alternatively, if the quotas are low or the proportion of winners in a sales contest is high, sales representatives may engage in low-risk prospects. Hence, if a firm would like to dampen high-risk behavior on the part of the salesforce, lowering quota levels or increasing the proportion of winners in sales contests might do so. Similarly, in order to reduce conservatism towards risk, moving up the quota levels or reducing the proportion of winners in sales contests could be useful. Our results extend beyond just salesforce management, to any situation where payoffs are based on reaching a certain threshold level in performance or are based on relative performance. For example, similar implications hold in tournaments for promotion to a limited number of top management positions in an organization, influencing the portfolio of R&D managers, and so on.", "e:keyword": ["Quotas", "Risk Taking", "Sales Contests", "Salesforce Compensation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.435", "e:abstract": "We organize the existing theoretical pricing research into a new two-level framework for industrial goods pricing. The first level consists of four pricing situations: New Product, Competitive, Product Line, and Cost-based. The second level consists of the pricing strategies appropriate for a given situation. For example, within the new product pricing situation, there are three alternative pricing strategies: Skim, Penetration, and Experience Curve pricing. There are a total of ten pricing strategies included in the framework. We then identified a set of cost, product, market, and information conditions which determine what pricing situation(s) a firm is facing as well as which strategies are appropriate within a given situation. Some of these determinant conditions are common to many pricing strategies (e.g., highly elastic demand) while others are unique to a given strategy within a particular pricing situation. For example, within the product line situation, the profitability of supplementary sales is a unique determinant of the Complementary Product pricing strategy (razor-and-blade pricing). Using this framework as a basis for an empirical study, we examined how well current industrial pricing practice matches the prescriptions from the existing research. Our sample consisted of 270 respondents (27% response rate). Of these, more than 50% indicated that they used more than one pricing strategy in formulating their most recent pricing decision for a high-value industrial product sold in the United States. As in previous research, Cost-Plus pricing was the most often cited pricing strategy (56% of the respondents). Since the respondents were able to indicate their use of more than one pricing strategy, the data are of the pick from  variety. In order to model the managers' pricing strategy choices, we constructed a stacked binary logit with a separate observation for each strategy within a given pricing situation. The signs of the determinant variables were estimated as interaction terms. The new product pricing strategies (skim, penetration, experience curve) were used for new models in the market. Skim pricing was used in markets with high levels of product differentiation by firms at a cost disadvantage due to scale. Penetration pricing was used by firms with a cost advantage due to scale in markets with high level of overall elasticity but low brand elasticity. Experience curve pricing was used for minor innovations by firms with low capacity utilization in markets with a high level of differentiation. The competitive pricing strategies (Leader, Parity, and Low-price Supplier) were used in mature markets. Parity pricing was used by firms in a poor competitive situation, i.e., high costs, low market share, low product differentiation. These firms were also unable to take advantage of high levels of elasticity since their capacity utilization was high. In contrast, the low-price supplier strategy was used by firms with low costs due to scale advantages. Since they have low utilization, these firms can take advantage of elastic brand demand. None of the determinants were significantly related to the choice of leader pricing. Product line pricing strategies (Bundling, Complementary Product, and Customer Value pricing) were more likely to be used by firms which sell substitute or complementary products. Bundle pricing was used for per-sale/contract pricing in markets with high levels of brand elasticity. Complementary product pricing (razor-and-blade) was used by firms that enjoyed high profitability on its supplementary sales. Using customer value pricing, a firm offers a stripped down version of its current products to appeal to more price sensitive segments or to leverage new distribution channels. This strategy was used to target a narrow segment in high growth markets where price changes are difficult to detect. Cost-based pricing was more likely to be used in markets where demand is very difficult to estimate. In such a situation, cost-based pricing makes a great deal of sense. In general, the results show that the managers' pricing strategy choices are consistent with normative pricing research. However, questions about how managers combine their strategies to arrive at a final price as well as the organizational influences on pricing strategies remain important areas for future research.", "e:keyword": ["Pricing Research", "Industrial Marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.455", "e:abstract": "Noble and Gruca (1999, this issue) provide useful insights into the pricing practices managers employ. Their findings indicate managerial pricing practices are heavily dominated by internal, cost-based approaches. Particularly relevant is the absence of value-based pricing practices. Noble and Gruca's findings indicate that the emerging market orientation work has not connected to pricing practice. This work poses a significant challenge for marketing theoreticians and educators: How can managers be helped to move beyond internally focused to externally-customer-focused pricing practice?", "e:keyword": ["Pricing", "Pricing Theory", "Market Orientation", "Marketing Strategy", "Marketing Education"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.3.458", "e:abstract": "The authors thank We would like to thank George E. Cressman, Jr. for his comments on their paper, Industrial Pricing: Theory and Managerial Practice. This study had its origins in the thesis of the first author, who entered a doctoral program in marketing after spending 18 years in the heavy equipment industry. In that period he spent a lot of time setting prices and trying to predict future price changes by competitors. His industry experience suggested that the market-oriented pricing strategies discussed in the marketing literature are not used as extensively in practice as we might like to believe. The results of the study do confirm this suspicion, although not to the extent suggested in the commentary. It is important to understand how many firms are operating with a purely internal focus for their pricing strategy.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.463", "e:abstract": "This research focuses on how consumers perceive the quality level of a complex stimulus (in our case, a service encounter) and how this perception affects consumers' overall assessment of the quality level of the firm. As such, it should be of interest to consumer behavior theorists as well as to those involved in marketing management issues. We start our presentation by developing a normative Bayesian data integration and updating model somewhat similar to that put forth by Rust et al. (1999). The major constructs of interest in this model are the person's belief about the overall service quality level of a firm and what will happen on the next service encounter. Two major results following from this baseline normative model are that the updated means of these two beliefs are identical to the dynamic updating equations put forth by Boulding et al. (1993), and that an underlying assumption of this model is that consumers form unbiased perceptions of the complex stimulus. Next, based on substantial empirical evidence from the behavioral literature, we incorporate into this baseline model a process by which consumers form nonrandom perceptions. Specifically, we postulate that a person's perception of a complex service encounter is a blend of the objective dimensions of the service encounter and the person's prior overall belief about the quality level of the firm. The relative weights placed on these two factors are determined at least in part by the experience level of the customer and the complexity of the service encounter. We then use this expanded model to compare and test different models of service quality and provide deeper insights into the process by which consumers form perceptions of both the service transaction and the overall service quality level of a firm. We test this model using data from two different experiments. In each experiment we manipulate the service provided and the person's initial and . Using both obtrusive and unobtrusive measures of the underlying constructs of our model, we employ multiple tests to determine the veracity of our expanded model relative to the baseline model. These tests support the expanded model specification. As found previously, and as predicted by a Bayesian updating process, consumers' prior beliefs influence their cumulative overall opinion of service quality (e.g., Anderson and Sullivan 1993, Bolton and Drew 1991, Boulding et al. 1993, Rust et al. 1999). Perhaps more importantly, these prior beliefs also influence their perceptions of the data themselves, which in turn affect their new (updated) overall opinion. These two different influences of prior beliefs on cumulative evaluations of quality constitute the double whammy referred to in the title. This double whammy effect has major managerial significance. Specifically, we see from our model development that all activities of a firm will be perceived in light of a person's prior beliefs and that these priors will be double counted relative to our baseline model. Thus, any marketing action taken by a firm will be perceived more positively or negatively depending on the person's prior belief about the quality level of the firm. As such, our model provides a formal explanation for the notion of brand equity as differential leverage in marketing activities as proposed by Keller (1993).", "e:keyword": ["Service Quality", "Bayesian Updating", "Consumer Behavior", "Confirmatory Bias", "Brand Equity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.485", "e:abstract": "Conventional wisdom seems to claim that, by lowering the cost of distribution and by making search easier for consumer, the introduction of the Internet is likely to intensify price competition. This paper intends to challenge this view by asking: When and how is the Internet likely to decrease the level of price competition between firms? To answer this question, we develop an analytic model with the following characteristics. On the demand side, consumers need to gather information on two types of product attributes: (which can be communicated on the Web at very low cost) and (for which physical inspection of the product is necessary). Consumers choose between two brands but are familiar with the nondigital attributes of only the brand purchased on the last purchase occasion. On the supply side, firms use traditional stores and the Internet to inform consumers about their products' attributes and to sell their products. In this setup, we show that the impact of the Internet on competition will be radically different depending on the relative importance of parameters describing the relevant shopping and distribution context. Specifically, we find that the introduction of the Internet might lead to monopoly pricing when (1) the proportion of Internet users is high enough, (2) when nondigital attributes are relevant but not overwhelming, (3) when consumers have a more favorable prior about the brand they currently own, and (4) when the purchase situation can be characterized by destination shopping. More surprising, we also show that in such cases, the use of the Internet not only leads to higher prices but can also discourage consumers from engaging in search. As such, an important message of the paper is that under some conditions the Internet might represent an opportunity for firms to leverage their brand loyalty and increase their profits. The intuition behind our results is the following. The Internet allows consumers to evaluate digital attributes easily, i.e., without visiting the stores. However, nondigital attributes can only be evaluated through physical presence. As such, for goods where both types of attributes are important, the introduction of the Net changes the effective cost of search for consumers. Without the Internet the cost of search is . With the introduction of the Net however, nonsearching consumers do not have to undertake the shopping trip at all because they can order products on the Net. Thus, in the presence of the Internet the cost of search is related to . In the case of destination shopping (i.e. when the fixed cost of undertaking the shopping trip is higher than the cost of visiting an additional store), the presence of the Internet creates higher effective search costs for consumers. Given this shift of paradigm in search costs due to the Internet, consumers may not take the risk of searching for products with better nondigital attributes, but instead, remain with the product they are familiar with. This results in increased consumer loyalty, which induces firms to increase their prices. Our results have important managerial implications. First, they provide guidelines for firms on when (i.e. for which product categories) they should consider expanding their distribution network to the Internet. In this respect, an important additional insight of the paper is that the Internet can lower price competition and lead to reduced consumer search if it is more expensive than the traditional distribution channel. This can easily be the case if distribution through the Internet represents additional costs such as the costs associated with shipping and handling and return policies. Second, the paper also provides guidelines on how to plan the firm's Internet strategy. Interestingly, the results suggest that with the general availability of the Internet the role of stores might actually become more important. While we do not explicitly model a dynamic market, our findings together with Klemperer's (1987) results suggest that stores might have a key role in consumer acquisition, while the Internet can help leverage the acquired customer base through demand fulfillment. This might imply that for certain product categories, firms should actually allocate additional resources to improve their in-store environment when considering the Internet as a complementary distribution channel.", "e:keyword": ["Internet", "Consumer Search", "Digital/Nondigital Attributes", "Competition", "Game Theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.504", "e:abstract": "Price promotions are used extensively in marketing for one simple reasonconsumers respond. The sales increase for a brand on promotion could be due to consumers accelerating their purchases (i.e., buying earlier than usual and/or buying more than usual) and/or consumers switching their choice from other brands. Purchase acceleration and brand switching relate to the primary demand and secondary demand effects of a promotion. Gupta (1988) captures these effects in a single model and decomposes a brand's total price elasticity into these components. He reports, for the coffee product category, that the main impact of a price promotion is on brand choice (84%), and that there is a smaller impact on purchase incidence (14%) and stockpiling (2%). In other words, the majority of the effect of a promotion is at the secondary level (84%) and there is a relatively small primary demand effect (16%). This paper reports the decomposition of total price elasticity for 173 brands across 13 different product categories. On average, we find that 25% of the elasticity is due to primary demand expansion (i.e., purchase acceleration) and 75% to secondary demand effects or brand switching. Thus, while Gupta's finding that the majority of promotional response stems from brand switching is supported, the average magnitude of the effect appears to be smaller than first thought. More important, there is ample evidence that promotions have a significant primary demand effect. The relative emphasis on purchase acceleration and brand switching varies systematically across categories, and the second goal of the paper is to explain this variation as a function of exogeneous covariates. In doing this, we recognize that promotional response is the consumer's reaction to a price promotion, and therefore develop a framework for understanding variability in promotional response that is based on the consumer's perspective of the benefits from a price promotion. These benefits are posited to be a function of: (i) category-specific factors, (ii) brand-specific factors, and (iii) consumer characteristics. The framework is formalized as a generalized least squares meta-analysis in which the brand's price elasticity is the dependent variable. Several interesting results emerge from this analysis.  Category-specific factors, brand-specific factors, and consumer demographics explain a significant amount of the variance in promotional response for a brand at both the primary and secondary demand levels.  Category-specific factors have greater influence on variability in promotional response and its decomposition than do brand-specific factors.  There are several instances where exogenous variables do not affect total elasticities yet significantly affect individual components of total elasticity. In fact, the lack of a significant relationship between the variables and total elasticity is often due to offsetting effects within two or more of the three behavioral components of elasticity. This is particularly true for brand-specific factors, which typically have no effect on total elasticity, yet have important effects on the individual behaviors.  There is some evidence to suggest that not all promotion-related increases in primary demand are due to forward-buyingin some cases promotions appear to increase consumption. We use these results to illustrate how category- and brand-specific factors work to drive primary and secondary demand elasticities in different directions. In short, this paper offers an empirical generalization of a key finding on promotional responsehow elasticities decompose across brand choice, purchase incidence, and stockpilingand new insights into factors that explain variance in promotional response. These findings are likely to be of interest to researchers who are concerned with theory development and the generalizability of marketing phenomena, and to managers who plan promotion campaigns.", "e:keyword": ["Price Elasticity", "Promotion", "Brand Choice", "Purchase Incidence", "Stockpiling", "Primary Demand", "Secondary Demand", "Meta-Analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.527", "e:abstract": "Consumers rank variety of assortment right behind location and price when naming reasons why they patronize their favorite stores. Consumers care about variety because they are more likely to find what they want when going to a store that offers more varied assortments. When tastes are not well formed or are dynamic, perceived variety matters even more because of the desire to become educated about what is available while maintaining flexibility. Variety perception also matters when the variety-seeking motive operates. Retailers care about variety because customers value variety. Therefore, it is important to understand how people perceive the variety contained in an assortment and how these perceptions influence satisfaction and store choice. Remarkably, except for a recent study by Broniarczyk et al. (1998), there has been no research aimed at understanding the variety perception process itself. We offer a general mathematical model of variety based on the information structure of an assortment, defined both by the multiattribute structure of the objects and their spatial locations. We impose a psychologically plausible set of restrictions on the general model and obtain a class of simpler estimable models of perceived variety. We utilize the model to develop assortments that vary widely in terms of their information structure and study the influence of three factors on variety perceptions: (a) information structure of each assortment (i.e., the attribute level differences between objects); (b) level of organization of the objects and hence their relative spatial positions; and (c) task orientations, promoting either analytic or holistic processing. We also investigate the influence of variety perception and organization on stated satisfaction and store choice. To summarize our major findings: 1. Information structure has a big impact on variety perceptions, though diminishing returns accompany increases in the number of attributes on which object pairs differ. 2. People are more influenced by local information structure (adjacent objects) than nonlocal information structure. Proximity matters. 3. Organization of the display can either increase or decrease variety perceptions. When people engage in analytic processing, organized displays appear to offer more variety. When processing is holistic, random displays are seen as more varied. 4. Both variety perceptions and organization drive stated satisfaction and store choice. People are more satisfied with and likely to choose stores carrying those assortments that are perceived as offering high variety and that are displayed in an organized rather than random manner. Our work provides a basic framework for thinking about variety. By helping retailers to understand the factors that drive variety perception, it may be possible to design more efficient, lower cost assortments without reducing variety perceptions and the probability of future store visits.", "e:keyword": ["Assortment", "Hamming Distance", "Store Choice", "Variety"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.547", "e:abstract": "We propose a conceptual frameworkwith the resource-based view (RBV) of the firm as its theoretical underpinningto explain interfirm differences in firms' profitability in high-technology markets in terms of differences in their functional capabilities. Specifically, we suggest that marketing, R&D, and operations capabilities, along with interactions among these capabilities, are important determinants of relative financial performance within the industry. This paper contributes to the RBV literature by proposing the input-output perspective to conceptualize the notion of capabilities. Specifically, this approach entails modeling a firm's functional activitiesviz., marketing, R&D and operationsas transformation functions that relate the productive factors/resources to its functional objectives, if the firm were to deploy these resources most efficiently. Any underattainment of the functional objective, then, is attributable to functional inefficiency, or equivalently, to a lower functional capability of the firm. The input-output conceptualization of a firm's capabilities is then estimated using the stochastic frontier estimation (SFE) methodology. SFE provides the appropriate econometric technique to empirically estimate the efficient frontier and hence the level of efficiency achieved by the various firms. Our study contributes to a number of literatures, both methodologically and substantively. First, it contributes both conceptually and methodologically to the RBV literature. Conceptually, our study suggests that firm capabilities can be viewed in an inputoutput framework. Methodologically, the study suggests the use of stochastic frontier estimation to operationalize and estimate firm capabilities. This methodology is, to the best of our knowledge, the first to allow the researcher/manager to capabilities from archival data. Substantively, our study contributes to the literature on market orientation by suggesting that a stronger market orientation of a firm should be reflected in a higher marketing capability. It also adds to the literature on design for manufacturability by explicating the complementarity among the various functional capabilities and offering empirical evidence on their relative importance in influencing a firm's performance. Finally, our study builds on prior literature that has highlighted the importance of marketingR&D coordination as important determinants of new product development and success. We highlight below some of our main findings.  A strong base of innovative technologies enhances a firm's sales by favorably influencing consumers' expectations about the externality benefits associated with its product. This suggests that a past track record of consistent innovation is a credible signal to current and potential customers of the firm's continued excellence in a technologically evolving market. Given the importance of influencing customers, managers need to tailor their marketing activities around the need to inform customers of the technological excellence of their firm. Thus, customers need to be informed of the innovative technologies that the firm possesses and of the future R&D initiatives undertaken by it. Similarly, any potential applications of innovative technology developed by the firm, and of technologies under development, should be emphasized.  Marketing capability has its greatest impact on the (quality-adjusted) innovative output for firms that have a strong technological base. In other words, firms with a strong R&D base are the ones with the most to gain from a strong marketing capability.  Marketing capability strongly influences the width of applicability of innovations, i.e., a firm's marketing capability enhances its ability to generate innovative technologies that have applications across a range of industries. This result carries a strong message for managers: A strong market orientation is one of the most fertile sources of ideas for innovation. Thus, marketing needs to be involved from the beginning of the innovation processnamely, right at the stage when technological ideas are being generated.  The most important determinant of a firm's performance is the interaction of marketing and R&D capabilities. This supports the assertion that firms in high-technology markets need to excel at two things: the ability to come up with innovations constantly, and the ability to commercialize these innovations into the kinds of products that capture consumer needs and preferences. This finding offers further evidence on the importance of coordination between R&D and marketing, as suggested in the extant marketing literature. Finally, using archival data, our methodology can be used to benchmark a firm's capabilities, with other firms in the industry, along various functional dimensions. This would be an important step in making more informed resource-allocation decisions. Thus, the firm can spend more money on those capabilities where it most lags the competition, or on those capabilities that are shown to have the maximum impact on firm performance.", "e:keyword": ["High-Technology Markets", "Resource-Based View", "Firm-Specific Capabilities", "Stochastic Frontier", "R&D and Innovation", "Patents and Patent Citations", "Cross-Functional Coordination", "Marketing-Manufacturing Interface", "Market Orientation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.569", "e:abstract": "Generally, profit predictions are made conditional upon a particular functional form. The typical caveat offered is that this is not the true demand model, but is instead some reasonable approximation. We show how the notion of an approximation can be explicitly represented using a random coefficient model. Our model nests the usual situation of complete model certainty as a special case. We go on to show how ignoring the uncertainty in functional form induced by approximation will lead to erroneous pricing decisions that may frequently lead to overpricing. For example, an inelastic, double-log demand model implies infinite optimal prices. This is clearly a nonsensical, analyst recommendation. We propose a more general form of the double-log model that allows for high confidence in the observed price range, but incorporates increased uncertainty about the adequacy of the double-log approximation as prices move beyond the observed range. The optimal pricing solutions for this new model are lower than those for the usual case with complete certainty. In fact, we find well-defined optimal pricing solutions even for inelastic double-log demand models. This is a finding of great practical importance, given that aggregate demand models tend to be inelastic for grocery categories, and that log demand models are frequently used (Hoch et al. 1995). We argue that the lack of recognizing uncertainty in the modeling process may partially account for why there is a seeming disparity between observed retail prices and the optimal prices implied by maximizing total category profits using estimated demand models (Little and Shapiro 1980). The problems of making optimal pricing decisions using double-log demand models calibrated with store-level scanner data have been recognized. Previous solutions are to constrain the results to achieve reasonable solutions (Reibstein and Gatignon 1984, Montgomery 1997) or to avoid these models altogether in favor of household choice models aggregated to the store-level (Vilcassim and Chintagunta 1995). Our assessment of the problem is that it is not necessarily an issue of model specification, but one of inference. In other words, double-log models fit well, but optimization leads to out-of-range predictions. Our suggestion is that inferences from an estimated demand model need to be approached with some caution. Specifically, uncertainty about predictions will always exist. If this uncertainty is incorporated into models such as the double-log form, then much better inferences can be made. It is our hope that this research will encourage others to think not only about model specification and estimation, but also inference.", "e:keyword": ["Approximations", "Demand Estimation", "Pricing Research", "Random Coefficient Models", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.584", "e:abstract": "There is theoretical and empirical evidence that consumers have limited cognitive resources and thus cannot maintain direct preferences for each choice alternative on the store shelves. Instead, they likely form their overall preferences for choice alternatives by evaluating the attributes describing each item. Rather than mapping the locations of and preferences for all choice alternatives in a multidimensional space, as is the current practice in marketing research, it is insightful to map the locations of and preferences for the attributes consumers use to evaluate the choice alternatives. The model proposed in this study unifies latent class preference models (choice models or conjoint models) with latent class multidimensional scaling (MDS) analysis. Dimensional restrictions are imposed on latent class preference models such that the locations of attribute levels and market response parameters can be mapped in reduced-dimension spaces. Interactions between attributes can be graphically examined, which is not feasible with the traditional MDS approach. Also, the effects of price reductions and promotions on the locations of attribute levels can be graphically examined. An empirical application with scanner panel data shows the capabilities and limitations of the proposed model. In addition to the managerial insights provided by the model, it is also much more parsimonious than existing methods, and it forecasts holdout choices significantly better. In the empirical application, a model with two-dimensional attribute maps has 50 fewer parameters than the best unrestricted latent class choice model, yet the fit is comparable. The predictive performance of our model is shown to be superior to that of latent class MDS approaches and latent class conjoint approaches.", "e:keyword": ["Brand Choice", "Choice Models", "Marketing Mix", "Scaling Methods", "Segmentation Research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.18.4.605", "e:abstract": "In many business sectors such as airlines, hotels, trucking, and media advertising, customers' arrivals and willingness to pay are uncertain. Managers must decide whether to quote a price low enough to guarantee early sales, or to quote a higher price and risk that some units remain unsold. In allocating capacity, they face a trade-off between two types of potential losses; (1) selling at a low price, and losing a better price later, and (2) waiting in vain to sell at a high price, and losing the opportunity of an earlier low price offer. Yield loss means that consumers who value the product most do not get to use it, and spoilage loss means that valuable products are wasted because no consumers get to use them. Sellers typically hedge against the risk of spoilage loss by selling some units early at low prices, and against the risk of yield loss by blocking some units in hope of selling them later at a high price. In this paper we show that the use of overselling with opportunistic cancellations can increase expected profits and improve allocation efficiency. Under this strategy, the seller deliberately oversells capacity if high-paying consumers show up, even when capacity is already fully booked. The seller then cancels the sale to some low-paying customers while providing them with appropriate compensation. We derive a new rule to optimally allocate capacity to consumers when overselling is used, and show that overselling helps limit the potential yield and spoilage losses. Yield loss is reduced because the seller can capture more high-paying customers by compensating low-paying customers who give up their right to the product. Spoilage loss is reduced because the compensation decreases the price spread perceived by the seller, and as a result, the seller is less anxious to speculate and block units. Overselling with opportunistic cancellations assures that the product will be sold to consumers who value it most. This means that everybody wins, and resources are allocated more efficiently than in conventional selling.", "e:keyword": ["Overselling", "Overbooking", "Yield Management", "Yield and Spoilage Losses", "Capacity Management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.1.1.15181", "e:abstract": "There is a revolution happeninga startling and amazing revolution that is altering everything from our traditional views of how advertising and communication media work to how people can and should communicate with each other. That revolution is the Internetthe massive global network of interconnected packet-switched computer networksand as the most important innovation since the development of the printing press, the Internet has the potential to radically transform not just the way individuals go about conducting their business with each other, but also the very essence of what it means to be a human being in society. Since the introduction of the first graphicallyoriented Web browser, Mosaic, in 1993, the Internet has experienced phenomenal growth, both in terms of the number of computers and devices connected to it and the number of individuals and firms providing and accessing content on it (Hoffman et al. 2000). The first significant commercial activity appeared on the Web by 1994and in the ensuing five years, the commercialization of the Internet has exploded. There are now very few countries and territories left in the entire world that do not have at least one host computer connected to the Internet (Rutkowski 1999). At the same time, electronic commerce, as a research area, a business, and, indeed, an entire new industry, is still very much in its infancy. There is much confusion and complexity and not nearly enough solid information.", "e:keyword": ["World Wide Web", "Commercialization of the Internet", "Internet Marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.1.4.15178", "e:abstract": "Despite the explosive growth of electronic commerce and the rapidly increasing number of consumers who use interactive media (such as the World Wide Web) for prepurchase information search and online shopping, very little is known about how consumers make purchase decisions in such settings. A unique characteristic of online shopping environments is that they allow vendors to create retail interfaces with highly interactive features. One desirable form of interactivity from a consumer perspective is the implementation of sophisticated tools to assist shoppers in their purchase decisions by customizing the electronic shopping environment to their individual preferences. The availability of such tools, which we refer to as for consumers, may lead to a transformation of the way in which shoppers search for product information and make purchase decisions. The primary objective of this paper is to investigate the nature of the effects that interactive decision aids may have on consumer decision making in online shopping environments. While making purchase decisions, consumers are often unable to evaluate all available alternatives in great depth and, thus, tend to use two-stage processes to reach their decisions. At the first stage, consumers typically screen a large set of available products and identify a subset of the most promising alternatives. Subsequently, they evaluate the latter in more depth, perform relative comparisons across products on important attributes, and make a purchase decision. Given the different tasks to be performed in such a two-stage process, interactive tools that provide support to consumers in the following respects are particularly valuable: (1) the initial screening of available products to determine which ones are worth considering further, and (2) the in-depth comparison of selected products before making the actual purchase decision. This paper examines the effects of two decision aids, each designed to assist consumers in performing one of the above tasks, on purchase decision making in an online store. The first interactive tool, a (RA), allows consumers to more efficiently screen the (potentially very large) set of alternatives available in an online shopping environment. Based on self-explicated information about a consumer's own utility function (attribute importance weights and minimum acceptable attribute levels), the RA generates a personalized list of recommended alternatives. The second decision aid, a (CM), is designed to help consumers make in-depth comparisons among selected alternatives. The CM allows consumers to organize attribute information about multiple products in an alternatives  attributes matrix and to have alternatives sorted by any attribute. Based on theoretical and empirical work in marketing, judgment and decision making, psychology, and decision support systems, we develop a set of hypotheses pertaining to the effects of these two decision aids on various aspects of consumer decision making. In particular, we focus on how use of the RA and CM affects consumers' search for product information, the size and quality of their consideration sets, and the quality of their purchase decisions in an online shopping environment. A controlled experiment using a simulated online store was conducted to test the hypotheses. The results indicate that both interactive decision aids have a substantial impact on consumer decision making. As predicted, use of the RA reduces consumers' search effort for product information, decreases the size but increases the quality of their consideration sets, and improves the quality of their purchase decisions. Use of the CM also leads to a decrease in the size but an increase in the quality of consumers' consideration sets, and has a favorable effect on some indicators of decision quality. In sum, our findings suggest that interactive tools designed to assist consumers in the initial screening of available alternatives and to facilitate in-depth comparisons among selected alternatives in an online shopping environment may have strong favorable effects on both the quality the efficiency of purchase decisionsshoppers can make much while expending substantially . This suggests that interactive decision aids have the potential to drastically transform the way in which consumers search for product information and make purchase decisions.", "e:keyword": ["Decision Making", "Online Shopping", "Electronic Commerce", "Decision Aids", "Recommendation Agents", "Consumer Behavior", "Information Search", "Consideration Sets", "Information Processing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.1.22.15184", "e:abstract": "Intuition and previous research suggest that creating a compelling online environment for Web consumers will have numerous positive consequences for commercial Web providers. Online executives note that creating a compelling online experience for cyber customers is critical to creating competitive advantage on the Internet. Yet, very little is known about the factors that make using the Web a compelling experience for its users, and of the key consumer behavior outcomes of this compelling experience. Recently, the flow construct has been proposed as important for understanding consumer behavior on the World Wide Web, and as a way of defining the nature of compelling online experience. Although widely studied over the past 20 years, quantitative modeling efforts of the flow construct have been neither systematic nor comprehensive. In large parts, these efforts have been hampered by considerable confusion regarding the exact conceptual definition of flow. Lacking precise definition, it has been difficult to measure flow empirically, let alone apply the concept in practice. Following the conceptual model of flow proposed by Hoffman and Novak (1996), we conceptualize flow on the Web as a cognitive state experienced during navigation that is determined by (1) high levels of skill and control; (2) high levels of challenge and arousal; and (3) focused attention; and (4) is enhanced by interactivity and telepresence. Consumers who achieve flow on the Web are so acutely involved in the act of online navigation that thoughts and perceptions not relevant to navigation are screened out, and the consumer focuses entirely on the interaction. Concentration on the navigation experience is so intense that there is little attention left to consider anything else, and consequently, other events occurring in the consumer's surrounding physical environment lose significance. Self-consciousness disappears, the consumer's sense of time becomes distorted, and the state of mind arising as a result of achieving flow on the Web is extremely gratifying. In a quantitative modeling framework, we develop a structural model based on our previous conceptual model of flow that embodies the components of what makes for a compelling online experience. We use data collected from a largesample, Web-based consumer survey to measure these constructs, and we fit a series of structural equation models that test related prior theory. The conceptual model is largely supported, and the improved fit offered by the revised model provides additional insights into the direct and indirect influences of flow, as well as into the relationship of flow to key consumer behavior and Web usage variables. Our formulation provides marketing scientists with operational definitions of key model constructs and establishes reliability and validity in a comprehensive measurement framework. A key insight from the paper is that the degree to which the online experience is compelling can be defined, measured, and related well to important marketing variables. Our model constructs relate in significant ways to key consumer behavior variables, including online shopping and Web use applications such as the extent to which consumers search for product information and participate in chat rooms. As such, our model may be useful both theoretically and in practice as marketers strive to decipher the secrets of commercial success in interactive online environments.", "e:keyword": ["Internet Marketing", "Electronic Commerce", "Online Consumer Behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.1.43.15180", "e:abstract": "This research examines the ability of six popular Web search engines, individually and collectively, to locate Web pages containing common marketing/management phrases. We propose and validate a model for search engine performance that is able to represent key patterns of coverage and overlap among the engines. The model enables us to estimate the typical additional benefit of using multiple search engines, depending on the particular set of engines being considered. It also provides an estimate of the number of relevant Web pages found by any of the engines. For a typical marketing/management phrase we estimate that the best search engine locates about 50% of the pages, and all six engines together find about 90% of the total. The model is also used to examine how properties of a Web page and characteristics of a phrase affect the probability that a given search engine will find a given page. For example, we find that the number of Web page links increases the prospect that each of the six search engines will find it. Finally, we summarize the relationship between major structural characteristics of a search engine and its performance in locating relevant Web pages.", "e:keyword": ["Capture/Recapture", "Hierarchical Bayes", "Marketing Information", "Probability Models", "World Wide Web"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.1.63.15182", "e:abstract": "The Internet has signi.cantly reduced the marginal cost of producing and distributing digital information goods. It also coincides with the emergence of new competitive strategies such as large-scale bundling. In this paper, we show that bundling can create economies of aggregation for information goods if their marginal costs are very low, even in the absence of network externalities or economies of scale or scope. We extend the Bakos-Brynjolfsson bundling model (1999) to settings with several different types of competition, including both upstream and downstream, as well as competition between a bundler and single good and competition between two bundlers. Our key results are based on the predictive value of bundling, the fact that it is easier for a seller to predict how a consumer will value a collection of goods than it is to value any good individually. Using a model with fully rational and informed consumers, we use the Law of Large Numbers to show that this will be true as long as the goods are not perfectly correlated and do not affect each other's valuations significantly. As a result, a seller typically can extract more value from each information good when it is part of a bundle than when it is sold separately. Moreover, at the optimal price, more consumers will find the bundle worth buying than would have bought the same goods sold separately. Because of the predictive value of bundling, large aggregators will often be more pro.table than small aggregators, including sellers of single goods. We find that these economies of aggregation have several important competitive implications: 1. When competing for upstream content, larger bundlers are able to outbid smaller ones, all else being equal. This is because the predictive value of bundling enables bundlers to extract more value from any given good. 2. When competing for downstream consumers, the act of bundling information goods makes an incumbent seem tougher to single-product competitors selling similar goods. The resulting equilibrium is less profitable for potential entrants and can discourage entry in the bundler's markets, even when the entrants have a superior cost structure or quality. 3. Conversely, by simply adding an information good to an existing bundle, a bundler may be able to profitably enter a new market and dislodge an incumbent who does not bundle, capturing most of the market share from the incumbent firm and even driving the incumbent out of business. 4. Because a bundler can potentially capture a large share of profits in new markets, single-product firms may have lower incentives to innovate and create such markets. At the same time, bundlers may have higher incentives to innovate. For most physical goods, which have nontrivial marginal costs, the potential impact of large-scale aggregation is limited. However, we find that these effects can be decisive for the success or failure of information goods. Our results have particular empirical relevance to the markets for software and Internet content and suggest that aggregation strategies may take on particular relevance in these markets.", "e:keyword": ["Bundling", "Internet", "Pricing", "Information Goods", "Software", "Competition", "Digital Goods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.1.83.15183", "e:abstract": "A fundamental dilemma confronts retailers with stand-alone sites on the World Wide Web and those attempting to build electronic malls for delivery via the Internet, online services, or interactive television (Alba et al. 1997). For consumers, the main potential advantage of electronic shopping over other channels is a reduction in search costs for products and product-related information. Retailers, however, fear that such lowering of consumers' search costs will intensify competition and lower margins by expanding the scope of competition from local to national and international. Some retailers' electronic offerings have been constructed to thwart comparison shopping and to ward off price competition, dimming the appeal of many initial electronic shopping services. Ceteris paribus, if electronic shopping lowers the cost of acquiring price information, it should increase price sensitivity, just as is the case for price advertising. In a similar vein, though, electronic shopping can lower the cost of search for quality information. Most analyses ignore the offsetting potential of the latter effect to lower price sensitivity in the current period. They also ignore the potential of maximally transparent shopping systems to produce welfare gains that give consumers a long-term reason to give repeat business to electronic merchants (cf. Alba et al. 1997, Bakos 1997). We test conditions under which lowered search costs should increase or decrease price sensitivity. We conducted an experiment in which we varied independently three different search costs via electronic shopping: search cost for price information, search cost for quality information within a given store, and search cost for comparing across two competing electronic wine stores. Consumers spent their own money purchasing wines from two competing electronic merchants selling some overlapping and some unique wines. We show four primary empirical results. First, for differentiated products like wines, lowering the cost of search for quality information reduced price sensitivity. Second, price sensitivity for wines common to both stores increased when cross-store comparison was made easy, as many analysts have assumed. However, easy cross-store comparison had no effect on price sensitivity for unique wines. Third, making information environments more transparent by lowering all three search costs produced welfare gains for consumers. They liked the shopping experience more, selected wines they liked more in subsequent tasting, and their retention probability was higher when they were contacted two months later and invited to continue using the electronic shopping service from home. Fourth, we examined the implications of these results for manufacturers and examined how market shares of wines sold by two stores or one were affected by search costs. When store comparison was difficult, results showed that the market share of common wines was proportional to share of distribution; but when store comparison was made easy, the market share returns to distribution decreased signi.cantly. All these results suggest incentives for retailers carrying differentiated goods to make information environments maximally transparent, but to avoid price competition by carrying more unique merchandise.", "e:keyword": ["Buyer Behavior", "Competitive Strategy", "Internet Marketing", "Price Sensitivity", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.2.105.11804", "e:abstract": "In collaborating to compete, firms forge different types of strategic alliances: same-function alliances, parallel development of new products, and cross-functional alliances. A major challenge in the management of these alliances is how to control the resource commitment of partners to the collaboration. In this research we examine both theoretically and experimentally how the type of an alliance and the prescribed profit-sharing arrangement affect the resource commitments of partners. We model the interaction within an alliance as a noncooperative variable-sum game, in which each firm invests part of its resources to increase the utility of a new product offering. Different types of alliances are modeled by varying how the resources committed by partners in an alliance determine the utility of the jointly-developed new product. We then model the interalliance competition by nesting two independent intra-alliance games in a supergame in which the groups compete for a market. The partners of the winning alliance share the profits in one of two ways: equally or proportionally to their investments. The Nash equilibrium solutions for the resulting games are examined. In the case of same-function alliances, when the market is large the predicted investment patterns under both profit-sharing rules are comparable. Partners developing new products in parallel, unlike the partners in a same-function alliance, commit fewer resources to their alliance. Further, the profit-sharing arrangement matters in such alliancespartners commit more resources when profits are shared proportionally rather than equally. We test the predictions of the model in two laboratory experiments. We find that the aggregate behavior of the subjects is accounted for remarkably well by the equilibrium solution. As predicted, profit-sharing arrangement did not affect the investment pattern of subjects in same-function alliances when they were in the high-reward condition. Subjects developing products in parallel invested less than subjects in same-function alliance, irrespective of the reward condition. We notice that theory seems to predict investments in low-reward conditions. Aplausible explanation for this departure from the normative benchmark is that subjects in the low-reward condition were influenced by altruistic regard for their partners. These experiments also clarify the support for the mixed strategy equilibrium: aggregate behavior conforms to the equilibrium solution, though the behavior of individual subjects varies substantially from the norm. Individual-level analysis suggests that subjects employ mixed strategies, but not as fully as the theory demands. This inertia in choice of strategies is consistent with learning trends observed in the investment pattern. A new analysis of Robertson and Gatignon's (1998) field survey data on the conduct of corporate partners in technology alliances is also consistent with our model of samefunction alliances. We extend the model to consider asymmetric distribution of endowments among partners in a same-function alliance. Then we examine the implication of extending the strategy space to include more levels of investment. Finally, we outline an extension of the model to consider cross-functional alliances.", "e:keyword": ["Strategic Alliances", "Experimental Economics", "Competitive Strategy", "Game Theory", "New Product Development"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.2.127.11805", "e:abstract": "The issue of power in the marketing channels for consumer products has received considerable attention in both academic and practitioner journals as well as in the popular press. Our objective in this paper is to provide an empirical method to measure the power of channel members and to understand the reasons (demand factors, cost factors, nature of channel interactions) for this power. We confine our analysis to pricing power in channels. We use methods from the game-theory literature in marketing on channel interactions to obtain the theoretical framework for our empirical model. This literature provides us a definition of powerone that is based on the proportion (or percentage) of channel profits that accrue to each of the channel members. There can be a variety of possible channel interactions between manufacturers and retailers in channels. The theoretical literature has examined some of these games. For example, Choi (1991) examines how channel profits for manufacturers and retailer vary if channel interactions are either vertical Nash, or if they are Stackelberg leaderfollower with either the manufacturer or the retailer being the price leader. Each of these three channel interaction games has different implications for profits made by manufacturers and retailers, and consequently for the relative power of the channel members. In contrast to the previous literature that has focused largely on the above three channel interaction games, our model extends the game-theoretic literature by allowing for a continuum of possible channel interactions between manufacturers and a retailer. Furthermore, for a given product market, we empirically estimate from the data where the channel interactions lie in this continuum. More critically, we obtain measures of how channel profits are divided between manufacturers and the retailer in the product market, where a higher share of channel profit is associated with higher channel power. We then examine how channel power is related to demand conditions facing various brands and cost parameters of various manufacturers. In going from game-theory-based theoretical models of channel interactions to empirical estimation, we use the new empirical industrial organization framework (Bresnahan 1988). As part of this structural modeling framework, we build retail-level demand functions for the various brands (manufacturer and private label) in a given product category. Given these demand functions, we obtain optimal pricing rules for manufacturers and the retailer. In determining their optimal prices, manufacturers and the retailer account for how all the players in the channel choose their optimal prices. That is, we account for dependencies in decision making across channel members. These dependencies are characterized by a set of conduct parameters, which are estimated from market data. The conduct parameters enable us to identify the nature of channel interactions between manufacturers and the retailer (along the continuum mentioned previously). In addition to the demand and conduct parameters, manufacturers' marginal costs are also estimated in the model. These marginal cost estimates, along with the manufacturer prices and retail prices available in our dataset, enable us to compute the division of channel profits among the channel members. Hence, we are able to obtain insights into who has pricing power in the channel. In the empirical application of the model, we analyze a local market for two product categories: refrigerated juice and tuna. In both categories, there are three major brands. The difference between them is that the private label has an insignificant market share in the tuna category. Our main empirical results show that the usual games examined in the marketing literature do not hold for the given data. We also .nd that the retailer's market power is very significant in both these product categories, and that the estimated demand and cost parameters are consistent with the estimated pattern of conduct between the manufacturers and the retailer. Given the evidence from the trade press of intense manufacturer competition in these categories, as well as the commodity nature of these products, the result of retailer power appears intuitive.", "e:keyword": ["Channel Power", "Private Labels", "Competitive Games"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.2.149.11807", "e:abstract": "This paper provides a method for nonparametrically modeling the relationship between consumer preference for product features, such as reliability or durability, and covariates that describe consumers and how they use the product. This relationship is of interest to firms designing and delivering products to a market because the extent to which consumers are sensitive to particular features determines the potential profitability of product offerings, and affects decisions relating to appropriate distribution outlets and advertising strategies. The successful identification of these relationships also aids in efficiently targeting marketing activities to specific segments of the consumer population. The relationship between consumer preference for product features and observable covariates is important but is typically unknown. In addition, these relationships are often deeply embedded in a model hierarchy and are not observed directly. For example, in models of household choice, the observed outcomes are multinomial with probabilities driven by latent utilities or values that consumers place on the choice alternatives. These utilities are in turn a function of characteristics, such as price and product features, which are differentially valued. Of primary interest is the relationship between consumer sensitivity to product characteristics and readily observed covariates such as household demographics or aspects of product usage. Because the relationships of interest are not directly observed, it is difficult to draw inferences about them without formal statistical models. This paper presents a three-level hierarchical Bayes model for modeling binary consumer preferences as a function of observable covariates. The hierarchical model nonparametrically estimates the relationships between consumer preferences for product features and the covariates without assuming a specific functional form. A nonparametric model is particularly useful in the exploratory analysis of consumer data in which the primary purpose of the analysis is to generate further questions rather than provide specific answers to well-posed questions. This type of analysis is frequently encountered in marketing where a series of studies are commissioned to better understand the nature of demand. The first level of the hierarchy in the Bayesian model relates the binary consumer choice to the sensitivities of the consumer to product attributes such as brand name, price, reliability, and durability. The second level of the hierarchy models the heterogeneity across consumers using functions that relate attribute sensitivities to observable covariates. This level of the hierarchy also allows each respondent to have unique demand coefficients by introducing random effect components. The third level of the hierarchy specifies a smoothness prior for each of the unknown functions used in the second level. The approach is flexible and works well both when the unknown function can be closely approximated by a linear function and when it cannot be. A Bayesian model selection technique is used to determine which functions can be modeled using a linear function and which ones should be modeled nonparametrically to provide the necessary flexibility to estimate the function accurately. The proposed methodology is illustrated using data from a survey of consumer preferences for features of marine outboard engines that was collected as part of a consulting project. Our analysis focuses on measuring consumer preferences for engine features and their relationships to two variables related to boat length and engine size. Consumer preferences for engine features were obtained through a national survey conducted over the telephone. Preferences were elicited by means of a pairwise evaluation in which respondents chose between two engines that were identical in every respect except for two engine features. The methodology can be modified to allow for more complex comparisons such as conjoint data collected in full profiles. The application of a Bayesian model selection procedure indicates that 4 of the 28 covariate relationships in the model are nonlinear, while the other 24 are linear. The preferences associated with these four functions are involved in 56% of the pairwise comparisons in the study. Therefore, in practice, if the nonlinear functions are not properly estimated there is the potential to draw misleading inferences regarding 56% of the pairwise choices. Firms can use the estimates of the functions relating preferences to covariates in a number of ways. First, they can use the covariates to determine the total number of consumers who have high demand for a particular product feature, and then they can target communication efforts to those individuals. Alternatively, the empirical results can be used as a basis of subsequent analysis to obtain a more complete characterization of a market segment.", "e:keyword": ["Consumer Preferences", "Cubic Smoothing Spline", "Hierarchical Bayes Model", "Markov Chain Monte Carlo", "Randan Effects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.2.163.11806", "e:abstract": "Manufacturers and distributors in marketing channels commonly establish prices, margins, and other trade terms through negotiations. These negotiations have significant impact on channel members' profit streams over the duration of the business relationship. We consider a situation where a manufacturer and an exclusive, independent distributor are negotiating the transfer (wholesale) price of a new product. The transfer price should lie between the manufacturer's production cost and the maximum resale price that the distributor can charge end consumers (consumers' reservation price). We assume that the negotiations occur in an incomplete and asymmetric information environment such that the manufacturer is uncertain about the consumers' reservation price, whereas the distributor knows it precisely because of proximity to the consumer. The negotiation is time-sensitive because of the threat of potential competitive entry. Both parties have identical opportunity costs of delay in reaching agreement. In this incomplete and asymmetric information environment, the negotiators must learn before they can reach agreement. However, each negotiator has an incentive to convince the other that the available surplus is smaller than it really is. Hence, a high (low) offer (counteroffer) has little credibility without opportunity costs of delay. For any given manufacturer offer, a distributor facing a low consumer reservation price has a small available surplus and therefore more incentive to delay agreement than if the price is high. Willingness to delay agreement and incur delay costs lends credibility to the price signal in an offer (counteroffer), providing a means for communicating credibly and facilitating agreement. Thus, with incomplete, asymmetric information and opportunity costs of delay, a signaling formulation with alternating offers and counteroffers captures key strategic characteristics of marketing channel negotiations. We adapt a game-theoretic model (Grossman and Perry 1986a, 1986b) to predict bargaining behavior and outcomes in this channel negotiation scenario. We derive both point predictions and directional implications from this sequential equilibrium (SE) bargaining model regarding how manufacturer uncertainty about distributor value (consumers' reservation price), opportunity cost of delay, and the actual reservation price (total surplus) should influence bargaining outcomes. The predictions are tested in two experiments. The point predictions serve as benchmarks against which we evaluate the observed bargaining outcomes, as we focus on testing the model's directional implications. We also explore the underlying bargaining process to assess the extent to which subjects conform to the SE signaling rationale in optimizing channel profits. Both experiments show that the point predictions of the SE model fall considerably short in describing bargaining behavior and outcomes. The players bargained suboptimally, took longer to agree, and could not extract the total available surplus. Nevertheless, the data are consistent with several directional predictions of the SE model. There is consistent support for the predicted directional effects of manufacturer uncertainty and consumer reservation prices. As expected, high uncertainty impeded efficient negotiation, eliciting high first offers from manufacturers and increasing bargaining duration. Also, higher reservation prices (higher surplus) lowered bargaining duration, increased bargaining efficiency, and raised profits for both parties. However, support for the predicted directional effects of opportunity cost of delay is mixed. Higher delay costs produced quicker agreements, but distributors did not benefit from their informational advantage. Although the directional results suggest that the SE model is a good representation of bargaining behavior, a closer analysis shows that the bargaining process data did not correspond to the specific signaling rationale of the SE model. Rather, these data suggest that the bargainers created simplified representations of the price negotiation and used heuristics to develop their offers and counteroffers. We observe two systematic patterns of deviations from the SE model. Some manufacturers may have used the counteroffer levels to infer the distributors' competitive stance and factored this into their responses. Thus, even though the distributor counteroffers carried signals of the consumer reservation price, the manufacturers delayed agreement because they either did not recognize the signal or thought it was unreliable. In other cases, the data are consistent with a simple, nonstrategic model (EMP) in which the manufacturer and the distributor divide the monetary payoff (surplus) equally. The results show that the effectiveness of signaling mechanisms depends not only on the economic characteristics of the bargaining situation, but also on shared individual and social contexts that influence how signals are transmitted and interpreted.", "e:keyword": ["Distribution Channels", "Margin Negotiation", "Sequential Bargaining", "Behavioral Game Theory", "Experimental Economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.2.185.11802", "e:abstract": "Recent work in marketing has drawn on behavioral decision theory to advance the notion that consumers evaluate attributes (and therefore choice alternatives) not only in absolute terms, but as from a reference point. The theory has important substantive and practical implications for the timing and execution of price promotions and other marketing activities. Choice modelers using scanner panel data have tested for the presence of these reference effects in consumer response to an attribute such as price. In applications of the theory of reference-dependent choice (Tversky and Kahneman 1991), some modelers report empirical evidence of loss aversion: When a consumer encounters a price above his or her established reference point (a loss), the response is greater than for a price below the reference point (a gain). Researchers have gone so far as to suggest that evidence for the so-called reference effect make it an empirical generalization in marketing (e.g., Kalyanaram and Winer 1995, Meyer and Johnson 1995). It is our contention that the measurement of loss aversion in empirical applications of the reference-dependent choice model is confounded by the presence of unaccounted-for heterogeneity in consumer price responsiveness. Our reasoning is that the kinked price response curve implied by loss aversion is confounded with the slopes of the response curves across segments that are differentially responsive to price. A more price-responsive consumer (with a steeper response function) tends to have a lower price level as a reference point. This consumer faces a larger proportion of prices above his reference point, thus the response curve is steeperin the domain of losses. Similarly, the less price-responsive consumer sees a greater proportion of prices below his reference point, so the response curve is less steep within the domain of gains. As a result, any cross-sectional estimate of loss aversion that does not take this into account will be biased upwardresearchers who do not control for heterogeneity in price responsiveness may arrive at incorrect substantive conclusions about the phenomenon. It is interesting to note that in this instance, failure to control for heterogeneity induces a bias in of finding an effect, rather than the more typical case of attenuation of the effect toward zero. We first test our assertion regarding the referencedependent model using scanner panel data on refrigerated orange juice and subsequently extend this analysis to 11 additional product categories. In all cases we find, as predicted, that accounting for price-response heterogeneity leads to lower and frequently nonsignificant estimates of loss aversion. We do, however, find some categories in which the effect does not disappear altogether. We also estimate loss aversion using a sticker shock model of brand choice in which the reference prices are . In line with the results of the majority of prior literature, we find smaller and insignificant estimates of loss aversion in this model. We show that this is because in the sticker shock model, there is no apparent correlation between the price responsiveness of the consumer and the representation of reference effects as losses or gains. Our findings strongly suggest that loss aversion may not in fact be a universal phenomenon, at least in the context of frequently purchased grocery products.", "e:keyword": ["Choice Models", "Reference Dependence", "Loss Aversion", "Sticker Shock", "Reference Price", "Empirical Generalization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.3.203.11801", "e:abstract": "An important product strategy for firms in mature markets is value-adding modifications to existing products. Marketing information that reveals consumers' preferences, buying habits, and lifestyle is critical for the identification of such product modifications. We consider two types of value-adding modifications that are often facilitated by marketing information: -type modifications that increase the attractiveness of a product to a firm's loyal customers, and -type modifications that allow a firm to increase the appeal of its product to a competitor's loyal customers. We examine two aspects of the markets for product modification information: (1) the manner in which retention and conquesting modifications affect competition between downstream firms, and (2) the optimal selling and pricing policies for a vendor who markets product modification information. We consider several aspects of the vendor's contracting problem, including how a vendor should package and target the information to the downstream firms and whether the vendor should limit the type of information that is sold. This research also examines when a vendor can gain by offering exclusivity to a firm. We address these issues in a model consisting of an information vendor facing two downstream firms that sell differentiated products. The model analyzes how information contracting is affected by differentiation in the downstream market and the quality of the information (in terms of how impactful the resulting modifications are). We analyze two possible scenarios. In the first, the information facilitates modifications that increase the appeal of products to the loyal customers of only one of the two downstream firms (i.e., one-sided information). In the second scenario, the information facilitates modifications that are attractive to the loyal consumers of both the firms (i.e., two-sided information). The effect of modifications on downstream competition depends on whether they are of the retention or the conquesting type. A retention-type modification increases the effective differentiation between the firms and softens price competition. Conquesting modifications, however, have benefits as well as associated costs. A conquesting modification of low impact reduces the effective differentiation between competing products and leads to increased price competition. However, when conquesting modifications are of sufficiently high impact, they also have the benefit of helping a firm to capture the customers of the competitor. The vendor's strategy for one-sided information always involves selling to one firm, the firm for which the modifications are of the retention type. When the identified modifications are of low impact, this result is expected because conquesting modifications are for downstream firms. However, even when the information identifies high-impact modifications (and positive profits are generated by selling the information as conquesting information), the vendor is strictly better off by targeting his information to the firm for which the modification is the retention type. With two-sided information, the equilibrium strategy is for the vendor to sell the complete packet of information (information on both retention and conquesting modifications) to both downstream firms. However, in equilibrium, both firms only implement retention-type modifications. The information on conquesting modifications is passive in the sense that it is never used by downstream firms. Yet the vendor makes strictly greater profit by including it in the packet. This obtains because the price charged for information depends critically on the situation an individual firm encounters by buying the information. The presence of conquesting information in the packet puts a nonbuyer in a worse situation, and this underlines the passive power of information. The vendor gains by including the conquesting information even though it is not used in equilibrium.", "e:keyword": ["Marketing of Information", "Information Packaging", "Selling Contracts", "Retention Modifications", "Conquesting Modifications", "Product Modifications", "Passive Power of Information"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.3.226.11796", "e:abstract": "In spite of the high financial stakes involved in marketing new motion pictures, marketing science models have not been applied to the market evaluation of motion pictures. The motion picture industry poses some unique challenges. For example, the consumer adoption process for movies is very sensitive to word-of-mouth interactions, which are difficult to measure and predict the movie has been released. In this article, we undertake the challenge to develop and implement MOVIEMODa prerelease market evaluation model for the motion picture industry. MOVIEMOD is designed to generate box-office forecasts and to support marketing decisions for a new movie after the movie has been produced (or when it is available in a rough cut) but before it has been released. Unlike other forecasting models for motion pictures, the calibration of MOVIEMOD does not require any actual sales data. Also, the data collection time for a product with a limited lifetime such as a movie should not take too long. For MOVIEMOD it takes only three hours in a consumer clinic to collect the data needed for the prediction of box-office sales and the evaluation of alternative marketing plans. The model is based on a behavioral representation of the consumer adoption process for movies as a macroflow process. The heart of MOVIEMOD is an interactive Markov chain model describing the macro-flow process. According to this model, at any point in time with respect to the movie under study, a consumer can be found in one of the following behavioral states: undecided, considerer, rejecter, positive spreader, negative spreader, and inactive. The progression of consumers through the behavioral states depends on a set of factors that are related to the marketing mix, as well as on a set of more general factors that characterize the movie-going behavior in the population of interest. This interactive Markov chain model allows us to account for word-of-mouth interactions among potential adopters and several types of word-of-mouth spreaders in the population. Marketing variables that influence the transitions among the states are movie theme acceptability, promotion strategy, distribution strategy, and the movie experience. The model is calibrated in a consumer clinic experiment. Respondents fill out a questionnaire with general items related to their movie-going and movie communication behavior, they are exposed to different sets of information stimuli, they are actually shown the movie, and finally, they fill outpostmovie evaluations, including word-of-mouth intentions.These measures are used to estimate the word-of-mouth parameters and other behavioral factors, as well as the movie-specific parameters of the model. MOVIEMOD produces forecasts of the awareness, adoption intention, and cumulative penetration for a new movie within the population of interest for a given base marketing plan. It also provides diagnostic information on the likely impact of alternative marketing plans on the commercial performance of a new movie. We describe two applications of MOVIEMOD: One is a pilot study conducted without studio cooperation in the United States, and the other is a full-fledged implementation conducted with cooperation of the movie's distributor and exhibitor in the Netherlands. The implementations suggest that MOVIEMOD produces reasonably accurate forecasts of box-office performance. More importantly, the model offers the opportunity to simulate the effects of alternative marketing plans. In the Dutch application, the effects of extra advertising, extra magazine articles, extra TV commercials, and higher trailer intensity (compared to the base marketing plan of the distributor) were analyzed. We demonstrate the value of these decision-support capabilities of MOVIEMOD in assisting managers to identify a final plan that resulted in an almost 50% increase in the test movie's revenue performance, compared to the marketing plan initially contemplated. Management implemented this recommended plan, which resulted in box-office sales that were within 5% of the MOVIEMOD prediction. MOVIEMOD was also tested against several benchmark models, and its prediction was better in all cases. An evaluation of MOVIEMOD jointly by the Dutch exhibitor and the distributor showed that both parties were positive about and appreciated its performance as a decision-support tool. In particular, the distributor, who has more stakes in the domestic performance of its movies, showed a great interest in using MOVIEMOD for subsequent evaluations of new movies prior to their release. Based on such evaluations and the initial validation results, MOVIEMOD can fruitfully (and inexpensively) be used to provide researchers and managers with a deeper understanding of the factors that drive audience response to new motion pictures, and it can be instrumental in developing other decision-support systems that can improve the odds of commercial success of new experiential products.", "e:keyword": ["Motion Pictures", "New Products", "Pretest Market Evaluation", "Forecasting", "Decision Support", "Markov Chains"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.3.244.11798", "e:abstract": "In practice, the rules in most open English auctions require participants to raise bids by a sizeable, discrete amount. Furthermore, some bidders are typically more aggressive in seeking to become the current bidder during competitive bidding. Most auction theory, however, has assumed bidders can place any tiny continuous bid increase, and recommend as optimal the tiniest possible increase. This article examines how incorporating discrete bidding and bidder aggressiveness affect optimal strategies for an important decision for auction sellers, which is setting the lowest acceptable bid at which to sell the property. We investigate two alternative methods sellers often use to enforce this decision. These are setting an irrevocable before the auction, and , where the seller or confederates pose as bona fide bidders and raise bona fide bids, unsuspected by bidders. These optimal strategies interest auction participants, especially sellers who must recognize the bidding rules and bidder aggressiveness they will encounter in actual auctions. We also examine how these strategies change with the auction context, such as the number of bidders, and how they differ from corresponding strategies already identified for continuous bidding. Our model examines open English auctions where bidders have independent, private valuations. We find that discrete bidding does affect these strategies, as does the aggressiveness of the bidder with the highest valuation, to the average aggressiveness of all other remaining bidders. We identify the seller's optimal discrete reserve, and show that if the highest valuator is relatively more (less) aggressive, this increases (decreases) from the optimal continuous reserve, and also increases (decreases) as the number of bidders increases. With continuous bidding, by contrast, this reserve is invariant to the number of bidders. As this bidder becomes relatively more aggressive, for a given number of bidders, the optimal discrete reserve increases, while as he or she becomes less aggressive, the seller's expected auction utility increases, which increases the set of auctions where discrete bidding generates higher seller welfare than continuous. We propose a covert shilling model that requires shilling sellers, and any confederates and auctioneers, to outwardly act no differently than with reserves, to avoid detection. We identify cases where the seller optimally shills once the bona fide bidding has stopped, and identify the corresponding optimal point to stop shilling and accept the next bona fide bid, if offered. This stopping point does not depend on where bona fide bidding stops, or aggressiveness, or the number of bidders, or on whether shill bids alternate with bona fide bids or are consecutively entered. We also find that the optimal lowest acceptable bid with shilling can be higher (lower) than that with reserves if the highest valuator is sufficiently unaggressive (aggressive). By comparison, in continuous bidding shilling and reserves yield identical lowest acceptable bids. Sometimes the seller using a shilling strategy optimally should not shill at all, and instead accept the bid where bona fide bidding stops. This can occur when that bid, or the number of bidders, is sufficiently high, or when the highest valuator is as, or less, aggressive than other bidders. Optimal shilling can be as practical to implement as reserves, because it does not require sellers to have any information beyond that needed in a reserve auction. If sellers shill optimally, they can never be worse off compared to using a reserve, and can be better off. Shilling can make bidders worse off, but can also make them better off when the seller using a shilling strategy optimally accepts bids below the optimal reserve. In these latter cases, shilling Pareto dominates reserves, ex ante. We provide numerical examples to illustrate these results. We discuss how our results might be affected if shilling is not covert, or bidders' valuations have a common value component rather than being independent, or by the rules used in many discrete bid Internet auctions.", "e:keyword": ["Auctions", "Internet Auctions", "Discrete Bidding", "Pricing", "Bidder Behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.3.266.11800", "e:abstract": "In a merchandise depth test, a retail chain introduces new products at a small sample of selected stores for a short period prior to the primary selling season and uses the observed sales to forecast demand for the entire chain. We describe a method for resolving two key questions in merchandise testing: (1) which stores to use for the test and (2) how to extrapolate from test sales to create a forecast of total season demand for each product for the chain. Our method uses sales history of products sold in a prior season, similar to those to be tested, to devise a testing program that would have been optimal if it had been applied to this historical sample. is defined as minimizing the cost of conducting the test, plus the cost of over- and understocking of the products whose supply is to be guided by the test. To determine the best set of test stores, we apply a -median model to cluster the stores of the chain based on a store similarity measure defined by sales history, and then choose one test store from each cluster. A linear programming model is used to fit a formula that is then used to predict total sales from test sales. We applied our method at a large retailer that specializes in women's apparel and at two major shoe retailers, comparing results in each case to the existing process used by the apparel retailer and to some standard statistical approaches such as forward selection and backward elimination. We also tested a version of our method in which clustering was based on a combination of several store descriptors such as location, type of store, ethnicity of the neighborhood of location, total store sales, and average temperature of the store location. We found that relative to these other methods, our approach could significantly improve forecasts and reduce markdowns that result from excessive inventory, and lost margins resulting from stockouts. At the apparel retailer the improvement was enough to increase profits by more than 100%. We believe that one reason our method outperforms the forward selection and backward elimination methods is that these methods seek to minimize squared errors, while our method optimizes the true cost of forecast errors. In addition, our approach, which is based purely on sales, outperforms descriptor variables because it is not always clear which are the best store descriptors and how best to combine them. However, the sales-based process is completely objective and directly corresponds to the retailer's objective of minimizing the understock and overstock costs of forecast error. We examined the stores within each of the clusters formed by our method to identify common factors that might explain their similar sales patterns. The main factor was the similarity in climate within a cluster. This was followed by the ethnicity of the neighborhood where the store is located, and the type of store. We also found that, contrary to popular belief, store size and location had little impact on sales patterns. In addition, this technique could also be used to determine the inventory allocation to individual stores within a cluster and to minimize lost demand resulting from inaccurate distribution across size. Finally, our method provides a logical framework for implementing micromerchandising, a practice followed by a significant number of retailers in which a unique assortment of merchandise is offered in each store (or a group of similar stores) tuned to maximize the appeal to customers of that store. Each cluster formed by our algorithm could be treated as a \"virtual chain\" within the larger chain, which is managed separately and in a consistent manner in terms of product mix, timing of delivery, advertising message, and store layout.", "e:keyword": ["Merchandise Testing", "Retailing", "Mathematical Programming"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.3.279.11799", "e:abstract": "We examine the problem of parallel imports: unauthorized flows of products across countries, which compete with authorized distribution channels. The traditional economics model of a discriminating monopolist that has different prices for the same good in different markets requires the markets to be separated in some way, usually geographically. The profits from price discrimination can be threatened by parallel imports that allow consumers in the high-priced region some access to the low-priced marketplace. However, as this article shows, there is a very real possibility that parallel imports may actually increase profits. The basic intuition is that parallel importation becomes another channel for the authentic goods and creates a new product version that allows the manufacturer to price discriminate. We propose a two-country, three-stage model to quantitatively study the effects and strategies. In the third stage, and in the higher priced country where parallel imports have entered, we characterize the resulting market segmentation. One segment of consumers stays with the authorized version as they place more value on the warranty and services that come with the authorized version. Another segment switches to parallel imports because a lower price is offered due to lack of country-specific features or warranties. Parallel imports also generate a third and new segment that would not have bought this product before. Unlike counterfeits that are fabricated by imitators, all parallel imports are genuine and sourced from the manufacturer in the lower-priced country through authorized dealers. Therefore, the manufacturer's global sales quantity should increase, but profit may rise or fall depending on the relative sizes and profitability of the segments. A profit-maximizing parallel importer should set price and quantity in the second stage after observing the manufacturer's prices in both countries. There will be a threshold of across-country price gap above which parallel imports would occur. In the first stage, the manufacturer can anticipate the possible occurrence of a parallel import, its price and quantity, and its effect on authorized sales in each country to make a coordinated pricing decision to maximize the global supply chain profit. Under some circumstances the manufacturer should allow parallel imports and under others should prevent them. Through a Stackelberg game we solve for the optimal pricing strategy in each scenario. We then find in one extension that when the number of parallel importers increases, the optimal authorized price gap should narrow, but the prices and quantities of parallel imports may rise or fall. In another extension, we .nd that when the manufacturer has other meanssuch as monitoring dealers, differentiating designs, and unbundling warrantiesto contain parallel imports, the authorized price gap can widen as a function of the effectiveness of nonpricing controls. In summary, parallel imports may help the manufacturer to extend the global reach of its product and even boost its global profit. If the manufacturer offers a discount version through its authorized dealers, it is running a high risk of confusing customers and tarnishing brand images. Parallel imports may cause similar concerns for the manufacturer, but unauthorized dealers are perceived as further removed from the manufacturer. Therefore, there is less risk of confusing consumers when parallel imports are channeled through unauthorized dealers. Furthermore, they are more nimble in diverting the product whenever their transshipment and marketing costs are small enough not to offset the authorized price gap and the valuation discount. This may explain why some manufacturers fiercely fight parallel imports, while others knowingly use this alternative channel.", "e:keyword": ["International Marketing", "Parallel Imports", "Pricing", "Intrabrand Competition", "Channel Conflict", "Stackelberg Game"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.4.297.11794", "e:abstract": "The number of brands in the marketplace has vastly increased in the 1980s and 1990s, and the amount of money spent on advertising has run parallel. Print advertising is a major communication instrument for advertisers, but print media have become cluttered with advertisements for brands. Therefore, it has become difficult to attract and keep consumers' attention. Advertisements that fail to gain and retain consumers' attention cannot be effective, but attention is not sufficient: Advertising needs to leave durable traces of brands in memory. Eye movements are eminent indicators of visual attention. However, what is currently missing in eye movementresearch is a serious account of the processing that takes place to store information in long-term memory. We attempt to provide such an account through the development of a formal model. We model the process by which eye fixations on print advertisements lead to memory for the advertised brands, using a hierarchical Bayesian model, but, rather than postulating such a model as a mere data-analysis tool, we derive it from substantive theory on attention and memory. The model is calibrated to eye-movement data that are collected during exposure of subjects to ads in magazines, and subsequent recognition of the brand in a perceptual memory task. During exposure to the ads we record the frequencies of fixations on three ad elements; brand, pictorial and text and, during the memory task, the accuracy and latency of memory. Thus, the available data for each subject consist of the frequency of fixations on the ad elements and the accuracy and the latency of memory. The model that we develop is grounded in attention and memory theory and describes information extraction and accumulation during ad exposure and their effect on the accuracy and latency of brand memory. In formulating it, we assume that subjects have different eye-fixation rates for the different ad elements, because of which a negative binomial model of fixation frequency arises, and we specify the influence of the size of the ad elements. It is assumed that the number of fixations, not their duration, is related to the amount of information a consumer extracts from an ad. The information chunks extracted at each fixation are assumed to be random, varying across ads and consumers, and are estimated from the observed data. The accumulation of information across multiple fixations to the ad elements in long-term memory is assumed to be additive. The total amount of accumulated information that is not directly observed but estimated using our model influences both the accuracy and latency of subsequent brand memory. Accurate memory is assumed to occur when the accumulated information exceeds a threshold that varies randomly across ads and consumers in a binary probit-type of model component. The effect of two media-planning variables, the ad's serial position in a magazine and the ad's location on the double page, on the brand memory threshold are specified. We formulate hypotheses on the effects of ad element surface, serial position, and location. The model is applied in a study involving a sample of 88 consumers who were exposed to 65 print ads appearing in their natural context in two magazines. The frequency of eye fixations was recorded for each consumer and advertisement with infrared eye-tracking methodology. In a subsequent indirect memory task, consumers identified the brands from pixelated images of the ads. Across the two magazines, fixations to the pictorial and the brand systematically promote accurate brand memory, but text fixations do not. Brand surface has a particularly prominent effect. The more information is extracted from an ad during fixations, the shorter the latency of brand memory is. We find a systematic recency effect: When subjects are exposed to an ad later, they tend to identify it better. In addition, there is a small primacy effect. The effect of the ad's location on the right or left of the page depends on the advertising context. We show how the model supports advertising planning and testing and offer recommendations for further research on the effectiveness of brand communication. In future research the model may be extended to accommodate the effects of repeated exposure to ads, to further detail the representation of strength and association of memory, and to include the effects of creative tactics and media planning variables beyond the ones we included in the present study.", "e:keyword": ["Brand Advertising", "Visual Attention", "Brand Memory", "Hierarchical Bayes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.4.313.11790", "e:abstract": "How should a firm decide whether or not to enter an untested market when a competing firm is vying for the same market? Should a firm always speed to the market in an effort to capitalize on pioneering advantages? We address those questions by developing a simple game-theoretical model that captures the most essential factors in a firm's market entry decision, such as market uncertainty, firm heterogeneity, competition, cannibalization, and order-of-entry effects. Our analysis shows that in a competitive context, both pioneering advantages and laggard's disadvantages can motivate a firm to speed to an untested market. Therefore, pioneering advantages alone are not an adequate guide for a firm to formulate its market entry strategy. The optimal decision may call for a firm to be a prudent laggard when pioneering advantages to the firm are substantial, or to become a market pioneer when facing pioneering disadvantages. We characterize different patterns of market entry as equilibrium outcomes for different configurations of the market reward structure and offer a conceptual framework for formulating market entry strategies that go beyond the conventional dichotomy: or . We show that the paradoxical phenomenon of disadvantaged pioneers can arise in a competitive context as the outcome of rational firms making rational choices. To show that pioneering advantages alone are not the right litmus test for market entry decisions, we apply our general framework to a concrete case where consumer preference or the premium that consumers are willing to pay for the pioneering brand gives rise to pioneering advantages and laggard's disadvantages. We conclude that the firm with a larger pioneering premium may choose to wait, while a firm with a smaller pioneering premium speeds to the market. Our analysis also sheds light on empirical research on pioneering advantages. Because firms may race into a market solely to avoid laggard's disadvantages rather than to capture pioneering advantages, pioneers are not necessarily the firms best positioned to establish, exploit, and maintain pioneering advantages. Therefore, it is not surprising that a significant percentage of pioneers fail, as documented by recent empirical research. Our normative investigation further suggests that this predicament in empirical research will not disappear even if we have complete data, use the right measurements, and employ perfect statistical techniques. Therefore, it is perhaps more fruitful to redirect our research effort in the search for pioneering advantages. Finally, we extend our analysis to incorporate the effect of cannibalization on an incumbent firm's market entry strategy. We conclude that cannibalization can motivate an incumbent firm to wait, as the conventional wisdom suggests, but it can also be an impetus for a firm to become a market pioneer. We offer supporting evidence for our analysis and discuss managerial implications of our conclusions.", "e:keyword": ["New Product Entry", "Competitive Strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.4.328.11789", "e:abstract": "Structural equation models are widely used in marketing and psychometric literature to model relationships between unobserved constructs and manifest variables and to control for measurement error. Most applications of structural equation models assume that data come from a homogeneous population. This assumption may be unrealistic, as individuals are likely to be heterogeneous in their perceptions and evaluations of unobserved constructs. In addition, individuals may exhibitdifferent measurement reliabilities. It is well-known in statistical literature that failure to account for unobserved sources of individual differences can resultin misleading inferences and incorrect conclusions. We develop a hierarchical Bayesian framework for modeling general forms of heterogeneity in partially recursive structural equation models. Our framework elucidates the motivations for accommodating heterogeneity and illustrates theoretically the types of misleading inferences that can result when unobserved heterogeneity is ignored. We describe in detail the choices that researchers can make in incorporating different forms of measurement and structural heterogeneity. Current random-coefficient models in psychometric literature can accommodate heterogeneity solely in mean structures. We extend these models by allowing for heterogeneity both in mean and covariance structures. Specifically, in addition to heterogeneity in measurement intercepts and factor means, we account for heterogeneity in factor covariance structure, measurement error, and structural parameters. Models such as random-coefficient factor analysis, random-coefficientsecond-order factor analysis, and random-coefficient, partially recursive simultaneous equation models are special cases of our proposed framework. We also develop Markov Chain Monte Carlo (MCMC) procedures to perform Bayesian inference in partially recursive, random-coefficient structural equation models. These procedures provide individual-specific estimates of the factor scores, structural coefficients, and other model parameters. We illustrate our approach using two applications. The first application illustrates our methods on synthetic data, whereas the second application uses consumer satisfaction data involving measurements on satisfaction, expectation disconfirmation, and performance variables obtained from a panel of subjects. Our results from the synthetic data application show that our Bayesian procedures perform well in recovering the true parameters. More importantly, we find that models that ignore heterogeneity can yield a severely distorted picture of the nature of associations among variables and can therefore generate misleading inferences. Specifically, we find that ignoring heterogeneity can result in inflated estimates of measurement reliability, wrong signs of factor covariances, and can yield attenuated model fit and standard errors. The results from the consumer satisfaction study show that individuals vary both in means and covariances and indicate that conventional psychometric methods are not appropriate for our data. In addition, we find that heterogeneous models outperform the standard structural equation model in predictive ability. Managerially, we show how one can use the individual-level factor scores and structural parameter estimates from the Bayesian approach to perform quadrantanalysis and refine marketing policy (e.g., develop a one-on-one marketing policy). The framework introduced in this paper and the inference procedures we describe should be of interest to researchers in a wide range of disciplines in which measurement error and unobserved heterogeneity are problematic. In particular, our approach is suitable for studies in which panel data or multiple observations are available for a given set of respondents or objects (e.g., firms, organizations, markets). At a practical level, our procedures can be used by managers and other policymakers to customize marketing activities or policies. Future research should extend our procedures to deal with the general nonrecursive structural equation model and to handle binary and ordinal data situations.", "e:keyword": ["Structural Equation Models", "Heterogeneity", "Hierarchical Bayes", "MCMC Procedures", "Metropolis-Hastings Algorithm", "Gibbs Sampling", "Customer Satisfaction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.4.348.11792", "e:abstract": "Academic work on sales compensation plans features agency models prominently, and these models have also been used to build decision aids for managers. However, empirical support remains sketchy. We conducted three experiments to investigate three unresolved predictions involving the incentive-insurance trade-off posited in the model. First, compensation should be less incentive loaded with greater effort-output uncertainty so as to provide additional insurance to a risk-averse agent. Second, flat wages should be used for verifiable effort so as to avoid unnecessary incentives. Third, less incentive-loaded plans should be used with more risk-averse agents so as to provide additional insurance. Our design implemented explicit solutions from a specific agency model, which offers greater internal validity, compared to extant laboratory designs that either did not implement explicit solutions or excluded certain parameters. In Experiment I, data from working manager subjects supported the first prediction but only when risk-averse agents undertook nonverifiable effort. We interpret this as disclosing the model's core circumstance, wherein it orders the data when the incentive-insurance trade-off is relevant. Thus, when verifiable effort made incentives moot, as is the case for the second prediction, the model failed to order the data. Building on these results, we reasoned that the third prediction should find support among risk-averse agents but not among risk-neutral agents, because insurance is a moot point with the latter agents. To this end, we added risk-neutral utility functions for agents in Experiment II. Data from MBA-candidate student subjects supported the predictions, but only when risk-averse agents undertook nonverifiable effort. In those cells in which the incentive-insurance trade-off was moot (either because of risk-neutrality or else verifiability), the data did not support the predictions. We confronted several validity threats to these results. To begin, Experiment I used the standard agency solution, which equalizes an agent's expected utility from the predicted plan with his expected utility from rejecting it. Subjects might have broken these ties on such grounds as fairness. To assess whether this confounded the results, we derived new solutions in Experiment II that broke ties in favor of the predicted plan (by a 10% margin in the expected utility). Our results were robust to this change. Second, our agents' behavior in Experiments I and II was much more consistent with predictions, compared to the principals' behavior, which broughtup task comprehension as a validity threat because our principals faced a more complex experimental task than the agents. To address this threat, we used three decision rounds in Experiment III to reduce the principals' task comprehension problems. A related validity threat arose from the relatively small gap in some cells between a principal's predicted expected utility and the principal's next best choice. To address this threat, we derived new solutions with larger gaps to make the principal's choices easier. The results were again robust to these changes, which removes these validity threats. We also addressed two alternative explanations. Might principals be predisposed to pick salary plus commission plans regardless of the model's predictions? If so, we should find such plans chosen uniformly across different experimental conditions. Pooling the data from our three experiments, we rejected this predisposition explanation by finding variation that was more consistent with treatment differences across cells. Second, mightagents choose higher effort levels because of a demand bias? If so, we should find agents picking high effort regardless of the plan actually offered to them. Using pooled data, we rejected this explanation by finding variation that was more consistent with a utility-maximizing reaction to the plan actually offered to them. Finally, we included manipulation checks to assess whether principals and agents perceived experimental stimuli identically, as per the common knowledge assumption in game theory. These data showed no differences between agents' and principals' perceptions of stimuli. Our experiments move the literature from simply asking whether the model works to pinpointing the circumstances in which itorders behavior. The primary stylized fact we uncovered is the persistent and striking lack of support for the agency model outside of the circumstance in which riskaverse agents undertake nonverifiable effort. The model's failure when there is no material insurance-incentive tradeoff deserves scrutiny in future work.", "e:keyword": ["Experimental Economics", "Agency Theory", "Sales Compensation", "Salesforce"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.4.366.11795", "e:abstract": "It is a popular contention that products launched today diffuse faster than products launched in the past. However, the evidence of diffusion acceleration is rather scant, and the methodology used in previous studies has several weaknesses. Also, little is known about why such acceleration would have occurred. This study investigates changes in diffusion speed in the United States over a period of 74 years (19231996) using data on 31 electrical household durables. This study defines diffusion speed as the time it takes to go from one penetration level to a higher level, and it measures speed using the slope coefficient of the logistic diffusion model. This metric relates unambiguously both to speed as just defined and to the empirical growth rate, a measure of instantaneous penetration growth. The data are analyzed using a single-stage hierarchical modeling approach for all products simultaneously in which parameters capturing the adoption ceilings are estimated jointly with diffusion speed parameters. The variance in diffusion speed across and within products is represented separately but analyzed simultaneously. The focus of this study is on description and explanation rather than forecasting or normative prescription. There are three main findings. 1. On average, there has been an increase in diffusion speed that is statistically significant and rather sizable. For the set of 31 consumer durables, the average value of the slope parameter in the logistic model's hazard function was roughly 0.48, increasing with 0.09 about every 10 years. It took an innovation reaching 5% household penetration in 1946 an estimated 13.8 years to go from 10% to 90% of its estimated maximum adoption ceiling. For an innovation reaching 5% penetration in 1980, that time would have been halved to 6.9 years. This corresponds to a compound growth rate in diffusion speed of roughly 2% between 1946 and 1980. 2. Economic conditions and demographic change are related to diffusion speed. Whether the innovation is an expensive item also has a sizable effect. Finally, products that required large investments in complementary infrastructure (radio, black and white television, color television, cellular telephone) and products for which multiple competing standards were available early on (PCs and VCRs) diffused faster than other products once 5% household penetration had been achieved. 3. Almost all the variance in diffusion speed among the products in this study can be explained by (1) the systematic increase in purchasing power and variations in the business cycle (unemployment), (2) demographic changes, and (3) the changing nature of the products studied (e.g., products with competing standards appear only late in the data set). After controlling for these factors, no systematic trend in diffusion speed remains unaccounted for. These findings are of interest to researchers attempting to identify patterns of difference and similarity among the diffusion paths of many innovations, either by jointly modeling the diffusion of multiple products (as in this study) or by retrospective meta-analysis. The finding that purchasing power, demographics, and the nature of the products capture nearly all the variance is of particular interest. Specifically, one does not need to invoke unobserved changes in tastes and values, as some researchers have done, to account for long-term changes in the speed at which households adopt new products. The findings also suggest that new product diffusion modelers should attempt to control not only for marketing mix variables but also for broader environmental factors. The hierarchical model structure and the findings on the systematic variance in diffusion speed across products are also of interest to forecasting applications when very little or no data are available.", "e:keyword": ["Diffusion", "New Product Research", "Empirical Generalizations", "Hierarchical Models", "Multilevel Analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.4.381.11793", "e:abstract": "With the increase in new product introductions in consumer packaged goods categories, supermarkets are reluctant to accept new products. Therefore, it is very important for manufacturers to convince retailers of the high-demand potential of their products. We study how a high-demand manufacturer can use advertising, slotting allowances, and wholesale prices to signal its high demand to retailers. Specifically, we examine the relative importance of advertising and slotting allowance in signaling demand. That is, when is it optimal for the manufacturer to use high advertising support, and when is it optimal for it to offer slotting allowance as a signal of its demand? We show that when a high-demand manufacturer is trying to signal its demand to retailers, advertising and slotting allowance are partial substitutes of one another in the sense that the manufacturer can increase one in order to compensate for a reduction in the other. We find that the high-demand manufacturer's signaling strategy depends on three factors: the retailer's stocking costs, the intensity of retail competition, and the advertising response rate in the given product market. We begin with a model of one manufacturer dealing with one retailer. The manufacturer has private information about the potential demand for its new product. The retailer is uncertain about the likely demand of the new product and is willing to accept the product only if it is convinced that the demand is high. We characterize the high-demand manufacturer's separating equilibrium strategies. We find that the slotting allowance plays an important role in signaling when the retailer's stocking costs are high and the advertising effectiveness is low. On the other hand, the manufacturer does not offer any slotting allowance, and advertising plays a bigger role when the stocking costs are low or the advertising effectiveness is high. We then examine the effects of retail competition on the manufacturer strategy. We find that the slotting allowance plays a more important role when the retail level competition is very intense. The manufacturer may have to offer a positive slotting allowance even in the absence of retailers' demand uncertainty when the retail competition is sufficiently intense. This result shows that the slotting allowance may have an important role to play even in the absence of signaling or screening considerations. Thus, our analysis of competitive setting provides an alternative explanation for slotting allowances. It also offers support to the views of many retailers who believe that slotting allowances can help retailers recover high stocking costs in highly competitive retail markets. In the presence of retailers' demand uncertainty, the manufacturer offers a higher slotting allowance in order to signal its high demand. We also investigate the effect of retailer's uncertainty about the effectiveness of the manufacturer's advertising. We show that if the high-demand manufacturer also has a higher advertising response rate, the manufacturer provides even higher advertising support to alleviate the retailer's advertising-related uncertainty. By increasing the advertising support, the manufacturer credibly tells the retailer; that it would not be optimal for the manufacturer to provide such high advertising support unless it had high enough advertising effectiveness.", "e:keyword": ["Channels of Distribution", "Game Theory", "New Product Introductions", "Signaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.19.4.390.11788", "e:abstract": "The objective of this paper is to investigate the firm's optimal advertising and pricing strategies when introducing a new product. We extend the existing signaling literature on advertising spending and price by constructing a model in which advertising is used both to raise awareness about the product and to signal its quality. By comparing the complete information game and the incomplete information game, we find that the high-quality firm will reduce advertising spending and increase price from their respective complete information levels. In the separating equilibrium, the high-quality firm will actually spend less on advertising than the low-quality firm, resulting in a negative correlation between product quality and advertising spending. What sets our analysis apart from previous studies is that we consider advertising spending not only as a signaling device but also as an informational device. When advertising spending is just a signaling device, it is purely a dissipative expense. It can be an effective signal of quality because only the high-quality firm can afford it; thus, consumers can infer the product's quality by its advertising spending. In this case, advertising spending and product quality are positively correlated. However, when advertising also serves the purpose of raising awareness, it endogenizes the size of the market for the firm, so it is not just a dissipative expense any more. Consider the low-quality firm's mimicking strategy in this case. When the low-quality firm is believed to be a highquality one, it can charge a much higher price than if its true quality were known. Given that its marginal cost is lower than the high-quality firm's, its profit margin will be much larger in mimicry than in revealing its true quality. Indeed, its profit margin will be even greater than the high-quality firm's. Therefore, the low-quality firm in mimicry has a strong incentive to increase its advertising spending from its optimal level when its true quality is known. To deter the low-quality firm's mimicking tendency, the high-quality firm should decrease its advertising spending so that mimicry is not as appealing to the low-quality firm as revealing its true quality. Indeed, the high-quality firm should reduce its advertising spending so much that it advertises less than the low-quality firm in equilibrium Many have interpreted signaling as burning money or throwing money down the drain. In the case of advertising, the claim is that its purpose is simply to show consumers that the firm can afford to squander money on advertising to signal its quality. Hence, the advertising content need not be informative. However, our results show that simply burning money is not enough to signal quality. How the money is burned is also important. When advertising raises awareness as well as signals quality, saving money rather than burning money is the correct signaling approach, although ultimately the high-quality firm will sacrifice some profit by reducing its market size. The intuition behind this result is that when information is incomplete, the high-quality firm cannot fully exploit its advantages. Whenever its advantages in quality and/or marginal costs are lessened, a firm will want to spend less on advertising.", "e:keyword": ["New Products", "Advertising Strategies", "Pricing", "Signaling Game", "Separating Equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.1.1.10197", "e:abstract": "Although price promotions have increased in both commercial use and quantity of academic research over the last decade, most of the attention has been focused on their effects on brand choice and brand sales. By contrast, little is known about the conditions under which price promotions expand short-run and long-run category demand, even though the benefits of category expansion can be substantial to manufacturers and retailers alike. This paper studies the category-demand effects of consumer price promotions across 560 consumer product categories over a 4-year period. The data describe national sales in Dutch supermarkets and cover virtually the entire marketing mix, i.e., prices, promotions, advertising, distribution, and new-product activity. We focus on the estimation of main effects (i.e., the dynamic category expansive impact of price promotions) as well as the moderating effects of marketing intensity and competition (both conduct and structure) on short- and long-run promotional effectiveness. The research design uses modern multivariate time-series analysis to disentangle short-run and long-run effects. First, we conduct a series of unit-root tests to determine whether or not category demand is stationary or evolving over time. The results are incorporated in the specification of vector-autoregressive models with exogenous variables (VARX models). The impulse-response functions derived from these VARX models provide estimates of the short- and long-term effects of price promotions on category demand. These estimates, in turn, are used as dependent variables in a series of second-stage regressions that assess the explanatory power of marketing intensity and competition. Several model validation tests support the robustness of the empirical findings. We present our results in the form of empirical generalizations on the main effects of price promotions on category demand in the short and the long run and through statistical tests on how these effects change with marketing intensity and competition. The findings generate an overall picture of the power and limitations of consumer price promotions in expanding category demand, as follows. Category demand is found to be predominantly stationary, either around a fixed mean or a deterministic trend. Although the total net short-term effects of price promotions are generally strong, with an average elasticity of 2.21 and a more conservative median elasticity of 1.75, they rarely exhibit persistent effects. Instead, the effects dissipate over a time period lasting approximately 10 weeks on average, and their long-term impact is essentially zero. By contrast, the successful introduction of new products into a category is more frequently associated with a permanent category-demand increase. Several moderating effects on price-promotion effectiveness exist. More frequent promotions increase their effectiveness, but only in the short run. The use of nonprice advertising reduces the category-demand effects of price promotions, both in the short run and in the long run. Competitive structure matters as well: The less oligopolistic the category, the smaller the short-run effectiveness of price promotions. At the same time, we find that the dominant form of competitive reaction, either in price promotion or in advertising, is no reaction. Short-run category-demand effectiveness of price promotions is lower in categories experiencing major new-product introductions. Finally, both the short- and long-run price promotion effectiveness is higher in perishable product categories. The paper discusses several managerial implications of these empirical findings and suggests various avenues for future research. Overall, we conclude that the power of price promotions lies primarily in the preservation of the status quo in the category.", "e:keyword": ["Category Demand", "Empirical Generalizations", "Long-Term Promotion Effects", "Competitive Strategy", "Marketing Mix", "Econometric Models", "Time-Series Analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.1.23.10201", "e:abstract": "Our research investigates the competitive ramifications of individual marketing and information management in today's information-intensive marketing environments. The specific managerial issues we address are as follows. First, what kinds of incentive environments do competing firms face when they can only target individual customers imperfectly? Second, does the improvement in an industry's targetability intensify price competition in the industry such that all competing firms become worse off? Third, should a firm share its customer knowledge so as to improve its rival's targetability? Fourth, how should an information vendor sell its information that can improve a firm's targetability? Finally, do competing firms have the same incentives to invest in their own targetability? To answer those questions, we develop a simple model  la Narasimhan (1988), in which each of the two competing firms have their own loyal customers and compete for common switchers. We assume that each firm can classify its own loyal customers and switchers correctly only with a less-than-perfect probability. This means that each firm's perceived customer segmentation differs from the actual customer segmentation. Based on their perceived reality, these two competing firms engage in price competition. As an extension, we also allow the competing firms to make their investment decisions to acquire targetability. We show that when individual marketing is feasible, imperfect, improvements in targetability by either or both competing firms can lead to win-win competition for both even if both players behave noncooperatively and the market does not expand. Win-win competition results from the fact that as a firm becomes better at distinguishing its price-insensitive loyal customers from the switchers, it is motivated to charge a higher price to the former. However, due to imperfect targetability, each firm mistakenly perceives some price-sensitive switchers as price-insensitive loyal customers and charges them all a higher price. These misperceptions thus allow its competitors to acquire those mistargeted customers without lowering their prices and, hence, reduce the rival firm's incentive to cut prices. This effect softens price competition in the market and qualitatively changes the incentive environment for competing firms engaged in individual marketing. A prisoner's dilemma occurs only when targetability in a market reaches a sufficiently high level. This win-win perspective on individual marketing has many managerial implications. First, we show that superior knowledge of individual customers can be a competitive advantage. However, this does not mean that a firm should always protect its customer information from its competitors. To the contrary, we find that competing firms can all benefit from exchanging individual customer information with each other at the nascent stage of individual marketing, when firms' targetability is low. Indeed, under certain circumstances, a firm may even find it profitable to give away this information unilaterally. However, as individual marketing matures (as firms' targetability becomes sufficiently high), further improvements in targetability will intensify price competition and lead to prisoner's dilemma. Therefore, it is not only prudent politics but also a business imperative for an industry to seize the initiative on the issue of protecting customer privacy so as to ensure win-win competition in the industry. Second, we show that the firm with a larger number of loyal customers tends to invest more in targetability when the cost of acquiring targetability is high. However, the firm with a smaller loyal base can, through information investment, acquire a higher level of targetability than the firm with a larger loyal base as long as the cost of acquiring targetability is not too high. As the cost further decreases, competing firms will all have more incentives to increase their investments in targetability until they achieve the highest feasible level. Third, an information vendor should make its information available nonexclusively (exclusively) when its information is associated with a low (high) level of targetability. When the vendor does sell its information exclusively, it should target a firm with a small loyal following if it can impart a high level of targetability to that firm. Finally, our analysis shows that an information-intensive environment does not doom small firms. In fact, individual marketing may provide a good opportunity for a small firm to leapfrog a large firm. The key to leapfrogging is a high level of targetability or customer knowledge.", "e:keyword": ["One-to-One Marketing", "Information Strategy", "Price Discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.1.42.10196", "e:abstract": "In a competitive marketplace, the effectiveness of any element of the marketing mix is determined not only by its absolute value, but also by its relative value with respect to the competition. For example, the effectiveness of a price cut in increasing demand is critically related to competitors' reaction to the price change. Managers therefore need to know the nature of competitive interactions among firms. In this paper, we take a theory-driven empirical approach to gain a deeper understanding of the competitive pricing behavior in the U.S. auto market. The ability-motivation paradigm posits that a firm needs both the ability and the motivation to succeed in implementing a strategy (Boulding and Staelin 1995). We use arguments from the game-theoretic literature to understand firm motivation and abilities in different segments of the auto market. We then combine these insights from the game-theoretic literature and the ability-motivation paradigm to develop hypotheses about competition in different segments of the U.S. auto market. To test our hypotheses of competitive behavior, we estimate a structural model that disentangles the competition effect from the demand and cost effects on prices. The theory of repeated games predicts that firms with a long-run profitability objective will try to sustain cooperative pricing behavior as a stable equilibrium when conditions permit. For example, markets with high concentration and stable market environments are favorable for sustaining cooperative behavior and therefore provide firms with the to cooperate. The theory of switching costs suggests that in markets in which a firm's current customers tend to be loyal, firms have a to compete very aggressively for new customers, recognizing the positive benefits of loyalty from the customer base in the long run. As consumer loyalty in the market increases, the gains from increasing market share by means of aggressive competitive behavior are more than offset by losses in profit margins. Firms therefore have the to price cooperatively. Empirically, we find aggressive behavior in the minicom-pact and subcompact segments, cooperative behavior in the compact and midsize segments, and Bertrand behavior in the full-size segment. These findings are consistent with our theory-based hypotheses about competition in different segments. In estimating a structural model of the auto market, we address several methodological issues. A particular difficulty is the large number of car models in the U.S. auto market. Existing studies have inferred competitive behavior only in markets with two to four products. They also use relatively simple functional forms of demand to facilitate easy estimation. Functional forms of demand, however, impose structure on cross-elasticities between products. Such structure, when inappropriate, can bias the estimates of competitive interaction. We therefore use the random coefficients logit demand model to allow flexibility in cross-elasticities. We also use recent advances in New Empirical Industrial Organization (NEIO) to extend structural estimation of competitive behavior to markets with a large number of products. We use the simulation-based estimation approach developed by Berry et al. (1995) to estimate our model. A frequent criticism of the NEIO approach is that its focus on industry-specific studies limits the generalizability of its findings. In this study, we retain the advantages of NEIO methods but partially address the issue of generalizability by analyzing competitive behavior in multiple segments within the auto industry to see whether there is a consistent pattern that can be explained by theory. Theoretical modelers can use our results to judge the appropriateness of their models in predicting competitive outcomes for the markets that they analyze. A by-product of our analysis is that we also get estimates of demand and cost apart from competitive interactions for the market. Managers can use these estimates to perform what-if analysis. They can answer questions about what prices to charge when a new product is introduced or when an existing product's characteristics are changed.", "e:keyword": ["Auto Market", "Competition", "Structural Models", "New Empirical Industrial Organization", "Game Theory", "Ability-Motivation Paradigm"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.1.61.10199", "e:abstract": "This paper is concerned with how retailers, supermarkets in particular, communicate price discounts and use unadvertised specials. A common practice for supermarkets is to communicate price deals on some products through newspaper advertisements, while communicating discounts on other products through in-store mechanisms such as shelf-talkers. This raises the question: So far as store choice is concerned, how might consumers take into account not only advertised prices at competing stores, but also expected prices of unadvertised goods? It also begs the question of why stores have unadvertised specials since their effect on store choice is not quite the same as the advertised discounts. Further, competing supermarkets advertise the same products part of the time, and different products at other times. They also tend to sometimes advertise a product in consecutive weeks, but sometimes not. Can these actions be part of a strategy? We formulate a game-theoretic model of retail competition by first extending the work of Lal and Matutes (1994) and then developing an alternative framework to answer these questions. Our model has two retailers, each of whom carries two goods. To simplify exposition, we assume that the stores are symmetric, the two goods are symmetric in their reservation prices, and are neither substitutes nor complements. Consumers are identical in their preferences and consumer heterogeneity is in the convenience that each store presents to a representative consumer. The stores may advertise the price of one good, reflecting the reality that stores do not advertise their whole assortment. They compete through advertising and prices to maximize profits. We thus recognize the strategic role of advertised prices and furthermore, we investigate the strategic role of unadvertised prices in retail competition. For this model, we derive a Rational Expectations Nash equilibrium in which each store randomly advertises the price of one good following a mixed strategy. Consumer expectations of the prices of the unadvertised goods are rational. We obtain three kinds of results. First, unadvertised specials occur in equilibrium, and induce temporal and cross-sectional variation in the identity of advertised goods, consistent with casual observation. In this equilibrium, the two stores advertise the same good part of the time and different goods at other times. When they advertise the same good they do not offer any unadvertised discount on the other good. However, when they advertise different goods, they offer an unadvertised discount on the good that they do not advertise. Intuitively, unadvertised discounts come about because stores randomize the identity of the advertised good in the mixed strategy equilibrium. If retailers were to advertise the same good at all times, they would have to compete intensely for store traffic and therefore discount the advertised good very deeply. And, having done so, they would find it optimal to set the unadvertised good at the reservation price and offer no discount on it. However, if stores randomize the advertised good as shown in this paper, both stores advertise the same good some of the time and at other times they advertise different goods. Because they advertise different goods some of the time, they do not fight intensely for store traffic on just one good, but rather they find it optimal to offer a discount on the unadvertised good also. As a result, an implication of our equilibrium for consumer choice is that unadvertised discounts affect store choice, and in equilibrium some consumers may shop around. Second, we obtain managerial insights into the role of unadvertised specials. They affect store choice, prevent consumer shopping around either fully or partly, and reduce head-to-head competition on the price of the advertised good. The most salient strategic implication of retailers' offering unadvertised discounts is to reduce competition among stores, and this is again due to the randomization strategy of the stores. In fact, stores can reduce head-to-head competition further by increasing the number of products in their assortment and randomizing on the advertised good from this assortment. Third, we provide a resolution of the Diamond (1971) paradox, which says that prices at competing stores approach the monopoly price. In our equilibrium, expected prices of both advertised and unadvertised goods are always below the monopoly price.", "e:keyword": ["Retailing", "Supermarkets", "Advertising", "Pricing", "Unadvertised Specials", "Competition", "Game Theory", "Consumer Choice", "Rational Expectations", "Diamond Paradox"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.1.82.10195", "e:abstract": "Sellers who plan to capitalize on the lifetime value of customers need to manage the sales potential from customer referrals proactively. To encourage existing customers to generate referrals, a seller can offer exceptional value to current customers through either excellent quality or a very attractive price. Rewards to customers for referring other customers can also encourage referrals. We investigate when referral rewards should be offered to motivate referrals and derive the optimal combination of reward and price that will lead to the most profitable referrals. We define a delighted customer as one who obtains a positive level of surplus above a threshold level and, consequently, recommends the product to another customer. We show that the use of referral rewards depends on how demanding consumers are before they are willing to recommend (i.e., on the delight threshold level). The optimal mix of price and referral reward falls into three regions: (1) When customers are easy to delight, the optimal strategy is to lower the price below that of a seller who ignores the referral effect but not to offer rewards. (2) In an intermediate level of customer delight threshold, a seller should use a reward to complement a low-price strategy. As the delight threshold gets higher in this region, price should be higher and the rewards should be raised. (3) When the delight threshold is even higher, the seller should forsake the referral strategy all together. No rewards should be given, and price reverts back to that of a seller who ignores referrals. These results are consistent with the fact that referral rewards are not offered in all markets. Our analysis highlights the differences between lowering price and offering rewards as tools to motivate referrals. Lowering price is attractive because the seller kills two birds with one stone: a lower price increases the probability of an initial purchase and the likelihood of referral. Unfortunately, a low price also creates a free-riding problem, because some customers benefit from the low price but do not refer other customers. Free riding becomes more severe with an increasing delight threshold; therefore, motivating referrals through low price is less attractive at high threshold levels. A referral reward helps to alleviate this problem, because of its pay for performance incentive (only actual referrals are rewarded.) Unfortunately, rewards can sometimes be given to customers who would have recommended anyway, causing a waste of company resources. The lower the delight threshold level, the bigger the waste and, therefore, motivating referrals through rewards loses attractiveness. Our theory highlights the advantage of using referral rewards in addition to lowering price to motivate referrals. It explains why referral programs are offered sometimes but not always and provides guidelines to managers on how to set the price and reward optimally.", "e:keyword": ["Referral Rewards", "Customer Referrals", "Customer Delight", "Word-of-Mouth"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.1.98.10198", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.16.1.iii", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.2.99.10191", "e:abstract": "Reward programs, a promotional tool to develop customer loyalty, offer incentives to consumers on the basis of cumulative purchases of a given product or service from a firm. Reward programs have become increasingly common in many industries. The best-known examples include frequent-flier programs offered by airlines, frequent-guest programs offered by hotels, and frequent-shopper programs offered by supermarkets. Despite the widespread business practice of reward programs, research efforts on reward programs, particularly in marketing, have been scarce. Our paper takes an important step towards understanding the design of reward programs and its implications on pricing strategies. We study a market that consists of two segments: heavy- and light-user segments. The key distinction between the two segments is that the heavy-user segment purchases in each period and thus is a candidate for the reward programs. In contrast, the light-user segment exits the market after one purchase and is not in a position to exploit reward programs. An important feature of our model is that we allow for different price sensitivity between heavy-user and light-user segments. Our model closely examines the type of rewards. A reward worth a dollar to the consumer might have different cost implications for the offering firm, depending on the type of reward. For example, cash rewards have higher unit reward cost () for the firm than a free product of the firm, such as an airline ticket or long-distance minutes (). Specifically, we examine an interesting puzzle observed in the marketplace. Several firms offer a cash reward or a product made by the firm, such as jackets, electronic items, etc. These firms could offer their own product as rewards and significantly lower their cost. We examine whether there is any reason for such a seemingly suboptimal practice. Our analysis shows that reward programs weaken price competition. By offering the incentives for repeat purchases, reward programs increase a firm's cost to attract competing firms' current customers. Because firms gain less from undercutting their prices, equilibrium prices go up. Moreover, as consumers become unwilling to switch because of potential rewards, the firm with a larger market share in the heavy-user segment charges higher prices. Therefore, a low price in the first period, which leads to a larger market share in the heavy-user segment, will always be followed by a high price in the second period. In our model, consumers are rational and can correctly anticipate firms' incentive to offer lower prices initially to enroll them into the reward programs. Our paper offers an explanation as to why the type and amount of reward may vary across the programs. We identify two determining factors for the selection of rewards: size and relative price sensitivity of the heavy-user segment. We find that in a market with a small heavy-user segment that is also much more price sensitive than the light-user segment, it is optimal for firms to offer the rewards. The intuition is based on the firms' incentive to exploit the price-insensitive light-user segment. By offering inefficient rewards, firms are able to commit to weaker competition and, therefore, higher prices. When the heavy-user segment is large or not very price sensitive, when compared to the light-user segment, competing firms should adopt the most efficient rewards to maximize their profit. This may well be the case in a number of real-world situations in which efficient rewards are quite prevalent. We also find that optimal reward amount has a negative relationship with unit reward cost. Because both firms use rewards to attract the heavy users, they tend to offer more when they adopt the more efficient rewards. Finally, our paper identifies the relationship between market characteristics and theimpact of reward programs on firms' profits and consumers' benefits. We find that firms gain from the adoption of reward programs as long as light users are not too price sensitive. When light users are very price sensitive, firms engage in intense price competition, thus benefiting little from the loyalty of heavy users created through rewards. Because reward programs increase market prices, light users, who do not get the reward, earn strictly lower benefit. In contrast, heavy users often stand to gain more from the reward program. In most cases, firms and the heavy users are better off at the expense of light users.", "e:keyword": ["Reward Programs", "Switching Cost", "Price Competition", "Game Theory", "Loyalty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.2.121.10194", "e:abstract": "We analyze data from a variety of sources, including historical data from a women's clothing catalog, a field study in that catalog, survey responses to catalog stimuli, and grocery store data for frozen juice, toothpaste, and tuna. The analysis yields three conclusions. First, sale signs are less effective at increasing demand when more items have them. Second, total category sales are maximized when some but not all products have sale signs. Third, placing a sale sign on a product reduces the perceived likelihood that the product will be available at a lower price in the future, but the effect is smaller when more products have sale signs. By ruling out alternative hypotheses, the findings suggest that moderation of the sale sign effect is in part due to reduced credibility when they are used on more products. The credibility argument is motivated in part by a recent paper that presented an equilibrium model predicting that customers who lack knowledge of market prices rely on point-of-purchase sale signs to help evaluate posted prices (Anderson and Simester 1998). The model predicts that sale signs increase demand but that the increase is smaller when more products have them. This moderating effect regulates how many sale signs stores use and makes customer reliance on these cues an equilibrium strategy. To evaluate whether the number of sale signs moderates their effectiveness we compare demand for items with sale signs when varying the number of sale signs on other products. For this comparison we use two datasets describing demand for products in a women's clothing catalog. The first dataset describes customer orders for the same set of items across three sequential issues of the catalog. The second dataset is a field test conducted by mailing different versions of the catalog to randomly selected customer samples. Reassuringly, although the datasets do not share the same limitations, the findings are consistent and indicate that sale signs are less effective when more products have them. In addition to the credibility explanation there are at least two alternative explanations for this result. First, using a sale sign to make a product more attractive may lead to substitution of demand from other sale items within a store, and so demand may vary even if there is no change in the credibility of the sale signs. To discriminate between the substitution and credibility explanations, we evaluate whether total category demand is maximized when some but not all products in the category have sale signs. The credibility explanation implies that adding another sale sign will eventually decrease total category sales when many items already have sale signs. In contrast, if adding a sale sign to an item leads solely to substitution from other products within (or outside) the category we will not observe a decrease in total category demand. We test the total category sales prediction using grocery store scan data describing total category demand for frozen juice, toothpaste, and canned tuna. The findings reveal that in all three categories there is a significant reduction in aggregate demand. A second alternative explanation is that sale signs focus customer attention on products with these cues. Distinguishing between the attention and credibility explanations is a difficult task. They both imply that sale signs deliver less information when there are too many sale signs. The credibility argument predicts that this occurs because the signs are noticed but not believed. The attention explanation predicts that the signs are less likely to be noticed when attention is diluted by a large number of sale signs. To discriminate between the credibility explanation and this attention effect we use survey measures to evaluate a third hypothesis. This hypothesis predicts both that placing a sale sign on a product reduces the perceived likelihood that the product will be available at a lower price in the next period, and that this effect is smaller when more items have them. In an attempt to control for the attention effect we focus subjects' attention on a set of focal items. Under these conditions it is unlikely that subjects overlook sale signs on these items. More important, the likelihood of overlooking sale signs on the focal items is unlikely to depend on how many other items also have sale signs. The results confirm both that the presence of a sale sign reduced subjects' expectations that an item would be available at a lower price in the future and that this effect is smaller when more items have sale signs.", "e:keyword": ["Sale Signs", "Retail Pricing", "Promotions", "Credibility", "Signaling", "Fashion Products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.2.143.10190", "e:abstract": "An outstanding problem in marketing is why some firms in a competitive market delegate pricing decisions to agents and other firms do not. This paper analyzes the impact of competition on the delegation decision and, in turn, the impact of delegation on prices and incentives. The theory builds on the simplest framework of competition in two dimensions: prices and (sales agents') effort. Specifically, we are interested in answering the following questions: (1) Does competition affect the price-delegation decision and, if yes, why? (2) How do prices vary under price-delegation and no-price-delegation scenarios? (3) Do the incentives to the sales representatives vary under the delegation and no-delegation scenarios? To address these issues, we build a game-theoretic model that consists of two firms selling through their sales representatives. These representatives are company employees. Sales are a function of prices and selling efforts. The risk-neutral firms decide whether or not to delegate the pricing decision to risk-averse sales representatives. The wages to the sales representatives consist of salary plus a commission on gross margins. The commission on gross margins can be adjusted, either through communicated marginal cost of production, which we call virtual marginal cost, or directly. The firm and the sales representative are assumed to have the same information about the market, that is, there is no information asymmetry. With competition in two dimensions, the strategic nature of decision variable depends on the relative intensity of competition. With unobservable contracts and risk-averse sales representatives, firms delegate the pricing decision when price competition is intense. Part of the uncertainty in demand is absorbed by the firm by keeping virtual marginal cost greater than the marginal cost of production. The competing firm infers this through the risk aversion of the sales representatives. Under price delegation the sales representatives' wages are higher, and they set a price that is higher than what the firms would have set themselves. This leads to softening of price competition that is to the advantage of both firms. When the effort competition is dominant, however, the firms prefer to make the pricing decision themselves, because this reduces the intensity of effort competition among agents. A single-instrument commission structure in which the firms adjust the virtual marginal cost is compared to a two-instrument commission structure in which the firms can adjust the commission rate, as well the virtual marginal cost. Under no price delegation, the two incentive schemes are the same. However, under price delegation the risk premium with the two-instrument scheme is lower. However, the prices and efforts are higher with the single-instrument scheme. When price competition is intense, the increase in risk premium with the single instrument scheme is more than compensated for by the increase in profits. This is the benefit of softening price competition through higher prices.", "e:keyword": ["Salesforce", "Delegation", "Agency Theory", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.2.170.10193", "e:abstract": "Sales contests are commonly used by firms as a short-term motivational device to increase salespeople's efforts. Conceptually, sales contests and piece-rate schemes, such as salary, commission, or quotas, differ in that in sales contests payment to salespeople is based on relative rather than absolute sales levels. Using the agency theoretic framework where the firm is risk neutral and the salespeople are risk averse, we examine how a firm should design an optimal contest to maximize its profit through stimulating salespeople's efforts. Specifically, we investigate how many salespeople should be given awards and how the reward should be allocated between the winners. Three commonly used sales contest formats are studied. In the first format, termed as Rank-Order Tournament, there are many winners and the amount of reward is based on relative rank achieved, with larger amounts awarded to higher ranks. We also examine two special cases of Rank-Ordered Tournament: a Multiple-Winners format, where the reward is shared equally, and a Winner-Take-All format, where a single winner gets the entire reward. We model salespeople's behavior by considering utility of the reward from achieving one of the winning ranks in the contest and assessing incremental chances of winning by exerting more effort. The analysis was done for two situations based on whether the total reward is large enough for salespeople to participate in the effort-maximizing sales contest or not. The analysis shows that factors impacting contest design include the salespeople's degree of risk aversion, number of salespeople competing in the contest, and degree of sales uncertainty (which reflects strength of the sales-effort relationship). The results show that salespeople exert lower effort when there are larger numbers of participants or when sales uncertainty is high. We find that the Rank-Order Tournament is superior to the Multiple-Winners contest format. In a Multiple-Winners format, the salesperson whose performance is just sufficient to win is better off than any of the other winners as he exerts the least effort to win but obtains as high a reward as any other winners. Specific recommendations on contest designs are obtained assuming that sales follow either a logistic or uniform distribution. Assuming that sales outcome is logistically distributed and the contest budget is high enough to ensure participation, our analysis shows that the total number of winners in a sales contest should not exceed half the number of the contestants. This result is due to the symmetric nature of the logistic distribution. Our analysis also indicates that the total number of winners should be increased and the spread decreased when salespeople are more risk averse. When salespeople are more risk averse, their marginal values for higher rewards become smaller. The spread should increase with ranks when rate of risk tolerance is high and decrease with ranks when the rate of risk tolerance is lower. In the extreme case of risk-neutral salespeople, the optimal design is a Winner-Take-All format. We also conclude that since the probability of winning the contest decreases with number of contestants, the optimal number of winners should increase and interrank spread decrease when there are a larger number of participants. If the firm does not allocate a large enough budget for salespeople to participate in the effort-maximizing sales contest, then the firm may increase the number of winners to more than half the sales-force. Increasing the number of winners and decreasing the spread are required to encourage the salespeople to participate, particularly when there are many participants who are risk averse. A counterintuitive result is that the number of winners should be reduced and the spread increased when sales uncertainty is high. Increasing sales uncertainty leads to lower equilibrium effort levels while keeping the expected utility of the contest rewards the same. Therefore, increased uncertainty results in higher participation incentive. The firm should thus relatively reduce the number of winners in high-uncertainty situations. Under the assumption of uniformly distributed sales, the recommendation is that a Winner-Take-All contest induces maximum efforts regardless of the level of risk aversion, number of players, or the degree of uncertainty. When the Winner-Take-All format does not meet the participation constraint, our analysis recommends offering a big reward to the top salesperson and a small reward to many other sales-people. The small reward should be just sufficient to ensure that all salespeople participate. Consistent with logistic distribution, the spread should decrease when salespeople are more risk averse or there are more players but should increase when sales uncertainty is larger. These results highlight that some of the conclusions drawn may be sensitive to distributional assumptions.", "e:keyword": ["Agency Theory", "Sales Contests", "Salesforce Compensation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.2.194.10192", "e:abstract": "Retailers' marketing objectives can be classified into three broad categories: attraction effects that focus on consumers' store-entry decisions, conversion effects that relate to consumers' decisions about whether or not to make a purchase at a store they are visiting, and spending effects that represent both dollar value and composition of their transactions. This paper proposes a framework that incorporates all three of these effect categories and examines their influence on store performance. Specifically, store sales are broken down into four components: front traffic, store-entry ratio, closing ratio, and average spending. Using inexpensive and readily available infrared and video imaging technology, it is possible to measure these four components in a wide variety of retail environments, allowing retailers to obtain a richer understanding of the effectiveness of promotional activities on store sales. A set of twelve hypotheses based on the economics of information and promotion literatures is proposed. These hypotheses relate the presence of various promotions (price, clearance, and new product), promotion scope, and the type of out-of-store communication vehicle used by retailers to each of the four store sales components. The proposed approach is then applied in two different empirical settings, both to test formally the hypotheses and to demonstrate more generally the richness of the information the approach can provide. The first application involves a Canadian apparel store that sells ladies' casual wear. The second application is based on a U.S. sporting-goods retail chain that sells a variety of sporting goods, including sportswear, sports shoes, and sports equipment. A joint model of four simultaneous equations using front traffic, store traffic, number of store transactions, and store sales as the endogenous variables is then formulated for the applications. Promotional factors are used as explanatory variables, along with a number of additional control variables (including length of operation, day of week, holidays, seasonality, and weather). Seemingly unrelated regression is used to estimate the model efficiently. A comparison model that includes only store sales as the endogenous variable is estimated for comparison with the joint model. Results from these applications indicate that the proposed framework provides more detailed information about promotional effectiveness than more traditional models of store performance. The effects of specific promotional decisions on store performance are described. Specifically, price promotions have little impact on front traffic, but positively affect store entry and likelihood that a consumer will make a purchase. The effect of price promotion on consumers' spending in a store is also significant, but varies in sign with the type of promotion employed. Second, while greater promotional scope enhances store entry, promotions with narrow scope seem to have negative impact on store traffic. The effects of promotion scope on store performance also seem to be moderated by the scope of merchandise carried by the retailer. Increased promotional scope appears to have a greater effect on store traffic and consumers' spending for a multicategory retailer than for a more focused seller. Third, clearance promotions have a weaker effect on store entry when compared to other multiple-category promotions, while new-product promotions have a positive impact on conversion. Finally, newspaper advertisements, when compared to targeted coupons, have a stronger effect on store attraction but a weaker effect on spending. In addition to understanding the key drivers of store sales, retailers are also interested in determining whether or not their promotions affect store profitability. An assessment of the profit impact cannot be based on the change in overall store sales because promotions may affect various items or product categories inside a store differentially, and gross margins may not be the same for all items or categories. Although gross margin and item- or category-specific sales were not available for the two applications studied, the paper describes how such information can be integrated with the output of the proposed joint model to arrive at a richer understanding of how promotions affect overall store profitability. Finally, managerial and academic implications of this work are described, and potential extensions of the joint model are suggested.", "e:keyword": ["Retailing and Wholesaling", "Marketing Mix", "Promotion", "Advertising and Media Research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.3.219.9765", "e:abstract": "Advance selling occurs when sellers allow buyers to purchase at a time preceding consumption (Shugan and Xie 2000). Electronic tickets, smart cards, online prepayments, and other technological advances make advance selling possible for many, if not all, service providers. These technologies lower the cost of making complex transactions at a greater distance from the seller's site. They also give sellers more control over advance selling by decreasing arbitrage. As technology enhances the capability to advance sell, more academic attention is vital. This paper strives to exploit these technologies by developing advance-selling strategies. Until recently, advance-selling research focused on the airline industry and specific characteristics of that industry. These characteristics included the price insensitivity of late arrivals (e.g., business travelers) compared with early arrivals (e.g., leisure travelers), demand uncertainty across flights on the same day, and capacity constraints. Recent findings by Shugan and Xie (2000) show that advance selling is a far more general marketing tool than previously thought. It does not require these industry-specific characteristics. It only requires the existence of buyer uncertainty about future valuations. Moreover, sellers without the ability to price discriminate can use advance selling to improve profits to the level of first-degree price discrimination. This finding is important because buyers are nearly always uncertain about their future valuations for most services (e.g., the utility of next year's vacation or a future college education). In this paper, we take the next step from Shugan and Xie (2000). We show that advance-selling profits do not come from buyer surplus, but from more buyers being able to purchase. We determine when and how to advance sell in a variety of situations, including situations with limited capacity, second-period arrivals, refunds, buyer risk aversion, exogenous credibility, continuous preference distributions, and premium pricing. We determine when advance selling improves profits and, when it does, how to set advance prices. We ask and answer seven questions. First, when should sellers advance sell? Second, how much can advance selling improve profits compared with only spot selling? Third, what factors impact the profitability of advance selling and how? Fourth, should advance prices be higher or lower or the same as spot prices? Fifth, how do capacity constraints impact advance-selling strategies? Sixth, should sellers limit the number of advance sales? Finally, what is the possible impact of buyer risk aversion? First, we provide precise conditions when sellers should advance sell. For example, without capacity constraints, we show that sellers should advance sell when marginal costs are sufficiently low to make it profitable to sell to buyers with low valuations and sufficiently high to convince buyers that the spot price will be higher than the advance price. Second, we find that advance selling can almost double the profits from optimal spot selling to early arrivals. We also show that advance selling has no impact on consumer surplus in markets with homogenous consumers and no capacity constraints. Therefore, advance selling can increase social welfare because seller profits increase. Third, we find that two very important factors impacting the profitability of advance selling are seller credibility and marginal costs. Buyers only advance buy when they expect an advantage from advance buying over spot buying. Without capacity constraints, sellers must credibly convince buyers that the advance price is at a discount to the spot price. We show that this condition is met under different circumstances. For example, large marginal costs can create credibility because buyers believe that these costs will lead to high spot prices. Fourth, we find (although optimal advance prices can be at a discount to the spot price) that sometimes a premium is optimal. Premiums are optimal when capacity is large (but limited) and marginal costs are not too large. Buyers advance purchase at a premium to spot prices when capacity is limited and spot prices are low. (Note that this is not a risk premium, and risk aversion is not required.) No prior research has suggested this strategy because that research relies on the assumption that early arrivals are more price sensitive than later ones. Without that assumption, premium advance pricing is sometimes optimal. Fifth, we find that binding capacity constraints can impact the profitability of advance selling in opposite ways. On one hand, capacity constraints create seller credibility. Buyers believe that spot prices will be high when they know spot capacity is limited (and, perhaps, more limited by advance sales). On the other hand, when capacity is limited, the need to increase sales from discounted advance prices diminishes. Sixth, consistent with Desiraju and Shugan (1999) we find that limiting advance sales can be profitable, but only under restrictive conditions. These conditions are: (1) selling to all early arrivals would leave insufficient capacity in the spot period to sell to all second-period arrivals with high valuations, (2) the optimal spot price is high, and (3) marginal costs are sufficiently small to make advance selling profitable. Finally, we find that buyer risk aversion can sometimes increase the profitability of advance selling. Our findings provide precise guidelines for a large number of service providers that will have the technical capability to advance sell. For those service providers, advance selling provides a creative pricing strategy that can potentially provide substantial improvements in profits.", "e:keyword": ["Pricing", "Advance Selling", "Advance Pricing", "Tickets", "State-Dependent Utility", "Services Marketing", "Dynamic Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.3.244.9764", "e:abstract": "Consumer goods manufacturers usually sell their brands to consumers through common independent retailers. Theoretical research on such channel structures has analyzed the optimal behavior of channel members under alternative assumptions of manufacturer-retailer interaction (Vertical Strategic Interaction). Research in Empirical Industrial Organization has focused on analyzing the competitive interactions between manufacturers (Horizontal Strategic Interaction). Decision support systems have made various assumptions about retailer-pricing rules (e.g., constant markup, category-profit-maximization). The appropriateness of such assumptions about strategic behavior for any specific market, however, is an empirical question. This paper therefore empirically infers (1) the Vertical Strategic Interaction (VSI) between manufacturers and retailer, (2) the Horizontal Strategic Interaction (HSI) between manufacturers simultaneously with the VSI, and (3) the pricing rule used by a retailer. The approach is particularly appealing because it can be used with widely available scanner data, where there is no information on wholesale prices. Researchers usually have no access to wholesale prices. Even manufacturers, who have access to their own wholesale prices, usually have limited information on competitors' wholesale prices. In the absence of wholesale prices, we derive formulae for wholesale prices using game-theoretic solution techniques under the specific assumptions of vertical and horizontal strategic interaction and retailer-pricing rules. We then embed the formulae for wholesale prices into the estimation equations. While our empirical illustration is using scanner data without wholesale prices, the model itself can be applied when wholesale prices are available. Early research on the inference of HSI among manufacturers in setting wholesale prices using scanner data (e.g., Kadiyali et al. 1996, 1999) made the simplifying assumption that retailers charge a constant margin. This assumption enabled them to infer wholesale prices and analyze competitive interactions between manufacturers. In this paper, we show that this model is econometrically identical to a model that measures retail-price coordination across brands. Hence, the inferred cooperation among manufacturers could be exaggerated by the coordinated pricing (category management) done by the retailer. We find empirical support for this argument. This highlights the need to properly model and infer VSI simultaneously to accurately estimate the HSI when using data at the retail level. Functional forms of demand have been evaluated in terms of the fit of the model to sales data. But recent theoretical research on channels (Lee and Staelin 1997, Tyagi 1999) has shown that the functional form has serious implications for strategic behavior such as retail passthrough. While the logit and linear model implies equilibrium passthrough of less than 100% (Lee and Staelin call this Vertical Strategic Substitute (VSS)), the multiplicative model implies optimal passthrough of greater than 100% (Vertical Strategic Complement (VSC)). Because passthrough rates on promotions have been found to be below or above 100% (Chevalier and Curhan 1976, Armstrong 1991), we empirically test the appropriateness of the logit (VSS) and the multiplicative (VSC) functional form for the data. We perform our analysis in the yogurt and peanut butter categories for the two biggest stores in a local market. We found that the VSS implications of the logit fit the data better than the multiplicative model. We also find that for both categories, the best-fitting model is one in which (1) the retailer maximizes category profits, (2) the VSI is Manufacturer-Stackelberg, and (3) manufacturer pricing (HSI) is tacitly collusive. The fact that the retailer maximizes category profits is consistent with theoretical expectations. The inference that the VSI is Manufacturer-Stackelberg reflects the institutional reality of the timing of the game. Retailers set their retail prices after manufacturers set their wholesale prices. Note that in the stores and product categories that we analyze, the two manufacturers own the dominant brands with combined market shares of about 82% in the yogurt market and 65% in the peanut butter market. The result is also consistent with a balance of power argument in the literature. The finding that manufacturer pricing is tacitly collusive is consistent with the argument that firms involved in long-term competition in concentrated markets can achieve tacit collusion. Managers use decision support systems for promotion planning that routinely make assumptions about VSI, HSI, and the functional form. The results from our analysis are of substantive import in judging the appropriateness of assumptions made in such decision support systems.", "e:keyword": ["Structural Models", "Horizontal Strategic Interaction", "Vertical Strategic Interaction", "Retailer Pricing", "Promotional Planning", "New Empirical Industrial Organization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.3.265.9767", "e:abstract": "Durable goods manufacturers often design product lines by segmenting their markets on quality attributesattributes that exhibit a more is better property for all consumers. Since products within a product line are partial substitutes, and consumers can self-select the products they want to purchase, multiproduct firms have to carefully consider the cannibalization problem in designing their product lines. Existing research has analyzed the cannibalization problem for a monopolist who faces consumers who differ in their quality valuations. If lower-quality products are sufficiently attractive, higher-valuation consumers may find it beneficial to buy lower-quality products rather than the higher-quality products targeted to them. That is, lower-quality products can potentially cannibalize higher-quality products. The cannibalization problem forces the firm to provide only the highest-valuation segment with its preferred (efficient) quality. All other segments get qualities lower than their preferred (efficient) qualities. When the cannibalization problem is very severe, the firm may not serve some of the lowest-valuation segments. However, not much is known about how and when the cannibalization problem affects product line design in an oligopoly. Also, consumers may differ not only in their quality valuations but also in their taste preferences. The objective of this paper is to fill these gaps by examining whether the cannibalization problem affects a firm's price and quality decisions in a model with consumer differences in quality valuations, as well as in their taste preferences, in both monopoly and duopoly settings. The paper addresses questions such as the following. With both types of consumer differences, should a firm, even a monopolist, provide efficient quality only to the top segment? Are there conditions under which other segments can also get their preferred quality levels? If so, how do consumer and firm characteristics affect the likelihood of different segments getting their preferred qualities? How does competition affect the firm's choice of qualities? I develop a model in which the market is made up of two segments, with one segment valuing quality more than the other. Consumers within each segment are distributed over Hotelling's (1929) linear city. Consumers in the two segments can have different taste preferences (transportation costs). Firm locations in the two segments may also be different. The paper begins with an analysis of the monopoly case. I find that when both segments are fully covered, the standard self-selection results of the high-valuation segment getting its preferred quality and the low-valuation segment getting less than its preferred quality do hold. Interestingly, when both segments are incompletely covered, under some conditions, the monopolist's price and quality choices are not determined by the cannibalization problem. In these cases, the monopolist finds it optimal to provide each segment with its preferred quality. Thus, the equilibrium quality levels in a second-degree price discrimination situation resemble the third-degree price discrimination solution. I characterize the relevant conditions in terms of consumer characteristics. I then consider the case of two firms competing in the market, each offering two productsone for the high-valuation segment and the other for the low-valuation segment. Here also both types of outcomes are possible, depending on consumers and firm characteristics. Under some conditions, the cannibalization problem does not affect the firms' price and quality choices, and each firm provides each segment with that segment's preferred quality. Each firm finds it optimal to serve both segments. When these conditions do not hold, only the high-valuation segment gets its preferred quality. I interpret the conditions necessary for these results to exist in terms of characteristics of the consumers and the firms. An interesting insight from the analysis is that as the taste preferences of the low-valuation segment become weaker (their transportation cost becomes lower), the more intense competition in the low-valuation segment makes it more attractive for the high-valuation consumers to buy the products meant for the low-valuation segment. This worsens the cannibalization problem, and the low-valuation segment may not get its preferred quality. On the other hand, when the taste preferences of the high-valuation segments are sufficiently weak, more intense competition in the high-valuation segment reduces that segment's incentives to buy the product meant for the low-valuation segment. This mitigates the cannibalization problem and makes it more likely for the low-valuation segment to get its preferred quality. Similarly, when firms are less differentiated in the low-valuation segment, stronger competition between the firms makes the cannibalization problem worse, and the low-valuation segment may not get its preferred quality. When the differentiation between the firms is sufficiently weak in the high-valuation segment, the high-valuation segment is more likely to be better off buying the product meant for it. As the high-valuation segment's incentives to buy the lower-quality product are reduced, the low-valuation segment is more likely to get its preferred quality.", "e:keyword": ["Cannibalization", "Product Line Design", "Price Discrimination", "Vertical Differentiation", "Horizontal Differentiation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.3.284.9768", "e:abstract": "Marketing scholars and practitioners frequently infer market responses from cross-sectional or pooled cross-section by time data. Such cases occur especially when historical data are either absent or are not representative of the current market situation. We argue that inferring market responses using cross-sections of multimarket data may in some cases be misleading because these data also reflect unobserved actions by retailers. For example, because the (opportunity) costs of doing so do not outweigh the gains, retailers are predisposed against promoting small share brands. As a consequence, local prices and promotion variables depend on local market sharesthe higher the local share, the higher the local observed promotion intensity. We refer to this reverse causation as an endogeneity. Ignoring it will inflate response estimates, because both the promotion effects on share as well as the reverse effects are in the same direction. In this paper, we propose a solution to this inference problem using the fact that retailers have trade territories consisting of multiple contiguous markets. This implies that the unobserved actions of retailers cause a measurable spatial dependence among the marketing variables. The intuition behind our approach is that by accounting for this spatial dependence, we account for the effects of the retailer's behavior. In this context, our study hopes to make the following contributions at the core of which lies the above intuition. First, we separate the market response effect from the reverse retailer effect by computing responses to price and promotion net of any spatialand therefore retailerinfluence. Second, underlying this approach is a new variance-decomposition model for data with a panel structure. This model allows to test for endogeneity of prices and promotion variables in the cross-sectional dimension of the data. This test aims to complement the one developed by Villas-Boas and Winer (1999), who test for endogeneity along the temporal dimension. Third, to illustrate the approach, we use Information Resources Inc. (IRI) market share data for brands in two mature and relatively undifferentiated product categories across 64 IRI markets. Whereas we only use data with very short time horizons to estimate price and promotion responses with the spatial model, we do have data over long time windows. We use the latter to validate the approach. Specifically, within-market estimates of price and promotion response are not subject to the same endogeneity because we hold the set of retailers constant. Therefore, comparing within- and across-market estimates of price and promotion responses is a natural way to validate the approach. Consistent with our argument, ignoring the reverse causation in the cross-sectional data leads to inferences of price and promotion elasticities that are farther away from zero than the elasticities obtained from within-market analysis. In contrast, cross-sectional spatial estimates and time-series estimates show convergent validity. From a practical point of view, this means it is possible to obtain reasonable within-market estimates of price and promotion elasticities from (predominantly) cross-sectional data. This may benefit marketing managers. The manager who would act on the inflated elasticities will over-allocate marketing resources to promotions because she ignores retailers' censorship of promotions on the basis of already existing high share. We explore other approaches to correct for the inference bias, and discuss further managerial issues and future research.", "e:keyword": ["Spatial Analysis", "Promotional Price Response", "Promotion Strategy", "Endogeneity Biases", "Variance-Decomposition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.3.300.9766", "e:abstract": "Price-matching guarantees are widely used in consumer and industrial markets. Previous studies argue that they are a marketing tactic that facilitates implicit price collusion. This is because once a store adopts this marketing tactic, its rivals can no longer steal its customers by undercutting its price, and hence they have little incentive to initiate price cuts. While a store with price-matching guarantees has no fear of losing customers to rivals' price cuts, it has every incentive to raise its own price to charge a higher price to its loyal consumers. A growing body of legal literature uses this argument today to justify calls for antitrust actions against stores employing this marketing tactic. However, this theoretical conclusion baffles practitioners and industry experts. In practice, sellers typically embrace this marketing tactic in response to heavier competition and a growing bargain consciousness among shoppers. The introduction of price-matching guarantees by a store is frequently interpreted by industry observers as the initiation of a price war rather than a signal of price collusion. This assertion is supported in many instances by the fact that stores that introduce price-matching guarantees also roll back their prices and typically suffer subsequent loss of profits. Most ironically, the favorite examples used by researchers to illustrate how price-matching guarantees can enforce price collusion, Crazy Eddie and Nobody Beats the Wiz, have subsequently either gone bankrupt or filed for federal bankruptcy protection. In this study we show that price-matching guarantees can indeed facilitate competition: The expected prices and profits can be strictly lower when all stores adopt price-matching guarantees than when they are not allowed to. This is because the adoption of price-matching guarantees generates not only a competition-dampening effect, which has been recognized in the literature, but also a competition-enhancing effect. This latter effect comes from the fact that price-matching guarantees encourage price search by those consumers who prefer to shop at a particular store but are mindful of saving opportunities. These consumers will have incentives to obtain the rival store's price when their favorite store offers price-matching guarantees to avail themselves of the lowest possible price at their favorite store. As a result, price-matching guarantees reduce the number of purchases at the store from those consumers who would have paid the full price and thus prompt a store to price more aggressively to bid for more incremental sales. This competition-enhancing effect can more than offset the competition-dampening effect in markets where consumers differ in their price search costs and store loyalty. Thus, our study casts doubt on the advisability of blanket prohibition on price-matching guarantees. Our argument relies only on consumer segmentation and on the phenomenon of periodic sales, both of which are common in retail markets. We arrive at our conclusion by incorporating and into the standard sales-promotion models. In contrast, the past literature on price-matching guarantees ignores the ubiquitous phenomenon of sales in retail markets and overlooks those consumers who prefer to shop at a particular store, but are alert to saving opportunities. As a result, it is troubled by two awkward conclusions. On the one hand, price-matching guarantees simply remove rivals' incentives to undercut in price and, hence, also their incentives to run sales. This implies that the adoption of price-matching guarantees in a market will eliminate the phenomenon of sales, which is obviously counterfactual. On the other hand, in equilibrium no consumer actually invokes price-matching guarantees, as each player has incentives to close any price gap in the market. This is obviously false, based on our casual observations and our conversations with store managers. Theoretical research on the subject thus far has been overwhelmingly one-sided, and empirical or experimental studies are conspicuously lacking. We hope that our conclusion will spark further research in both directions. A healthy debate will broaden our perspective on an issue of great importance in formulating public policies and in managerial decision making.", "e:keyword": ["Price Guarantees", "Price Promotions", "Competitive Strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.3.315.9763", "e:abstract": "Firms often search enthusiastically for distinguishing traits that they may use to price discriminate between segments. Yet there are occasions in which firms forgo the opportunity to price discriminate and instead charge a single price. Traditional explanations for why retailers forgo the opportunity to price discriminate focus on the cost of discriminating, including operational costs, explicit discrimination costs, and implicit discrimination costs. In this paper we identify an additional reason for why firms may forgo an opportunity to price discriminate. By revealing that a product is being sold to a broad range of segments, a retailer implicitly claims that the product is suitable for each segment. However, claiming that a premium-quality product is suitable for price-sensitive consumers undermines the credibility of a retailer's quality claim. The signaling explanation was motivated by extensive discussions over more than a year with a major catalog retailer that sells premium-quality jewelry and gifts. Discussions with managers revealed that they were reluctant to use any price-discrimination mechanism that signals their products are targeted at price-sensitive customers. For example, the catalog does not include sale or clearance sections and does not target more price-sensitive customers by using separate items. However, management was under some pressure to consider installment-billing offers, which allow customers to pay over a series of periods rather than in a lump sum. Management feared that offering installment billing may adversely affect customers' quality perceptions and demand. To investigate this issue, we develop a general game-the-oretic model, illustrate how the model extends to installment billing, and conduct a large-scale field test. The general model illustrates how selling to multiple segments may lead to an adverse quality signal. We illustrate how the model extends to installment-billing offers in a direct-mail catalog. Installment-billing offers allow customers to spread the total payment over a series of payments. All customers have the option of using installment billing, and customers who use the plan receive an economic benefit (an interest-free loan). We would normally expect this type of offer to increase demand or, at a minimum, leave demand unchanged. However, because installment-billing offers target credit-constrained customers, we predicted that the introduction of installment billing would prompt an unfavorable quality inference and reduce demand among quality-sensitive customers. We empirically investigated this prediction in a large-scale field test with a catalog that offers premium-quality jewelry and gifts. Two versions of the catalog were created: a test version that contained an installment-billing offer, and a control version in which installment billing was not offered. Importantly, the prices in both the test version and control version were identical. Approximately 240,000 catalogs were mailed, and customers were randomly assigned to either the test version or control version. Results show that the installment-billing offer (test version) was associated with both a reduction in the number of orders received and a reduction in aggregate revenue. Offering installment billing resulted in approximately $15,000 in lost revenue. The only plausible explanation for this counterintuitive finding appears to be the signaling theory. To investigate the long-term effects, the catalog agreed to survey their customers to measure how an offer of installment billing affects their customers' quality perceptions. Similar to the field test, two versions of a catalog were created, and customers were randomly mailed a catalog, along with a short survey. Respondents were asked to browse through the catalog and return their responses in a replypaid envelope. The findings are consistent with customer beliefs in the signaling model: Offering installment billing lowers the perceived quality of the items in the catalog. The field test and survey findings were both statistically significant and managerially relevant. Together, the results convinced the catalog not to include installment-billing offers in future catalogs.", "e:keyword": ["Signaling", "Price Discrimination", "Installment Billing", "Promotions", "Quality Perceptions", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.331.9753", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.337.9762", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.349.9758", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.357", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.360.9754", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.364.9752", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.373.9761", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.382.9757", "e:abstract": "Trade promotions are temporary price cuts that manufacturers offer retailers to encourage them to reduce retail prices. While trade promotion spending as a percentage of marketing budget has increased dramatically, the inefficiency of trade promotion represents the number-one concern among manufacturers, as indicated by recent trade surveys. At the heart of this dissatisfaction lies manufacturers' concern regarding widespread retailer opportunism with low retail pass through. Our objective is to develop a simple game-theoretic framework to examine the strategic considerations that underlie a retailer's decision to pass through a trade deal. In particular, we are interested in answering the following questions: (I) What and how do product-market characteristics impact the extent of retail opportunism? (II) How can the manufacturer alleviate the retail pass-through problem by strategically supplementing trade promotions with advertising trade deals directly to consumers? To address these issues, we consider a stylized channel with a single manufacturer who serves two customer segments through a single (focal) retailer. We implicitly capture the essence of retail competition by allowing customers to have an outside option: other retailers that customers might search if they deem the price at the focal retailers to be too high. Customers differ in their valuation for the manufacturer's product and in the costs they incur when searching for a better price at other retailers. While customers are unaware of the existence of a trade deal in any particular time period, through prior experience they know the frequency of such deals and, furthermore, they update their beliefs about the occurrence of a deal by observing the posted retail price. The retailer decides whether to pass through a deal or not, recognizing the impact of his pass-through policy on customers' search propensity, and hence, their willingness to pay. The main message of the paper is that in an environment where manufacturer offers trade promotions, a retailer may not have the incentive to pass a low wholesale price onto consumers because consumers do not have perfect information about ongoing trade promotions. When consumers observe a high price at the focal retailer and yet are not sure if a trade promotion is on, they may not look around for a low price. Therefore, the retailer can price opportunistically to gain a higher margin by not passing a low price to consumers. However, if the retailer never passes savings on, consumers can infer opportunistic pricing based on prior knowledge of trade promotion frequency and have a higher tendency to shop elsewhere, thus reducing sales volume. The retailer resolves the conflicting incentives by occasionally charging a low price when a trade promotion is on, while posting a high price on other occasions. We find that the extent of retail opportunism depends on product-market characteristics, such as the retailer's clientele and the heterogeneity in consumer search costs, as well as on the characteristics of the manufacturer's trade promotion policy, such as the frequency of trade promotion and the depth of discount offered. When the low-valuation consumers have search costs that make them exit the market when the focal retailer posts a high price, the manufacturer will intervene by advertising his trade promotion directly to consumers, thus performing a channel coordination function. We consider several extensions of the base modelexplicit retail competition, differentiated retailers, and heterogeneity in consumers' knowledge about the frequency of trade dealsand show that our results still hold.", "e:keyword": ["Trade Promotion", "Price Uncertainty", "Customer Search", "Retail Pass Through", "Channel Coordination", "Semiseparating Sequential Equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.405.9756", "e:abstract": "Many product categories, from pizzas to real estate, present buyers with purchase decisions involving complex area judgments. Does a square look larger or smaller than a circle? How much smaller does a circle of 8-inch diameter look when compared to one with a 10-inch diameter? In this paper, we propose a psychophysical model of how consumers make area comparison judgments. The model involves consumers making effort-accuracy trade-offs that lead to heuristic processing of area judgments and systematic shape- and size-related biases. The model is based on four propositions: P1. Consumers make an initial comparison between two figures based on a single dimension; P2. The dimension of initial comparisonthe primary dimensionis the one that is most salient to consumers, where salience is figure and context dependent; P3. Consumers insufficiently adjust an initial comparison using a secondary dimension, which we assume to be orthogonal to the primary dimension used for the initial comparison; and P4. The magnitude by which the initial comparison is adjusted is directly related to the relative salience of the secondary dimension versus the primary dimension. The model predicts that a single linear dimension inappropriately dominates the two-dimensional area comparison task and that contextual factors affect which linear dimension dominates the task. The relative use of the second dimension depends on its relative salience, which can be influenced in a variety of ways. The model extends the area estimation literature in cognitive psychology by exploring new biases in area estimation and is able to resolve controversial effects regarding which shape is perceived to be bigger, the square or the circle, by incorporating contextual factors into model specifications. A set of six studiesfive laboratory experiments and one field experimentsystematically test model predictions. Study 1 is a process study that shows that when two dimensions are available to make an area comparison judgment, people choose one of those to be the primary dimension, with the other being the secondary dimension. Furthermore, it shows that the choice of the primary dimension is dependent on its relative salience that can be contextually manipulated via manner of visual presentation. Studies 2 and 3 show how the use of a diagonal versus the side of a square (contextually determined) can affect whether a square is perceived to be smaller or larger than a circle of the same area. Study 3 extends the investigation to the domain of the price people are willing to pay for pizzas of different shapes, presented differently. Study 4, a field study, demonstrates external validity by showing that purchase quantities are greater when a circular package is expected to contain less than a rectangular package of the same volume in a domain where consumption goal is constant (cream cheese with a bagel). Studies 5 and 6 examine ways in which one can increase the salience of the secondary dimension, in a size estimation task, i.e., judging the rate of increase of area. While Study 5 does so via contextual visual cues (incorporating lines that draw one's attention to the underused dimension), Study 6 does the same using semantic cues that direct attention to a single dimension (e.g., diameter) or the total area and comparing these with a visual presentation of the figure. Overall, results suggest that the manner in which information is presented affects the relative salience of dimensions used to judge areas, and can influence the price consumers are willing to pay. Underlining the external validity of these findings, container shape can significantly affect quantity purchased and overall sales. The paper highlights biases in area comparison judgments as a function of area shape and size. The model is parsimonious, demonstrates good predictive ability, and explains seemingly contradictory results in the cognitive psychology literature. Implications for pricing, product design, packaging, and retailing are suggested.", "e:keyword": ["Consumer Behavior", "Experiments", "Judgment Biases", "Package Design", "Information Processing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.426.9759", "e:abstract": "To be used effectively, market knowledge and information must be structured and represented in ways that are parsimonious and conducive to efficient managerial decision making. This manuscript proposes a new latent structure spatial model for the representation of market information that meets this requirement. When applied to a priori defined (e.g., socioeconomic) segments, our proposed methodology provides a new way to display marketing data parsimoniously via dimension reduction through a factor-analytic specification. In post hoc studies, we simultaneously derive market segments from the data and represent the structure of market information within each of the unobserved, derived groups/segments. We summarize all relevant information concerning derived market segments via a series of maps that prove conducive to the quick and accurate dissemination of customer and competitor market information. The associations between the variables are captured in a reduced space, where each variable is represented by a vector that emanates from the origin and terminates on a hypersphere of unit (the vector length is arbitrary) radius (e.g., a unit circle in a two-dimensional space). The angles between the variable vectors capture the correlation structure in the reduced space. The method is very general and can be utilized to identify latent structures in a wide range of marketing applications. We present an actual commercial marketing application involving the (normalized) prescription shares (of specialists) of ethical drugs to demonstrate the effectiveness of representing market information in this manner and to reveal the advantage of the proposed methodology over a more general finite mixture-based method. The proposed methodology derives three segments that tend to group specialists with respect to the stage of adoption of innovation in this therapeutic category. The specialists in the first group appear to be laggards because they prescribe more of the older class of brands. However, they also have a higher-than-average preference for a newer and somewhat cheaper brand. This suggests that some of the specialists belonging to this segment may be price sensitive, while others may exhibit a slower adoption cycle, replacing the older class with the newer brands, and thus, skip one stage in the cycle of innovation. The specialists in the second segment are heavy users of the newer class of brands but are not particularly fast to adopt the latest brands. Finally, the last segment clearly consists of innovators. Traditionally, pharmaceutical marketers have viewed specialists in one of two extremesall specialists are the same (i.e., the market has only one segment) or all specialists are very different (i.e., the market consists of 10,000+ segments of one physician each). Not surprisingly, this analysis suggests a more moderate perspective: specialists adopt new products at different rates.", "e:keyword": ["Spatial Models", "Market Segmentation", "Latent Structure Analysis", "Maximum Likelihood Estimation", "Ethical Drugs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.20.4.442.9751", "e:abstract": "Two issues that have become increasingly important while estimating the parameters of aggregate demand functions to study firm behavior are the of marketing activities (typically, price) and across consumers in the market under consideration. Ignoring these issues in the estimation of the demand function parameters can lead to biased and inconsistent estimates for the effects of marketing activities. Endogeneity and heterogeneity have achieved prominence in large measure because of the increasing popularity of logit models to characterize demand functions using data. The logit model accounts for purchase incidence and brand choice by including a no-purchase alternative in the consumer's choice set. This allows for category sales to change as a function of the marketing activities of brands in the category. There are three issues with using the logit model with the no-purchase option to characterize demand when studying competitive interactions among firms. (1) The marketing literature dealing with brand choice behavior at the consumer level has found that the IIA restriction is not appropriate, as each brand in the choice set is more similar to some brands than it is to others. (2) Studies have found that the purchase incidence decision is distinct from the brand choice decision. Hence, it may not be appropriate to model the no-purchase decision as just another alternative in the choice set with the IIA restriction holding across all brands and the no-purchase option. (3) Even if the distinction between the purchase incidence and brand choice decisions is accounted for via, for example, a nested logit specification, accounting for the purchase incidence decision with aggregate data requires assumptions for computing the share of the no-purchase alternative which is otherwise unobserved. In this paper, we propose a probit model as an alternative to the logit model to specify the aggregate demand functions of firms competing in oligopoly markets. The probit model avoids the IIA property that affects the logit model at the individual consumer level. Furthermore, the probit model can naturally account for the distinction between the purchase incidence and brand choice decisions due to the general covariance structure assumed for the utilities of the alternatives. We demonstrate how the parameters of the proposed model can be estimated using aggregate time series data from a product market. In the estimation, we account for the endogeneity of marketing variables as well as for heterogeneity across consumers. Our results indicate that both endogeneity as well as heterogeneity need to be accounted for even after allowing for a non-IIA specification at the individual consumer level. Specific to our data, we also find that ignoring endogeneity has a bigger impact on the estimated price elasticities than ignoring the effects of heterogeneity. A comparison of the elasticities obtained from the probit model with those from the corresponding logit specification indicates that the of elasticities obtained from the probit model across brands is larger than that obtained from the logit. The results have implications for issues such as firm-level pricing. In addition to specifying a probit model and providing comparisons with the logit model, the paper also addresses the third issue raised above. We propose a simple alternative to the purchase incidence/brand choice specification by decomposing the demand for a brand into a category demand equation and a conditional brand choice share equation. We provide a comparison of results from this specification to those from the specification that includes the no-purchase alternative and find that estimated elasticities are sensitive to the specification used. We also estimate the demand function parameters using a traditional specification such as the double-logarithmic model. Here, we find that the estimated elasticities could be signed in such a manner as to be not useful for firm-level pricing decisions. One of the key limitations of the proposed model is that while it accounts for the purchase incidence and brand choice decisions of households, it does not account for differences across consumers in their purchase quantities. The model and analysis are best suited for product categories in which consumers typically make single-unit purchases. Another limitation is more practical in nature. While recent advances have been made in computing probit probabilities, it could nevertheless be a challenge to do so when the number of alternatives is large.", "e:keyword": ["Heterogeneity", "Endogeneity", "Probit Model", "Logit Model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.16.1.iii", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.21.1.1.162", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.21.1.14.159", "e:abstract": "People consume products in a variety of environments. They drink beer, for example, by themselves, with close friends, on the beach, when playing cards, at tailgate parties, and while having dinner with their boss. Within these environments, an individual may prefer Schaefer beer when drinking alone, Budweiser when having a party, Corona when lying on the beach, and Heineken when dining out. Preferences change across environments because the benefits sought by the consumer change. Consumers may feel thirsty while lying on the beach, and they may want to display refined tastes while dining out. Moreover, the effect of environment may not be homogeneous, as some people enjoy meeting new people in social gatherings while others may prefer to visit with those who are more familiar. Even though consumers face the same objective environment, different motivating conditions and brand preferences may arise. It is important for marketing managers to understand how brand preferences change across people, environments, and motivating conditions and, more importantly, which product attributes are associated with these changes. Communication and positioning decisions are more likely to be effective if the relationships among objective environment, motivating conditions, and preferences for brand attributes are known. If motivating conditions are uniquely associated with individuals across environments, or with environments across individuals, then the basis of marketing analysis is at the individual or environmental level. If, however, motivating conditions arise from the intersection of individuals and their environments, then analysis conducted at the individual or environmental level will be insufficient to understand human behavior. In such a case, firms may want to view different environments as distinct markets, each with its own pattern of heterogeneous wants and competitive environment. In this paper, the influence of objective environments and motivating conditions on brand preference is investigated. The mathematical model is based on the economic framework of utility maximization and discrete choice, and it accommodates three challenges that arise in modeling variation in brand preference. First, consumer consideration sets and purchase histories can vary widely across individuals in a relevant universe. Because brand preferences are the dependent variables in our analysis, our method must be able to accommodate a large number of brands to avoid restricting its measured variation as the objective environment and motivating conditions change. We propose a method using partial ranking data, combined with pairwise trade-off data, to obtain estimates of brand preference for all brands in our study. Second, the model must allow for multiple effects, leading to both within-person and across-person heterogeneity in preferences. Variation in brand preference is investigated within a hierarchical Bayes model in which motivating conditions are related to brand preference through a regression model in the random effects specification. Third, it is often counterintuitive for respondents to express preferences for attribute combinations that do not actually exist. A statistical method model is proposed for decomposing aggregate brand preferences into preferences for core and extended product attributes. Data are collected from a national survey of consumer off-premises beer consumption. A total of 842 respondents from six different geographic markets participated. Data include preferred brand sets under different objective environments, brand choice rankings, product attributes, and motivating conditions. Effect sizes for respondent and objective environment are both large. We found that the level of explained variance in brand and attribute preference attributable to motivating conditions is greater than that accounted for by a simple interaction of respondent and environmental effects, suggesting that motivations provide a more sensitive description of variation in brand preference. Our findings indicate that 1) across individuals the objective environment is associated with heterogeneous, not homogeneous, motivating conditions; 2) within an individual, motivating conditions may change with variation in the objective environment; and 3) motivating conditions are related to preferences for specific attributes. Our results imply that the unit of analysis for marketing is properly a person-activity occasion. Brands, for example, are used in individual instances of behaviora brand performs well or poorly on individual occasions of use. The relevant universe is enumerated in person-activity occasions rather than in respondents. For some activities, such as doing the laundry, the occasions may typically occur in relatively unchanging environments, and it may be appropriate to allow respondents to summarize over occasions of the activity. For other activities, such as snacking or drinking beer, the activity may occur in distinct kinds of environment. In the case of such activities, it is appropriate to allow for the effect of changing environments to manifest themselves, if present. Doing so may require sampling from the relevant universe of person-activity occasions over an appropriate time frame. The design must be such as to record intraindividual variability due to changes in the environment for action.", "e:keyword": ["Extended Choice Models", "Hierarchical Bayes", "Unit of Analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.1.32.155", "e:abstract": "The objective of the present research is to study consumer decisions to utilize a line of credit. The life-cycle hypothesis from economics argues that consumers should intertemporally reallocate their incomes over their life stream to maximize lifetime utility. One form of intertemporal allocation is to use past income (in the form of savings) in the future. A second form is the use of future income in the present. This can only be done if consumers have access to a temporary pool of money that they can draw from and replenish in the futurea function performed by consumer credit. However, our research reinforces prior findings that consumers are unable to correctly value their future incomes, and that they lack the cognitive capability to solve the intertemporal optimization problem required by the life-cycle hypothesis. Instead, we argue that consumers use information such as the credit limit as a signal of their future earnings potential. Specifically, if consumers have access to large amounts of credit, they are likely to infer that their lifetime income will be high and hence their willingness to use credit (and their spending) will also be high. Conversely, consumers who are granted lower amounts of credit are likely to infer that their lifetime income will be low and hence their spending will be lower. However, based on research in the area of consumer skepticism and inference making, we also argue for a moderating role of the credibility associated with the credit limit. Specifically, we argue that the above effect of credit availability would be particularly strong for consumers who believe that the credit limit credibly signals their future earnings potential (i.e., a nave consumer who has limited experience with consumer credit). However, as consumers gain experience with credit, they start discounting credit availability as a predictor of their future and start questioning the validity of the process used to set the credit limit. Hence, with experience the effect of credit limit on the willingness to use credit should be attenuated. We test these predictions in five separate studies. In the first experimental study, we manipulate credit limit and credibility and pose subjects with a hypothetical purchase opportunity. Consistent with our prediction, credit limit impacted the propensity to spend, but only when the credibility was high. In the second experimental study, we replicate these findings even when subjects were given information about their expected future salaries, and also show that the credit limit influences their expectation of future earnings potential. In the third study, we show that the mere availability (and increase) of current liquidity cannot explain our findings. In the fourth study, we conduct a survey of consumers in which we measure a number of demographic characteristics and also ask them for their propensity to spend in a given purchase situation. In the fifth study we use the Survey of Consumer Finances (SCF) dataset, a triennial survey of U.S. families that is designed to provide detailed information on the use of financial services, spending behaviors, and selected demographic characteristics. Results from both studies 4 and 5 provide further support for our proposed frameworkcredit limits influence spending to a greater extent for consumers with lower credibility: younger consumers and less-educated consumers. Across all studies we achieved triangulation by using a variety of approaches (surveys and experiments), subjects types (young students and older consumers), nature of predictor variables (manipulated and measured), dependent measures (purchase likelihood, credit card balance, new charges), and methods of analysis (ANOVA and regression), and consistently found that increasing credit limits on a credit card increases spending, especially when the credibility of the limit is high. This paper joins a growing body of literature in marketing and behavioral decision theory that goes beyond the traditional domains of inquiry (e.g., product choice, effects of marketing mix variables) and focuses on consumer decisions relating to the appropriate use of income to finance consumption. Our framework differs from prior research on the effect of payment mechanisms on spending in two significant ways. First, we are interested in the effects of the availability of credit on spending, and not necessarily in the effect of the transaction format that is associated with each payment mechanism. Second, while prior research has studied the point-of-purchase and historic (i.e., prepurchase) effects of credit, the present research is concerned with the availability of credit in the future. Specifically, our framework is invariant to the current and prior usage of credit by the consumer.", "e:keyword": ["Consumer Credit", "Credit Cards", "Intertemporal Choice", "Mental Accounting", "Self-Control"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.1.54.158", "e:abstract": "In a recent article, Broniarczyk et al. (1998) report an interesting finding that the availability of consumers most preferred alternative in an assortment positively influences their perceptions of assortment size. This finding points to the impact of a hitherto unexplored retail strategic dimension, what we call, commitment to assortment consistency. So far, researchers have been examining retail strategies that pertain to assortment width and depth. The consistency factor featuring in the assortments offered by some retailers has largely gone unnoticed or ignored in the extant literature. We seek to address this dimension in our research. By consistency in assortment we mean the tacit promise made by a retailer to carry a given set of brands, sizes, flavors, colors, etc. from one period to next, so that a consumer who looks for his preferred brand (or size, flavor, etc.) will be able to find that brand for sure at that retail store. Of course, not all retailers commit themselves to carrying a consistent assortment. For example, if someone walks into a warehouse club such as Sams Choice, he may be able to buy branded product at a lower price but not brand or size (or color, flavor, etc.) he looks for. One reason for this inconsistency is that these stores make a bulk of their purchases during trade deals that are offered by the branded manufacturers and hence have less say on what they can carry in a given period. In the apparel market, an example of a store that does not carry a consistent assortment is Ross Dress for Less. If we draw a continuum from a point of no commitment (to assortment contents) to a point of full commitment, it is rather obvious that one can locate retailers such as Sams Choice at its low end and retailers such as Macys at its top end. While it is obvious that the mere existence of the consumer segment that looks for consistent assortments will drive some retailers to adopt such commitments to consistent assortments (C2C for short), what is less obvious is that this strategy is affected (negatively) by supply side factors such as the availability of trade deals. This is because while opportunistic buying helps a retailer to reduce his acquisition costs, it introduces inconsistency in the assortment. It is also important to note that although consumers may seek particular brands, their final choice of a retailer is affected also by price and location of the retailers. Thus, it is not clear how a retailer would react in a competitive environment even if a sizable segment of the market seeks consistent assortment. Apart from adopting C2C strategy, offering a service oriented shopping environment (such as having more knowledgeable store personnel and a well-lit parking lot) is another way by which a retailer can increase the store traffic. Our research question is: In a market served by two retailers, what supply and demand conditions would enable one or both retailers to adopt C2C and/or offer service? Out of the various retail market structures possible, one particular structure is of importance to us. This is the market where only one retailer adopts C2C and offers service as well. Our focus on this market structure is motivated by two factors: prevalence of this retail structure in many markets, and availability of enough data from one such market, the Dutch flower market, which we use to validate our theoretical predictions. By using a three-stage game theoretic formulation, we show that in equilibrium only one retailer would adopt C2C and offer service as well if the other retailer does not have a high acquisition cost advantage in the supplier market, and if the cost of offering service is neither too high nor too low. Another interesting result we get is that even when a large section of the market seeks a consistent assortment, it will not be profitable for both the retailers to adopt C2C. This is because a retailer not adopting C2C can still attract customers by passing on his supply side savings to them through low prices and engaging in less price-based competition with the C2C retailer, while adopting C2C in retaliation would bring down the profits of both the retailers. We actually measure the model parameters in the Dutch flower retail market and show that with these values the model predicts the equilibrium outcome (i.e., one retailer alone offering both C2C and service) that characterizes the structure of this market. We carry out sensitivity analysis to demonstrate the robustness of the model prediction.", "e:keyword": ["Assortment Consistency", "Competition", "Service", "Retail Strategy", "Dutch Flower Market"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.1.74.160", "e:abstract": "Asymmetrically positioned retailers, who vary in the quality/in-store service offered, are increasingly using promotional advertisingthe practice of advertising sale prices on familiar merchandise linesto compete for customers who are willing to comparison shop. The objective of this paper is to examine the role of promotional advertising for stores that vary in their quality positioning in competing for customers using a game-theoretic model. Our focus is on two key retail promotional advertising decisions: the frequency with which to advertise price reductions and the accompanying depth of discount. We consider a stylized duopolistic retail market with the two stores that differ in their service positioning. We assume that each store enjoys a relative advantage in serving a subset or segment of customers who regularly visit it and whom we call patrons of the store. We assume that it costs more to shop at the less-frequented store. We further assume that consumers are only partially informed about the prevailing retail priceswhile they perfectly know the posted price at the store that they patronize, they are uncertain about the price at the other store and have rational expectations about these prices. Consumers in this market differ on three dimensions: preference for service, shopping costs, and store switching costs. We explicitly consider two consumer segments differing in their willingness to pay for service. Furthermore, we assume store switching is more costly for the high-valuation segment. We allow for within-segment heterogeneity by assuming that consumers differ in their shopping costs. Our analysis shows that if promotional advertising is not too costly, the equilibrium strategies of the competing retailers entail occasionally posting its regular price but not advertising that price and on other occasions posting its sale price and advertising that price. The analysis also suggests that promotional advertising is driven by offensive (traffic-building) as well as defensive (consumer-retention) considerations. Furthermore, the relative importance of offensive and defensive considerations is influenced by the service positioning of the stores. Specifically, relative to the low-service store, promotional advertising by the high-service store is driven more by offensive consideration than defensive consideration. Finally, a store's service positioning impacts its frequency of promotional advertising and the depth of discount that it offers during sale. Specifically, relative to the low-service store, the high-service store offers advertised sales more frequently but with shallower discounts. These results follow from the fact that differences in service positioning lead to a natural consumer self-selection. Specifically, the consumer-mix of the high-service store comprises a higher fraction of the high-valuation consumers who are less sensitive to promotional advertising due to their higher store switching costs. Thus, if the low-service retailer were to build store traffic by targeting the customer mix of the high-service retailer (motivated by offensive consideration), it has to offer deeper discounts; yet the demand enhancement is lower. Thus, relative to the high-service store, promotional advertising is not that attractive for the low-service store. However, the low-service store still relies on offering discounted prices occasionally to retain its customer base. Thus when using promotional advertising to attract and retain customers, the high-service store should rely more on the frequency cue, while the low-quality store should rely more on the magnitude cue. We provide empirical support for the key predictions of our analytical model by collecting and analyzing retail promotional advertisements for stores that vary in their level of in-store service, published in major newspapers in a large U.S. metropolitan city. We collected data from 813 advertisements across 14 different product groups in the mens and women's categories. The data are consistent with the model's predictions. Our theory and empirical analysis should be of interest to both academics and practitioners, particularly those in the area of channel management and promotional advertising.", "e:keyword": ["Retail Competition", "Store Positioning", "Promotional Advertising", "Shopping Cost", "Traffic Building", "Customer Retention"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.1.97.161", "e:abstract": "As firms jockey to position themselves in emerging markets, firms need to evaluate the relative attractiveness of market expansion in different countries. Since the attractiveness of a market is a function of the eventual market potential and the speed at which the product diffuses through the market, a better understanding of the determinants of market potential and diffusion speed across different countries is of particular relevance to firms deliberating their market expansion strategies. Despite a recent spurt in research on multinational diffusion, there exist significant gaps in the literature. First, existing studies tend to limit their analysis to industrialized countries, thus reducing the ability to generalize the insights to many emerging markets. Second, these studies tend to focus on the coefficients of external and internal influence in the Bass diffusion model but do not analyze the determinants of market potential. Third, the choice of variables that affect the parameters of the Bass diffusion model has been rather limited. In this paper, we seek to address these gaps in the literature. To address the scope issue, we assembled a novel dataset that captures the diffusion of 6 products in 31 developed and developing countries from Europe, Asia, and North and South America. The set of countries in our dataset encompasses 60% of the world population and includes such emerging economies as China, India, Brazil, and Thailand. This should provide us with a stronger basis to make empirical generalizations about the diffusion process. For firms seeking to expand into emerging international markets, our findings about penetration potential have considerable significance. For example, we find that for the set of products that we analyze the average penetration potential for developing countries is about one-third (0.17 versus 0.52) of that for developed countries. We also find that it takes developing countries on average 17.9% (19.25 versus 16.33 years) longer to achieve peak sales. Thus, despite the well-known positive effect of product introduction delays on diffusion speed, we find that developing countries still continue to experience a slower adoption rate, compared to that of developed countries. Our study also investigated the impact of several new macroenvironmental variables on penetration potential and speed. For example, our findings indicate that a 1% change in international trade or urbanization level can potentially change the penetration potential by about 0.5% and 0.2% respectively. These are some of the key variables projected to change significantly over the coming years for developing countries. While business managers have relatively little influence on such variables, our findings can still serve as valuable empirical guide for the variables that they should consider in evaluating diverse international markets and in performing sensitivity analysis with respect to their projected trends. Finally, our study also holds implications for managers seeking to combine information about past diffusion patterns across products and countries for better prediction. We pool information efficiently across multiple products and countries using a Hierarchical Bayes estimation methodology. By sharing information across countries and products in a single, coherent framework, we find that this pooling approach leads to substantial improvements in prediction accuracy. Our technique is particularly superior in predicting sales and BDM parameter values in the early years of new product introduction in a new country, when forecast estimates are managerially most useful. We also decompose the variance in the BDM model parameters into product, country, and product-country components. These results give guidelines to managers about which market experience they should weigh more to arrive at forecasts of market potential and diffusion speed. We find that while past experiences of other products in a country (country effects) are relatively more useful to explain penetration level (cumulative sales), past experiences in other countries where a product was earlier introduced (product effects) are more useful to explain the coefficients of external and internal influence (and thus the speed with which the product will attain peak sales).", "e:keyword": ["Diffusion", "International Marketing", "Hierarchical Bayes", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.1.0.157", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.21.2.119.147", "e:abstract": "There are now available a number of new subscription services that comprise a dual pricing system of a monthly access fee (rental) and a per-minute usage charge. Examples include cellular phones, the Internet, and pay TV. The usage and retention of such services depend on the absolute and relative prices of this dual system. For instance, a moderate access fee but a low-usage charge might initially appeal to customers, but later a low-usage customer might find the monthly fee unjustified and thereby relinquish the service. Providers of such services, therefore, usually offer several pricing packages to cater to differing customer needs. The purpose of this study is to derive a revenue-maximizing strategy for subscription services, that is, the combination of access and usage price that maximizes revenue over a specified time period. An additional objective is to determine access and usage price elasticities because they have historically played an important role in theoretical pricing models. The application area is the cellular phone market, but for a new rather than an existing product. To help gauge the likely usage rates and customer retention, a field experiment is conducted in which several alternative price combinations are used. Specifically, a sample of potential residential customers (most of whom did not have an existing cell phone) were divided into four treatment groups. The first group were not charged an access fee but did have to pay a small per-minute usage charge. The second group also paid a small usage charge but in addition had three access price increases over the duration of the trial. The third group paid no access fee but had usage charge increases, while the fourth group had both access fee and usage charge increases. Usage levels for each respondent are recorded, as is their month of dropout if they discontinue the service. An initial examination of the data shows that higher access fees result in higher customer attrition, and higher usage cost results in lower usage. Furthermore, usage and retention are related in that declining usage levels over time often signal impending customer attrition. Hence, two phenomena need to be modeled: usage of the service and customer retention conditional on usage. Some seasonal effects are also observed and are allowed for in the model. Modeling customer attrition simultaneously with usage is important because ignoring customer attrition will likely result in an underestimate of price sensitivity. This results from a censoring effect, whereby respondents who remain in the trial tend to be wealthier, and hence, less price sensitive. Given the known problems of ignoring customer attrition, we develop a theoretical model of usage, which explicitly incorporates attrition by extending a time-series model introduced by Hausman and Wise (1979). We make two extensions of the Hausman and Wise model. The first is to generalize it from two to many time periods and the second is to allow for respondent heterogeneity by incorporating latent classes. We fit the model by maximum likelihood and find that a two-segment model is best. In addition, we examine the predictive validity of our model and find it to be reasonably good. In general, the results show that access and usage prices have different relative effects on demand and retention. There are five key results. First, access price has some effect on usage but a much stronger effect on retention. Second, usage price has a strong effect on usage and a moderate effect on retention, in that if usage price increases so much that usage declines, then lower usage levels results in higher attrition. Third, access price elasticity is about half that of usage price, with both elasticities generally being much smaller than 1, indicating relative inelasticity for this particular service. Fourth, customer attrition rate (churn) is much more sensitive to access than usage price and, last, if just observed usage is examined and customer attrition is ignored, then price sensitivity is very likely to be substantially underestimated (on the order of 45% in our case). Finally, when developing the revenue-maximizing price combination we allow for the cost of customer acquisition by using some typical advertising-to-sales ratios for the telecommunications industry. We find that the revenue maximizing price is $27.70 per month for the access fee and $0.81 per minute for the airtime charge. These values are in line with current access fees and usage costs in the given market.", "e:keyword": ["Attrition", "Field Experiment", "Pricing", "Price Elasticity", "Subscription", "Telecommunications"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.2.139.149", "e:abstract": "Many established industries, such as the online service industry, the telecommunication industry, or the fitness club industry, are access service industries. When using services in these industries, consumers pay for the privilege of accessing the firm's facilities but do not acquire any right to the facility itself. A firm's pricing decisions in access industries frequently come down to a simple choice among pricing, pricing, or pricing. However, it is not so simple for firms in those industries to make this choice. Access service firms typically face a mix of consumers who have intrinsically different usage rates. A key characteristic of access service firms, however, is that the cost of providing an additional minute of usage is typically negligible, as long as the firm has the necessary capacity to serve its customers. Service capacity, which corresponds to the total available time on a firm's system, is often limited. In this paper, we show that service capacity and consumer usage heterogeneity are two important factors that determine a firm's optimal choice. We develop a model that incorporates these two salient characteristics shared by access industries and study what determines a firm's choice among the three alternative pricing structures ( pricing, pricing, or pricing). Our analysis shows that, in the presence of consumer usage heterogeneity, service capacity mediates a firm's optimal choice in a complex, yet predictable way. A firm's choice also hinges on whether heavy or light users are more valuable in terms of their willingness-to-pay on a per-unit-capacity basis. The presence of both consumer usage heterogeneity and capacity constraints prompts a firm to choose its pricing structure to attract a desired customer mix and to price discriminate. As a result, two-part tariff pricing is not always optimal in access industries, and a firm's pricing structure can vary in a complex way with the interaction of those two factors. Specifically, we show that when light users are more valuable, a firm may use a two-part tariff or a flat fee, depending on whether the firm is constrained by its service capacity, but never charge a usage price alone or offer any signing bonus (a negative flat fee or a flat payment to customers). When heavy users are more valuable, a firm may choose to set a usage price, a signing bonus plus a usage price, or flat fee. Interestingly, regardless of whether heavy or light users are more valuable in an access service industry, only flat rate pricing is a sustainable pricing structure once the industry has developed sufficient excess capacity. We also show that the optimal pricing strategy in access industries can have some intriguing, nonintuitive implications that have not been explored elsewhere. For instance, when the industry capacity is unevenly distributed between competing firms, the large-capacity firm may well be advised to increase, rather than to decrease, its price to accommodate the small firm. It would be too costly and too tactless for the large firm to do otherwise. In fact, the strategy of accommodation calls on the larger firm to retreat in both light and heavy user markets and leave more of its capacity idle and more of the market demand unmet when the small firm's capacity (hence, the industry capacity) increases. This implies that incremental policy measures that encourage the growth of smaller companies in the presence of a large company can be welfare-decreasing because the growth of smaller firm can force the retreat of a large company at the expense of market coverage. Today, services account for two-thirds to three-quarters of the GNP, not only in the United States but also in many industrial countries. Access industries are growing rapidly to exert profound impact on today's economy. However, service pricing in general and pricing access services in particular have not received adequate attention in the literature. In this paper, we take the first step in understanding how capacity constraints and consumer usage heterogeneity mediate the choice of pricing structures in both monopolistic and competitive contexts.", "e:keyword": ["Pricing Strategy", "Service Pricing", "Competitive Strategies", "Access Services", "Capacity", "Equilibrium Models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.2.160.154", "e:abstract": "The identification of geographic target markets is critical to the success of companies that are expanding internationally. Country borders have traditionally been used to delineate such target markets, resulting in accessible segments and cost efficient entry strategies. However, at present such \"countries-as-segments\" strategies may no longer be valid. In response to the accelerating trend toward global market convergence and within-country fragmentation of consumer needs, cross-national consumer segmentation is increasingly used, in which consumers in different countries are grouped based on the similarities in their needs, ignoring the country borders. In this paper, we propose new methodology that helps to improve the identification of spatial segments by using information on the location of consumers. Our methodology identifies spatial segments based on consumer needs and at the same time uses spatial information at the subcountry level. We suggest that segments of consumers are likely to demonstrate spatial patterns and develop a hierarchical Bayes approach specifying several types of spatial dependence. Rather than assigning consumers to segments, we identify spatial segments consisting of predefined regions. We develop four models specifying different types of spatial dependence. Two models characterize situations of and , which represent existing approaches to international segmentation. The other two models accommodate within and of segments and are new to the segmentation literature. The models account for within-segment heterogeneity in multiattribute-based segmentation, covering numerous applications in response-based market segmentation. We show that the models can be estimated using Gibbs sampling, where for the spatial contiguity model, a rejection sampling procedure is proposed. We conduct an analysis of synthetic data to assess the performance of the most restrictive spatial segmentation model in situations where spatial patterns do or do not underlie the data-generating process. Data for which the true properties are known were analyzed with models of spatial contiguity and spatial independence of segments. The results indicate that a substantial improvement in parameter recovery may be realized if a spatial pattern underlies the data-generating process, but that the spatial-independence model may provide a better alternative when this is not the case. We empirically illustrate our approach in the setting of international retailing, using survey data collected among consumers in seven countries of the European Union. A store image measurement instrument was used. This instrument is based on the multiattribute model of store image formation, with overall evaluations of stores as a dependent variable and image perceptions as predictor variables. The segmentation basis consists of (latent) importances of store image attributes, i.e., product quality, service quality, assortment, pricing, store atmosphere, and location. We argue that store image attribute importances are likely to display spatial variation and expect spatial concentration of segments, or even contiguity, to occur. We apply and compare the four spatial segmentation models to the store image data. The countries-as-segments model receives lowest support from the data, less than that of the spatially independence model, which is in line with the current notion that consumer preferences cut across national borders. However, the spatially contiguity model and spatial-association model demonstrate the best fit. Although the differences between the various models are not very large, we find support, consistent across the two fit indices, for the spatial models. Substantive results are presented for the spatial contiguity model. We identified five spatial segments that cut across borders. The segments give rise to different retail positioning strategies, and their importance estimates and location demonstrate face validity.", "e:keyword": ["International Market Segmentation", "MCMC Estimation", "Spatial Information"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.2.178.148", "e:abstract": "People are frequently exposed to potentially attractive events that are subsequently and unexpectedly reversed and to potentially painful events, which are also unexpectedly reversed. In the process of being returned to the initial asset position, does the sequence in which the positive and negative events occur matter? This issue of the combined effect of pleasurable and painful stimuli has received scant theoretical or empirical attention. We attempt to fill this lacuna in the literature by studying the retrospective evaluation of surprises that return individuals to their original economic state. Although such surprises do not change an individual's original economic state, we argue that the individual's psychological state changes, and the final affective state is, among other things, a function of the sequence in which the events occur. From a theoretical standpoint, several perspectives can be brought to bear on the issue. For instance, one reading of mental accounting, based on prospect theory's value function, would predict that losses should dominate gains, and therefore, regardless of sequence, people should be unhappy when exposed to two economically equivalent outcomes of different signs. Conversely, the literature on intertemporal choice would suggest that a series that ends on an up note is preferred to a series that ends on a down note, because people like to defer gratification so that they may savor positive outcomes. Similarly, people apparently have a preference for \"happy endings.\" Finally, the extant literature on \"recency effects\" would predict that the last event in a series should have a disproportionate influence on overall affect. Our model relies on a shift in the reference point to explain how a surprising reversal of an event will lead to a nonzero evaluation of the sequence. We suggest that people's reference points shift immediately but imperfectly after a stimulus is presented. Intuitively, this implies that the first stimulus will shift the reference point in its direction, as a result of which the evaluation of a sequence of events in which an initial event is unexpectedly reversed will be more favorable if the first event is a loss than if it is a gain. This model captures the unanticipated nature of the second event (i.e., the surprise element) by allowing the first event to move the reference point. Consequently, by the time the next event occurs the reference point has been updated, as a result of which the zero economic outcome of the sequence yields nonzero utility. We further posit that the magnitude of the reference point shift should be affected by the time elapsed between the two stimuli. Specifically, the reference point shifts gradually with time, until it is fully updated. Consequently, the final affective state of the sequence is also a function of the temporal distance between the two events. The main predictions of the model were empirically supported first in a survey using a mall-intercept sample. Subsequently, we conducted a study of student subjects involving a coin-tossing game in which real money was at stake and in which subjects in one condition experienced the second outcome after a two-day delay. Our results from this second study supported the model's prediction regarding the impact of the elapsed time between the events. The experimental tasks involved surprising reversals of initial outcomes, thus ensuring that \"savoring/dread\" types of explanations (which require that subjects anticipate the second event) could not be operating. Finally, in a series of three follow-up studies, we tested the claim that the magnitude of outcomes would have an impact on observed affect, and consistent with our theory and contrary to recency predictions, we observed similar results across different magnitudes. While theoretically interesting, we should also note that our research is of potential pragmatic significance. People's reactions to a series of events is of considerable interest to marketers desirous of generating enhanced attitude, affect, purchase intention, and the like without offering economic inducements such as rebates, coupons, or other costly discounts. Additionally, public policy officials may be interested in protecting people from being manipulated into purchasing a product simply because of changes in the sequence in which a series of offers is made by the merchant.", "e:keyword": ["Sequences", "Surprises", "Judgment and Decision Making"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.2.197.153", "e:abstract": "The increasing availability of customer information is giving many firms the ability to reach and customize price and other marketing efforts to the tastes of the individual consumer. This ability is labeled as consumer addressability. Consumer addressability through sophisticated databases is particularly important for direct-marketing firms, catalog retailers such as L.L Bean and Land's End, credit card-issuing banks, and firms in the long-distance telephone market. We examine the strategic implications of consumer addressability on competition between database/direct marketing firms. We address questions such as: In a competitive environment, how should firms invest in addressability? Will future improvements in the degree of addressability increase or mitigate the intensity of competition between the firms? Will greater addressability always be beneficial for firms? We model competition between two firms in a market where consumers differ on a horizontal attribute of product differentiation. The market comprises consumers located on a linear attribute space and firms located at the ends of the line. We represent the degree of addressability (or the reach of a firm's database) as the proportion of consumers at each point in the market who are in the firm's database. Consequently, the firm can offer these consumers customized prices. Consumer addressability creates two effects that govern the competition between firms: a \"surplus extraction\" effect because a firm might address a consumer who is not reached by its competitor and a \"competitive\" effect that is created by the set of consumers who can be addressed by both firms. The key results of the paper pertain to when the addressability decision is endogenous. When the extent of market differentiation (or consumer heterogeneity in preferences for a product/brand attribute), as well as the incremental cost of addressability, are sufficiently large, firms make symmetric investments in equilibrium. Given high costs, firms choose sufficiently low levels of addressability. Low addressability and high levels of market differentiation both help reduce price competition, which facilitates symmetric choice of addressability by the firms in equilibrium. However, when market differentiation and the cost of incremental addressability become small, firms face the prospect of destructive competition. As a result, they strategically differentiate in their choice of addressability to mitigate this competition. Interestingly, even in the extreme case when incremental addressability is costless, not every firm chooses full addressability in equilibrium. This has useful implications for direct marketing. Given that the advances in information technology should improve the ability of firms to address their consumers, it might indeed not be desirable for all direct marketing firms to indefinitely pursue greater addressability as costs of doing so decline. The analysis also shows an interesting effect of market differentiation in addressable markets: Equilibrium profits can decrease with an increase in market differentiation when the marginal cost of addressability is sufficiently high. Finally, we discuss the competitive outcome that would result when firms compete with addressable as well as uniform posted prices.", "e:keyword": ["Customized Pricing", "Direct Marketing", "Database Marketing", "Consumer Addressability", "Marketing Information", "Individual Marketing", "Competitive Price Discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.2.209.151", "e:abstract": "Assessment of accurate market size and early adoption patterns is essential to strategic decision making of managers involved in new-product launches. This article proposes methodology that explains changes in parameter estimates of the Bass model, (coefficient of innovation), (coefficient of imitation), and (market penetration rate) by direction of \"extra-Bass\" skew in the data, or equivalently, by underlying heterogeneity of the population. This research shows significantly opposite patterns of these parameter estimates, depending on skew of the diffusion curve detected by a generalized model, i.e., the gamma/shifted Gompertz (G/SG) model, which embeds the Bass model as a special case. The G/SG model originally presented in Bemmaor (1994) is based on two assumptions: (1) Individual-level times to first purchase are distributed shifted Gompertz and (2) individual-level propensity to buy follows a gamma distribution across the population. We assume that the scale parameter of the shifted Gompertz distribution is constant across consumers. The advantage the G/SG model has over alternative diffusion models such as the nonuniform influence model is that its cumulative distribution function takes a closed-form expression. In line with Van den Bulte and Lilien (1997), we analyze these opposite patterns from simulated data using the G/SG model as the true model and 12 real adoption data sets. The patterns are: (1) as the level of censoring decreases, the estimates of and decrease and those of increase when data exhibit more right skew than the Bass model and (2) the estimates of and increase and those of q decrease when data exhibit more left skew than the Bass model. For the simulated data, we manipulated four dimensions: (1) \"extra-Bass\" skew in the data, (2) ratio , (3) speed of diffusion, and (4) error variance. Both results of the simulated data and the real adoption data sets confirm the existence of two opposite patterns of parameter estimates of the Bass model depending on \"extra-Bass\" skew. When the model is correctly specified with simulated data, estimates of increase and those of decrease for both the Bass and the G/SG models. The estimates of increase as one adds data points only for the G/SG model. No significant tendency in parameter estimates of was detected for the Bass model. As for ill-conditioning issues, systematic changes in the parameter estimates of the G/SG model can be substantially larger in some cases than those obtained with the Bass model, even though the data were generated by taking the G/SG model as the true one. Therefore, model complexity can aggravate the tendency for parameters to change systematically as one adds data points. The forecasting results from the simulated data show the supremacy of the G/SG model. It provides more accurate results than the Bass model in the one-step ahead, two-step ahead, and three-step ahead forecasts. With the real data set, the G/SG model provides more accurate one-step ahead forecasts than the Bass model, but the model's forecasting performance deteriorates more rapidly than the Bass model when one shifts to two-step ahead and three-step ahead forecasts. The systematic changes in parameter estimates are larger for the more complex model. Our research shows that the G/SG model is a flexible model used to analyze the systematic changes in parameter estimates when specification error and ill-conditioning occur. As our findings incorporate two possible types of parameter estimate bias, compared to the previous single-direction view, they can provide essential information to enhance forecasting accuracy of products and services using new technological innovations. Our forecasting results of simulated and real adoption data raise a question about the optimal horizon of forecasting in applying flexible models of diffusion. The G/SG model also provides grounds to investigate jointly \"the speed of takeoff\" and \"the diffusion speed after takeoff\".", "e:keyword": ["Diffusion", "New-Product Diffusion", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.2.0.150", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.223.145", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.229.143", "e:abstract": "Consumers are often observed to purchase more than one variety of a product on a given shopping trip. The simultaneous demand for varieties is observed not only for packaged goods such as yogurt or soft drinks, but in many other product categories such as movies, music compact disks, and apparel. Multinomial (MN) choice models cannot be applied to data exhibiting the simultaneous choice of more than one variety. The random utility interpretation of either the MN logit or probit model uses a linear utility specification that cannot accommodate interior solutions with more than one variety (alternative) chosen. To analyze data with multiple varieties chosen requires a nonstandard utility specification. Standard demand models in the economics literature exhibit only interior solutions. We propose a demand model based on a translated additive utility structure. The model nests the linear utility structure, while allowing for the possibility of a mixture of corner and interior solutions where more than one but not all varieties are selected. We use a random utility specification in which the unobservable portion of marginal utility follows a log-normal distribution. The distribution of quantity demanded (the basis of the likelihood function) is derived from these log-normal random utility errors. The likelihood function for this class of models with mixtures of corner and interior solutions is a mixed distribution with both a continuous density portion and probability mass points for the corners. The probability mass points must be calculated by integrals of the log-normal errors over rectangular regions. We evaluate these high-dimensional integrals using the GHK approximation. We employ a Bayesian hierarchical model, allowing household-specific utility parameters. Our utility specification related to the approach of Wales and Woodland (1983) who employ a translated quadratic utility function. Wales and Woodland were only able to study, at the most, three varieties because there was no practical way to evaluate the utility function at that time. In addition, the quadratic utility specification is not a globally valid utility function, making welfare computations and policy experiments questionable. Hendel (1999) and Dube (1999) present an alternative approach in the utility function which is constructed by summing up over unobservable consumption occasions. While only one variety is consumed on each occasion, the marginal utilities of varieties change over the consumption occasions, giving rise to a simultaneous purchase of multiple varieties. Our Bayesian inference approach allows us to obtain individual household estimates of utility parameters. Household utility estimates are used to compute the value of each variety. We compute a compensating value for the removal of each flavor; that is, we compute the monetary equivalent of the household's loss in utility from removal of a flavor. These calculations show that households highly value popular flavors and would incur substantial utility losses from removal of these flavors from the yogurt assortment. Next we consider the implications of our model for retailer assortment and pricing policies. Given limited shelf space, only a subset of the possible varieties can be displayed for purchase at any one time. If consumers value variety, then a retailer with lower variety must compensate the consumers in some way, such as a lower price level. We see this trade-off between price and variety across different retailing formats. Discount or warehouse format retailers often have both lower variety and lower prices. To measure this trade-off, we explore the utility loss from reduction in variety and find the reductions in price that will compensate for this utility loss. These price reduction calculations must be based on a valid utility structure. Heterogeneity in tastes is critical in these utility computations and policy experiments. We find that a relatively small fraction of households with extreme preferences dominate the compensating value computations. That is, some households are observed to purchase mostly or exclusively one variety. These households must be heavily compensated for the removal of this variety from the assortment. In some retailing contexts, customization of the assortment is possible at the customer level. We show that such customization virtually eliminates any utility loss from reduction in variety.", "e:keyword": ["Variety", "Corner Solutions", "Assortment", "Translated Utility"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.251.146", "e:abstract": "Marketing channel interactions typically feature three characteristics that have not been incorporated together in an analytic study: (1) the parties can do business repeatedly over time, often under different terms of trade (e.g., prices may vary), (2) the terms that the seller offers one buyer may be different from those she offers another, giving each interaction the flavor of bilateral monopoly bargaining, and (3) the buyer and seller come to the interaction uncertain about the valuations each holds for the good, but they do know each other's for valuation. The seller might, for example, come to the bargaining table aware that the buyer has a strong reputation for being willing to pay only low prices, and the buyer might come aware that the seller is strongly reputed for high cost and is, therefore, willing to offer only high prices. The latter characteristic raises an interesting question: When engaged in a marketing channel interaction, what type of reputation is best for a buyer or seller to take to the bargaining table? In this paper, we answer that question by incorporating each of the characteristics that typify channel interactions in a formal game-theoretic bargaining model. We determine how the reputations that buyers and sellers bring to the bargaining table affect their equilibrium strategies and payoffs. Our analysis shows that, in general, the best reputation for the seller to take to the bargaining table is one that makes the buyer nearly certain in his belief that the seller's cost is high, a result that matches intuition. The best reputation for the buyer, however, is counterintuitive. We show that an increase in the buyer's reputed willingness to pay can actually cause the seller to offer a price. The best reputation for the buyer to take to the bargaining table is, therefore, one that makes the seller believe that there is a significant chance that he is willing to pay a high price. This result is new to the literature and brings with it immediate managerial implications that we discuss. Our analysis also shows that modeling the buyer as a forward-looking strategic player yields different results than does following the normal convention of modeling the buyer as a nonstrategic price-taker. We discuss why future research on channels and on reference-dependent utility theory should consider these differences.", "e:keyword": ["Bargaining", "Channels of Distribution", "Game Theory", "Pricing Research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.273.137", "e:abstract": "The Internet has led to a large number of third-party sources that offer high-quality information about firms's products at little or no cost to consumers. As a result, many of these sources have grown in popularity, extending well-beyond the usual reach of traditional third parties such as and . For example, the online version of offers, at no cost to consumers, information about new products, existing products, long-term tests, and buyers' guides, all relating to the automotive industry. AvWeb.com delivers weekly aviation news and new product reviews to its readers, and a large number of websites follow developments on computer platforms such as the Apple Macintosh. In this paper we analyze how the provision of third-party information affects the division of profits in a multiproduct distribution channel. To illustrate, consider the competition between Microsoft and Apple in the operating systems (OS) market and their channel relationship to CompUSA, a retailer that sells both Macs and Windows-based PCs. Consider two pieces of thirdparty information. First, suppose that CNET, an Internet technology site, reviews the newest upgrade of the MacOS and writes that the new user interface is even easier to use than previously. Second, suppose that an article in the technology section of the notes that changes in Apple's networking support now enable Macs to be better integrated into PC networks. These two pieces of information are similar in the sense that they both express good news about the MacOS and thus they both can be expected to benefit Apple by increasing consumer demand for Macs. One might also expect that in both cases CompUSA will capture some of the gains that come from the increased demand for Macs and that Microsoft will lose because the good news about the MacOS will induce some consumers to choose Macs over Windows-based PCs. However, we will show that this intuition is incorrect. The two reviews can have surprisingly different implications for the profits of Microsoft and CompUSA. The reason is that the two reviews differ on one crucial dimension: the group of customers for whom they are primarily relevant. The CNET review talks about improvements in the customer interfaceprecisely what Apple's consumers care about. The review talks about compatibility with prevailing PC standardsimportant to consumers who care relatively more about compatibility and who are thus more likely to prefer Windows (Apple's consumers). We show that good news about the MacOS that is more relevant to Apple's core consumers (the CNET review) benefits Microsoft but harms CompUSA, while good news about the MacOS that is more relevant to Apple's noncore consumers (the review) has the opposite effect. It harms Microsoft but benefits CompUSA. Stated more generally, our main result is that when third-party information affects consumers' product valuations, the type of information that induces the change is critical to understanding which firms gain and which firms lose. In particular, depending on the type of third-party information, we find that (1) a retailer can be harmed by good news about a product that it carries; (2) a manufacturer can gain from good news about a rival's product; and (3) good news about a product category need not benefit all the manufacturers in that category. There are three novel features of the analysis. First, we derive the equilibrium division of profit among firms when a retailer sells the products of competing manufacturers, and we have done so while placing few restrictions on the feasible set of contracts. Second, we show how this equilibrium division of profit lends itself to a simple graphical interpretation that depicts which firms gain and which firms lose from third-party information. Third, we provide a taxonomy of information types and identify the key features of each type that cause profit incentives to vary. In particular, we conceptualize information as having three components, namely (1) the products to which the information pertains, (2) whether the information is positive or negative, and (3) the consumers to whom the information is relevant. We show that all three information components play a role in determining the change in each firm's profit. Our framework can also be used to analyze a variety of other settings of interest; for example, it can be used to analyze profit incentives when the retailer has bargaining power, when there is downstream competition, and when there are non-information-based changes in consumers' valuations. In addition, our framework may be used both to analyze the effects on profits of persuasive advertising and to predict advertising content.", "e:keyword": ["Game Theory", "Channel Coordination", "Third-Party Information", "Advertising", "Internet", "Distribution Channel"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.294.140", "e:abstract": "The service-profit chain (SPC) is a framework for linking service operations, employee assessments, and customer assessments to a firm's profitability (Heskett et al. 1994). The SPC provides an integrative framework for understanding how a firm's operational investments into service operations are related to customer perceptions and behaviors, and how these translate into profits. For a firm, it provides much needed guidance about the complex interrelationships among operational investments, customer perceptions, and the bottom line. Implementing the SPC is a pervasive problem among most service firms, and several attempts have been made to model various aspects of the SPC. However, comprehensive approaches to model the SPC are lacking, as most studies have only focused on discrete aspects of the SPC. There is a need for approaches that combine data such as measures of operational inputs, customer perceptions and behaviors, and financial outcomes from multiple sources, providing the firm with not only comprehensive diagnosis and assessment but also with implementation guidelines. Importantly, an approach that is sensitive to and can accommodate the strengths and weaknesses of such data sets is required. We outline and illustrate such an approach in this paper. Our approach has the potential to both identify and quantify the benefits of implementing a service strategy, especially for firms having multiple units (e.g., banks with branches, retail outlets, and so forth). The implementation approach is illustrated using data from a national bank in Brazil. We used customer surveys from more than 500 branches of the bank. Each individual customer's marketing survey data was linked to a number of operational metrics. First, behavioral measures of retention, such as the length of the customer's relation with the bank, the deposit amount, and number of transactions with the bank, were obtained and merged with the survey data. Second, the main branch used by each customer was identified and operational inputs (e.g., number of employees, number of available automated teller machines (ATMs)) used at that branch were obtained and merged with the data set. This data set was used to model the SPC at a and level. The analysis consisted of a structural-equation model that identified the critical conceptual relationships that parsimoniously articulate the SPC for this bank. For instance, from among a variety of attribute-level perceptions, the bank was able to identify those perceptions that were critical determinants of behavioral intentions. Similarly, from a variety of available behavioral metrics, the bank was able to identify those behaviors most relevant to profitability. The utilized Data Envelopment Analysis (DEA) and provides customized feedback to each branch in implementing the strategic model. It provides each branch with a metric of its relative efficiency in translating inputs such as employees and ATMs into relevant strategic outcomes such as customer intentions and behaviors. Our illustration shows how top management can use the strategic and operational analysis in tandem. Whereas the strategic model provides the key relationships and metrics that are needed to ensure that all subunits of the firm follow a consistent strategy, the operational analysis enables each branch to benchmark its unique position so that the branch can implement the strategic model in the most efficient way. Thus, . For this bank, the operational analysis shows that for a branch to achieve superior profitability, it is important that the branch manager not only be efficient in achieving superior satisfaction (as indicated in positive behavioral intentions) but also be efficient in translating such attitudes and intentions into relevant behaviors. In other words, superior satisfaction alone is not an unconditional guarantee of profitability.", "e:keyword": ["Service", "Profitability", "Service-Profit Chain", "Retention", "Satisfaction", "Banking", "Financial Services"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.318.139", "e:abstract": "The literature on cross-national diffusion models is gaining increased importance today due to the needs of present day managers. New product sales growth in a given nation or society is affected by many factors (Rogers 1995), and of these, sociocontagion (or word of mouth) has been found to be the most important factor that characterizes the diffusion process (Bass 1969, Moore 1995). Hence, it is interesting and perhaps challenging to analyze what would happen if a new product diffuses in parallel in two neighboring but culturally different countries. Not only will we expect the diffusion process in the two countries to be different, but we will also expect some interaction among them, especially if the two societies mingle with each other. There are two streams of research in cross-national diffusion. The first type focuses on exploring the differences between diffusion processes in two countries and finding out whether those differences can be attributed to social and cultural differences between the countries involved. Examples of this type of research are found in Takada and Jain (1991), Gatignon et al. (1989), Helsen et al. (1993), and Kumar et al. (1998). These studies did find some relationship between the cultural differences of the countries studied and the differences in the diffusion process. The second stream of research focuses on modeling explicitly the interaction between the diffusion processes in two countries. The interaction is typically captured through lead-lag effect (Eliashberg and Helsen 1996, Kalish et al. 1995), where the sales process in the lead country (i.e., the country where the product was first introduced) is modeled to affect the sales process in the lag country (i.e., the country where the product was introduced a few years later). Another method to study the interaction among the diffusion processes in two countries was suggested by Putsis et al. (1997), who used a mixing model to empirically explore the existence of such interactions. These studies basically observed that, when a new product is introduced early in one country and with a time lag in subsequent countries, the consumers in the lag countries learn about the product from the lead country adopters, resulting in a faster diffusion rate in the lag countries. Ganesh and Kumar (1996) formulized this effect as the learning effect and, subsequently, Ganesh et al. (1997) found this learning effect to be influenced by country-specific factors (cultural similarity, economic similarity, and time lag elapsed between the lead and the lag countries) and product-specific factors (continuous vs. discontinuous innovation and the presence or absence of a standardized technology). A careful analysis of the extant literature on the second stream of research would reveal that neither the learning effect model nor the mixing model can be modified to accommodate the other model. Our contribution to the literature exactly addresses this point. In this paper, an alternative framework is proposed that has two unique features. First, the framework is flexible enough to not only account for the lead country affecting the lag countries and vice versa, but also to accommodate the simultaneous interaction among countries in explaining the diffusion processes in the countries concerned. Using multiple product categories and a variety of new product introduction situations, we empirically demonstrate the flexibility and efficiency of our proposed framework. We found strong evidence of all types of interactions, namely, lead lag, lag lead, and simultaneous, which evidence suggests that one cannot afford to omit any of the interactions. The second unique feature of our paper is the estimation procedure that we used. Because statistical estimation of a dynamic process that includes lead-lag, lag-lead, and simultaneous types of causality within a single framework is not straightforward, we suggest an iterative estimation procedure for the estimation. This new procedure not only proved to be flexible in accommodating different types of interaction, but also converged rather quickly in all of the cases that we empirically tested. Noting that the statistical properties of these estimators are not generally available, we carried out a simulation exercise that clearly revealed the efficiency of the proposed estimation procedure. After analyzing the interaction, we went further and showed that the magnitude of the cross-national influences is affected by certain country-specific and product-specific factors. The flexibility of the proposed method over the existing methods is demonstrated through obtaining superior forecasts with the proposed method. Several interesting insights for managers concerned with formulating international marketing strategies are offered.", "e:keyword": ["Multinational Diffusion", "Iterative Estimation", "Lead-Lag", "Lag-Lead", "And Simultaneous Effects", "International Marketing Strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.331.144", "e:abstract": "In recent years, interest in category management has surged, and as a consequence, large retailers now systematically review their product assortments. Variety is a key property of assortments. Assortment variety can determine consumers' store choice and is only gaining in importance with today's increasing numbers of product offerings. To support retailers in managing their assortments, insight is needed into the influence of assortment composition on consumers' variety perceptions, and appropriate measures of assortment variety are required. This paper aims to extend the assortment variety model recently proposed by Hoch et al. (1999) in Marketing Science. It conceptualizes assortment variety from an attribute-based perspective and compares this with the productbased approach of Hoch, Bradlow, and Wansink (HBW). The attribute- based approach offers an alternative viewpoint for assortment variety. Attribute- and product-based approaches reflect basic conceptualizations of assortment variety that assume substantially different perception processes: a consumer comparing products one-by-one versus a consumer examining attributes across products in the assortment. While the product-based approach focuses on the dissimilarity between product pairs in an assortment, the attribute-based approach that we propose focuses on the marginal and joint distributions of the attributes. We conjecture and aim to show that an attribute-based approach suffices to predict consumers' perceptions of assortment variety. In operationalizing the attribute-based approach, two measures of assortment variety are described and compared to productbased measures. These two measures relate to the dispersion of attribute levels, e.g., if all products have the same color or different colors, and the dissociation between attributes, e.g., if product color and size are unrelated. The ability of product-based and attributed- based measures to predict consumers' perceptions of assortment variety is assessed. The product-based measures () tap the dissimilarity of products in an assortment across attributes. The attribute-based measures tap the dispersion of attribute levels across products () and the dissociation between product attributes (1) in an assortment. In two studies, we examine the correlations between these measures in a well-behaved environment (study 1) and the predictive validity of the measures for perceived variety in a consumer experiment (study 2). Study 1, using synthetic data, shows that the attribute-based measures tap specific aspects of assortment variety and that the attribute-based measures are less sensitive to the size of assortments than product-based measures are. Whereas HBW focus on assortments of equal size, study 1 indicates that an extension to assortments of unequal size results in summed Hamming measures that correlate highly with assortment size. The latter is important when assortments of different size are compared. Next, we examine how well the measures capture consumers' perception of variety. Study 2, a consumer experiment, shows that the attribute-based measures account best for consumers' perceptions of variety. Attribute- based measures significantly add to the prediction of consumers' perceptions of variety, over and above the product-based measures, while the reverse is not the case. Interestingly, this study also indicates that assortment size may not be a good proxy for perceived assortment variety. The findings illustrate the value of an attribute-based conceptualization of assortment variety, since these measures (1) correlate only moderately with assortment size and (2) suffice to predict consumers' perceptions of assortment variety. In the final section we briefly discuss how attribute-based and product-based measures can be used in assortment management, and when productand attribute-based approaches may predict consumers' variety perceptions. We discuss how an attribute-based approach can identify which attribute levels and attribute combinations influence consumers' perceptions of variety most, while a productbased approach can identify influential products. Both approaches have applications in specific situations. For instance, an attributebased approach can identify influential attributes in an ordered, simultaneous presentation of products, while a product-based approach can assess the impact of sequential presentations of products better. In addition, we indicate how the random-intercept model estimated in study 2 can be further extended to capture the influence of, e.g., consumer characteristics.", "e:keyword": ["Retailing", "Product Assortment", "Variety Perception", "Variety Measurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.342.141", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.21.3.347.138", "e:abstract": "Marketers have long struggled with developing forecasts for new products before their launch. We focus on one data sourceadvance purchase ordersthat has been available to retailers for many years but has rarely been tied together with postlaunch sales data. We put forth a duration model that incorporates the basic concepts of new product diffusion, using a mixture of two distributions: one representing the behavior of innovators (i.e., those who place advance orders) and one representing the behavior of followers (i.e., those who wait for the mass market to emerge). The resulting mixed-Weibull model specification can accommodate a wide variety of possible sales patterns. This flexibility is what makes the model well-suited for an experiential product category (e.g., movies, music, etc.) in which we frequently observe very different sales diffusion patterns, ranging from a rapid exponential decline (which is most typical) to a gradual buildup characteristic of sleeper products. We incorporate product-specific covariates and use hierarchical Bayes methods to link the two customer segments together while accommodating heterogeneity across products. We find that this model fits a variety of sales patterns far better than do a pair of benchmark models. More importantly, we demonstrate the ability to forecast new album sales before the actual launch of the album, based only on the pattern of advance orders.", "e:keyword": ["Advance Selling", "Diffusion", "Forecasting", "Entertainment Marketing", "Hierarchical Bayes Analysis", "Stochastic Models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.4.369.132", "e:abstract": "We argue that: (1) whether articles contain numeric data should be irrelevant to the evaluation process; (2) the desirability of numeric real, numeric synthetic, or nonnumeric data depends on the research objective; (3) assumptions can and should sometimes substitute for additional data; and (4) equal scrutiny should be given to data collection procedures, regardless of whether the researcher influences the collection or not. Finally, rather than focusing on data, evaluation of research should focus on whether the research provides compelling evidence for the conclusions.", "e:keyword": ["Numeric Data", "Nonnumeric Data", "Synthetic Data", "Marketing Theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.4.378.134", "e:abstract": "We examine the retailer's store brand positioning problem. Our game-theoretic model helps us identify a set of conditions under which the optimal strategy for the retailer is to position the store brand as close as possible to the stronger national brand. In three empirical studies, we examined whether market data are consistent with some of the implications of our model. In the first study, using observational data from two US supermarket chains, we found that store brands are more likely to target stronger national brands. Our second study estimated cross-price effects in 19 product categories, and found that only in categories with high-quality store brands, store brand and the leading national brand compete more intensely with each other than with the secondary national brand. In a third product perception study, we found that although explicit targeting by store brands influenced consumer perceptions of physical similarity, it had no influence on consumers' perceptions of overall or product quality similarity. While it appears that retailers do follow a positioning strategy consistent with our model, it changes buying behavior in the intended fashion only if the store brand offers quality comparable to the leading national brands.", "e:keyword": ["Store Brands", "Private Labels", "Positioning", "Retailing", "Game Theory", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.4.398.130", "e:abstract": "A firm contemplating improvements to its product attributes would be interested in the dollar value the market attaches to any potential product modification. In this paper, we derive a measure of market value such that the comparison of the measure against the incremental unit cost of the attribute improvement is key in deciding whether or not the attribute improvement is profitable. Competition from other brands, the potential for market expansion, and heterogeneity in customer preference structures are explicitly modeled using the multinomial logit framework. The analysis yields a closed form expression for the market's value for an attribute improvement (MVAI). A key result we obtain is that customers should be differentially weighted based on their probability of purchasing the firm's product. In particular, customers who exhibit a very high or very low probability of choosing the firm's product should receive less weight in detemining MVAI. Because the probability of choice varies across products, the answer to the question of how much the market values an improvement depends on which firm is asking the question. It is shown that customers whose utilities have a greater random component should be weighted less. Furthermore, the measure developed is robust to the influence of outliers in the sample. An empirical illustration of the MVAI measure in the context of a new product development study is provided. The study illustrates the advantages of the proposed measure over currently used approaches and explores the possibility of competitive price reactions.", "e:keyword": ["New Product Development", "Product Positioning", "Multibrand Competition", "Conjoint Analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.4.412.135", "e:abstract": "An interesting phenomenon has been the emergence of infomediaries in the form of Internet referral services in many markets. These services offer consumers the opportunity to get price quotes from enrolled brick-and-mortar retailers and direct consumer traffic to particular retailers who join them. This paper analyzes the effect of referral infomediaries on retail markets and examines the contractual arrangements that they should use in selling their services. We identify the conditions necessary for the infomediary to exist and explain how they would evolve with the growth of the Internet. The role of an infomediary as a price discrimination mechanism leads to lower online prices. Perhaps the most interesting result is that the referral infomediary can unravel (i.e., no retailer can get any net profit gain from joining) when its reach becomes too large. The analysis also shows why referral infomediaries would prefer to offer geographical exclusivity to joining retailers.", "e:keyword": ["Referral Services", "Infomediaries", "Intermediaries", "Internet", "Price Discrimination", "Retail Competition", "Exclusive Contracts"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.4.435.136", "e:abstract": "Marketing researchers and practitioners are interested in consumer loyalty because of its managerial consequences. Previous empirical studies find that consumers are loyal not only to a brand, but also to a firm (umbrella brand). That is, even when firms offer products, consumers tend to continue to purchase from the same firm. This repeat-purchase behavior might result from or from . The meaning of state dependence is that the current choice behaviorally depends on the previous one. The traditional model of state dependence assumes that the previous choice affects the current . This study suggests another source of state dependence: The previous choice affects the current . Specifically, the model assumes that the consumer (a) knows the attributes of the product offered by the firm from which he/ she purchased in the previous period, (b) is uncertain about the attributes of the new products offered by the other firms, (c) can obtain full information about the attributes of all the products through a costly search, and (d) if the consumer decides not to search, he/she purchases the new product offered by the firm from which he/she purchases in the previous period. It is shown that state dependence can result either from the effect of previous choices on the current utility or from its effect on the current information set. This theoretical result raises the following question: What kind of data does a researcher need in order to distinguish between the two sources of state dependence? This study shows that the two sources can be distinguished with a standard panel data set. In other words, although the new source of state dependence is based on the search activity of consumers, there is an that enables a researcher to detect such activity even without direct data on search. The empirical distinction is possible because the behavioral implications of the two sources of state dependence are different. They differ in the effect of product attributes on the repeat-purchase probability. The following example partially illustrates this result: There are two firms and ; the consumer purchased a product from firm in period  1; the only product attribute is ; and the utility is a linear function of . One aspect of our findings is that in the traditional model of state dependence a change in both and that leaves the difference between them, (  ), unchanged (neutral change, hereinafter) has no effect on the repeat-purchase probability. However, such a change affect the repeat-purchase probability in the asymmetric information model of state dependence. This is only one aspect of the findingthe implications of the models differ in a more general fashion. The intuition of this result is the following. A neutral change has no effect on the repeat-purchase probability in the traditional model of state dependence, because it does not affect the difference between the utilities from both alternatives. In the asymmetric model of state dependence the consumer's decision process consists of two stages. First, he decides whether to search for information about the other alternative or not. Then, if he searches for information, he chooses the alternative that maximizes his utility. In the second stage, a neutral change has no effect on choices, since such a change does not affect the difference between the utilities from the two alternatives. In the first stage, the consumer knows , but does not know . It turns out that in this stage a neutral change affect the search decision. When, for example, both 's decrease and the utility is a positive function of , the probability of search increases, and thus the repeat-purchase probability decreases. The proposed source of state dependence is examined using structural estimation and panel data on television viewing choices in the United States. Controlling for both observed and unobserved heterogeneity, it is found that the suggested source is more important in creating repeat-purchase than the traditional one for most of the population (71%). This indicates that what was considered by previous studies to result from the dependence of consumer utility on their previous choices is at least partially due to the effect of the previous choices on consumers' information set. The distinction between the two sources of repeat-purchase is important because ignoring the informational explanation may lead to incorrect theoretical and empirical conclusions. For example, price discounts to induce trial are more important for consumers whose utility depends on previous choices, while advertising is more effective for those whose information set depends on previous choices.", "e:keyword": ["State Dependence", "Umbrella Brand Loyalty", "Incomplete Information", "Search", "Search Cost", "Television Viewing Choices"]}, {"@id": "http://dx.doi.org/10.1287/mksc.21.4.455.131", "e:abstract": "A computationally attractive model for the analysis of conjoint choice experiments is the mixed multinomial logit model, a multinomial logit model in which it is assumed that the coefficients follow a (normal) distribution across subjects. This model offers the advantage over the standard multinomial logit model of accommodating heterogeneity in the coefficients of the choice model across subjects, a topic that has received considerable interest recently in the marketing literature. With the advent of such powerful models, the conjoint choice design deserves increased attention as well. Unfortunately, if one wants to apply the mixed logit model to the analysis of conjoint choice experiments, the problem arises that nothing is known about the efficiency of designs based on the standard logit for parameters of the mixed logit. The development of designs that are optimal for mixed logit models or other random effects models has not been previously addressed and is the topic of this paper. The development of efficient designs requires the evaluation of the information matrix of the mixed multinomial logit model. We derive an expression for the information matrix for that purpose. The information matrix of the mixed logit model does not have closed form, since it involves integration over the distribution of the random coefficients. In evaluating it we approximate the integrals through repeated samples from the multivariate normal distribution of the coefficients. Since the information matrix is not a scalar we use the determinant scaled by its dimension as a measure of design efficiency. This enables us to apply heuristic search algorithms to explore the design space for highly efficient designs. We build on previously published heuristics based on relabeling, swapping, and cycling of the attribute levels in the design. Designs with a base alternative are commonly used and considered to be important in conjoint choice analysis, since they provide a way to compare the utilities of pro- files in different choice sets. A base alternative is a product profile that is included in all choice sets of a design. There are several types of base alternatives, examples being a socalled outside alternative or an alternative constructed from the attribute levels in the design itself. We extend our design construction procedures for mixed logit models to include designs with a base alternative and investigate and compare four design classes: designs with two alternatives, with two alternatives plus a base alternative, and designs with three and with four alternatives. Our study provides compelling evidence that each of these mixed logit designs provide more efficient parameter estimates for the mixed logit model than their standard logit counterparts and yield higher predictive validity. As compared to designs with two alternatives, designs that include a base alternative are more robust to deviations from the parameter values assumed in the designs, while that robustness is even higher for designs with three and four alternatives, even if those have 33% and 50% less choice sets, respectively. Those designs yield higher efficiency and better predictive validity at lower burden to the respondent. It is noteworthy that our best choice designs, the 3- and 4-alternative designs, resulted not only in a substantial improvement in efficiency over the standard logit design but also in an expected predictive validity that is over 50% higher in most cases, a number that pales the increases in predictive validity achieved by refined model specifications.", "e:keyword": ["Conjoint Choice", "Design Efficiency", "Heterogeneity", "Base-Alternative"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.1.12848", "e:abstract": "We argue that research problems are only interesting relative to some external audience. Interesting academic research should impact, at least, that external audience. Hence, we should target our research toward specific external audiences. Several foreboding trends that exacerbate the urgency of this targeting are discussed. To facilitate the targeting task, a partial list of fifteen possible audiences for academic research in marketing is identified. We discuss some of them, including practitioners, in detail. For example, we conclude that, for our research to be interesting to practitioners, practitioners must have the ability to improve and to make better decisions with enhanced understanding. Finally, we strongly suggest that we focus our research on fundamental problems in marketing. These are problems with the property that external audiences would first look to the marketing literature for answers.", "e:keyword": ["Academic Research", "External audiences for research", "Practitioners and academic research", "Marketing discipline"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.16.12844", "e:abstract": "Manufacturer trade promotion spending is second only to cost of goods sold as a profit-and-loss item, yet manufacturers often lose money on these deals as a result of forward-buying by retailers. The search for more effective forms of trade promotion and the availability of scanners at cash registers has led to the emergence of a new type of trade dealthe scan-backthat gives retailers a discount on unitsduring the promotion rather than on units. We develop a theory to compare retailer pricing decisions and profitability under scan-back and traditional off-invoice trade deals. We show that, when the terms of the trade deal are identical, retailers prefer off-invoice trade deals and manufacturers prefer scan-backs. Manufacturers can, however, redesign the scan-back to leave the retailer weakly better off and leave themselves strictly better off. Using proprietary data from the beverage category, we conduct an empirical analysis and find that during the promotion period scan-back trade deals, relative to off-invoice deals: (1) Do not cause excess ordering and (2) generate higher retail sales through lower retail prices. Implications for researchers and managers are discussed.", "e:keyword": ["Trade promotion", "Scan-back", "Off-invoice", "Forward-buying", "Empirical analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.40.12847", "e:abstract": "We introduce the idea of a massively categorical variable, a variable such as zip code that takes on too many values to treat in the standard manner. We show how to use a massively categorical variable directly as an explanatory variable. As an application of this concept, we explore several of the issues that analysts confront when trying to develop a direct marketing campaign. We begin by pointing out that the data contained in many of the common sources are masked through aggregation in order to protect consumer privacy. This creates some difficulty when trying to construct models of individual level behavior. We show how to take full advantage of such data through a hierarchical Bayesian variance components (HBVC) model. The flexibility of our approach allows us to combine several sources of information, some of which may not be aggregated, in a coherent manner. We show that the conventional modeling practice understates the uncertainty with regard to its parameter values. We explore an array of financial considerations, including ones in which the marginal benefit is non-linear, to make robust model comparisons. To implement the decision rules that determine the optimal number of prospects to contact, we develop an algorithm based on the Monte Carlo Markov chain output from parameter estimation. We conclude the analysis by demonstrating how to determine an organization's willingness to pay for additional data.", "e:keyword": ["Direct marketing", "Categorical variables", "Hierarchical bayes analysis", "Variance components", "Decision theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.58.12849", "e:abstract": "We offer an econometric framework that models consumer's consideration set formation as an outcome of her costly information search behavior. Because frequently purchased products are characterized by frequent price promotions of varying depths of discounts, a consumer faces significant uncertainty about the prices of the brands. The consumers engage in a fixed-sample search strategy that results in their discovering the posted prices of a subset of the available brands. This subset is referred to as the consumer's consideration set. The proposed model is estimated using the scanner data set for liquid detergents. Our key empirical results are: (i) consumers zincur significant search costs to discover the posted prices of the brands; (ii) whereas in-store displays and feature ads do not influence consumers' quality perceptions of the brands, they significantly reduce search costs for observing the prices of the brands; (iii) per capita income of consumer's household significantly increases her search costs; and (iv) the consumers' price sensitivity is seriously underestimated if we were to assume that consumers get to know all the posted prices at zero cost. The proposed model is also estimated for the ketchup category to enable us to do cross-category comparisons of consumers' price search behavior", "e:keyword": ["Price uncertainty", "Consumer search", "Consideration set", "Quality uncertainty", "Consumer learning", "Bayesian updating", "Structural model", "Econometric estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.85.12842", "e:abstract": "Internet Shopping Agents (ISAs) allow consumers to costlessly search many online retailers and buy at the lowest price. One would expect these ISAs to subject sellers to intense price competition that results in uniform low prices. Yet, Internet retailers have joined these ISAs. Furthermore, the prices charged by inside retailers can vary substantially. We examine the impact of ISAs on market competition. An ISA creates differentiation in the pricing strategies of ex-ante identical retailers: Some retailers join the ISA due to mass of consumers that they can potentially win, while others stay out and extract surplus from their loyal consumers, while others stay our and extract surplus from their loyal consumers. The equilibrium inside pricing is such that the average price charged can increase or decrease when more retailers join, depending on whether or not the reach of the ISA is independent of the number of joining retailers. When the reach is endogenous, there exist a unique number of inside retailers.", "e:keyword": ["Consumer search", "Comparison shopping", "Shopping agents", "Internet intermediaries", "Internet retailing", "Game theory", "Electronic commerce"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.107.12850", "e:abstract": "This paper develops a model for capturing continuous heterogeneity in the joint distribution of reservation prices for products and bundles. Our model is derived from utility theory and captures both within-and among-subject variability. Furthermore, it provides dollarmetric reservation prices and individual-level estimates that allow the firm to target customers and develop customized and nonlinear pricing policies. Our experiments show that, regardless of whether the products are durables or nondurables, the model captures heterogeneity and predicts well. Models that assume homogeneity perform poorly, especially in predicting choice of the bundle. Furthermore, the methodology is robust even when respondents evaluate few profiles. Self-stated reservation prices do not have any informational content beyond that contained in the basic model. The direct elicitation method appears to understate (overstate) the variation in reservation prices across consumers for low-priced (high-priced) products and bundles. Hence this method yields biased demand estimates and leads to suboptimal product-line pricing policy. The optimization results show that the product-line pricing policy depends on the degree of heterogeneity in the reservation prices of the individual products and the bundle. A uniformly high-price strategy for all products and bundles is optimal when heterogeneity is high. Otherwise, a hybrid strategy is optimal.", "e:keyword": ["Bundling", "Reservation prices", "Optimal pricing", "Consumer heterogeneity", "Multinomial probit models", "Conjoint analysis", "Bayesian models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.131.12843", "e:abstract": "We construct a multistage game-theoretic model of advertising and price competition in a differentiated products duopoly, in which proportions of consumers exhibit latent inertia in favor of repeat purchase. Advertising simultaneously plays the dual role in reducing such inertia through awareness and enhancing perceived brand value (persuasion). We derive the advertising price cross-effects and provide a theoretical reconciliation of the longstanding debate in the marketing literature regarding the impact of advertising on price sensitivity. We characterize the nature of equilibria under symmetry and show that when a large proportion of consumers exhibit inertial tendencies, then a multiplicity of equilibria exists. Marketing implications and comparative statics are discussed. Numerical simulations for asymmetric firms are presented, wherein we show that advertising is not a useful competitive tool for small firms. However, advertising spending by the large firm provides a halo effect for the average prices in the category, which has a positive externality on the small firm's profits. In the absence of the small brand advertising, larger brand shares encourage firms to allocate higher expenditures on advertising to enhance the perceived value of their brand, which in turn shore up the average prices in the industry from which all firms benefit.", "e:keyword": ["Consumer inertia", "Duopoly", "Advertising competition", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.147.12845", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.22.2.151.16038", "e:abstract": "Effectively solving problems requires proper organization. Like other academic disciplines, our discipline must organize around our fundamental problems rather than around our procedures (i.e., tools, approaches, methods). Only that organization ensures healthy debate and constructive communication on relevant research questions. Three initiatives might help foster a transition from a procedure-based to a problem-based organization. One initiative is compartmentalized reviews. Rather than only assigning reviews based on the technical procedures used in a manuscript (e.g., experiments), we assign at least one reviewer, whose expertise is in the problem domain (e.g., advertising), to review only that part of the manuscript (e.g., relevancy to advertisers). Another initiative is to avoid dichotomous certification (i.e., correct or incorrect) for procedures. All procedures yield evidence that forms a multidimensional continuum from circumstantial to overwhelming. Sometimes, precision in stating the conclusion is more important than precision in the procedure. Finally, research streams on marketing questions are essentialno one article is definitive. To foster these streams,must encourage (to some degree) articles that expand on research previously published in. Whenpublishes an article, it has some obligation to give some priority to manuscripts that build on the same topic (i.e., not to label them as incremental).", "e:keyword": ["Organization of Marketing", "Reviewing", "Referees", "Research Streams"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.2.161.16040", "e:abstract": "We ask the question, when should the most highly skilled salespeople sell the best products? Our main result is that the highly skilled reps should sell better products when the task is very complex and worse products when the task is very simple. This is shown using a general analytical model of selling in which sales are a joint function of the salesperson's skill and the complexity of the selling task. Complexity varies across products and industries. Intuitively, when the selling task is complex, few salespeople oflevel of ability will be successful with a low-quality product. Therefore, the high-skill rep's value is higher on the better product. Conversely, when the task is simple, salespeople ofability can sell the better product fairly easily so the high-skill rep's impact is more pronounced on the worse product. This general result offers insight into many key problems: Which salespeople should we hire? How should we organize our salespeople? How should we allocate training funds? We show that the insights hold for salespeople that eithervalue or simplythe customer about the product's value. Finally, we contrast this set of questions with the question ofsalespeople the firm should hire. We find that the firm that has the biggest sales force does not always have the best.", "e:keyword": ["Sales Management", "Selling", "Hiring Policies", "Game Theory", "Agency Theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.2.188.16041", "e:abstract": "Sales takeoff is vitally important for the management of new products. Limited prior research on this phenomenon covers only the United States. This study addresses the following questions about takeoff in Europe: 1) Does takeoff occur as distinctly in other countries, as it does in the United States? 2) Do different categories and countries have consistently different times-to-takeoff? 3) What economic and cultural factors explain the intercountry differences? 4) Should managers use a sprinkler or waterfall strategy for the introduction of new products across countries? We gathered data on 137 new products across 10 categories and 16 European countries. We adapted the threshold rule for identifying takeoff (Golder and Tellis 1997) to this multinational context. We specify a parametric hazard model to answer the questions above. The major results are as follows: 1) Sales of most new products display a distinct takeoff in various European countries, at an average of six years after introduction. 2) The time-to-takeoff varies substantially across countries and categories. It is four times shorter for entertainment products than for kitchen and laundry appliances. It is almost half as long in Scandinavian countries as in Mediterranean countries. 3) While culture partially explains intercountry differences in time-to-takeoff, economic factors are neither strong nor robust explanatory factors. 4) These results suggest distinct advantages to a waterfall strategy for introducing products in international markets.", "e:keyword": ["International New Product Growth", "New Product Takeoff", "New Product Growth", "International Diffusion", "Diffusion of Innovations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.2.209.16037", "e:abstract": "Indirect network externality (INE) effect exists when the utility of a product increases with the greater availability of compatible complementary products. We demonstrate that INE effects can vary by product attributes, with externality-sensitive attributes gaining more from increased availability of complementary products than other attributes. Past research has assumed that the benefit of increased availability of complementary products (e.g., software) accrues to theproduct (e.g., hardware). Utilizing data covering a period from 1985 to 1995 on compact disc player prices, attributes, and CD titles releases, and using a hedonic price approach, we find significant positive interactions between CD title availability and two attributes of the CD player, namely, changer capacity and its oversampling rate. In addition to INE-attribute interactions, increased availability of CD titles is found to have a significant positive impact on the overall price of CD players, which is consistent with past research. Collectively, these effects of INE have helped reduce the yearly decline in the price of CD players. The finding that INE effects differentially affect different attributes can help managers in decisions such as pricing, timing of introduction, and changing the levels of INE-sensitive attributes.", "e:keyword": ["Indirect Network Externality", "Hedonic Price", "Externality Sensitive Attributes", "CD Player Prices"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.2.222.16036", "e:abstract": "Exclusive advertising on a given media outlet is usually profitable for an advertiser because consumers are less aware of competing products. However, for such arrangements to exist, media must benefit as well. We examine conditions under which such exclusive advertising contracts benefit both advertisers and media outlets (referred to as ) by illustrating that exclusive equilibria arise in a theoretical model of the media, advertisers, and consumers who participate in both the product and media markets. In the model, stations sell advertising space to advertisers and broadcast advertising messages to consumers. Conditions leading to higher equilibrium levels of advertising can be unprofitable for advertisers because high levels of advertising make consumers better informed and thus lead to fiercer product price competition. As a result, media stations may be less profitable as well because their payoff is determined as a fraction of the advertiser surplus generated in the product market. Stations mitigate this effect by offering advertisers exclusive advertising contracts. With such contracts, consumers are less informed about competing products, yielding higher producer surplus. It is profitable for stations to offer exclusivity when commercial advertising is an important means for advertisers to inform consumers about their products.", "e:keyword": ["Advertising", "Market Structure", "Media"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.2.246.16035", "e:abstract": "Slotting allowances are a relatively recent trend, particular to the retail food industry. These allowances are lump-sum, up-front transfer payments from manufacturer to retailer when the manufacturer launches a new product. The practice has attracted some scrutiny because of uncertainty about its purposes and consequences. We draw from the extant literature to identify factors that potentially influence the relative magnitude of slotting allowances. Based on analysis of primary survey data from retailers and manufacturers, we observe that charging and paying of slotting allowances are affected by the relative strength of the players. Among retailers, the relative magnitude of slotting fees increases with retailers' informational advantage over the manufacturer about the likely success of the new product, even when retailers recognize that the product is likely to be successful. Additionally, and consistent with the first finding, retailers with lower costs (i.e., potentially more efficient and powerful retailers) received higher slotting allowances. Furthermore, retailers charge higher slotting fees, even when concerns about manufacturers' fulfilling postlaunch advertising commitments are minimal, implying that relatively powerless manufacturers are asked to provide credible commitments regarding postlaunch activitiesare asked to pay relatively high slotting fees. Among manufacturers, the relative magnitude of slotting fees paid is lower for those who have a strong market share position. We discuss the theoretical, managerial, and public policy implications of our findings.", "e:keyword": ["Slotting Allowances", "Information Asymmetry", "New Product Introductions", "Retail Food Industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.2.271.16034", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.22.3.273.17743", "e:abstract": "We propose and test new adaptive question design and estimation algorithms for partial profile conjoint analysis. Polyhedral question design focuses questions to reduce a feasible set of parameters as rapidly as possible. Analytic center estimation uses a centrality criterion based on consistency with respondents' answers. Both algorithms run with no noticeable delay between questions. We evaluate the proposed methods relative to established benchmarks for question design (random selection, D-efficient designs, adaptive conjoint analysis) and estimation (hierarchical Bayes). Monte Carlo simulations vary respondent heterogeneity and response errors. For low numbers of questions, polyhedral question design does best (or is tied for best) for all tested domains. For high numbers of questions, efficient fixed designs do better in some domains. Analytic center estimation shows promise for high heterogeneity and for low response errors; hierarchical Bayes for low heterogeneity and high response errors. Other simulations evaluate hybrid methods, which include self-explicated data. A field test (330 respondents) compared methods on both internal validity (holdout tasks) and external validity (actual choice of a laptop bag worth approximately $100). The field test is consistent with the simulation results and offers strong support for polyhedral question design. In addition, marketplace sales were consistent with conjoint-analysis predictions.", "e:keyword": ["New Product Research", "Measurement", "Internet Marketing", "Estimation and Other Statistical Techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.3.304.17739", "e:abstract": "Bayesian methods have become widespread in marketing literature. We review the essence of the Bayesian approach and explain why it is particularly useful for marketing problems. While the appeal of the Bayesian approach has long been noted by researchers, recent developments in computational methods and expanded availability of detailed marketplace data has fueled the growth in application of Bayesian methods in marketing. We emphasize the modularity and flexibility of modern Bayesian approaches. The usefulness of Bayesian methods in situations in which there is limited information about a large number of units or where the information comes from different sources is noted. We include an extensive discussion of open issues and directions for future research.", "e:keyword": ["Bayesian Statistics", "Decision Theory", "Marketing Models", "Critical Review"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.3.329.17740", "e:abstract": "We develop an econometric model to study a setting in which a new product is launched first in its domestic market and only at a later stage in foreign markets, and where the product's performance (demand) and availability (supply) are highly interdependent over time within and across markets. Integrating literature on international diffusion, success-breeds-success trends, and the theatrical motion picture industrythe focus of the empirical analysiswe develop a dynamic simultanenous-equations model of the drivers and interrelationship of the behavior of consumers (audiences) and retailers (exhibitors). Our findings emphasize the importance of considering the endogeneity and simultaneity of audience and exhibitor behavior, and challenge conventional wisdom on the determinants of box office performance (which is predominantly based on modeling frameworks that fail to account for the interdependence of performance and availability). Specifically, we find that variables such as movie attributes and advertising expenditures, which are usually assumed to influence audiences directly, mostly influence revenuesdirectly, namely through their impact on exhibitors' screen allocations. In addition, consistent with the idea that the buzz for a movie is perishable, we find that the longer is the time lag between releases, the weaker is the relationship between domestic and foreign market performancean effect mostly driven by foreign exhibitors' screen allocations.", "e:keyword": ["Dynamic Simultaneous Equations Modeling", "International Release Strategies", "Entertainment Marketing", "Motion Picture Distribution and Exhibition", "Channel Management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.3.355.17742", "e:abstract": "This paper studies dynamic competition in markets characterized by the introduction of technologically advanced next-generation products. Firms invest in new product effort in an attempt to attain industry leadership, thus securing high profits and benefiting from advantages relevant for the success of future product generations. The analysis reveals that when the current leader possesses higher research and development (R&D) competence, it tends to investin R&D than rivals and to retain its lead position. The leader's investment exhibits an inverse-U pattern as this advantage increases. In contrast, when the leader enjoys an advantage that originates from the persistence of reputation, it investsthan its followers. Now, followers' investment exhibits an inverse-U pattern as reputation advantage increases. Depending on the extent of leader reputation, industry structure can either exhibit frequent leadership shifts or prolonged incumbent dominance. The basic framework is extended to allow investments in additional marketing variables (e.g., advertising). Interestingly, the leader takes advantage of strong demand for its current product by focusing more on advertising, whereas the follower expends more on R&D. By shedding light on the implications of industry position for investment incentives and market evolution, the analysis provides valuable insights for formulating marketing strategy in fast-paced, high-tech business environments.", "e:keyword": ["High-Technology Marketing", "New Product Development", "Dynamic Capabilities", "Technological Competition", "Markov-Perfect Equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.3.371.17736", "e:abstract": "There is strong theoretical and empirical evidence supporting the idea that first-to-market leads to an enduring market share advantage. In sharp contrast to these findings, we find that at the business unit level being first-to-market leads, on average, to a long-term. This result holds for a sample of consumer goods as well as a sample of industrial goods and leads to questions about the validity of first mover advantage, in and of itself, as a strategy to achieve superior performance. We replicate the typical demand-side pioneering advantage but find an even, which is the source of the pioneering profit disadvantage. In an extended analysis, we show that first-to-market leads to an, which, depending on the sample or profit measure, lasts for about 12 to 14 years before turning into a disadvantage. Moreover, we show that pioneers differentially benefit from a lack of consumer learning, a strong market position and patent protection. These three moderating factors together can actually help pioneers achieve a sustainable profit advantage over later entrants. Finally, we find strong support for the theoretical argument that the entry order decision should be treated as endogenous in empirical estimation.", "e:keyword": ["Pioneering Advantage", "Firm Performance", "New Product Marketing", "IV-Estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.3.393.17737", "e:abstract": "Consumers often have to decide whether to acquire information in high-stakes decision domains. We study women in mammography waiting rooms to test how a false-alarm result (i.e., an indication that a malady is present when a more accurate follow-up test reveals it is not) affects willingness to get retested. In Study 1 we show that, given a false-alarm result, life-threatening test consequences are associated with more disutility for future testing than when test consequences are less significant; this does not hold for normal test results. In Study 2 in the mammography context, we show that patients receiving a false-alarm result experienced more stress, were less likely to believe that a positive mammography result indicated cancer, and more likely to delay mammography than patients receiving normal results unless they were also told that they may be vulnerable to breast cancer in the future. We show that delays in planned adherence following a false-alarm result can be mitigated by an information intervention. Finally, we have preliminary evidence that a previous history of false-positive results can cause a consumer to both react more negatively to emotional stress and respond more positively to coping information.", "e:keyword": ["Value of Information", "Decision Making Under Uncertainty", "Medical Decision Making", "Stress", "Cancer False-Positive", "Patient Preferences", "Mammography", "Medical Testing", "High-Stakes Decisions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.3.411.17738", "e:abstract": "Several twostage choice models (consideration stage plus choice stage) have been proposed in the marketing literature. We extend this literature by developing a more general model that incorporates how consumer search influences the degree to which they consider various brands. To test the validity and value of our model, we operationalized it with data obtained from Peapod, an online grocer, where we tracked consumers search processes. We demonstrate that our model performs better than competing models on all the key criteria. New choice models, such as the ones proposed here, are necessary for deriving managerially relevant understanding of choice behavior in online markets. Our empirical results suggest that consumers search both their internal memory and external information at the store to determine the degree to which they consider various brands. Consumers are also heterogeneous with respect to their capability to process external information. For some consumers, external information search dramatically increases the degree to which they consider various brands; but, for others, it has little impact on their consideration. We also find that certain features (e.g., personal lists) reduce consideration set sizes, whereas other features (e.g., sort) increase consideration set sizes.", "e:keyword": ["Consideration Sets", "Consumer Search", "Online Shopping", "Fuzzy Set Theory", "Choice Models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.4.437.24904", "e:abstract": "Rankings of MBA programs by journalists have had a dramatic impact on business schools. Now, we face the prospect of journal rankings. Journal rankings impose a single dimension for evaluating journals that will inevitably hurt some journals. However, there are steps that researchers can take to keep their journals strong. For example, we should all support our professional organizations, subscribe to relevant journals, and count journal service in promotion decisions. In sum, this article identifies ten steps all researchers should take.", "e:keyword": ["Academic Journals", "Rankings", "Citations", "Third-Party Evaluations", "Marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.4.442.24910", "e:abstract": "This research examines the methods, viability, and benefits of pooling scanner panel choice data with compatible preference data from designed choice experiments. The fact that different choice data sources have diverse strengths and weaknesses suggests it might be possible to pool multiple sources to achieve improved models, due to offsetting advantages and disadvantages. For example, new attributes and attribute levels not included in the scanner panel data can be introduced via the choice experiment, while the scanner panel data captures preference dynamics, which is, at best, difficult with experimental data. Our application, involving liquid laundry detergent, establishes the feasibility and desirability of doing such augmentations of scanner panel data: The joint scanner panel/choice experiment model has significantly better prediction performance on a holdout data set than does a pure scanner panel model. Thus, we extend the concept of choice into another domain and demonstrate that data enrichment can add significantly to one's understanding of preferences reflected in scanner panel data.", "e:keyword": ["Data Enrichment", "Choice Models", "Scanner Panel", "Choice Experiment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.4.461.24907", "e:abstract": "In this study we compare consumer brand loyalty in online and traditional shopping environments for over 100 brands in 19 grocery product categories. The online purchase data come from a large traditional grocery retailer that also operates an online store for its products. The offline data corresponds to the exact same brands and categories bought in traditional stores by a panel of homes operated by ACNielsen for purchases made in the same city and over the same time period. We compare the observed loyalty with a baseline model, a new segmented Dirichlet model, which has latent classes for brand choice and provides a very accurate model for purchase behavior. The results show that observed brand loyalty for high market share brands bought online is significantly greater than expected, with the reverse result for small share brands. In contrast, in the traditional shopping environment, the difference between observed and predicted brand loyalty is not related to brand share.", "e:keyword": ["Brand Choice", "Probability Models", "Internet Shopping"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.4.477.24911", "e:abstract": "People often need to trade off between the probability and magnitude of the rewards that they could earn for investing effort. The present paper proposes that the conjunction of two simple assumptions (relating effort-induced reward expectations to prospect theory's value function) provides a parsimonious theory that predicts that the nature of the required effort will have a systematic effect on such trade-offs. Using the case of frequency (or loyalty) programs, a series of five studies involving both real and hypothetical choices demonstrated that (a) the presence (as opposed to absence) of effort requirements enhances the preference for sure-small rewards over large-uncertain rewards; (b) the preference for reward certainty is attenuated when the effort activity is intrinsically motivating; and (c) continuously increasing the effort level leads to an inverted-U effect on the preference for sure-small over largeuncertain rewards. The studies also employ process measures and examine the mechanisms underlying the impact of the effort stream on the trade-off between the certainty and magnitude of rewards. The final section discusses the theoretical implications of this research as well as the practical implications with respect to frequency programs and other types of incentive systems.", "e:keyword": ["Effort and Reward", "Decisions Under Uncertainty", "Frequency/Loyalty/Reward Programs", "Incentive Systems", "Intrinsic Motivation", "Psychology of Rewards", "Risky Choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.4.503.24909", "e:abstract": "We study sequential search behavior in a generalized \"secretary problem\" in which a single object is to be selected from a set ofalternatives. Alternatives are inspected in a random order, one at a time, and only the rank order of the current alternative relative to the ones that have already been observed can be ascertained. At each period, the consumer may either accept the current alternative, continue to search and pay a fixed cost, or recall an alternative that has already been inspected. A recalled alternative is assumed to be available with a known probability. The consumer's goal is to select the overall best alternative from the fixed set. We describe the results of an experiment designed to test the optimal model and compare it to a behavioral decision model that incorporates local patterns of the observed sequence. Both set size and search cost are manipulated experimentally in a 2x2 factorial design. Our results show that cost and set size affect the amount of search in the predicted direction. However, in the two no-cost conditions subjects search too little in comparison to the optimal model, whereas in the two cost conditions they search too much. The behavioral decision rule that we propose provides a possible account for the observed pattern of the behavioral regularities.", "e:keyword": ["Search Behavior", "Sequential Choice Models", "Behavioral Decision Rules", "Information Processing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.4.520.24906", "e:abstract": "In this paper, we develop an analytical approach to modeling consumer response to banner ad exposures at a sponsored content Web site that reveals significant heterogeneity in (unobservable) click proneness across consumers. The effect of repeated exposures to banner ads is negative and nonlinear, and the differential effect of each successive ad exposure is initially negative, though nonlinear, and levels off at higher levels of passive ad exposures. Further, significant correlations between session and consumer click proneness and banner exposure sensitivity suggest gains from repeated banner exposures when consumers are less click prone. For a particular number of sessions, more clicks are generated from consumers who revisit over a longer period of time, than for those with the same number of sessions in a relatively shorter timeframe. We also find that consumers are equally likely to click on banner ads placed early or late in navigation path and that exposures have a positive cumulative effect in inducing click-through in future sessions. Our results have implications for online advertising response measurement and dynamic ad placement, and may help guide advertising media placement decisions.", "e:keyword": ["Advertising and Media Research", "Clickstream Data", "Computer-Mediated Environments", "Online Consumer Behavior", "Random-Coefficient Models", "Internet", "World Wide Web"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0060", "e:abstract": "There are many critical concerns (including the accounting for endogeneity) when one is properly estimating response functions. However, it is sometimes (certainly not always) better to leave some variables exogenous when building mathematical models intended to help decision makers. The exogenous variables allow the decision maker to better adapt the mathematical model to different situations and to incorporate myriad variables and constraints outside of the model.", "e:keyword": ["Endogeneity", "Decision models", "Game theory", "Constrained models", "Marketing theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0040", "e:abstract": "We use the results of three large-scale field experiments to investigate how the depth of a current price promotion affects future purchasing of first-time and established customers. While most previous studies have focused on packaged goods sold in grocery stores, we consider durable goods sold through a direct mail catalog. The findings reveal different effects for first-time and established customers. Deeper price discounts in the current period future purchases by first-time customers (a positive long-run effect) but future purchases by established customers (a negative long-run effect). Overall, the results show evidence of several long-run effects: forward buying, selection, customer learning, and increased deal sensitivity. Short-run metrics that ignore these effects overstate the overall change in demand for established customers. The implication is that if prices are set based on short-run elasticity, then they will be too low. Among first-time customers, the short-run metrics underestimate the total increase in demand. If prices are set based on short-run elasticity, then they will be too high.", "e:keyword": ["Price promotions", "Pricing", "Long-term effects", "Forward buying", "Purchase acceleration", "Deal sensitivity", "Catalogs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0026", "e:abstract": "The issue of delegating pricing responsibility to the salesforce is of interest to marketing academics and practitioners. It has been shown by Lal (1986) that under certain situations with information asymmetry, it is more profitable for the firm to delegate pricing authority to the salesforce than to have centralized pricing. In this paper we re-examine situations where information asymmetry exists and analyze its effect on the decision of the firm to set a price or delegate pricing responsibility to the salesforce. Using contract theory, we find that when the salesperson's private information can be revealed to the firm through contracting, centralized pricing performs at least as well as price delegation. We derive the optimal centralized pricing contract under a set of standard assumptions used in the economics and business literature.", "e:keyword": ["Pricing research", "Compensation", "Salesforce", "Delegation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0034", "e:abstract": "This paper empirically investigates the determinants of retailers' pricing decisions. It finds that competitor factors explain the most variance in retailer pricing strategy. Only in the cases of price-promotion coordination and relative brand price do category and chain factors explain much variance in retailer pricing. These findings are derived from a simultaneous equation model of how underlying dimensions of retailers' pricing strategies are influenced by variables representing the market, chain, store, category, brand, customer, and competition. The optical scanner data base describes 1,364 brand-store combinations from six categories of consumer packaged goods in five U.S. markets over a two-year time period. Our study classifies retailers' pricing strategies based on four underlying dimensions: price consistency, price-promotion intensity, price-promotion coordination, and relative brand price. These four pricing dimensions are statistically related to: (1) competitor price and deal frequency (competitor factors), (2) storability and necessity (category factors), (3) chain positioning and size (chain factors), (4) store size and assortment (store factors), (5) brand preference and advertising (brand factors), and (6) own-price and deal elasticities (customer factors). These findings are useful to retailers profiling alternative pricing strategies, and to manufacturers customizing the levels of marketing support spending for different retailers.", "e:keyword": ["Retailing", "Pricing", "Promotion", "Competitive strategy", "Econometric analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0046", "e:abstract": "At the heart of a new product sales-forecasting model for consumer packaged goods is a multiple-event timing process. Even after controlling for the effects of time-varying marketing mix covariates, this timing process is not a stationary one, which means the standard interpurchase time models developed within the marketing literature are not suitable for new products. In this paper, we develop a dynamic changepoint model that captures the underlying evolution of the buying behavior associated with the new product. This extends the basic changepoint framework, as used by a number of statisticians, by allowing the changepoint process itself to evolve over time. Additionally, this model nests a number of the standard multiple-event timing models considered in the marketing literature. In our empirical analysis, we show that the dynamic changepoint model accurately tracks (and forecasts) the total sales curve as well as its trial and repeat components and other managerial diagnostics (e.g., percent of triers repeating).", "e:keyword": ["Sales forecasting", "Trial/repeat", "New product research", "Duration models", "Nonstationarity", "Changepoint models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0041", "e:abstract": "For several of the largest supermarket product categories, such as carbonated soft drinks, canned soups, ready-to-eat cereals, and cookies, consumers regularly purchase assortments of products. Within the category, consumers often purchase multiple products and multiple units of each alternative selected on a given trip. This multiple discreteness violates the single-unit purchase assumption of multinomial logit and probit models. The misspecification of such demand models in categories exhibiting multiple discreteness would produce incorrect measures of consumer response to marketing mix variables. In studying product strategy, these models would lead to misleading managerial conclusions. We use an alternative microeconomic model of demand for categories that exhibit the multiple discreteness problem. Recognizing the separation between the time of purchase and the time of consumption, we model consumers purchasing bundles of goods in anticipation of a stream of consumption occasions before the next trip. We apply the model to a panel of household purchases for carbonated soft drinks.", "e:keyword": ["Multiple discreteness", "Structural modeling", "Customer behavior", "Brand choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0033", "e:abstract": "Recommendations often play a positive role in the decision process by reducing the difficulty associated with choosing between options. However, in certain circumstances recommendations play a less positive and more undesirable role from the perspectives of both the recommending agent or agency and the person receiving the recommendation. Across a series of four studies, we explore consumer response when recommendations by experts and intelligent agents contradict the consumer's initial impressions of choice options. We find that unsolicited advice that contradicts initial impressions leads to the activation of a reactant state on the part of the decision maker. This reactance, in turn, leads to a behavioral backlash that results not only in consumers ignoring the agents' recommendations but in intentionally contradicting them.", "e:keyword": ["Recommendations", "Intelligent agents", "Decision support systems", "Internet marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0022", "e:abstract": "Utility maximizing solutions to economic models of choice for goods with either discrete quantities or non-linear prices cannot always be obtained using standard first-order conditions such as Kuhn-Tucker and Roy's identity. When quantities are discrete, there is no guarantee that derivatives of the utility function are equal to derivatives of the budget constraint. Moreover, when prices are nonlinear, as in the case of quantity discounts, first-order conditions can be associated with the minimum rather than the maximum value of utility. In these cases, the utility function must be directly evaluated to determine its maximum. This evaluation can be computationally challenging when there exist many offerings and when stochastic elements are introduced into the utility function. In this paper, we provide an economic model of demand for substitute brands that is flexible, parsimonious, and easy to implement. The methodology is demonstrated with a scanner panel data set of light-beer purchases. The model is used to explore the effects of price promotions on primary and secondary demand, and the utility of product assortment.", "e:keyword": ["Bayesian analysis", "Econometric models", "Pricing research", "Product management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0035", "e:abstract": "Prior work in marketing has suggested that advertising levels beneath which there is essentially no sales responseare rarely encountered in practice. Because advertising policies settle into effective ranges through early trial and error, thresholds cannot be observed directly, and arguments for their existence must be based primarily on a \"statistical footprint,\" that is, on relative fits of a range of model types. To detect possible threshold effects, we formulate a switching regression model with two \"regimes,\" in only one of which advertising is effective. Mediating the switch between the two regimes is a logistic function of category-specific dynamic variables (e.g., order of entry, time in market, number of competitors) and advertising levels, nesting a variety of alternative formulations, among them both standard concave and S-shaped responses. A sequence of comparisons among parametrically related models strongly suggests: that threshold effects exist; that market share response to advertising is not necessarily globally concave; that superior fit cannot be attributed to model flexibility alone; and that dynamic, environmental, competitive, and brand-specific factors can influence advertising effectiveness. These effects are evident in two evolving durables categories (SUVs and minivans), although not in the one mature, nondurable category (liquid detergent) studied.", "e:keyword": ["Advertising", "Econometric models", "Product management", "Switching regression", "Dynamic models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0042", "e:abstract": "Competitive behavior in commercial television broadcasting is modeled to examine program choice and the effects of more channels being available on firm strategy. Specifically, broadcasters compete by selecting both the \"type\" and quality level of a program to offer, but do not compete on price. We obtain five major results. First, a comparison of monopoly and duopoly markets indicates that broadcasters in an industry with a larger number of competitors may provide programs of lower quality compared to broadcasters in an industry with a smaller number. Second, in terms of viewer welfare, having more channels available is not necessarily \"better.\" Third, broadcasters tend to choose an intermediate level of differentiation in terms of the types of programs they provide, resulting in a \"counterprogramming\" strategy. In other words, avoidance of price competition is not required for competitors to differentiate themselves from each other. Fourth, if one broadcaster starts the evening with a higher-quality (higher-rated) program than its competitor, its second program should also be of higher quality. Finally, a broadcaster's first program should be of equal or higher quality than its second program. Put another way, it always behooves a broadcaster to \"lead with its best.\"", "e:keyword": ["Competition", "Competitive strategy", "Entertainment marketing", "Game theory", "Market structure", "Media", "Product policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0044", "e:abstract": "In several markets, consumers can gain further information regarding how well a product fits their preferences only by experiencing it after purchase. This could then generate loyalty for the products tried first. This paper considers a model in which consumers learn in the first period about the product they buy and then make choices in the second period about the competing products, given what they learned in the first period. The paper finds that if the distribution of valuations for each product is negatively (positively) skewed, a firm benefits (is hurt) in the future from having a greater market share todaythe brand loyalty characteristic. With negative skewness, two effects are identified: On one hand, marginal forward-looking consumers are less price sensitive than myopic consumers, and this is a force toward higher prices. On the other hand, forward-looking firms realize that they gain in the future from having a higher market share in the current period and compete more aggressively in prices. For similar discount factors for consumers and firms, the latter effect dominates. The paper also characterizes the importance of consumer learning effects on the market outcome.", "e:keyword": ["Consumer learning", "Brand loyalty", "Dynamic competition", "Experience goods", "Lifetime value of customers", "Forward-looking consumers", "Forward-looking firms"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0030", "e:abstract": "The price for a product may be set too low, causing the seller to leave money on the table, or too high, driving away potential buyers. Contingent pricing can be useful in mitigating these problems. In contingent pricing arrangements, price is contingent on whether the seller succeeds in obtaining a higher price within a specified period. We show that if the probability of obtaining the high price is not too high, sellers profit from using contingent pricing while economic efficiency increases. The optimal contingent pricing structure depends on the buyer's risk attitudea deep discount is most profitable if buyers are risk prone. A consolation reward is most profitable if buyers are risk averse. To motivate buyers to participate in a contingent pricing arrangement, the seller must provide sufficient incentives. Consequently, buyers also benefit from contingent pricing. In addition, because the buyers with the highest willingness-to-pay get the product, contingent pricing increases the efficiency of resource allocation.", "e:keyword": ["Pricing", "Price risks", "Contingent selling formats", "Standbys", "Price discrimination", "Pricing under uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0028", "e:abstract": "We develop a brand choice model with learning based on the Kalman filter methodology. The model enables us to separate the effects of contemporaneous marketing promotions from the impact of the perceived quality valuation accrued through product usage over time. We also account for idiosyncratic consumer learning and preferences. The results point to the presence of heterogeneity in the valuation carryover coefficients across consumers and brands. In contrast to our expectations, a higher price is not important for most of the consumers in the sample. The model enables us to compare brands in terms of their memorability, which determines brand salience on the next purchase occasion. Our findings suggest that price promotions may be deficient as a tool to increase market share in the studied product category. The proposed model is applicable to other consumer goods contingent on consumers' being sufficiently motivated to learn their own preferences via personal experience. Brand managers can use the model for comparative diagnostics and market performance simulation under different price and promotion scenarios. This paper is instructive to the application of a relatively new methodology; we illustrate the analytical potential of the model by demonstrating its inferential power in a specific marketing context.", "e:keyword": ["Brand choice", "Buyer behavior", "Consumer learning", "Hierarchical Bayes analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0078", "e:abstract": "Consulting and scholarly research often have very different objectives with respect to advancements in practice, theory, and observation (e.g., data collections). For example, consulting often emphasizes immediate benefits, specialized applications, and a focus on only the key variables. Scholarly research often emphasizes replicability, generalizability, and introducing previously uninvestigated variables. However, these activities complement each other, and each activity is important for the advancement of the other. Benefiting from that complementarity requires the literature to bridge knowledge gained from each activity. It is unnecessary for every researcher to try to bridge theory and practice by working on the interface between academics and practice. However, it is critical that some researchers do so. This issue of examines several excellent applications of that provide detailed microexaminations of the fundamental marketing practices that we seek to understand and improve. Beyond demonstrating how to solve specific problems in practice, in my opinion, the articles and commentaries in this issue also illustrate at least the following ideas: Short-term tactics can produce significant short-term advantages. Existing models in the literature can be useful with proper implementation. State-of-the-art research is most useful for infrequent decisions. Finally, knowing the decision-making context is essential for determining which variables to include in the analysis.", "e:keyword": ["Marketing models", "Scholarly research", "Industry practice", "Academic consulting", "Theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0066", "e:abstract": "In 2003 the INFORMS Society for Marketing Science (ISMS) introduce and conducted its inaugural Practice Prize Competition. The reports and papers that follow are the finalists from that competition, representing the best examples of rigor plus relevance that our profession produces.", "e:keyword": ["Marketing science practice", "Database marketing", "Diffusion", "Prelaunch forecasting", "Market entry", "Defensive strategy", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0063", "e:abstract": "We introduce Dynamic Multilevel Modeling (DMLM) to a multicatalog-brand environment to determine the optimal frequency, size, and customer segmentation of direct marketing activities. This optimization method leverages multicatalog-brand effects including the utilization of prior customer ordering behavior, maximization of customer value and customer share, and economies of scale and scope in printing and mailing. This enhancement of the original DMLM-approach is called Dynamic Multidimensional Marketing (DMDM). With DMLM alone, Rhenania, a German direct mail order company, turned its catalog mailing practices around and consequently rose from the number 5 to the number 2 market position. The DMLM approach was so effective that two major competitors could be bought out. Improvements provided by DMDM were threefold: more efficient resource allocation across all catalog brands, more accurate customer microsegmentation, and more effective reactivation. Presently, the company's target is to transform single-brand customer relationships into two- or three-brand relationships with higher revenue per customer. As a consequence, the Rhenania group's performance was decoupled from the overall market trend.", "e:keyword": ["Customer value analysis", "Database marketing", "Direct marketing", "Dynamic investment analysis", "Mail-order business", "Managerial decision making", "Markov processes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0057", "e:abstract": "Research on the product life cycle (PLC) has focused primarily on the role of diffusion. This study takes a broader theoretical perspective on the PLC by incorporating informational cascades and developing and testing many new hypotheses based on this theory. On average, across 30 product categories, the authors find that: (i) New consumer durables have a typical pattern of rapid growth of 45% per year over 8 years. (ii) This period of growth is followed by a slowdown when sales decline by 15% and stay below those of the previous peak for 5 years. (iii) Slowdown occurs at 34% population penetration and about 50% of ultimate market penetration. (iv) Products with large sales increases at takeoff tend to have larger sales declines at slowdown. (v) Leisure-enhancing products tend to have higher growth rates and shorter growth stages than nonleisure-enhancing products. Time-saving products tend to have lower growth rates and longer growth stages than nontime-saving products. (vi) Lower probability of slowdown is associated with steeper price reductions, lower penetration, and higher economic growth. (vii) A hazard model can provide reasonable predictions of the slowdown as early as the takeoff. The authors discuss the implications of these findings.", "e:keyword": ["Product life cycles", "Sales takeoff", "Cascades", "New product growth", "Innovation", "Product management", "Diffusion", "High-tech marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0045", "e:abstract": "Although negotiating over prices with sellers is common in many markets such as automobiles, furniture, services, consumer electronics, etc., it is not clear how a haggling price policy can help a firm gain a strategic advantage or whether it is even sustainable in a competitive market. In this paper, we explore the implications of haggling and fixed prices as pricing policies in a competitive market. We develop a model in which two competing retailers choose between offering either a fixed price or haggling over prices with customers. There are two consumer segments in our analysis. One segment, the , has a lower opportunity cost of time and a lower haggling cost than the other segment, the . When both retailers follow the same pricing policy, then a haggling policy is more profitable than a fixed-price policy only when the proportion of nonhagglers is sufficiently high. We find two kinds of prisoners' dilemma: under some conditions, a more profitable haggling policy can be broken by a fixed-price policy, and under other conditions, a fixed-price policy can be broken by a haggling policy. Surprisingly, we show that under some conditions, an asymmetric outcome with one retailer haggling and the other offering a fixed price is also an equilibrium.", "e:keyword": ["Competitive strategy", "Marketing strategy", "Price discrimination", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0021", "e:abstract": "This paper proposes the (ARM), first used by Aalen (1980), to explain households' interpurchase times. Unlike the Proportional Hazard Model (PHM), first proposed by Cox (1972), the ARM incorporates the effects of covariates on the individual hazard function in an (as opposed to ) manner. While a large number of previous studies on interpurchase timing have dealt with the question of correctly specifying the parametric distribution for interpurchase times, no study has explicitly investigated the question of correctly specifying the effects of covariates in the model. This study looks at this issue. We propose an ARM that is suitable for purchase-timing data, and compare its empirical performance to that of the PHM and the Accelerated Failure Time Model (AFTM) using scanner panel data on laundry detergents, paper towels, and toilet tissue. We find that the ARM not only estimates and validates the observed interpurchase times better than existing models, but also recovers a time-varying price elasticity and shows a high degree of robustness in the estimated covariate effects to alternative parametric specifications of the baseline hazard. The estimates of covariate parameters under the PHM, on the other hand, are highly sensitive to alternative parametric specifications of the baseline hazard.", "e:keyword": ["Purchase timing models", "Additive risk model", "Proportional hazard model", "Accelerated failure time model", "Log-logistic hazard"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0058", "e:abstract": "This paper examines four alternative product strategies available to an innovating firm in markets with network effects: single-product monopoly, technology licensing, product-line extension, and a combination of licensing and product-line extension. We address three questions. First, what factors affect the attractiveness of each of the four product strategies? Second, under what conditions will any particular strategy dominate the others? Third, what is the impact of licensing fees on the profitability of a licensing strategy? We show that offering a product line utilizes consumer heterogeneity to increase the total user base and is superior to free licensing when the innovator's cost of producing a low-quality product is low and network effects are weak. However, because of the advantage of licensing in generating a larger installed base, free licensing can dominate line extension when network effects are strong, even if the innovator suffers no cost disadvantage compared to the competitor. We also show that paid licensing trumps free licensing when the clone product has a high quality or a low cost, regardless of network effect. Finally, strong network effects make a lump-sum fee more profitable than a royalty fee (or a combination of both) because a royalty fee reduces the licensee's production.", "e:keyword": ["Network effects", "New product strategy", "Innovation management", "Licensing", "Product line", "Competitive strategy", "Technological standards", "Installed base"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0047", "e:abstract": "This paper examines a market where the provision of information service is costly, but information service has the characteristics of a public good. Consumers, on the other hand, can use the information service to make an informed purchase decision and derive higher utility from consuming their ideal product. However, after receiving the information service from an information service provider, consumers can easily free ride by purchasing at low-price sellers who do not provide any information service. The paper examines the competition where sellers compete by providing information service for horizontally differentiated products and where technology reduces consumers' search cost. It is found that in this market a seller needs to establish itself as an information service provider in order to make positive profits, even when there is free riding. A seller, however, cannot make positive profits by free riding all the time. Also, with an increase in competition in the information service market, sellers have reduced incentives to provide information service. It is also found that in this market a decrease in search cost may increase or decrease social welfare.", "e:keyword": ["Free riding", "Search cost", "Electronic markets", "Electronic commerce"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0024", "e:abstract": "We propose a utility-theoretic brand-choice model that accounts for four different sources of state dependence: 1. effects of lagged choices (), 2. effects of serially correlated error terms in the random utility function (), 3. effects of serial correlations between utility-maximizing alternatives on successive purchase occasions of a household (), and 4. effects of lagged marketing variables (). Our proposed model also allows habit persistence to be a function of lagged marketing variables, while accommodating the effects of unobserved heterogeneity in household choice parameters. This model is more flexible than existing state-dependence models in marketing and labor econometrics. Using scanner panel data, we find structural state dependence to be the most important source of state dependence. Marketing-mix elasticities are systematically understated if state-dependence effects are incompletely accounted for. The Seetharaman and Chintagunta (1998) model is shown to recover spurious variety-seeking effects while overstating habit-persistence effects. Ignoring habit persistence type 1 leads to an underestimation, while ignoring habit persistence type 2 leads to an overestimation of structural state-dependence effects. We find lagged promotions to have carryover effects on habit persistence. Ignoring one or more sources of state dependence underestimates the total incremental impact of a sales promotion. We draw implications for manufacturer pricing.", "e:keyword": ["Brand choice", "State dependence", "Habit persistence", "Lagged choices", "Lagged utilities", "Serial correlation", "Distributed lags", "Marketing carryover", "Random utility"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0050", "e:abstract": "While there is a growing literature on investigating the Internet clickstream data collected for a single site, such datasets are inherently incomplete because they generally do not capture shopping behavior across multiple websites. A customer's visit patterns at one or more other sites may provide relevant information about the timing and frequency of his or her future visit patterns at the site of interest. We develop a stochastic timing model of cross-site visit behavior to understand how to leverage information from one site to help explain customer behavior at another. To this end, we incorporate two sources of association in browsing patterns: one for the observable outcomes (i.e., arrival times) of two timing processes and the other for the latent visit propensities across a set of competing sites. This proposed multivariate timing mixture model can be viewed as a generalization of the univariate exponential-gamma model. In our empirical analysis, we show that a failure to account for both sources of association not only leads to poor fit and forecasts, but also generates systematically biased parameter estimates. We highlight the model's ability to make accurate statements about the future behavior of the zero class (i.e., previous nonvisitors to a given site) using summary information (i.e., recency and frequency) from past visit patterns at a competing site.", "e:keyword": ["Internet browsing behavior", "Data integration", "Multivariate duration models", "Customer acquisition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0048", "e:abstract": "When selling a product line, a firm has to consider the costs of communicating about the different products to the consumers. This may affect the product line design in general, and which products or services are offered in particular. The problem is that firms have to communicate to consumers, possibly through advertising, to make them consider buying the products that firms are selling. This results in the firm offering a smaller number of products than is optimal when advertising has no costs. This effect is greater the extent of consumer confusion about the advertising messages, and is reduced by a greater ability to target advertising. When offering vertically differentiated products (second-degree price discrimination), under general conditions it is optimal to advertise so that one has a greater proportion of sales of a lower-quality product than if advertising had no cost. This situation also allows the firm to charge a lower price for the high-quality product and offer a higher quality of the low-quality product than it would if advertising were without cost.", "e:keyword": ["Communication strategies", "Advertising", "Product policy", "Product line", "Segmentation", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0061", "e:abstract": "Sales promotions generate substantial short-term sales increases. To determine whether the sales promotion bump is truly beneficial from a managerial perspective, we propose a system of store-level regression models that decomposes the sales promotion bump into three parts: cross-brand effects (secondary demand), cross-period effects (primary demand borrowed from other time periods), and category-expansion effects (remaining primary demand). Across four store-level scanner datasets, we find that each of these three parts contribute about one third on average. One extension we propose is the separation of the category-expansion effect into cross-store and market-expansion effects. Another one is to split the cross-item effect (total across all other items) into cannibalization and between-brand effects. We also allow for a flexible decomposition by allowing all effects to depend on the feature/display support condition and on the magnitude of the price discount. The latter dependence is achieved by local polynomial regression. We find that feature-supported price discounts are strongly associated with cross-period effects while display-only supported price discounts have especially strong category-expansion effects. While the role of the category-expansion effect tends to increase with higher price discounts, the roles of cross-brand and cross-period effects both tend to decrease.", "e:keyword": ["Econometric models", "Market response models", "Sales promotion", "Regression and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0038", "e:abstract": "Consumers incur many transaction costs in purchasing and using most products. This paper examines the effects of a reduction in such consumer transaction costs caused by market-level technological advances. Using a model in which consumers are simultaneously heterogeneous in their transaction costs and in their marginal valuations of product quality, this paper highlights two mechanisms that can cause such reductions in consumer transaction costs to lower consumer surplus and reduce consumer share of the total social surplus. Specifically, market-level technological advances reduce different consumers' transaction costs by different amounts and increase their reservation prices by different amounts, which can lead to: (i) product design changes that many current consumers do not like and (ii) homogenization in consumer reservation prices that allows a seller to extract more surplus through its pricing policy. This paper also shows that consumers may be better off with seller-induced higher consumer transaction costs. Finally, the paper shows how, depending on the nature of the quality production process, such reductions in consumer transaction costs can either lower or raise product qualities and consumer prices.", "e:keyword": ["Technological advances", "Transaction costs", "Consumer homogenization", "Product features", "Product pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0052", "e:abstract": "The authors present a model of free sample effects and evidence from two field experiments on free samples. The model incorporates three potential effects of free samples on sales: (1) an acceleration effect, whereby consumers begin repeat purchasing of the sampled brand earlier than they otherwise would; (2) a cannibalization effect, which reduces the number of paid trial purchases of the brand; and (3) an expansion effect, which induces purchasing by consumers who would not consider buying the brand without a free sample. The empirical findings suggest that, unlike other consumer promotions such as coupons, free samples can produce measurable long-term effects on sales that can be observed as much as 12 months after the promotion. The data also show that the effectiveness of free sample promotions can vary widely, even between brands in the same product category. Application of the model to the data from the two experiments reveals that the magnitude of acceleration, cannibalization, and expansion effects varies substantially across the two free sample promotions. These and other findings suggest that the model can be a useful tool for obtaining insights into the nature of free sample promotions.", "e:keyword": ["Free samples", "Samples", "Promotions", "Field experiments", "Incremental volume", "Coupons", "Incremental sales", "Sampling programs", "Consumer products", "Panel data", "Repeat purchasing", "Long-term effects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0036", "e:abstract": "Store brand entry has become a key issue in marketing as it may structurally change the performance of and the interactions among all market players. Based on their multivariate time-series analysis, the authors demonstrate permanent performance effects of store brand entry, typically benefiting the retailer, the consumers, and premium-brand manufacturers, while harming second-tier brand manufacturers. For the , they consistently find two of store brand entry: . This increase in unit margins implies that the retailer strengthens its bargaining position vis--vis national brand manufacturers. However, store brand entry only rarely yields category expansion and does not create store traffic or revenue benefits. Second, do not obtain lower prices on all national brands, only on some second-tier brands. However, they benefit from enlarged product assortment and intensified promotional activity that lowers average price paid for two out of four categories. For the , store brand entry is typically beneficial for national brands, but not for national brands. Often, premium brands experience lower long-term price sensitivity and higher revenues, whereas second-tier brands experience higher long-term price sensitivity and lower revenues.", "e:keyword": ["Structural change", "Manufacturers versus retailers", "Store brand entry", "Unit root tests", "Vector-autoregressive models", "Long-term price elasticity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0032", "e:abstract": "Many theories of consumer behavior involve thresholds and discontinuities. In this paper, we investigate consumers' use of screening rules as part of a discrete-choice model. Alternatives that pass the screen are evaluated in a manner consistent with random utility theory; alternatives that do not pass the screen have a zero probability of being chosen. The proposed model accommodates conjunctive, disjunctive, and compensatory screening rules. We estimate a model that reflects a discontinuous decision process by employing the Bayesian technique of data augmentation and using Markov-chain Monte Carlo methods to integrate over the parameter space. The approach has minimal information requirements and can handle a large number of choice alternatives. The method is illustrated using a conjoint study of cameras. The results indicate that 92% of respondents screen alternatives on one or more attributes.", "e:keyword": ["Conjoint analysis", "Noncompensatory decision process", "Hierarchical Bayes", "Revealed choice", "Attribute screening", "Consideration sets", "Elimination by aspects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0062", "e:abstract": "This paper presents an initial examination of an emerging business model, the Name-Your-Own-Price (NYOP) channel, as popularized by priceline.com. Focusing on how to optimally structure such market interactions, I ask whether it is more profitable to restrict individuals to a single bid, as is currently done by , or conversely, to allow consumers to continue bidding if the previous offer was rejected. I find that both market structures yield the same expected profit. In practice, a single-bid policy may not be perfectly enforceable, especially in the Internet environment, because a sophisticated user can circumvent such a policy by camouflaging one's identity or otherwise manipulating the bidding procedure. Thus, single-bid restriction is likely to result in , the case in which some consumers are limited to a single bid while other, sophisticated users may rebid. I ask whether such surreptitious bidding is detrimental to the NYOP firm and find that profits are lower than if such opportunistic behavior were absent. Surprisingly, I find that the impact of the number of repeat bidders on profits is not monotonic. Thus, if it is prohibitively costly or logistically infeasible for the NYOP firm to eliminate surreptitious rebidding behavior, the firm may, in fact, benefit from encouraging, rather than discouraging, users to rebid. The direction that increases profits depends on the percentage of sophisticated bidders.", "e:keyword": ["Priceline", "Name-your-own-price channel", "Sequential search", "Bidding", "Pricing", "e-commerce", "Reverse auctions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0051", "e:abstract": "One of the main problems associated with early-period assessment of new product success is the lack of sufficient sales data to enable reliable predictions. We show that managers can use spatial dimension of sales data to obtain a predictive assessment of the success of a new product shortly after launch time. Based on diffusion theory, we expect that for many innovative products, word of mouth and imitation play a significant role in the success of an innovation. Because word-of-mouth spread is often associated with some level of geographical proximity between the parties involved, one can expect clusters of adopters to begin to form. Alternatively, if the market reaction is widespread reluctance to adopt the new product, then the word-of-mouth effect is expected to be significantly smaller, leading to a more uniform pattern of sales (assuming that there are no external reasons for clustering). Hence, the less uniform a product's distribution, the higher its likelihood of generating a contagion process and therefore of being a success. This is also true if the underlying baseline distribution is nonuniform, as long as it is an empirical distribution known to the firm. We use a spatial divergence approach based on cross-entropy divergence measures to determine the distance between two distribution functions. Using both simulated and real-life data, we find that this approach has been capable of predicting success in the beginning of the adoption process, correctly predicting 14 of 16 actual product introductions in two product categories. We also discuss the limitations of our approach, among them the possible confusion between natural formation of geodemographic clusters and word-of-mouth-based clusters.", "e:keyword": ["New products", "Innovation diffusion", "Spatial analysis", "Complexity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0049", "e:abstract": "When is inducing intrabrand competition (via nonexclusive distribution) an optimal strategy? To address this issue, a static model is developed to examine two settings. The manufacturer uses exclusive distributors in the first setting and nonexclusive distributors in the second. The analysis indicates that the choice of distribution rests critically on whether the manufacturer can effectively extract surplus from the distributors. Due to a variety of institutional reasons, the distributors' liability is often limited in performing on behalf of the manufacturer; such limited liability restricts how much of the distributors' surplus can be extracted. When the distributors' surplus cannot be fully extracted, the manufacturer may prefer nonexclusive distribution even when distributors can free-ride on each other's efforts.", "e:keyword": ["Channels of distribution", "Agency theory", "Intrabrand competition", "Free-riding", "Limited liability", "Vertical contractual restrictions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0056", "e:abstract": "In this paper, we provide theoretical arguments and empirical evidence for how Genetic Algorithms (GA) can be used for efficient estimation of macro-level diffusion models. Using simulations we find that GA and Sequential Search-Based-Nonlinear Least Squares (SSB-NLS) provide comparable parameter estimates when the data including peak sales are being used, for a range of error variances, and true parameter values commonly encountered in the literature. From empirical analyses we find that the forecasting performance of the GA estimates is better than that of SSB-NLS, Augmented Filter, Hierarchical Bayes, and Kalman Filter when only pre-peak sales data is available for estimation. When sales data until the peak time period are available for estimation, SSB-NLS is able to obtain parameter estimates when the starting values provided are the estimates from using GA. The estimates from GA are not biased and do not change in a systematic fashion when post-peak sales data are used, whereas the estimates from SSB-NLS are biased and change in a systematic fashion. Summarizing, we find that GA may be better suited for diffusion model estimation under the three conditions where SSB-NLS has been found to have problems.", "e:keyword": ["Bass model", "Starting values", "Systematic change and bias", "Closed-form solution", "Nonlinear least squares", "Genetic algorithms", "Pre-peak sales forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0096", "e:abstract": "Academic research in marketing often and rightfully tends to either build on well-established past research topics or follow well-established practices in industry. However, as technology advances, it might be possible to foresee some more enduring trends and focus research on future issues rather than on past issues. One approach would be to study emerging technologies with rapidly declining costs. Each of these emerging technologies spawns myriad applications that have the potential to dramatically impact existing markets. Interesting research topics include the study of the impact of these applications on different market participants (e.g., final consumers, the seller, the seller of complementary services, intermediaries, information providers, competitors, other industries). Research topics also include the optimal structure for products and services, given these new applications, as well as which intermediary should offer particular services. Research topics also include the interactive ability to rapidly customize marketing strategy by identifying individuals at particular points in time and under particular demand conditions. Five of these technologies include enhanced search services, biometrics and smart cards, enhanced computational speed, M-commerce, and GPS tracking.", "e:keyword": ["Research in marketing", "Scholarly and academic research topics", "Research fads"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0082", "e:abstract": "Franchisees within large branded chains loudly complain of a form of channel conflict known as encroachment or impact. Encroachment occurs when franchisors add new units of their brand proximately to their franchisees' existing units. Franchisees claim that their revenues have substantially decreased as a result of encroaching same-brand entry. The topic of encroachment has not only dominated franchisee association agendas and trade journal headlines but has also become a hot topic for politicians and policymakers. Yet, until now, evidence of encroachment has been strictly anecdotal. This paper provides the first systematic evidence of encroachment. Using revenue data from the Texas lodging industry in the 1990s, I find that when franchisors approve new same-brand units in the vicinity of incumbent units, these new units cannibalize the incumbents' revenues. In contrast to the result for franchisors, the addition of a new unit by company-owned brands in the vicinity of same-brand units is associated with an increase in the incumbents' revenues. This contrast suggests that encroaching behavior is caused by incentives that result from the governance form of franchising and is not simply an outcome that accompanies all expansion. This finding informs theory on governance forms and exclusive territories. Implications for practitioners and policy are also discussed.", "e:keyword": ["Channels of distribution", "Franchising", "Encroachment", "Cannibalization", "Market entry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0080", "e:abstract": "Buyer search costs for price are changing in many markets. Through a model of buyer and seller behavior, I consider the effects of changing search costs on prices both when product differentiation is fixed and when it is endogenously determined in equilibrium. If firms cannot change product design, lower buyer search costs for price lead to increased price competition. However, if product design is a decision variable, lower search costs for price may also lead to higher product differentiation, which decreases price competition. In this case, the overall effect of lower buyer search costs for price may even be higher prices, lower social welfare, and higher industry profits. The result is especially interesting because recent technological changes, such as Internet shopping, can affect the market structure through lowering buyer search costs.", "e:keyword": ["Product design", "Product positioning", "Search costs", "Market structure", "Endogenous firm decisions", "e-commerce"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0072", "e:abstract": "This paper proposes a descriptive model of the spatial and temporal evolution of retail distribution for new packaged goods. The distribution model postulates separate processes for local market entry by manufacturers, and adoption by retailers given entry. Of special interest is whether retail adoption occurs along a competitive network with retailers as nodes and overlapping trade areas of these retailers as links. The model is calibrated on data covering the introduction of two very successful new brands in the frozen pizza category. For these brands, manufacturers sequentially enter markets based on spatial proximity to markets already entered (spatial evolution), and on whether chains in these markets adopted previously elsewhere (market selection). A retail chain adopts new brands based on the adoption timing of competing chains within its trade territory (competitive contagion) and on the fraction of its trade area in which the new brand is available (trade area coverage). The effects of market selection and of trade area coverage create dependencies between market entry and retail adoption. Because of these dependencies the attraction of a particular market as a lead market depends on its location in the geographic structure of the U.S. retail trade.", "e:keyword": ["Spatial diffusion", "Network diffusion", "Retail distribution", "New product research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0070", "e:abstract": "The literature on structural equation models is unclear on whether and when multicollinearity may pose problems in theory testing (Type II errors). Two Monte Carlo simulation experiments show that multicollinearity can cause problems under certain conditions, specifically: (1) when multicollinearity is extreme, Type II error rates are generally unacceptably high (over 80%), (2) when multicollinearity is between 0.6 and 0.8, Type II error rates can be substantial (greater than 50% and frequently above 80%) if composite reliability is weak, explained variance (<sup>2</sup>) is low, and sample size is relatively small. However, as reliability improves (0.80 or higher), explained variance <sup>2</sup> reaches 0.75, and sample becomes relatively large, Type II error rates become negligible. (3) When multicollinearity is between 0.4 and 0.5, Type II error rates tend to be quite small, except when reliability is weak, <sup>2</sup> is low, and sample size is small, in which case error rates can still be high (greater than 50%). Methods for detecting and correcting multicollinearity are briefly discussed. However, since multicollinearity is difficult to manage after the fact, researchers should avoid problems by carefully managing the factors known to mitigate multicollinearity problems (particularly measurement error).", "e:keyword": ["Multicollinearity", "Measurement error", "Structural equation models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0054", "e:abstract": "Standard diffusion models capture social contagion only coarsely and do not allow one to operationalize different contagion mechanisms. Moreover, there is increasing skepticism about the importance of contagion and, as has long been known, S-shaped diffusion curves can also result from heterogeneity in the propensity to adopt. We present hypotheses about conditions under which specific contagion mechanisms and income heterogeneity are more pronounced, and test these hypotheses using a meta-analysis of the / ratio in applications of the Bass diffusion model. The ratio is positively associated with the Gini index of income inequality in a country, supporting the heterogeneity-in-thresholds interpretation. The ratio also varies as predicted by the Gamma-Shifted Gompertz diffusion model, but the evidence vanishes after controlling for national culture. As to contagion, the / ratio varies with the four Hofstede dimensions of national culturefor three of them in a direction consistent with the social contagion interpretation. Furthermore, products with competing standards have a higher / ratio, which is again consistent with the social contagion interpretation. Finally, we find effects of national culture only for products without competing standards, suggesting that technological effects and culturally moderated social contagion effects might not operate independently from each other.", "e:keyword": ["Diffusion of innovations", "Social contagion", "Income heterogeneity", "National culture", "Meta-analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0071", "e:abstract": "Managers are very interested in word-of-mouth communication because they believe that a product's success is related to the word of mouth that it generates. However, there are at least three significant challenges associated with measuring word of mouth. First, how does one gather the data? Because the information is exchanged in private conversations, direct observation traditionally has been difficult. Second, what aspect of these conversations should one measure? The third challenge comes from the fact that word of mouth is not exogenous. While the mapping from word of mouth to future sales is of great interest to the firm, we must also recognize that word of mouth is an outcome of past sales. Our primary objective is to address these challenges. As a context for our study, we have chosen new television (TV) shows during the 19992000 seasons. Our source of word-of-mouth conversations is Usenet, a collection of thousands of newsgroups with diverse topics. We find that online conversations may offer an easy and cost-effective opportunity to measure word of mouth. We show that a measure of the dispersion of conversations across communities has explanatory power in a dynamic model of TV ratings.", "e:keyword": ["Word of mouth", "Diffusion of innovations", "Measurement", "Networks and marketing", "New product research", "Internet marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0055", "e:abstract": "The main objective of this paper is to provide a decision-support system of micro-level customized promotions, primarily for use in online stores. Our proposed approach utilizes the one-on-one and interactive nature of the Internet shopping environment and provides recommendations on . We address the issue by first constructing a joint purchase incidence-brand choice-purchase quantity model that incorporates how variety-seeking/inertia tendency differs among households and change over time for the same household. Based on the model, we develop an optimization procedure to derive the optimal amount of price discount for each household on each shopping trip. We demonstrate that the proposed customization method could greatly improve the effectiveness of current promotion practices, and discuss the implications for retailers and consumer packaged goods companies in the age of Internet technology.", "e:keyword": ["Customized promotions", "Profit optimization", "Internet marketing", "Decision support system", "Personalized marketing", "Econometric models", "Purchase incidence", "Brand choice", "Purchase quantity", "Variety-seeking", "Inertia"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0073", "e:abstract": "Clickstream data provide information about the sequence of pages or the path viewed by users as they navigate a website. We show how path information can be categorized and modeled using a dynamic multinomial probit model of Web browsing. We estimate this model using data from a major online bookseller. Our results show that the memory component of the model is crucial in accurately predicting a path. In comparison, traditional multinomial probit and first-order Markov models predict paths poorly. These results suggest that paths may reflect a user's goals, which could be helpful in predicting future movements at a website. One potential application of our model is to predict purchase conversion. We find that after only six viewings purchasers can be predicted with more than 40% accuracy, which is much better than the benchmark 7% purchase conversion prediction rate made without path information. This technique could be used to personalize Web designs and product offerings based upon a user's path.", "e:keyword": ["Personalization", "Multinomial probit model", "Hierarchical Bayes models", "Hidden Markov chain models", "Vector autoregressive models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0075", "e:abstract": "Long-term marketing effectiveness is a high-priority research topic for managers, and emerges from the complex interplay among dynamic reactions of several market players. This paper introduces restricted policy simulations to distinguish four dynamic forces: consumer response, competitor response, company inertia, and company support. A rich marketing dataset allows the analysis of price, display, feature, advertising, and product-line extensions. The first finding is that consumer response differs significantly from the net effectiveness of product-line extensions, price, feature, and advertising. In particular, net sales effects are stronger and longer-lasting than consumer response. Second, this difference is not due to competitor response, but to company action. For tactical actions (price and feature), it takes the form of , as promotions last for several weeks. For strategic actions (advertising and product-line extensions), by other marketing instruments greatly enhances dynamic consumer response. This company action negates the postpromotion dip in consumer response, and enhances the long-term sales benefits of product-line extensions, feature, and advertising. Therefore, managers are urged to evaluate company decision rules for inertia and support when assessing long-term marketing effectiveness.", "e:keyword": ["Long-term marketing effectiveness", "Dynamic consumer and competitor response", "Company inertia and support", "Vector autoregressive  models", "Impulse-response functions", "Policy simulation restrictions", "Postpromotion dip"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0025", "e:abstract": "The paper Manufacturers Returns Policies and Retail Competition by Padmanabhan and Png (1997) argues that returns policies intensify retail competition and therefore raise the manufacturers profits. They reach that result through a problematic method to solve the game. Particularly, in the game where the manufacturer accepts returns, they unreasonably assume the retailers would never face stock constraints, thus changing the retail competition from a Cournot-like competition to a Bertrand one. Actually, in a game where retailers first order stocks and then compete by choosing prices, even if a manufacturer offers full return policies, the retailers can still use insufficient stocks as quantities precommitment in order to uphold retail prices. Hence, the retailers still face stock constraints at the final stage of the game. The nature of the game is essentially unaffected by returns policies when demand is certain. This note shows that returns policies do not intensify retail competition in the model proposed by Padmanabhan and Png (1997).", "e:keyword": ["Returns policies", "Retail competition", "Demand uncertainty", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0091", "e:abstract": "Returns policies are common in many sectors of retail distribution. Padmanabhan and Png (1997) showed that with demand uncertainty, a returns policy could improve manufacturer profitability under certain conditions. Wang (2004) showed that returns policies do not change manufacturer profitability when demand is certain and retailing is competitive. We show that returns policies do increase manufacturer profitability by attenuating retailer price competition when demand is low and intensifying competition when demand is high. Importantly, this effect holds only in the presence of demand uncertainty. Further, the conditions under which a returns policy raises the manufacturer's profit are weaker when retailing is a duopoly than when retailing is a monopoly. This suggests that returns policies serve both to manage competition and mitigate demand uncertainty.", "e:keyword": ["Returns policies", "Retail competition", "Demand uncertainty", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0084", "e:abstract": "Conventional wisdom suggests that one of the goals of manufacturer advertising is to reduce the cross-price elasticity between products (make one's own and rivals' products appear to be substitutable in the eyes of consumers). Conventional wisdom also suggests that, all else being equal, retailers will be able to obtain better terms of trade from manufacturers the substitutable are the manufacturers' products. It follows that retailers should be opposed to advertising that has the effect of reducing cross-price elasticities and thus that manufacturer advertising can be a source of channel conflict. We show that these conventional wisdoms need not hold when only some consumers are exposed to the advertising messages. Using a Hotelling model of demand, we show that (1) manufacturers can be worse off from advertising that reduces the cross-price elasticities between their products, (2) channel conflict need not arise, even when the sole purpose of advertising is to affect cross-price elasticities, and (3) depending on its bargaining power, a retailer can be better off when the manufacturers' products are perceived to be less substitutable.", "e:keyword": ["Advertising", "Differentiation", "Channel conflict", "Bargaining", "Channel coordination", "Distribution channel", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0112", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0114", "e:abstract": "features many and diverse articles that analyze competitive responsiveness. Although recent editorials (e.g., Shugan 2002) suggest that competitive responsiveness is only a part of a comprehensive competitive marketing strategy, it remains a vital part. For that reason and many others, is particularly proud of this special issue edited by David J. Reibstein and Dick R. Wittink. Before introducing and vigorously applauding both the editors and authors of this excellent special issue, we emphasize that competitive responsiveness raises numerous issues, including whether one can forecast outcomes of new policies based on past observations made under old policies (i.e., the Lucas critique) and decisions regarding which variables should be considered endogenous (Shugan 2004), i.e., determined within the model. Perhaps, normative models are inherently perishableevolution in market structure requires modifications over time. Also, although complete consistency within the world of the model is aesthetically pleasing, imposing industry-specific exogenous constraints (that might appear unrelated to the modeling assumptions) is sometimes necessary.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0111", "e:abstract": "No abstract available", "e:keyword": ["Marketing strategy", "Competitive strategy", "Game theory", "Competitiveness"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0077", "e:abstract": "This research uses Procter & Gamble's value pricing initiative as a context for testing whether actual competitor and retailer response to a major policy change can be predicted using a game-theoretic model. We first estimate demand functions for P&G and competitor brands from the period before value pricing was initiated. We then formulate a dynamic manufacturer-retailer Stackelberg model that includes P&G, a national-brand competitor, and a retailer. The model takes P&G's move as given and prescribes the price and promotion response of the competitors and the retailer. We substitute the estimated demand parameters into the model to obtain prescriptions for each competitor and the retailer, and see whether these prescriptions are related to the actual response. We find that the dynamic game-theoretic model calibrated with empirical estimates of demand parameters has significant predictive power. We also test the predictive power of two benchmark models. The first is based on the reaction function approach of Leeflang and Wittink (Leeflang, Peter S. H., Wittink, Dick R. 1992. Diagnosing competitive reactions using (aggregated) scanner data. 3957.), and the second is a simplification of our dynamic model where the retailer is not strategic. The dynamic game-theoretic model performs better than either benchmark.", "e:keyword": ["Dynamic game-theoretic models", "Manufacturer-retailer Stackelberg", "Competitor response", "Retailer response", "Pricing decisions", "Promotion decisions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0083", "e:abstract": "Companies spend millions of dollars on advertising to boost a brand's image and simultaneously spend millions of dollars on promotion that many believe calls attention to price and erodes brand equity. We believe this paradoxical situation exists because both advertising and promotion are necessary to compete effectively in dynamic markets. Consequently, brand managers need to account for interactions between marketing activities and interactions among competing brands. By recognizing interaction effects between activities, managers can consider interactivity trade-offs in planning the marketing-mix strategies. On the other hand, by recognizing interactions with competitors, managers can incorporate strategic foresight in their planning, which requires them to look forward and reason backward in making optimal decisions. Looking forward means that each brand manager anticipates how other competing brands are likely to make future decisions, and then by reasoning backward deduces one's own optimal decisions in response to the best decisions to be made by all other brands. The joint consideration of interaction effects and strategic foresight in planning marketing-mix strategies is a challenging and unsolved marketing problem, which motivates this paper. This paper investigates the problem of planning marketing mix in dynamic competitive markets. We extend the Lanchester model by incorporating interaction effects, constructing the marketing-mix algorithm that yields marketing-mix plans with strategic foresight, and developing the continuous-discrete estimation method to calibrate dynamic models of oligopoly using market data. Both the marketing-mix algorithm and the estimation method are general, so they can be applied to any other alternative model specifications for dynamic oligopoly markets. Thus, this dual methodology augments the decision-making toolkit of managers, empowering them to tackle realistic marketing problems in dynamic oligopoly markets. We illustrate the application of this dual methodology by studying the dynamic Lanchester competition across five brands in the detergents market, where each brand uses advertising and promotion to influence its own market share and the shares of competing brands. Empirically, we find that advertising and promotion not only affect the brand shares (own and competitors') but also exert interaction effects, i.e., each activity amplifies or attenuates the effectiveness of the other activity. Normatively, we find that large brands underadvertise and overspend on promotion, while small brands underadvertise and underpromote. Finally, comparative statics reveal managerial insights into how a specific brand should respond optimally to the changes in a competing brand's situation; more generally, we find evidence that competitive responsiveness is asymmetric.", "e:keyword": ["Continuous-discrete estimation", "Dynamic competition", "Interaction effects", "Marketing-mix planning", "Strategic foresight", "Two-point boundary value problem"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0069", "e:abstract": "How do competitors react to each other's price-promotion and advertising attacks? What are the reasons for the observed reaction behavior? We answer these questions by performing a large-scale empirical study on the short-run and long-run reactions to promotion and advertising shocks in over 400 consumer product categories over a four-year time span. Our results clearly show that the most predominant form of competitive response is passive in nature. When a reaction does occur, it is usually retaliatory in the same instrument, i.e., promotion attacks are countered with promotions, and advertising attacks are countered with advertising. There are very few long-run consequences of any type of reaction behavior. By linking reaction behavior to both cross- and own-effectiveness, we further demonstrate that passive behavior is often a sound strategy, while firms that do opt to retaliate often use ineffective instruments, resulting in spoiled arms. Accommodating behavior is observed in only a minority of cases, and often results in a missed sales opportunity when promotional support is reduced. The ultimate impact of most promotion and advertising campaigns depends primarily on the nature of consumer response, not the vigilance of competitors.", "e:keyword": ["Empirical generalizations", "Advertising and price-promotion effects", "Competitive strategy", "Time-series analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0064", "e:abstract": "The ability to keep up with changing technology is critical for a company's long-term survival. However, companies have to balance the risk of rushing into new areas and potentially cannibalizing their existing business against the risk of missing the emerging market. This paper investigates when incumbents enter into new market niches created by technological innovation. We argue that market conditions and company-specific characteristics do not suffice to explain incumbents' entry timing, but that entry is a contagious process. Our results demonstrate that incumbents are more likely to respond to innovations in their industry when their counterparts do so. In particular, we show that incumbents are affected by the entry of firms that are similar in size and resources. When a highly similar company enters the new market, it raises the probability that the company enters itself beyond levels based solely on the attractiveness of the market.", "e:keyword": ["Market entry", "Competition", "New product research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0029", "e:abstract": "We study three determinants of the levels of price and detailing effort across geographic markets: the within-market response to each variable, the nature of interfirm strategic interactions both within that market and across markets. We empirically examine the interactions of Prozac, Zoloft, and Paxil across the United States, the United Kingdom, Germany, France, and Italy. Our results indicate that all three factors driving marketing mix interactions are at play in this product category. The U.S. market is less price-sensitive than the European markets. Detailing elasticities are comparable across the United States, Germany, and Italy, while the United Kingdom and France show greater elasticity. For the U.S. market, we find that almost all deviations from Nash pricing and detailing levels are due to within-market interactions. In the U.K. market, deviations from Nash prices come about due mostly to across-market interactionsboth with the United States as well as the rest of Europe, whereas deviations from Nash detailing levels are mainly due to across-market interactions with the United States. For Italy, we observe that both within and across-market interactions affect price and detailing levels. Overall, the pattern of interactions makes observed prices more similar across countries than prices implied by the estimated elasticities. This underscores the importance of considering within- and across-market interactions in developing multimarket strategy.", "e:keyword": ["Marketing strategy", "International marketing", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0087", "e:abstract": "We investigate differences in the dynamics of marketing decisions across geographic markets empirically. We begin with a linear-quadratic game involving forward-looking firms competing on prices and advertising. Based on the corresponding Markov perfect equilibrium, we propose estimable econometric equations for demand and marketing policy. Our model allows us to measure empirically the strategic response of competitors along with economic measures such as firm profitability. We use a rich dataset that combines sales, marketing mix, factor cost, and advertising cost data for eighteen geographic markets in the frozen entre category. We find that larger markets tend to be less price-sensitive and more profitable than smaller markets. We also find evidence of positive carryover of own advertising on own demand. In terms of consumer substitution patterns, we find that the role of advertising (in our data) seems to be more category-building (complementary) than share-stealing (competitive). The complementary role is stronger in larger markets. On the supply side, we find that firms make smaller adjustments to own advertising as goodwill goes up. Consistent with cross-advertising effects on demand, firms make smaller (larger) adjustments to advertising in response to competitive goodwill in the less competitive larger (in the more competitive smaller) markets. Finally, we find that consumer welfare decreases (increases) in larger (smaller) markets when firms move to a zero-advertising regime.", "e:keyword": ["Competition", "Advertising", "Multiple geographic markets", "Structural models", "Markov perfect equilibrium", "Dynamics", "Packaged goods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0074", "e:abstract": "Normative models typically suggest that prices rise in periods of high demand and cost. However, in many markets, prices fall when demand or costs rise. This inconsistency occurs because the normative models assume that competitive intensity does not change with demand and cost conditions over time. We therefore introduce the notion of by suggesting that it is important not only to account for the of demand and cost on prices (e.g., higher demand means higher prices), but also the of demand and cost changes on competition (e.g., higher demand could cause more competition and, hence, lower prices). We develop a general, unified framework to empirically model the direct and indirect effects of demand and cost shifts on pricing in differentiated product markets. Our approach allows us to measure the indirect effect of multiple demand and cost drivers on competitive intensity and test predictions from alternative theories of repeated games. The empirical application is to the U.S. photographic film industry, where there are two main players, Kodak and Fuji. We find that the indirect effects are highly significant and comparable in magnitude to the direct effects. Competitive intensity is greater in periods of high demand and lower cost and is moderated by whether demand or costs are expected to grow or decline. Interestingly, we find asymmetries in the competitive responses of Kodak and Fuji. While Kodak is sensitive to demand factors, Fuji is sensitive to costs. Our results suggest that market characteristics such as observability of competitor prices can be an important determinant of how competitive intensity is affected by demand and cost conditions.", "e:keyword": ["Pricing research", "Competition", "Competitive strategy", "Game theory", "Estimation and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0053", "e:abstract": "I provide a general formulation of the channel pass-through problem as a comparative static of the retail price equilibrium, and I analyze the impact of category management and retail competition on pass-through, focusing on brand and retailer differences, and the nature of the cost change being passed throughwhether it is brand specific, retailer specific, both, or neither. With category management, a retailer's response to a brand-specific cost change is not limited to that brand; in general, a retailer will also change the prices of other brands. The cross-brand effect can be positive or negative, and, depending on its sign, it either enhances or attenuates pass-through. I explain the cross-brand effect as an interaction between two forces: a demand-substitution force that pushes for a negative cross-brand effect, and a strategic-complementarity force that pushes for a positive cross-brand effect. Retail competition adds another layer of strategic complementarity, causing other retailers to respond even for retailer-specific cost changes and increasing pass-through of categorywide cost changes. But its effect for brand-specific cost changes is ambiguous. I apply the theory to two commonly used demand functionslinear demand and nested logitand show that they have significantly different pass-through properties. The paper concludes with a discussion of how the theory relates to the empirical literature, including the companion piece by Besanko et al. (Besanko, D., J-P. Dub, S. Gupta. 2005. Own-brand and cross-brand retail pass-through. (1) 123137.)", "e:keyword": ["Pass-through", "Retailing", "Category management", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1030.0043", "e:abstract": "In this paper we describe the pass-through behavior of a major U.S. supermarket chain for 78 products across 11 categories. Our data set includes retail prices and wholesale prices for stores in 15 retail price zones for a one-year period. For the empirical model, we use a reduced-form approach that focuses directly on equilibrium prices as a function of exogenous supply- and demand-shifting variables. The reduced-form approach enables us to identify the theoretical pass-through rate without specific assumptions about the form of consumer demand or the conduct of a category-pricing manager. Thus, our measurements of pass-through are not constrained by specific structure on the underlying economic model. The empirical pricing model includes costs of all competing products in the category on the right-hand side (not only the cost of the focal brand) and yields estimates of both own-brand and cross-brand pass-through rates. Our results provide a rich picture of the retailer's pass-through behavior. We find that pass-through varies substantially across products and across categories. Own-brand pass-through rates are, on average, more than 60% for 9 of 11 categories, a finding that is at odds with the claims of manufacturers about retailers in general. Importantly, we find substantial evidence of cross-brand pass-through effects, indicating that retail prices of competing products are adjusted in response to a change in the wholesale price of any given product in the category. We find that cross-brand pass-through rates are both positive and negative. We explore determinants of own-brand and cross-brand pass-through rates and find strong evidence in multiple categories of asymmetric retailer response to trade promotions on large versus small brands. For example, brands with larger market shares, and brands that contribute more to retailer profits in the category, receive higher pass-through. We also find that trade promotions on large brands are less likely than small brands to generate positive cross-brand pass-through, i.e., induce the retailer to reduce the retail price of competing smaller products. On the other hand, small share brands are disadvantaged along three dimensions. Trade promotions on small brands receive low own-brand pass-through generate positive cross-brand pass-through for larger competing brands. Moreover, small share brands do not receive positive cross pass-through from trade promotions on these larger competitors. We also find that store brands are similarly disadvantaged with respect to national brands.", "e:keyword": ["Pricing", "Promotion", "Retailing", "Channels of distribution", "Econometric models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0076", "e:abstract": "Much of the empirical research on competitive reactions describes how or why rivals react to a firm's past actions, but stops short of examining whether managers attempt to such reactions, which we call strategic competitive reasoning. In three exploratory studies, we find evidence of managers' thinking about competitors' past and future behavior, but little incidence of strategic competitive reasoning. Competitive intelligence experts and other experienced managers' assessment of the results suggests that the relatively low incidence of strategic competitor reasoning is due to perceptions of low returns from anticipating competitor reactions more than to the high cost of doing so. Both the difficulty of obtaining competitive information and the uncertainty associated with predicting competitor behavior contribute to these perceptions. The paper suggests both a need for research on competitive behavior and an opportunity to influence and improve managerial judgment and decision making.", "e:keyword": ["Competitive reaction", "Competitive analysis", "Marketing strategy", "Managerial decision making"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0086", "e:abstract": "This paper describes the development and application of a marketing model to help set an incumbent's defensive marketing strategy prior to a new competitor's launch. The management problem addressed is to assess the market share impact of a new entrant in the residential Australian long distance telephone call market and determine the factors that would influence its dynamics and ultimate market appeal. The paper uses probability flow models to provide a framework to generate forecasts and assess the determinants of share loss. We develop models at two levels of complexity to give both simple, robust forecasts and more detailed diagnostic analysis of the effect of marketing actions. The models are calibrated prior to the new entrant's launch, enabling preemptive marketing strategies to be put in place by the defending company. The equilibrium level of consideration of the new entrant was driven by respondents' strength of relationship with the defender and inertia, while trial was more price-based. Continued use of the defender depends on both service factors and price. The rate at which share loss eventuates is negatively related to the defender's perceived responsiveness, saving money being the only reason to switch, and risk aversion. Prelaunch model forecasts, validated six months after launch using both aggregate monthly sales data and detailed tracking surveys, are shown to closely follow the actual evolution of the market. The paper provides a closed-form multistate model of the new entrant's diffusion, a methodology for the prelaunch calibration of dynamic models in practice, and insights into defensive strategies for existing companies facing new entrants.", "e:keyword": ["Defensive strategy", "Brand choice", "Diffusion", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0065", "e:abstract": "Building on the observation that competitive dynamics and market evolution are inextricably linked and underresearched, we propose a road map to guide and stimulate future research in the area. A number of rationales have been proposed to explain why there is relatively little research directed toward understanding the links between competitive dynamics and market evolution; these include the predominance of different research paradigms in each area, a lack of data appropriate for analyzing the two areas together, and the difficulty of obtaining robust and significant results with analysis that is by definition complex (it must consider factors and outcomes both across firms and over time). Using this last rationale as a starting point, we develop a series of research propositions related to key relationships where (a) insignificant or contradictory results have been obtained (in extant research) or (b) researchers have yet to delve. The propositions are designed to deepen our understanding of the relationship between the areas. Throughout the analysis, the key to developing the propositions is to recognize the importance of moderating factors, mediating factors, and covariates. In addition, where the approach to empirically test a proposition is new, we propose categories, measures, and comparisons that can be used.", "e:keyword": ["Competition", "Market growth", "Market potential", "Competitive response", "Market structure"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.147.12845", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0124", "e:abstract": "Brand loyalty and the more modern topics of computing customer lifetime value and structuring loyalty programs remain the focal point for a remarkable number of research articles. At first, this research appears consistent with firm practices. However, close scrutiny reveals disaffirming evidence. Many current so-called loyalty programs appear unrelated to the cultivation of customer brand loyalty and the creation of customer assets. True investments are up-front expenditures that produce much greater future returns. In contrast, many so-called loyalty programs are shams because they produce liabilities (e.g., promises of future rewards or deferred rebates) rather than assets. These programs produce short-term revenue from customers while producing substantial future obligations to those customers. Rather than showing trust by committing to the customer, the firm asks the customer to trust the firmthat is, trust that future rewards are indeed forthcoming. The entire idea is antithetical to the concept of a customer asset. Many modern loyalty programs resemble old-fashioned trading stamps or deferred rebates that promise future benefits for current patronage. A true loyalty program invests in the customer (e.g., provides free up-front training, allows familiarization or customization) with the expectation of greater future revenue. Alternative motives for extant programs are discussed.", "e:keyword": ["So-called loyalty programs", "Reward programs", "Customer assets", "Customer liabilities", "Deferred rebates", "Customer lifetime value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0090", "e:abstract": "This research examines whether a low-ranking member in a high-status category (e.g., a low-end model of a high-end brand) or a high-ranking member in a low-status category (e.g., a high-end model of a low-end brand) is favored, holding the objective qualities of the items constant. Brand equity research suggests that the quality of a brand is more important than the ranking of a product within a brand. Our research documents a robust whereby a high-ranking product in a low-status category is favored over a low-ranking product in a high-status category even when information on competing categories is made available. We explain this effect in terms of narrow focusing and evaluability, and we identify boundary conditions of the effect.", "e:keyword": ["Brand choice", "Brand product management", "Ranking effect", "Narrow focusing", "Evaluability"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0099", "e:abstract": "Sellers often explicitly suggest to buyers that they compare one option to other (reference) options. Building on the notion that loss aversion is more pronounced when comparisons are explicit rather than implicit, we propose that the mere fact that consumers are explicitly told to make particular comparisons induces more risk-averse, cautious choice and bidding behavior. This proposition was supported in a field experiment involving real online auctions, in which comparisons among listings either were done spontaneously by bidders or were encouraged using an explicit instruction to compare the focal auction with adjacent listings. Results showed that explicit reference points (1) diminished the influence of adjacent auction prices on the focal auctions price; (2) led participants to submit fewer, lower, and later bids; (3) increased the incidence of sniping; (4) decreased bidding frenzy; and (5) decreased the tendency to bid on multiple items simultaneously. The impact of explicit comparisons on risk-averse behavior was further tested in a very different context using a laboratory choice experiment. In that study, explicit instructions to compare option sets increased the tendency to choose the compromise, low-risk, and all-average alternatives. We discuss the theoretical and practical implications of this research.", "e:keyword": ["Explicit reference points", "Comparisons", "Consumer choice", "Online bidding behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0089", "e:abstract": "Product reviews by third parties are growing in popularity. This paper examines when and how a manufacturing firm should adapt its marketing strategies to such reviews. For example, should a firm receiving an unfavorable review reduce its price or adjust its advertising? Should a winning product of a product review (e.g., editors choice) boost its advertising expenditure to spread the good news? How should firms strategic responses to product reviews differ across different types of product reviews (description vs. recommendation) and different advertising media (the reviewers publication vs. other media)? We develop a theory to address these issues and derive firms optimal responses to product reviews under different product/market/review/media conditions. We show that firms should choose rather than as a strategic variable in response to product reviews when enough consumers value horizontal product attributes. Surprisingly, we find that using a review-endorsed (i.e., advertisements containing third-party award logos) to broadcast its victory can hurt the winning product of a product review. Also, it is not necessarily wise for the winning products to boost to spread the good news. Data from two industriesprinters and running shoesare used to illustrate some of our findings.", "e:keyword": ["Pricing", "Advertising", "Third-party infomediaries", "Product review information", "Information asymmetry", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0092", "e:abstract": "We show that when a single media content distributor (such as a television cable company or an Internet provider) delivers advertising messages on behalf of multiple competing brands, it can sometimes utilize customized advertising to implement monopoly pricing. Even though such monopolistic pricing can be implemented with varying degrees of customization of commercials, product revenues and consumer surplus are highest when the distributor chooses the highest level of customization feasible. Consumers would, obviously, prefer aggressive price competition in product markets. However, given that collusion on prices is facilitated anyway, when the distributor acts as a common agent, the welfare of consumers is enhanced when commercials are better aligned with their preferences.", "e:keyword": ["Personalized advertising", "Media content distributor", "Common agency", "Price competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0081", "e:abstract": "The retail trade today is increasingly dominated by large, centrally managed power retailers. In this paper, we develop a channel model in the presence of a dominant retailer to examine how a manufacturer can best coordinate such a channel. We show that such a channel can be coordinated to the benefit of the manufacturer through either quantity discounts or a menu of two-part tariffs. Both pricing mechanisms allow the manufacturer to charge different effective prices and extract different surpluses from the two different types of retailers, even though they both have the appearance of being fair. However, quantity discounts and two-part tariffs are not equally efficient from the manufacturers perspective as a channel coordination mechanism. Therefore, the manufacturer must judiciously select its channel coordination mechanism. Our analysis also sheds light on the role of street money in channel coordination. We show that such a practice can arise from a manufacturers effort to mete out minimum incentives to engage the dominant retailer in channel coordination. From this perspective, we derive testable implications with regard to the practice of street money.", "e:keyword": ["Distribution channels", "Channel power", "Channel coordination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0094", "e:abstract": "We argue that standardized information disclosure (information using a common format and uniform metrics) creates asymmetric opportunities for firms, which affects their strategies and survival. We test our predictions using a longitudinal, quasi-experimental field study, involving the Nutrition Labeling and Education Act of 1990 (NLEA), and we focus on firm market share within a category as a key asymmetry. Findings indicate that, in general, the NLEA had no effect on firm responses. However, when accounting for firm differences, we observe that the NLEA led to (1) an increase in small-share firm exits and (2) a greater increase in distribution for large-share firms. No concurrent increase in price by large-share firms following the NLEA was observed. We conclude by discussing the implications of these effects for firm strategy, the design of public policy, and theories regarding the impact of information on markets.", "e:keyword": ["Public policy", "Marketing strategy", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0098", "e:abstract": "Todays managers are very interested in predicting the future purchasing patterns of their customers, which can then serve as an input into lifetime value calculations. Among the models that provide such capabilities, the Pareto/NBD counting your customers framework proposed by Schmittlein et al. (1987) is highly regarded. However, despite the respect it has earned, it has proven to be a difficult model to implement, particularly because of computational challenges associated with parameter estimation. We develop a new model, the beta-geometric/NBD (BG/NBD), which represents a slight variation in the behavioral story associated with the Pareto/NBD but is vastly easier to implement. We show, for instance, how its parameters can be obtained quite easily in Microsoft Excel. The two models yield very similar results in a wide variety of purchasing environments, leading us to suggest that the BG/NBD could be viewed as an attractive alternative to the Pareto/NBD in most applications.", "e:keyword": ["Customer base analysis", "Repeat buying", "Pareto/NBD", "Probability models", "Forecasting", "Lifetime value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0088", "e:abstract": "It has been shown in the behavioral decision making, marketing research, and psychometric literature that the structure underlying preferences can change during the administration of repeated measurements (e.g., conjoint analysis) and data collection because of effects from learning, fatigue, boredom, and so on. In this research note, we propose a new class of hierarchical dynamic Bayesian models for capturing such dynamic effects in conjoint applications, which extend the standard hierarchical Bayesian random effects and existing dynamic Bayesian models by allowing for individual-level heterogeneity around an aggregate dynamic trend. Using simulated conjoint data, we explore the performance of these new dynamic models, incorporating individual-level heterogeneity across a number of possible types of dynamic effects, and demonstrate the derived benefits versus static models. In addition, we introduce the idea of an unbiased dynamic estimate, and demonstrate that using a counterbalanced design is important from an estimation perspective when parameter dynamics are present.", "e:keyword": ["Heterogeneity", "Empirical utility functions", "Dynamic models", "Bayesian analysis", "Conjoint analysis", "Unbiased dynamic estimates"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0097", "e:abstract": "In this research, we investigate the behavior of Cronbachs coefficient alpha and its new standard error. We systematically analyze the effects of sample size, scale length, strength of item intercorrelations, and scale dimensionality. We demonstrate the beneficial effects of sample size on alphas standard error and of scale length and the strengths of item intercorrelations (effects that are substitutes in their benefits) on both alpha and its standard error. Our findings also speak to this adage: Heterogeneity within the item covariance matrix (e.g., through multidimensionality or poor items) negatively impacts reliability by decreasing the precision of the estimation. We also examined the question of equilibrium scale length, showing the conditions for which it is optimal to add no items, or one, or multiple items to a scale. In terms of best practices, we recommend that researchers report a confidence interval or standard error along with the coefficient alpha point estimate.", "e:keyword": ["Measurement", "Survey research", "Reliability", "Coefficient alpha"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0132", "e:abstract": "The two reports and two articles (as well as a Commentary) that follow are the finalists from the 2004 ISMS Practice Prize Competition, representing the best examples of rigor plus relevance that our profession produces.", "e:keyword": ["Marketing science practice", "Sales territory design", "Sales forecasting", "Optimal product mix", "Advertising models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0133", "e:abstract": "Sales territory alignment is the assignment of accounts and their associated selling activities to salespeople and teams. Models, systems, processes, and wisdom have evolved over 1,500 project implementations for 500 companies with 500,000 sales territories. Optimization models have evolved over time to explicitly consider travel time along road networks and customer disruption. Personal computers with continually increasing speeds and storage capabilities, the Internet, and mapping databases have enabled the development of systems that communicate alignments visually to sales managers. Because of their combinatorial complexity, multiple conflicting objectives, and personnel aspects that touch everyone in the salesforce, the alignment models were unable to completely solve the sales territory alignment issues faced by companies. Consequently, processes that add local managerial knowledge were used to communicate and enhance model-derived solutions, while achieving very high implementation rates. The territory alignment team gains knowledge with every sales territory alignment. Alignment insights get codified. Alignment experts improve every model-derived solution. This wisdom becomes part of subsequent alignments and triggers further innovation. Over time, the role of processes and wisdom becomes larger than the role of the models and systems.", "e:keyword": ["Salesforce", "Sales management", "Sales territory alignment", "Segmentation", "Change management", "Model implementation", "Pharmaceuticals"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0134", "e:abstract": "No abstract available", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0135", "e:abstract": "We discuss the development and implementation of , a sales forecasting model, by pack size, category, channel, region, customer account and a Web-based decision support system (DSS) for consumer packaged goods. In addition to capturing the effects of such variables as past sales, trend, own and competitor prices and promotional variables, and seasonality, the model accounts for the effects of temperature, significant holidays, new product introductions, trading day corrections, and adjustments to the wholesale level. In general, the model forecasts sales volume satisfactorily for a leading consumer packaged goods company. The DSS enables top- and mid-level executives in sales, marketing, strategic planning, and finance to develop accurate forecasts of sales volume, plan prices, and promotional activities over a long time horizon; to track sales response to marketing actions over time; and to simulate forecast scenarios based on possible marketing decisions and other variables. is being rolled out for more users and more divisions in the company. The key take-aways are that successful development and implementation of a rigorous marketing science model require a strong internal champion, a careful balance between modeling sophistication and practical relevance, good diagnostic features, regular validations, and greater attention to the development of a fast and responsive DSS.", "e:keyword": ["Forecasting", "Econometric models", "Channels", "Decision support system", "Marketing mix", "Pricing", "Promotions", "Strategic planning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0159", "e:abstract": "The Practice Prize Reports consist of one article with two parts as follows: Sinha, Ashish, J. Jeffrey Inman, Yantao Wang, Joonwook Park. Attribute drivers: A factor analytic choice map approach for understanding choices among SKUs and Tellis, Gerard J., Rajesh K. Chandy, Deborah MacInnis, Pattana Thaivanich. Modeling the microeffects of television advertising: Which ad works, when, where, for how long, and why?", "e:keyword": ["Product management", "Choice models", "Assortment", "Advertising response", "Wear-in", "Wear-out", "Carry-over effect", "Long-term effect", "Ad creative", "Ad cues"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0103", "e:abstract": "The rapid advance in information technology now makes it feasible for sellers to condition their price offers on consumers prior purchase behavior. In this paper we examine when it is profitable to engage in this form of price discrimination when consumers can adopt strategies to protect their privacy. Our baseline model involves rational consumers with constant valuations for the goods being sold and a monopoly merchant who can commit to a pricing policy. Applying results from the prior literature, we show that although it is to price so as to distinguish high-value and low-value consumers, the merchant will never find it to do so. We then consider various generalizations of this model, such as allowing the seller to offer enhanced services to previous customers, making the merchant unable to commit to a pricing policy, and allowing competition in the marketplace. In these cases we show that sellers will, in general, find it profitable to condition prices on purchase history.", "e:keyword": ["Internet marketing", "Personalized marketing", "Price discrimination", "Targeting", "Privacy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0109", "e:abstract": "Almost universally, research and practice suggest that a brand that increases its product assortment, or variety, should benefit through increased market share. In this paper, we show this is not always the case. We introduce the construct assortment type and demonstrate that the effect of assortment size on brand share is systematically moderated by assortment type. We define an alignable assortment as a set of brand variants that differ along a single, compensatory dimension such that choosing from that assortment only requires within-attribute trade-offs. In contrast, we define a nonalignable assortment as a set of brand variants that simultaneously vary along multiple, noncompensatory dimensions, demanding between-attribute trade-offs. In turn, we argue that an alignable assortment can efficiently meet the diverse tastes of consumers, thereby increasing brand share, but that a nonalignable assortment increases both the cognitive effort and the potential for regret faced by a consumer, thereby decreasing brand share. We term this effect overchoice. Across three studies, we provide evidence of overchoice and tie the effect to the effort and regret brought about by nonalignability. In the process, we demonstrate that simplification of information presentation, reversibility of choice, and a reduction in underlying nonalignability serve to reduce or eliminate this effect.", "e:keyword": ["Brand assortments", "Brand choice", "Decision making", "Product policy", "Variety"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0105", "e:abstract": "In alliances jointly developing product and market, we first investigate how (a) the number of networks competing to develop a product, (b) the number of alternative technology platforms, and (c) market sensitivity to product development expenditures affect investments of partnering firms. We find that, in equilibrium, when the number of either competing networks or technologies increases, investments are more likely to be directed toward market, rather than product, development. Second, we consider the case in which firms continue to jointly develop a product but compete individually in the market. Our analysis suggests that forcing alliance partners to compete individually might not attenuate the underinvestment problem associated with new product alliances. Third, we extend the model to consider sequential market entry with rewards based on the order of entry, technology spillover, endogenous market size, and asymmetric technologies. Finally, key predictions of the basic model are tested in two experiments. The aggregate results provide strong support to the qualitative implications of the equilibrium solution but only mixed support to its quantitative predictions.", "e:keyword": ["New product research", "Alliances", "Experimental economics", "Game theory", "Two-stage competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0100", "e:abstract": "We introduce methods from statistical learning theory to the field of conjoint analysis for preference modeling. We present a method for estimating preference models that can be highly nonlinear and robust to noise. Like recently developed polyhedral methods for conjoint analysis, our method is based on computationally efficient optimization techniques. We compare our method with standard logistic regression, hierarchical Bayes, and the polyhedral methods using standard, widely used simulation data. The experiments show that the proposed method handles noise significantly better than both logistic regression and the recent polyhedral methods and is never worse than the best method among the three mentioned above. It can also be used for estimating nonlinearities in preference models faster and better than all other methods. Finally, a simple extension for handling heterogeneity shows promising results relative to hierarchical Bayes. The proposed method can therefore be useful, for example, for analyzing large amounts of data that are noisy or for estimating interactions among product features.", "e:keyword": ["Choice models", "Data mining", "Econometric models", "Hierarchical Bayes analysis", "Marketing tools", "Regression and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0110", "e:abstract": "Over the years, researchers have found that promotion makes consumers switch brands and purchase earlier or more. However, it is unclear how promotion affects consumption, especially for product categories that are perceived to be versatile and substitutable. In this paper, we propose a dynamic structural model with endogenous consumption under promotion uncertainty to analyze the promotion effect on consumption. This model recognizes consumers as rational decision makers who form promotion expectations and plan their purchase and consumption decisions in light of promotion schedule. Applying the proposed model to packaged tuna and yogurt, we find that endogenous consumption responds to promotion as a result of forward-looking and stockpiling behavior. This is the first empirical paper that recognizes consumption as an endogenous decision variable and proposes a structural model to offer behavioral explanations on whether, how, and why promotion encourages consumption for product categories with flexible consumption.", "e:keyword": ["Promotion", "Consumption", "Category expansion", "Dynamic structural model", "Forward-looking consumers"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0101", "e:abstract": "Discrete choice models of aggregate demand, such as the random coefficients logit, can handle large differentiated products categories parsimoniously while still providing flexible substitution patterns. However, the discrete choice assumption may not be appropriate for many categories in which we expect consumers may purchase more than one unit of the selected item. We derive the aggregate demand system corresponding to a discrete/continuous household-level model of demand. We also propose a method-of-simulated-moments procedure that provides consistent estimates of the structural parameters when only aggregate data are available. The procedure also enables the researcher to control both for the potential endogeneity of marketing variables as well as potential heterogeneity in consumer tastes. Using our aggregate estimates, we can measure the decomposition of price elasticities into incidence, brand choice, and purchase quantity components. We also propose several empirical tests to assess the validity of the discrete/continuous demand system versus that of the logit model. In several simulation experiments, we demonstrate the robustness of this model across datasets in which quantity choices may or may not be important. Our empirical calibration to store-level data in the refrigerated orange juice category indicates a considerable improvement in fit of the observed aggregate sales using the discrete/continuous model.", "e:keyword": ["Discrete/continuous demand system", "Logit demand system", "Aggregate data", "Price endogeneity", "Primary and secondary demand"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0117", "e:abstract": "An important question that firms face in advertising is developing effective media strategy. Major improvements in the quality of consumer information and the growth of targeted media vehicles allow firms to precisely target advertising to consumer segments within a market. This paper examines advertising strategy when competing firms can target advertising to different groups of consumers within a market. With targeted advertising, we find that firms advertise more to consumers who have a strong preference for their product than to comparison shoppers who can be attracted to the competition. Advertising less to comparison shoppers can be seen as a way for firms to endogenously increase differentiation in the market. In addition, targeting allows the firm to eliminate wasted advertising to consumers whose preferences do not match a products attributes. As a result, the targeting of advertising increases equilibrium profits. The model demonstrates how advertising strategies are affected by firms being able to target pricing. Target advertising leads to higher profits, regardless of whether or not the firms have the ability to set targeted prices, and the targeting of advertising can be more valuable for firms in a competitive environment than the ability to target pricing.", "e:keyword": ["Media precision", "Advertising", "Targeting", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0107", "e:abstract": "We provide a fully personalized model for optimizing multiple marketing interventions in intermediate-term customer relationship management (CRM). We derive theoretically based propositions on the moderating effects of past customer behavior and conduct a longitudinal validation test to compare the performance of our model with that of commonly used segmentation models in predicting intermediate-term, customer-specific gross profit change. Our findings show that response to marketing interventions is highly heterogeneous, that heterogeneity of response varies across different marketing interventions, and that the heterogeneity of response to marketing interventions may be partially explained by customer-specific variables related to customer characteristics and the customers past interactions with the company. One important result from these moderating effects is that relationship-oriented interventions are more effective with loyal customers, while action-oriented interventions are more effective with nonloyal customers. We show that our proposed model outperformed models based on demographics, recency-frequency-monetary value (RFM), or finite mixture segmentation in predicting the effectiveness of intermediate-term CRM. The empirical results project a significant increase in intermediate-term profitability over all of the competing segmentation approaches and a significant increase in intermediate-term profitability over current practice.", "e:keyword": ["Customer relationship management", "One-to-one marketing", "Personalization", "Customer heterogeneity", "Segmentation", "Direct marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0102", "e:abstract": "Delegating pricing decisions to the salesforce has been a salient issue for marketing academics and practitioners. We examine this issue in a competitive market using standard agency theory with symmetric and asymmetric information. Under symmetric information we find that the optimal contracts have a nice property that allows managers to reach the upper bound on firms profit by using either centralized or delegated pricing, and hence there are no incentive-based reasons to prefer centralized versus delegated contract types. Thus, managers can focus on the design of the optimal incentive scheme without worrying about the contract type. Under asymmetric information we find that there always exists an equilibrium where all firms use centralized pricing that is either unique or payoff equivalent to equilibria that have a combination of contract types. These results are robust to a large class of agent and market parameters. However, if restrictions are exogenously imposed on contract form or observability, the above results are no longer true, as in earlier work. We demonstrate our results by providing explicit solutions under the Holmstrom and Milgrom (1987) framework.", "e:keyword": ["Pricing research", "Compensation", "Salesforce", "Agency theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0108", "e:abstract": "Adaptive metric utility balance is at the heart of one of the most widely used and studied methods for conjoint analysis. We use formal models, simulations, and empirical data to suggest that adaptive metric utility balance leads to partworth estimates that are relatively biasedsmaller partworths are upwardly biased relative to larger partworths. Such relative biases could lead to erroneous managerial decisions. Metric utility-balanced questions are also more likely to be inefficient and, in one empirical example, contrary to popular wisdom, lead to response errors that are at least as large as nonadaptive orthogonal questions. We demonstrate that this bias is because of endogeneity caused by a winners curse. Shrinkage estimates do not mitigate these biases. Combined with adaptive metric utility balance, shrinkage estimates of heterogeneous partworths are biased downward relative to homogeneous partworths. Although biases can affect managerial decisions, our data suggest that, empirically, biases and inefficiencies are of the order of response errors. We examine viable alternatives to metric utility balance that researchers can use without biases or inefficiencies to retain the desired properties of (1) individual-level adaptation and (2) challenging questions.", "e:keyword": ["Conjoint analysis", "Efficient question design", "Adaptive question design", "Internet market research", "e-commerce", "Product development"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1040.0106", "e:abstract": "We examine box-office sales in the context of a market share model. This is accomplished by developing a combination of a sliding-window logit model and a gamma diffusion pattern in a hierarchical Bayes framework. We show that accounting for the full choice set available every week not only increases the fit of weekly movie sales but also leads to parameter estimates that depict a richer picture of the movie industry. We show that movie studios appear to have a good understanding of the products they produce, knowing when to support them and when not to. We also show that the effect of the number of opening week screens is overestimated in traditional models. Our research indicates that actors have a direct and directors an indirect effect on consumers movie choice. Releasing a movie contemporaneously with other movies of the same genre adversely affects box-office performance all around. Releasing a movie against movies of the same Motion Picture Association of America (MPAA) rating hurts its sales in the beginning, but there is a displacement effect, which leads to a less severe sales loss in the long run.", "e:keyword": ["Entertainment marketing", "Motion picture distribution and exhibition", "Movie choice", "New product research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0174", "e:abstract": "Past research reveals an extraordinary number and variety of transaction games, often with different rules. For example, buy and sell offers can be take-it-or-leave-it, irrevocable, of limited duration, negotiable, contingent on events, et cetera. The possible sets of rules seem endless. Past (often, very insightful) research has focused on optimization, given particular rules of the game. This focus often overlooks why players choose to play the game. Indeed, assuming that an exchange will occur makes the marketing function (e.g., facilitating exchanges) inconsequential. Unlike inescapable market games between rival firms, buyers and sellers often choose whether to play transaction games. Hence, game design (i.e., setting the rules of the game) becomes vital, because the design determines both the likelihood of desirable outcomes (e.g., the best transaction price) and whether (or how many) players will choose to play. We need more research revealing the desirability of various rule sets for different target groups and revealing rules that enhance the benefits to all players. For example, a particular auction game might provide sellers with liquidity (i.e., faster transactions) while providing buyers with unique items at bargain prices. We should also explore the interaction of rules and player benefits (e.g., liquidity, anonymity, likelihood of a transaction, etc.).", "e:keyword": ["Transaction games", "Game theory", "Auctions", "Rules of the game", "Game design", "Player benefits"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0118", "e:abstract": "Many emerging technologies exhibit path-dependent demands driven by positive network feedback. Such network effects profoundly impact marketing strategists' thinking in today's network economy. However, the significant network externalities expected by many people often fail to materialize in the emerging technology market. We analyze this phenomenon in the context of a technology distribution channel. By studying cheap-talk strategies under information asymmetry, we show that incentive-compatible contracts are essential for achieving credible information transmission. In our model, the better-informed technology vendor has an incentive to inflate the retailer's ex ante belief of network externalities when a wholesale price contract is adopted. When properly termed revenue-sharing contracts are implemented, there are information-efficient cheap-talk equilibria where truthful information transmission is mutually beneficial. When the vendor's information is imperfect, even revenue-sharing contracts cannot guarantee credible information transmission if there is significant prior belief disparity between the vendor and the retailer. This study demonstrates how information-inefficient equilibria (e.g., information blockage) arise because of the conflict of interest or the conflict of opinion among channel members. It also explores the role of cheap talk in facilitating channel coordination.", "e:keyword": ["Behavioral game theory", "Channels of distribution", "Cheap-talk game", "Conflict of opinion", "Demand signaling", "Emerging technologies", "Network externalities", "Revenue sharing", "Strategic information transmission"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0142", "e:abstract": "This paper draws on the quality profitability emphasis framework of Rust, Moorman, and Dickson (2002) (Rust, Roland T., Christine Moorman, Peter R. Dickson. 2002. Getting returns from service quality: Revenue expansion, cost reduction, or both. (October) 724.) to examine the association between customer satisfaction and long-term financial performance among firms that a dual emphasis (focusing on both revenue-expansion and cost-reduction simultaneously, rather than solely emphasizing one over the other). Using a longitudinal data set of 77 firms from the United States, we test this hypothesis and find that the association between customer satisfaction and long-term financial performance is positive and relatively stronger for firms that successfully a dual emphasis. We build on the work of Rust, Moorman, and Dickson (2002), who investigated the financial impact of engaging in the process of achieving a dual emphasis. Collectively, these studies show that while a dual emphasis is desirable for long-run financial success, the a dual emphasis may not be as financially rewarding in the short run. Firms pursuing a dual emphasis need to consider both short- and long-term consequences of their strategy.", "e:keyword": ["Customer satisfaction", "Services marketing", "Marketing strategy and metrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0119", "e:abstract": "To increase the sales of their products through advertising, firms must integrate their brand-advertising strategy for capturing market share from competitors and their generic-advertising strategy for increasing primary demand for the category. This paper examines whether, when, and how much brand advertising versus generic advertising should be done. Using differential game theory, optimal advertising decisions are obtained for a dynamic duopoly with symmetric or asymmetric competitors. We show how advertising depends on the cost and effectiveness of each type of advertising for each firm, the allocation of market expansion benefits, and the profit margins determined endogenously from price competition. We find that generic advertising is proportionally more important in the short term and that there are free-riding effects leading to suboptimal industry expenditure on generic advertising that worsen as firms become more symmetric. Due to free-riding by the weaker firm, its instantaneous profit and market share can actually be higher. The effectiveness of generic advertising and the allocation of its benefits, however, have little effect on the long-run market shares, which are determined by brand-advertising effectiveness. Extensions of the model show that market potential saturation leads to a decline in generic advertising over time.", "e:keyword": ["Advertising", "Generic advertising", "Differential games", "Dynamic duopoly", "Optimal control"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0128", "e:abstract": "This paper investigates the competitive market for mass-customized products. Competition leads to surprising conclusions: Manufacturers customize only one of a product's two attributes, and each manufacturer chooses the same attribute. Customization of both attributes cannot persist in an equilibrium where firms first choose customization and then choose price, because effort to capture market with customization makes a rival desperate, putting downward pressure on prices. Equilibrium involves partial or no customization. In partial customization, rival firms do not differentiate their mass-customization programs: If firms customize different attributes, many more consumers are indifferent between the two firms. The elasticity of demand is increased and the resulting price war makes differentiated customization unprofitable. If firms customize the same attribute of a two-attribute product, they should concentrate on the attribute with the smaller heterogeneity in consumers' preferences. We incorporate consumers effort in portraying their preferences as a cost of interaction and provide public policy findings on the well-being of these consumers: When this cost is low, consumers are better off with customization than with standard goods, but firms choose too little customization. The loss in consumer surplus is sometimes captured by the firms, but for low interaction costs, firms' profit-driven behavior is economically inefficient.", "e:keyword": ["Customization", "Product differentiation", "Competition", "Game theory", "Personalization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0152", "e:abstract": "Marketing scholars have reflected upon the marketing discipline's internal evolution before. However, no prior study has assessed the globalization of authorship in our discipline, let alone assessed its consequences for the field. This paper addresses the following two questions: (1) Is there evidence of increasing globalization of authorship in the marketing discipline? (2) If so, does it help or hinder the field? Our work shows empirically how the globalization of our discipline evolved, how U.S. dominance is fading, and which countries experienced a rise in productivity of their affiliate and native scholars. Globalization hinders the field, because it has a negative effect on the impact of several major journals (most importantly, the and the ). Globalization helps the field, because it has a positive effect on the diversity of our discipline. Important implications of our research are: (1) Journals and sponsoring organizations should strive for more international meetings. (2) Editors, reviewers, and authors should pay more attention to the global relevance of the research they publish, review, and submit. (3) Individual researchers should aim to be part of the global community of marketing scientists through, for instance, international research visits.", "e:keyword": ["Globalization", "Journal impact", "Journal diversity", "Journal citations", "Philosophy of science", "Bibliometrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0123", "e:abstract": "Many marketing problems require accurately predicting the outcome of a process or the future state of a system. In this paper, we investigate the ability of the support vector machine to predict outcomes in emerging environments in marketing, such as automated modeling, mass-produced models, intelligent software agents, and data mining. The support vector machine (SVM) is a semiparametric technique with origins in the machine-learning literature of computer science. Its approach to prediction differs markedly from that of standard parametric models. We explore these differences and benchmark the SVM's prediction hit-rates against those from the multinomial logit model. Because there are few applications of the SVM in marketing, we develop a framework to position it against current modeling techniques and to assess its weaknesses as well as its strengths.", "e:keyword": ["Automated modeling", "Choice models", "Kernel transformations", "Multinomial logit model", "Predictive models", "Support vector machine"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0121", "e:abstract": "This research investigates the impact of a large-scale assortment reduction on customer retention, utilizing a model we develop to explore the effect on sales at both the store level and the category level simultaneously. We apply our model to a data set provided by an online grocer. The data contain detailed household purchase records for every category in the store. Our results indicate that the reduction in assortment overall store sales, a result that contrasts with that of all of the recent studies on assortment reductions (Food Marketing Institute. 1993. Variety or duplication: A process to know where you stand. Prepared by Willard Bishop Consulting and Information resources, Inc., in cooperation with Frito Lay; Drze, Xavier, Stephen J. Hoch, Mary E. Purk. 1994. Shelf management and space elasticity. (4) 301326; Broniarczyk, Susan M., Wayne D. Hoyer, Leigh McAlister. 1998. Consumers' perceptions of the assortment offered in a grocery category: The impact of item reduction. (May) 166176; Boatwright, Peter, Joseph C. Nunes. 2001. Reducing assortment: An attribute-based approach. (July) 5063; Boatwright, Peter, Joseph C. Nunes. 2004. Correction note for Reducing assortment: An attribute-based approach. . Forthcoming). We find the reduction had a negative effect on both shopping frequency and purchase quantity, and we find that the decline in shopping frequency resulted in a greater loss than did the reduction in purchase quantities. We also find that the impact of the assortment cut varies widely by category, with less-frequently purchased categories more adversely affected. The variation in the assortment reduction's impact across categories suggests that managers compare select categories in order to moderate the overall loss in sales.", "e:keyword": ["Customer retention", "Product assortment", "Efficient assortment", "Category management", "Variety", "Hierarchical Bayes", "COM-Poisson"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0145", "e:abstract": "As the online channel matures, many firms are finding that an understanding of how their online customers' preferences and choices vary across geographical markets can be very useful. In this paper, we propose a spatial multinomial model of customer choice and illustrate how spatial modeling of choices of online customers across geographical markets provides useful insights in the context of a product mix and pricing decision of an online book publisher. The spatial multinomial model specifically accounts for the spatial correlations among customer choices among different product formsprint and PDF. The estimation results obtained using data generated from an online experiment show that the spatial model accounts for the geographical variation in many of the unobserved effects possibly due to locational differences and price sensitivities. The resultant spatial decision maps provide useful predictions as to how purchase rates vary across geographical markets as a function of the price differential between product forms, with implications for targeting customers through local market advertising, direct marketing, and cross-channel promotion.", "e:keyword": ["Spatial model", "Mixed multinomial logit", "Pricing", "Digital products", "Internet"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0149", "e:abstract": "Understanding the lead-lag relationship between distribution and demand is an important and challenging issue for all marketers. It is particularly challenging in the movie industry, where the very short lifespan and decaying revenue and exhibition patterns of motion pictures means that the associated time series are short and nonstationary, rendering existing econometric methods unreliable. We propose an alternate method that uses state-space diagrams to determine lead-lag relationships. Straightforward to apply and interpret, it takes advantage of the eyes ability to see patterns that algebra-based formulations cannot easily recognize. A number of validation tests are provided to illustrate the usefulness and limitations of the method. We study the weekly data for 231 major movies released in 20002001. While econometric methods do not provide consistent results, the graphical method of visually inferred causality clearly shows a pattern that demand leads distribution for most movies. In other words, the dominant industry pattern is one of movie exhibitors monitoring box office sales and then responding with screen allocation decisions. The managerial implications of these findings are discussed.", "e:keyword": ["Distribution", "Marketing tools", "Movies", "State-space diagrams", "Time series"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0196", "e:abstract": "Despite some misconceptions, consumer rationality is a property of the researcher rather than the consumer. Consumers become more rational as we are better able to predict their behavior or other important outcomes influenced by their behavior. Perfect rationality results when we achieve accurate predictions. Consequently, at least for many articles, consumers are becoming more rational as we find better ways to predict. However, some experimental consumer behavior articles find the opposite. The difference between experimental and statistical controls explains the divergence in conclusions. Experimental controls test rationality based on whether previously absent variables exhibit significant explanatory power holding known explanatory variables constant. Statistical controls test rationality based on the incremental explanatory power of previously absent variables after accounting for known explanatory variables. Moreover, experimental tests tend to isolate consumer behavior predictions while statistical tests check for sufficient accuracy to choose among different firm strategies. Both perspectives are correct but ask very different questions.", "e:keyword": ["Bounded rationality", "Experiments", "Predicted choice", "Consumption", "Consumer behavior", "Econometrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0122", "e:abstract": "This paper examines optimal advertised quality, actual quality, and price for a firm entering a market. It develops a two-period model where advertised quality influences expectations, and hence trial and the gap between actual quality and expectations determines satisfaction, which in turn impacts second-period sales. In such situations a company makes a choice between advertising high quality and getting trial, but little repeat; and advertising low quality and getting low trial, but high repeat. Results are derived by numerical methods, as well as analytically for a special case of the model. The model suggests it is optimal to overstate quality when (i) customers rely relatively less on advertising to form quality expectations, and (ii) customers' intrinsic satisfaction with a product is high. These results are consistent with deceptive advertising cases at the FTC, which showed more deception for unknown firms and for firms whose customers were more satisfied. They are also consistent with the decisions made by future managers (MBAs), except that the respondents would advertise higher (versus lower) quality when advertising was effective.", "e:keyword": ["Customer satisfaction", "Customer expectations", "Deceptive advertising", "New products", "Advertised and actual quality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0140", "e:abstract": "This paper considers the decision problem of a firm that is uncertain about the demand, and hence profitability, of a new product. We develop a model of a decision maker who sequentially learns about the true product profitability from observed product sales. Based on the current information, the decision maker decides whether to scrap the product. Central to this decision problem are sequential information gathering, and the option value of scrapping the product at any point in time. The model predicts the optimal demand for information (e.g., in the form of test marketing), and it predicts how the launch or exit policy depends on the firm's demand uncertainty. Furthermore, it predicts what fraction of newly developed products should be launched on average, and what fraction of these products will fail, i.e., exit. The model is solved using numerical dynamic programming techniques. We present an application of the model to the case of the U.S. ready-to-eat breakfast cereal industry. Simulations show that the value of reducing uncertainty can be large, and that under higher uncertainty firms should strongly increase the fraction of all new product opportunities launched, even if their point estimate of profits is negative. Alternative, simpler decision rules are shown to lead to large profit losses compared to our method. Finally, we find that the high observed exit rate in the U.S. ready-to-eat cereal industry is optimal and to be expected based on our model.", "e:keyword": ["New product strategy", "Product launch", "Product exit", "Managerial decision making under uncertainty", "Bayesian learning", "Numerical dynamic programming", "Dynamic structural models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0150", "e:abstract": "Shipping-fee schedules are an important but underresearched element of the marketing mix for direct marketers. This paper provides an empirical study on the impact of shipping and handling charges on consumer-purchasing behavior. Using a database from an online retailer that has experimented with a wide variety of shipping-fee schedules, we investigate the impact of shipping charges on order incidence and order size. We use an ordered probability model that is generalized to account for the effects of nonlinear and discontinuous shipping fees on purchasing decisions, and to accommodate heterogeneity in response parameters. Results show that consumers are very sensitive to shipping charges and that shipping fees influence order incidence and basket size. Promotions such as free shipping and free shipping for orders that exceed some size threshold are found to be very effective in generating additional sales. However, the lost revenues from shipping and the lack of response by several segments are substantial enough to render such promotions unprofitable to the retailer. Heterogeneity across consumers also suggests interesting opportunities for the retailer to customize the shipping and other marketing-mix promotion offerings.", "e:keyword": ["Shipping fees", "Direct and Internet retailing", "Nonlinear pricing", "Promotions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0136", "e:abstract": "We develop, estimate, and test a response model of order timing and order volume decisions of catalog customers and derive a Bayes rule for optimal mailing strategies. The model integrates the and components of the response; incorporates the of the firm; and uses a Bayesian framework to determine the optimal mailing rule for each catalog customer. The we propose for optimal mailing strategy allows for a broad set of objectives to be realized across the time horizon, such as profit maximization, customer retention, and utility maximization with or without risk aversion. We find that optimizing the objective function over multiple periods as opposed to a single period leads to higher expected profits and expected utility. Our results indicate that the cataloguer is well advised to send fewer catalogs than its current practice in order to maximize expected profits and utility.", "e:keyword": ["Catalog mailing", "Database marketing", "Econometric models", "Hierarchical Bayes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0151", "e:abstract": "This paper investigates whether the tendency to buy store brand is category specific, or an enduring consumer trait. We develop a multicategory brand-choice model with a factor-analytic structure on the covariance matrix of the coefficients. The methodology allows us to elicit the basic latent tendency for a household to buy store brands, while controlling for other causes such as price sensitivity. The model is applied to a set of ten food and nonfood product categories. We find strong evidence of correlations in household preferences for store brands across categories. Using a two-dimensional factor structure, we find that one of the factors explains a substantial amount of variation in store-brand preference, while the other factor explains price sensitivity. The presence of these factors in all categories indicates that there are unobservable household-level traits that are non-category specific, i.e., stable across product categories. Using data from five holdout categories, we find that household estimates of these latent factors are very useful in predicting demand for store brands in new categories. Other potential applications for store managers are discussed.", "e:keyword": ["Store brands", "Multicategory choice models", "Heterogeneity", "Frequent-shopper data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0143", "e:abstract": "Minimizing strife through vertical integration is commonly seen as the holy grail for long-term success in product distribution. In this paper, we take a different slant, showing that sometimes a separated channel that embodies a degree of discord can be helpful, particularly when a long-term view is taken. This result is shown in the context of durable goods manufacturing. The quandary of durable goods production is that once demand for a certain time frame is met, there is a subsequent temptation to flood the market with additional goods. As such, consumers are reluctant to buy immediately, instead opting to wait for discounted prices. This problem can be alleviated by a degree of channel discord: High wholesale prices ensure future sales will slow to a trickle.", "e:keyword": ["Channel coordination", "Double marginalization", "Durable goods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0120", "e:abstract": "In this note, we explore channel interactions in an information-intensive environment where the retailer can implement personalized pricing and the manufacturer can leverage both personalized pricing and entry into a direct distribution channel. We study whether a retailer can benefit from personalized pricing and how upstream personalized pricing or entry into a direct distribution channel affects the allocation of channel profit. We find that the retailer is worse off because of its own or upstream personalized pricing, even when the retailer is a monopoly. However, it may still be optimal for the retailer to embrace personalized pricing in order to reap the strategic benefit of deterring the manufacturer from selling direct and targeting end consumers.", "e:keyword": ["Channel management", "Personalized pricing", "CRM", "Entry deterrence"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0202", "e:abstract": "The case method of teaching and the corresponding Socratic Method predate the discovery of the scientific method for advancing knowledge and problem solving. The case method applies known principles (e.g., laws) to specific situations while the scientific method focuses on discovering principles. Although the case method might be effective at teaching leadership and persuasion skills, it can lack the spirit of inquiry and the worship of the truth associated with the scientific method. Moreover, unlike legal cases, business cases lack precedent (i.e., stare decisis), the foundation of written law, and rigorous adjudication. More importantly, the traditional case method of teaching often ignores important research findings. Consequently, it helps destroy the link between academic research and classroom learning. Students lose the benefit of important research findings while leaving the classroom with false confidence about what they know. Researchers lose an incentive to do research relevant to their students. Eventually, there is less research worth teaching, and fewer students value the knowledge learned through painstaking research. Although we might covet the skill of persuasion, time might gradually elevate previously less persuasive managers who have better skills with analysis and collecting relevant information. Great teaching requires great content, in addition to active learning.", "e:keyword": ["Case method", "Teaching business", "Learning", "M.B.A. education", "Scholarly research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0169", "e:abstract": "When purchase and consumption decisions are separated in time and when future utility is state dependent, consumers may desire to pursue consumption flexibility by purchasing different products together (multiple buying). This paper analyzes the effects of consumption flexibility on competing firms' marketing mix decisions, in a model in which future preference uncertainty exists and consumers differ in their preferred product location on a horizontal attribute. The analysis shows that the nature of price competition in such markets is dependent upon whether consumer multiple buying (and thus primary demand) is endogenously induced. When preference uncertainty is important, the firms are involved in a flexibility trap in which primary demand is expanded but profits decrease with the spread of consumer heterogeneity. This counter-intuitive result is caused by the firms being induced to over-cut prices to increase primary demand when consumption flexibility is important. In response to this, the firms may configure their products to alleviate the adverse effect of consumer heterogeneity. For example, if preference uncertainty is important, the firms may choose to minimize differentiation on the horizontal attribute, or extend the current product line, to deal with the flexibility trap. The implications of allowing for positive salvage value, uncertainty heterogeneity, preference correlation, and state-dependent preference configuration are also investigated.", "e:keyword": ["Consumption flexibility", "Horizontal differentiation", "Market expansion", "Positioning", "Preference uncertainty", "Price competition", "Product line extension"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0138", "e:abstract": "Asymmetric pricing or asymmetric price adjustment is the phenomenon where prices rise more readily than they fall. We offer and provide empirical support for a new theory of asymmetric pricing in wholesale prices. Wholesale prices may adjust asymmetrically in the small but symmetrically in the large, when retailers face cost of price adjustment. Such retailers will not adjust prices for small changes in their costs. Manufacturers then see a region of inelastic demand where small wholesale price changes do not translate into commensurate retail price changes. The implication is asymmetrica small wholesale price increase is more profitable because manufacturers will not lose customers from higher retail prices; yet, a small decrease is less profitable, because it will not lower retail prices; hence, there is no extra revenue from greater sales. For larger changes, this asymmetry in the behavior of wholesale price vanishes as the price adjustment cost is compensated by the increase in retailers' revenue resulting from correspondingly large retail price changes. We present a formal economic model of a channel with forward-looking retailers and cost of price adjustment, test the derived propositions on the behavior of manufacturer prices using a large supermarket scanner data set, and find that the results are consistent with the predictions of our theory. We then discuss the implications for asymmetric pricing, channels, and cost of price adjustment literatures, as well as public policy.", "e:keyword": ["Asymmetric pricing", "Asymmetric price adjustment", "Channel pricing", "Cost of price adjustment", "Menu cost", "Wholesale price", "Channel of distribution", "Retailing", "Economic model", "Scanner data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0137", "e:abstract": "Chat rooms, recommendation sites, and customer review sections allow consumers to overcome geographic boundaries and to communicate based on mutual interests. However, marketers also have incentives to supply promotional chat or reviews in order to influence the consumers' evaluation of their products. Moreover, firms can disguise their promotion as consumer recommendations due to the anonymity afforded by online communities. We explore this new setting where advertising and word of mouth become perfect substitutes because they appear indistinguishable to the consumer. Specifically, we investigate here whether word of mouth remains credible and whether firms choose to devote more resources promoting their inferior or superior products. We develop a game theoretic model in which two products are differentiated in their value to the consumer. Unlike the firms, the consumers are uncertain about the products' quality. The consumers read messages online that help them decide on the identity of the superior product. We find a unique equilibrium where online word of mouth is persuasive despite the promotional chat activity by competing firms. In this equilibrium, firms spend more resources promoting inferior products, in striking contrast to existing advertising literature. In addition, we discuss consumer welfare implications and how other marketing strategies might interact with promotional chat.", "e:keyword": ["Advertising", "Word-of-mouth", "Source credibility", "Internet marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0126", "e:abstract": "Firms often differentiate their product lines vertically to capture consumers' differential willingness to pay for quality. Additionally, many firms offer products varying not in quality but in characteristics such as scent, color, or flavor, that relate to horizontal differentiation. For example, in the yogurt category, each manufacturer carries several product lines differing in quality and price, but within each line there is an assortment of flavors that is uniformly priced. To better understand these product-line pricing strategies, we address two key issues. First, how do consumers perceive product-line and flavor attributes? Second, given consumers' preferences, is the current strategy of pricing product lines differently, but offering all flavors within a product line at the same price, optimal? We find that consumers value line attributes more than flavor attributes. Our analysis reveals that firms exploit these differences in consumer preferences by using product lines as a price discrimination tool. However, firms' profits would not significantly increase if they were to price flavors within a product line differently. Therefore, the current pricing policy of setting different prices for product lines, but uniform prices for all flavors within a line, appears to be on target.", "e:keyword": ["Product-line pricing", "Competitive strategy", "Product assortment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0168", "e:abstract": "The e-marketplace has emerged as an important electronic shopping environment that, according to a recent report, may evolve into a dominant force in Internet marketing. We investigate an e-marketplace with online stores offering competing products. We find that featuring is associated with a price premium and serves as a mechanism to mitigate competition among online stores. In essence, featuring facilitates a convenience-price trade-off. Those consumers who find that the cost of searching outweighs the benefit of a lower expected price shop at the featured store where the price is higher on average. We endogenize consumers' decisions on shopping at the featured store or use the search engine to shop at the low-price store and examine the robustness of our results in an -store setting.", "e:keyword": ["Nonprice advertising", "Competitive strategy", "e-commerce", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0141", "e:abstract": "How can supermarkets use the vast data they have to design strategies to compete for large-basket shoppers, potentially their most profitable customers? We say, analyze the data to glean basket composition of heterogeneous consumers. Of theoretical and practical interest is the question, will our suggestion improve supermarket profits, and if so, by what pricing strategies? By answering this we can also answer the question, do results of extant research on single-product marketing that using such information intensifies competition and lowers profits, generalize to multiproduct marketing by supermarkets that carry several products, and whose patrons purchase multiple items? We derive equilibrium data-based intelligent pricing strategies that also produce increased supermarket profits. What is more interesting is our result that data analysis is profitable whether or not a competitor undertakes data analysis. If not too costly, implementing data analysis is a dominant strategy. Moreover, with comprehensive data analysis connecting basket information and household data, stores can increase profits further by targeting rewards at individual households, thereby segmenting the market more completely. In contrast to past research findings, we show that for supermarkets, even under competition, use of information on past purchases enables intelligent pricing, better segmentation, and higher profits.", "e:keyword": ["Basket size", "Competition", "Consumer data-analytics", "Game theory", "Supermarkets", "Price discrimination", "Intelligent pricing", "Retailing", "Reward cards", "Reward programs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0215", "e:abstract": "One research function is proposing new scientific theories; another is testing the falsifiable predictions of those theories. Eventually, sufficient observations reveal valid predictions. For the impatient, behold statistical methods, which attribute inconsistent predictions to either faulty data (e.g., measurement error) or faulty theories. Testing theories, however, differs from estimating unknown parameters in known relationships. When testing theories, it is sufficiently dangerous to cure inconsistencies by adding observed explanatory variables (i.e., beyond the theory), let alone unobserved explanatory variables. Adding ad hoc explanatory variables mimics experimental controls when experiments are impractical. Assuming unobservable variables is different, partly because realizations of unobserved variables are unavailable for validating estimates. When different statistical assumptions about error produce dramatically different conclusions, we should doubt the theory, the data, or both. Theory tests should be insensitive to assumptions about error, particularly adjustments for error from unobserved variables. These adjustments can fallaciously inflate support for wrong theories, partly by implicitly under-weighting observations inconsistent with the theory. Inconsistent estimates often convey an important messagethe data are inconsistent with the theory! Although adjustments for unobserved variables and ex post information are extraordinarily useful when estimating known relationships, when testing theories, requiring researchers to make these adjustments is inappropriate.", "e:keyword": ["Unobserved heterogeneity", "Scientific method", "Falsification", "Statistical validation", "Errors in the variables", "Popper falsification"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0178", "e:abstract": "The abundance of highly disaggregate data (e.g., at five-second intervals) raises the question of the optimal data interval to estimate advertising carryover. The literature assumes that (1) the optimal data interval is the interpurchase time, (2) too disaggregate data causes a disaggregation bias, and (3) recovery of true parameters requires assumption of the underlying advertising process. In contrast, we show that (1) the optimal data interval is what we call , (2) too disaggregate data does not cause any disaggregation bias, and (3) recovery of true parameters does not require assumption of the advertising process but only data at the unit exposure time. These results hold for any linear dynamic model linking sales with current and past advertising.", "e:keyword": ["Advertising carryover", "Duration of ad effects", "Optimal data interval", "Interpurchase time", "Interexposure time"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0175", "e:abstract": "We examine the relationship between objective and perceived quality for 241 products in 46 product categories over a period of 12 years. On average, we find that the effect of a change in objective quality is not fully reflected in customer perceptions of quality until after about six years. In the first year after a quality change, only about 20% of the total effect over time is realized. These effects are significantly larger and quicker for a decrease in quality relative to an equivalent increase. Interestingly, we also find that brand reputation has a double advantage. High-reputation brands are rewarded three years quicker for an increase in quality and punished one year slower for a decrease in quality compared to low-reputation brands. These differences in response time are a meaningful measure of brand equity. Finally, we examine the differences in quality effects across several product- and category-specific variables and discuss the implications of our findings.", "e:keyword": ["Quality", "Quality perception", "Lagged effects", "Carryover duration", "Brand management", "Reputation effects", "Consumer learning", "Product management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0193", "e:abstract": "In a model of vertical differentiation, the principal concern of this paper is to identify sufficient conditions for producing a higher- or lower-quality good to be more profitable (in terms of profits and profit margin). Our basic model considers a scenario where the firms' quality levels are fixed and they engage in price competition. Here, our results are three. First, we develop the notion of and show that its (increasing or decreasing) monotonicity in product quality implies that of firm profitability in equilibrium. A firm's relative cost efficiency refers to its quality-adjusted cost (dis)advantage relative to its immediate competitors, for a given distribution of consumer tastes. Second, selling a higher-quality good is more profitable when (defined as the ratio between a firm's quality and unit cost) is increasing in quality. Third, we also establish a set of lower and upper bounds on each firm's profitability. The basic model is then extended in two directions. We examine unit cost functions that demonstrate monotone profitability, even when both quality and price are endogenous variables. We also show that the spirit of relative cost efficiency and its associated sufficient condition hold valid for a logconcave consumer distribution.", "e:keyword": ["Vertical differentiation", "Profitability", "Bertrand competition", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0158", "e:abstract": "We extend the Schmittlein et al. model (1987) of customer lifetime value to include satisfaction. Customer purchases are modeled as Poisson events, and their rates of occurrence depend on the satisfaction of the most recent purchase encounter. Customers purchase at a higher rate when they are satisfied than when they are dissatisfied. A closed-form formula is derived for predicting total expected dollar spending from a customer base over a time period (0, ]. This formula reveals that approximating the mixture arrival processes by a single aggregate Poisson process can systematically underestimate the total number of purchases and revenue. Interestingly, the total revenue is increasing and convex in satisfaction. If the cost is sufficiently convex, our model reveals that the aggregate model leads to an overinvestment in customer satisfaction. The model is further extended to include three other benefits of customer satisfaction: (1) satisfied customers are likely to spend more per trip on average than dissatisfied customers, (2) satisfied customers are less likely to leave the customer base than dissatisfied customers, and (3) previously satisfied customers can be more (or less) likely to be satisfied in the current visit than previously dissatisfied customers. We show that all the main results carry through to these general settings.", "e:keyword": ["Customer satisfaction", "Customer value analysis", "Hidden Markov model", "Nonstationarity", "Stochastic processes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0170", "e:abstract": "The marketing literature has suggested two prominent decision mechanisms through which in-store display and feature advertising can affect brand choice, which I call and . The primary objective of this study is to propose an integrated brand choice model that incorporates these two possible behavioral mechanisms, which have been shown to work by previous research. The model allows consumers to use various combinations of the decision mechanisms with different probabilities and thus enables one to assess the extent to which each effect might occur in actual purchase data and to investigate how consumers might differ in their tendencies to engage in these decision processes. By incorporating these likely behavioral mechanisms, the proposed model alleviates the problems caused by multicollinearity and produces sensible parameter estimates for the joint effects of promotion vehicles (in-store display, feature ad, and price discount), which contributes to better managerial decision making.", "e:keyword": ["Brand choice model", "Behavioral mechanisms", "Joint promotion effects", "Consideration set formation", "Price-cut proxy", "Econometric models", "Promotion decisions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0182", "e:abstract": "This study investigates how people's satisfaction judgments are modified after they interact with other group members. It integrates research on customer satisfaction and social influence to develop hypotheses about how an individual's satisfaction is influenced by discrepancies between her expectations about the satisfaction of other group members and their actual opinions as revealed in group discussion. It also considers how this effect is moderated by the individual's susceptibility to social influence and perceptions of group cohesiveness. Two empirical studies demonstrate significant social influence effects on satisfaction judgments in groups. Study One analyzes group satisfaction data collected over time using a mixed-effects regression. It shows that an individual's perceived discrepancy between others' satisfaction judgments and expected group satisfaction has an important influence on her postdiscussion satisfaction judgments. Moreover, individuals discount the prediscussion satisfaction judgments of other group members in favor of perceived satisfaction and its discrepancy with expectations. Group cohesiveness accentuates the perceived discrepancy with expected group satisfaction. Study Two analyzes survey data from dyads drawn from a cross-sectional sample of organizational buyers who purchase from the same supplier. It models the decision maker's satisfaction with a service supplier as a function of end-user satisfaction. It shows that social influence effects exist in purchasing groups within organizations. Both studies demonstrate that individual-level postdiscussion satisfaction judgments tend to become more extreme, a phenomenon we call .", "e:keyword": ["Satisfaction", "Social influence", "Organizational buying decisions", "Group cohesiveness", "Escalation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0192", "e:abstract": "This paper extends the scanner-based choice literature by explicitly incorporating individual-level brand-preference data. We illustrate our model using a unique data set that combines survey and scanner data collected from the same individuals. The addition of individual-specific brand-preference information significantly improves fit and prediction. Furthermore, this observed heterogeneity better explains choice than does unobserved heterogeneity in the standard scanner model's parameters. More importantly, we find that the standard model underestimates the importance of consumers' brand preferences and overestimates both brand loyalties and price sensitivities. Brand loyalty is overestimated because models without preference information confound state dependence, heterogeneity, and preference effects. Price sensitivities are inflated because the average preference-based consumer is implicitly assumed to be more willing to switch from his preferred brand than is the real preference-based consumer. Further, standard models overestimate the heterogeneity in price and loyalty sensitivities and misidentify both price- and loyalty-sensitive consumers. The managerial implications of our findings and the applicability of our methodology when survey data are collected infrequently and for only a subsample of consumers are pursued. We demonstrate that even under these circumstances better populationwide pricing and promotion decisions are identified and more accurate targeting results.", "e:keyword": ["Discrete choice", "Heterogeneity", "Scanner data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0195", "e:abstract": "When making product choices, consumers are influenced by the preferences of other consumers, such as family members, friends, neighbors, and colleagues. Preference interdependence among family members is likely to be significant because of cohabitation and strong emotional ties. To estimate the preference interdependence, we specify a simultaneous equation model and propose a Bayesian estimation approach. Unlike existing models that use a spatial autoregressive structure to capture the interdependence of consumer preferences, we are able to estimate the potential asymmetry in the preference interdependence among family members in a more flexible way. In a simulation study, we show that models that ignore interdependence of preferences yield biased estimates of consumers' sensitivity to observed attribute preferences. In an empirical application, we estimate the interdependence of the viewership of television programs between husbands and wives in 481 households. We find that wives' viewing behavior depends more strongly on their husbands' viewing behavior than husbands' viewing behavior depends on their wives' viewing behavior. There exist significant differences in parameter estimates of dependence across categories of television programs. Differences in levels of spousal interdependence across households are partially explained by the age and the education level of the spouses.", "e:keyword": ["Hierarchical Bayesian analysis", "Interdependent preferences", "Simultaneous equation models", "Television programs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0148", "e:abstract": "For many consumers who use loans to acquire an expensive durable such as a car, the market value of their current (used) durable is less than the outstanding loan amount. If these consumers want to replace their used durable with a new one, they might not be able to do so because of the added burden of paying off the amount of negative equity. We develop and solve a model in which a durable goods manufacturer gives consumers a cash rebate, so that they can get out of the negative equity problem. We find that under certain conditions, the manufacturer offering a lower durability product is more likely to give cash rebates and, when such rebates are given, to offer a greater cash rebate. These rebates also lead to higher prices net of the rebate compared with the situation without any such rebates. We empirically test some of our equilibrium results and find support for our model predictions.", "e:keyword": ["Consumer rebates", "Promotions", "Durable goods", "Automobiles"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0198", "e:abstract": "In contrast to other marketing activities, advertising is usually outsourcedhistorically, to an integrated full-service advertising agency that provides both creative and media services. Recently, major changes have occurred, specialized media shops have appeared, and large holding companies with several agencies and a separate media shop have emerged. This paper describes how firms in the current industry structure choose their agents. A theoretical section specifies the firm's decision process in obtaining the creative and media components. Conditions under which one or both could be effectively produced internally are identified. In the prevalent case when these components are outsourced, we evaluate whether they can be more efficiently produced by a bundled full-service agency or by unbundling to separate agents. An empirical section tests the theory implications with cross-sectional data on U.S. firm choices of advertising agents. The bundled full-service agency is still widely used. Firms with large advertising budgets unbundle to take advantage of media discounts obtained by market-making media shops. Internal agencies are used when firms have internal creative abilities, or have low-level requirements in both creative and media. The creative and media agencies within holding companies are found to act independently.", "e:keyword": ["Advertising agencies", "Media shops", "Outsourcing", "Unbundling", "Double marginalization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0181", "e:abstract": "A brand choice model with heterogeneous price-threshold parameters is used to investigate a three-regime piecewise-linear stochastic utility function. The model is used to explore the relationships between aspects of consumer price sensitivity and price thresholds using hierarchical Bayes modeling with the Markov chain Monte Carlo (MCMC) method. This study contributes to the modeling literature on discontinuous likelihoods in choice models. The empirical application using our scanner panel data set shows that the reference effect and loss aversion are more marked after price thresholds are taken into heterogeneous price response models. Furthermore, loss aversion is attenuated by using price thresholds than by an aggregate (homogeneity) model without price thresholds.", "e:keyword": ["Discontinuous likelihoods", "Reference effect", "Price threshold", "Latitude of price acceptance", "Brand choice", "Bayesian MCMC", "Heterogeneity", "Scanner panel data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.22.1.147.12845", "e:abstract": "The Editor in Chief and Area Editors of are indebted to their guest area editors and the ad hoc reviewers, who provide expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest area editors and ad hoc reviewers who served from January to December 2005. We gratefully appreciate their support.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0236", "e:abstract": "Despite the invaluable contribution of marketing folks (e.g., making markets work), they fail to enjoy the same freedom of speech as others. This fact is particularly egregious because unlike other groups that can use threats, force, or coercion, marketing folks rely only on speech. Although the U.S. Constitution never mentions commercial speech, the courts invented the concept to censor marketing folks. The cloudy rational was that consumers need special protection from marketing folks (e.g., advertising). Naturally, censorship leads to abuse. Powerful incumbents use censorship covertly against new entrants. Politicians use censorship surreptitiously to promote their own political goals. If consumers need protection, it is certainly from the misleading statements of those with freedom of speechpoliticians, attorneys, the news media, and the censors.", "e:keyword": ["Freedom of speech", "Commercial speech", "Censorship", "Advertising", "Marketing", "Regulation", "Branding"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0166", "e:abstract": "Idea generation (ideation) is critical to the design and marketing of new products, to marketing strategy, and to the creation of effective advertising copy. However, there has been relatively little formal research on the underlying incentives with which to encourage participants to focus their energies on relevant and novel ideas. Several problems have been identified with traditional ideation methods. For example, participants often free ride on other participants' efforts because rewards are typically based on the group-level output of ideation sessions. This paper examines whether carefully tailored ideation incentives can improve creative output. I begin by studying the influence of incentives on idea generation using a formal model of the ideation process. This model illustrates the effect of rewarding participants for their impact on the group and identifies a parameter that mediates this effect. I then develop a practical, web-based asynchronous ideation game, which allows the implementation and test of various incentive schemes. Using this system, I run two experiments that demonstrate that incentives do have the capability to improve idea generation, confirm the predictions from the theoretical analysis, and provide additional insight on the mechanisms of ideation.", "e:keyword": ["Idea generation", "New product research", "Product development", "Marketing research", "Agency theory", "Experimental economics", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0180", "e:abstract": "Managers commonly use customer feedback data to set goals and monitor performance on metrics such as Top 2 Box customer satisfaction scores and intention-to-repurchase loyalty scores. However, analysts have advocated a number of different customer feedback metrics including average customer satisfaction scores and the number of net promoters among a firm's customers. We empirically examine which commonly used and widely advocated customer feedback metrics are most valuable in predicting future business performance. Using American Customer Satisfaction Index data, we assess the linkages between six different satisfaction and loyalty metrics and COMPUSTAT and CRSP data-based measures of different dimensions of firms' business performance over the period 19942000. Our results indicate that average satisfaction scores have the greatest value in predicting future business performance and that Top 2 Box satisfaction scores also have good predictive value. We also find that while repurchase likelihood and proportion of customers complaining have some predictive value depending on the specific dimension of business performance, metrics based on recommendation intentions (net promoters) and behavior (average number of recommendations) have little or no predictive value. Our results clearly indicate that recent prescriptions to focus customer feedback systems and metrics solely on customers' recommendation intentions and behaviors are misguided.", "e:keyword": ["Customer satisfaction", "Marketing metrics", "Marketing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0188", "e:abstract": "We develop a demand model for technology products that captures the effect of changes in the portfolio of models offered by a brand as well as the influence of the dynamics in its intrinsic preference on that brand's performance. To account for the potential correlation in the preferences of models offered by a particular brand, we use a nested logit model with the brand (e.g., Sony) at the upper level and its various models (e.g., Mavica, FD, DSC, etc.) at the lower level of the nest. Relative model preferences are captured via their attributes and prices. We allow for heterogeneity across consumers in their preferences for these attributes and in their price sensitivities in addition to heterogeneity in consumers' intrinsic brand preferences. Together with the nested logit assumption, this allows for a flexible substitution pattern across models at the aggregate level. The attractiveness of a brand's product line changes over time with entry and exit of new models and with changes in attribute and price levels. To allow for time-varying intrinsic brand preferences, we use a state-space model based on the Kalman filter, which captures the influence of marketing actions such as brand-level advertising on the dynamics of intrinsic brand preferences. Hence, the proposed model accounts for the effects of brand preferences, model attributes and marketing mix variables on consumer choice. First, we carry out a simulation study to ensure that our estimation procedure is able to recover the true parameters generating the data. Then, we estimate our model parameters on data for the U.S. digital camera market. Overall, we find that the effect of dynamics in the intrinsic brand preference is greater than the corresponding effect of the dynamics in the brand's product line attractiveness. Assuming plausible profit margins, we evaluate the effect of increasing the advertising expenditures for the largest and the smallest brands in this category and find that these brands can increase their profitability by increasing their advertising expenditures. We also analyze the impact of modifying a camera model's attributes on its profits. Such an analysis could potentially be used to evaluate if product development efforts would be profitable.", "e:keyword": ["Econometric models", "Hi-tech marketing", "Advertising", "Product line attractiveness", "Product development", "Nested logit models", "Kalman filter"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0176", "e:abstract": "This paper provides an empirical study of entry by a Wal-Mart supercenter into a local market. Using a unique frequent-shopper database that records transactions for over 10,000 customers, we study the impact of Wal-Mart's entry on consumer purchase behavior. We develop a joint model of interpurchase time and basket size to study the impact of competitor entry on two key household decisions: store visits and in-store expenditures. The model also allows for consumer heterogeneity due to observed and unobserved factors. Results show that the incumbent supermarket lost 17% volumeamounting to a quarter million dollars in monthly revenuefollowing Wal-Mart's entry. Decomposing the lost sales into components attributed to store visits and in-store expenditures, we find that the majority of these losses were due to fewer store visits with a much smaller impact attributed to basket size. We also find that Wal-Mart lures some of the incumbent's best customers, and that retention of a small number of households can significantly reduce losses at the focal store. Finally, certain observed household characteristics such as distance to store, shopping behavior, and product purchase behavior are found to be useful in profiling the defectors to Wal-Mart. Implications and strategies for supermarket managers to compete with Wal-Mart are discussed.", "e:keyword": ["Entry", "Retail competition", "Wal-Mart supercenter", "Frequent-shopper data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0209", "e:abstract": "Firms invest heavily in different types of business-to-business relationship marketing activities in the belief that such programs bolster their bottom line. In this study, we develop and test a conceptual model that links customer-specific relationship marketing investments to short-term, customer-specific financial outcomes. Data from a matched set of 313 business customers covered by 143 salespeople of 34 selling firms indicate that investments in social relationship marketing pay off handsomely, financial relationship marketing investments do not, and structural relationship marketing investments are economically viable for customers serviced frequently. We conceptualize relationship marketing in a context involving nested participants (customers, salespeople, selling firms) and employ a hierarchical linear modeling approach to account for observations that are not independent. Across the three hierarchical levels, the impact of the financial, social, and structural components of relationship marketing investments and the potential moderating factors offer valuable insights into contextual factors and managerial strategies for leveraging relationship marketing investments. In an attempt to suggest normative guidelines to managers, we extend our analysis to a simple resource allocation model that describes the optimal mix of relationship marketing resources based on firm strategies.", "e:keyword": ["Relationship marketing", "Customer relationship management", "Marketing strategy", "Financial outcomes", "Allocation", "Hierarchical linear modeling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0211", "e:abstract": "Consumer choice in surveys and in the marketplace reflects a complex process of screening and evaluating choice alternatives. Behavioral and economic models of choice processes are difficult to estimate when using stated and revealed preferences because the underlying process is latent. This paper introduces Bayesian methods for estimating two behavioral models that eliminate alternatives using specific attribute levels. The elimination by aspects theory postulates a sequential elimination of alternatives by attribute levels until a single one, the chosen alternative, remains. In the economic screening rule model, respondents screen out alternatives with certain attribute levels and then choose from the remaining alternatives, using a compensatory function of all the attributes. The economic screening rule model gives an economic justification as to why certain attributes are used to screen alternatives. A commercial conjoint study is used to illustrate the methods and assess their performance. In this data set, the economic screening rule model outperforms the EBA and other standard choice models and provides comparable results to an equivalent conjunctive screening rule model.", "e:keyword": ["Elimination by aspects", "Consideration sets", "Attribute screening", "Noncompensatory decision processes", "Conjoint analysis", "Hierarchical Bayes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0219", "e:abstract": "The rapid rate of knowledge obsolescence in many high-technology markets makes it imperative for firms to renew their technological bases constantly. Given its critical importance, excellence in renewal of technological base would serve as a . Drawing on past literature, we identify this dynamic capability associated with acquiring and utilizing external technological know-how with the notion of (AC). We ask the following questions: (a) What would cause some firms to have a higher AC than others? and, (b) What is the impact of AC on a firm's profitability? We build a conceptual framework suggesting that marketing, R&D, and operations capabilities have a significant positive impact on a firm's AC. We test our framework on a data set of firms in high-technology markets. Using an econometric technique called stochastic frontier estimation, we the AC of firms from an observation of the know-how they actually absorb. We find that firm-specific capabilities significantly impact AC. Also, we find that AC has a significant impact on profitability and that this impact is moderated by the pace of technological change: the greater the pace of change, the greater the impact.", "e:keyword": ["Absorptive capacity", "Dynamic capabilities", "High-technology markets", "Resource-based view", "Marketing capability", "Organizational learning", "Knowledge acquisition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0199", "e:abstract": "In this study, we examine firms' incentive to offer customized products in addition to their standard products in a competitive environment. We offer several key insights. First, we delineate market conditions in which firms will (will not) offer customized products in addition to their standard products. Surprisingly, we find that when firms offer customized products they are able to not only expand demand, but can also the prices of their standard products relative to when they do not. Second, we find that when a firm offers customized products it is a dominant strategy for it to also offer its standard product. This result highlights the role of standard products and the importance of retaining them when firms offer customized products. Third, we identify market conditions under which ex ante symmetric firms will adopt symmetric or asymmetric customization strategies. Fourth, we highlight how the degree of customization offered in equilibrium is affected by market parameters. We find that the degree of customization is lower when both firms offer customized products relative to the case when only one firm offers customized products. Finally, we show that customizing products under competition does not lead to a prisoner's dilemma.", "e:keyword": ["Degree of product customization", "Mass customization", "Standard products", "Competition", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0183", "e:abstract": "This paper shows that the analysis of Liu et al. (2004) contains a substantive errorthe asserted pure-strategy Nash equilibrium leading to their Theorems 1 and 2 is really not an equilibrium. We show that in their model, either pure-strategy Nash equilibria do not exist or, unlike their asserted main result, when a pure-strategy equilibrium exists, increasing the number of commercial television broadcasters does not result in lower-quality programs. Possible modifications of Liu et al.'s model that may help restore the desired result are discussed.", "e:keyword": ["Imperfect competition", "Game theory", "Market structure", "Media"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0233", "e:abstract": "Liu et al. [Liu, Y., D. S. Putler, C. B. Weinberg. 2004. Is having more channels really better? A model of competition among commercial television broadcasters. (1) 120133] examine the television broadcast industry using a model in which profit-maximizing broadcasters seek to gain viewers by choosing the type of program to offer and by spending money to set program quality, allowing broadcasters to sell access to those viewers (through inserted advertisements) at a fixed rate per viewer. Wu and Chou [Wu, C., S. Chou. 2006. Commentary on Is having more channels really better? A model of competition among commercial television broadcasters. (5) 541545] argue that the duopoly result for a certain range of the cost parameter in Liu et al. is not a pure strategy Nash equilibrium. They further propose some modifications to the original model to restore Liu et al.'s results. In this reply, we demonstrate how a single strategy, not included in the strategy space of the Liu et al. duopoly model leads to the difference between our analysis and that of Wu and Chou. While we had intended to rule out this strategy, the text was not entirely clear on this issue; Wu and Chou's comment provides an opportunity to clarify the situation. We provide both empirical and theoretical support for excluding this strategy, which allows us to focus on the more plausible competitive situations in television broadcasting. We also reply to Wu and Chou's other comments on several issues, such as the relative importance of program type versus quality.", "e:keyword": ["Competition", "Competitive strategy", "Entertainment marketing", "Game theory", "Market structure", "Media", "Product policy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0251", "e:abstract": "We have observed conspicuous changes in the 25 years after Frank M. Bass, John D. C. Little, and Donald G. Morrison begot . benefited from five subsequent editors and fifty different area editors. New submissions grew from 40 to over 320. Published articles grew from 16 to over 45 per year. We discuss six possible developments for the next 25 years. (1) The Internet's extraordinary search capabilities will diminish the distinctiveness of each journal. (2) The Internet will allow any researcher to publish research without journals. (3) Faster dissemination is inevitable. (4) Many economical electronic journals will enter the market. (5) The business model of print journals must change. (6) We will publish new forms of content (e.g., videos, blogs, running reviews, and subsequent comments by the authors).", "e:keyword": ["Electronic journals", "Print journals", "Citations", "Impact of the Internet"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0258", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0237", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0234", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0139", "e:abstract": "Given the growth of the service sector, and advances in information technology and communications that facilitate the management of relationships with customers, models of service and relationships are a fast-growing area of marketing science. This article summarizes existing work in this area and identifies promising topics for future research. Models of service and relationships can help managers manage service more efficiently, customize service more effectively, manage customer satisfaction and relationships, and model the financial impact of those customer relationships. Models for managing service have often emphasized analytical approaches to pricing, but emerging issues such as the trade-off between privacy and customization are attracting increasing attention. The trade-offs between productivity and customization have also been addressed by both analytical and empirical models, but future research in the area of service customization will likely place increased emphasis on e-service and truly personalized interactions. Relationship models will focus less on models of customer expectations and length of relationship, and more on modeling the effects of dynamic marketing interventions with individual customers. The nature of service relationships increasingly leads to financial impact being assessed within customer and across product, rather than the traditional reverse, suggesting the increasing importance of analyzing customer lifetime value (CLV) and managing the firm's customer equity.", "e:keyword": ["Services marketing", "Relationship marketing", "Customer satisfaction", "Service quality", "Service productivity", "Customization", "Service design", "e-service", "Service demand", "Pricing of services", "Service guarantees", "Complaint management", "Customer retention", "Customer relationship management", "Word of mouth", "Customer lifetime value", "Customer equity", "Return on quality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0156", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0171", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0172", "e:abstract": "No abstract available.", "e:keyword": ["Service", "Service-profit chain", "Marketing discipline", "Marketing theory", "Audience for marketing science"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0173", "e:abstract": "No abstract available.", "e:keyword": ["Service-recovery strategies", "Customer-managed interactions", "Self-service technologies"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0165", "e:abstract": "No abstract available.", "e:keyword": ["Customer relationship management", "Service channel", "CRM programs", "Pricing of services", "Development of customer demand", "Cross-selling", "Communication campaign", "Adaptive learning", "Dynamic marketing interventions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0189", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0154", "e:abstract": "No abstract available.", "e:keyword": ["Service models", "Service failure", "Customer satisfaction", "Switching costs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0161", "e:abstract": "The recent marketing literature reflects a growing interest in structural models, stemming from (1) the desire to test a variety of behavioral theories with market data, and (2) recent developments that facilitate estimation of and inference for these models. Whether one should always go through the effort of developing such tightly parameterized models with the associated computational burden of estimating them and whether it pays off to make strict behavioral assumptions in terms of better decisions remain open questions. To shed some light on these issues, we provide examples of structural approaches to consumer choice and demand as well as examples where the goal is to study the nature of competition in the marketplace. From that review comes our discussion of issues in the development and application of structural models, including their estimation, testing, and validation, their applicability in the practice of marketing, and their usefulness for normative as well as descriptive purposes.", "e:keyword": ["Structural models", "Heterogeneity", "Competition", "Endogeneity", "Dynamic demand models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0235", "e:abstract": "In their article, Structural Modeling in Marketing: Review and Assessment, Chintagunta, Erdem, Rossi, and Wedel (2006) provide a comprehensive survey of the contributions to the empirical marketing literature made by researchers using structural econometric modeling. More importantly, their review poses the question of whether structural methods should become more prominent in marketing research. Addressing that question requires a careful consideration of the potential gains of employing structure in this context, as well as the compromises necessary for implementation. Instead of specifically referencing many of the interesting papers cited by the authors, I will focus my comment on evaluating the value of structural approaches in marketing in more general terms.", "e:keyword": ["Structural modeling", "Empirical industrial organization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0184", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0186", "e:abstract": "No abstract available.", "e:keyword": ["Structural models", "Marketing models", "Behavioral theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0190", "e:abstract": "Structural models integrate behavioral and psychological decision theory into economics models and are more aligned with the true underlying economic primitives of the consumers. This allows researchers to investigate more behavior-driven and process-oriented customer decision processes such as learning of product attributes, formation of a consideration sets, stockpiling, and flexible consumption that cannot be easily handled by traditional marketing models. Chintagunta et al. (2006) gives an excellent review on the development and applications of structural models in marketing for modeling both consumer demand and firm competition. Recent interests in consumer research have diversified from frequently purchased packaged goods to services, high-tech, Internet, and information industries. Consumers are observed to be more sophisticated, long-term oriented, risk averse, and rational when making purchase and consumption decisions in these product categories. Structural models are better choices to capture the nature of the sophisticated decision process under the new marketing environment and engineering. Given the thorough review of Chintagunta et al. (2006), I will focus only on the dynamic structural demand models that have the components of information processing, rational expectation, and/or endogenous decisions to trade off current and future utilities, and I discuss some current marketing issues that can be most appropriately addressed by these models.", "e:keyword": ["Structural model", "Information processing", "Rational expectations", "Optimal decision", "Hyperbolic discounting", "Information asymmetry", "Signaling", "Searching", "Learning", "e-commerce", "Dynamic and interactive marketing intervention", "Decision support system"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0194", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0232", "e:abstract": "Chintagunta, Erdem, Rossi, and Wedel (2006) (CERW) discuss many different issues related to the use of structural models in marketing. They use examples of structural models that involve both consumer demand and supply-side competition to provide a critical assessment of the strengths and weaknesses of structural modeling and its future in marketing. While they have done a very nice job, the purpose of this commentary is to provide additional discussion of the three issues raised in their paper.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0201", "e:abstract": "There has been rapidly growing interest in structural models, and the review paper by Chintagunta et al. (2006) is a timely contribution. The paper identifies the key issues and provides an excellent assessment. A contemporaneous paper by Erdem et al. (2005) also offers a critical examination on some of the issues. My comments are in three areas. First, I selectively revisit some of the key issues in the paper and, in doing so, I hope I shed additional insight on these issues. Second, I discuss some of the recent papers that build on established psychological consumer behaviors. Third, I offer a brief list of potentially significant substantive problems for which structural models will be insightful.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0177", "e:abstract": "The motion picture industry has provided a fruitful research domain for scholars in marketing and other disciplines. The industry has high economic importance and is appealing to researchers because it offers both rich data that cover the entire product lifecycle for many new products and because it provides many unsolved puzzles. Although the amount of scholarly research in this area is rapidly growing, its impact on practice has not been as significant as in other industries (e.g., consumer packaged goods). In this article, we discuss critical practical issues for the motion picture industry, review existing knowledge on those issues, and outline promising research directions. Our review is organized around the three key stages in the value chain for theatrical motion pictures: production, distribution, and exhibition. Focusing on what we believe are critical managerial issues, we propose various conjecturesframed either as research challenges or specific research hypothesesrelated to each stage in the value chain and often involved in understanding consumer movie-going behavior.", "e:keyword": ["Motion picture industry", "Entertainment industry", "Review", "Research and models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0155", "e:abstract": "No abstract available.", "e:keyword": ["Motion pictures", "New product research", "Product placement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0157", "e:abstract": "No abstract available.", "e:keyword": ["Motion picture industry", "Entertainment industry", "File sharing and piracy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0164", "e:abstract": "The motion picture industry features challenging management problems and appealing research opportunities. Based on research findings, three challenges for managers are the need to (1) view movies as just one part of the product line, (2) consider the evolution of the first-run exhibition channel, and (3) take a global perspective in decision making. For researchers, the motion picture industry is a data-intensive natural laboratory that allows for empirical testing of important research questions. The study of competitive dynamics and contractual issues are highlighted as topics of particular relevance.", "e:keyword": ["Motion picture industry", "Entertainment industry", "Applications"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0163", "e:abstract": "In this comment, I discuss some research issues at the interface of marketing and operations particularly relevant to the motion picture industry. The major focus of my comments will be on the exhibition component of the motion picture value chain. Based on research findings and available data, I discuss the following issues: dynamic and interesting characteristics of the motion picture industry, the applicability of management science tools to artistic products, the practitioners' viewpoint, and the possibility of moving from specific to general research problems (and vice versa) in this field. Four promising research areas have been identified for marketing academics and researchers: (i) an integrated scheduling approach, (ii) relationship management and contract design, (iii) the role of forecasting accuracy in movie decision support systems, and (iv) the impact of digital conversion of movies on operations scheduling.", "e:keyword": ["Motion picture industry", "Operations", "Scheduling", "Decision support systems"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0185", "e:abstract": "No abstract available.", "e:keyword": ["Movies", "Consumer choice models", "Marketing channels", "Models and intuition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0187", "e:abstract": "No abstract available.", "e:keyword": ["Motion pictures", "Contact theory", "Talent compensation contracts", "Profit participation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0250", "e:abstract": "We observe a disproportional number of movies that vividly portray business and businesspeople with an unfavorable bias, often depicting ordinary business activity as zero-sum and sometimes depicting it as callous, immoral, and criminal. These movies also often aggrevate existing economic misconceptions that might include what we could call folk marketing. Folk marketing includes false ideas, such as marketing being a zero-sum game (rather than adding value), marketing research being intrusive clandestine surveillance (rather than advocating the buy viewpoint), and secrecy about market data being evidence of nefarious activities (rather than simply hiding strategies from competitors). Marketing scholars need to combat vigorously these false ideas. Moreover, when advertisements sponsor movies, it might be necessary to consider the conjoined movie content and the consistency of that content with the desired brand image.", "e:keyword": ["Antibusiness movies", "Motion pictures", "Films", "Motion picture industry", "Bias", "Villains", "Perceptions of businesspeople"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0200", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0144", "e:abstract": "Innovation is one of the most important issues in business research today. It has been studied in many independent research traditions. Our understanding and study of innovation can benefit from an integrative review of these research traditions. In so doing, we identify 16 topics relevant to marketing science, which we classify under five research fields: - Consumer response to innovation, including attempts to measure consumer innovativeness, models of new product growth, and recent ideas on network externalities; Organizations and innovation, which are increasingly important as product development becomes more complex and tools more effective but demanding; - Market entry strategies, which includes recent research on technology revolution, extensive marketing science research on strategies for entry, and issues of portfolio management; - Prescriptive techniques for product development processes, which have been transformed through global pressures, increasingly accurate customer input, Web-based communication for dispersed and global product design, and new tools for dealing with complexity over time and across product lines; - Defending against market entry and capturing the rewards of innovating, which includes extensive marketing science research on strategies of defense, managing through metrics, and rewards to entrants. For each topic, we summarize key concepts and highlight research challenges. For prescriptive research topics, we also review current thinking and applications. For descriptive topics, we review key findings.", "e:keyword": ["Innovation", "New products", "Consumer innovativeness", "Diffusion models", "Network externalities", "Strategic entry", "Defensive strategy", "Ideation", "Rewards to entrants", "Metrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0221", "e:abstract": "The need to understand the relationships among customer metrics and profitability has never been more critical. These relationships are pivotal to tracking and justifying firms' marketing expenditures, which have come under increasing pressure. The objective of this paper is to integrate existing knowledge and research about the impact of customer metrics on firms' financial performance. We investigate both unobservable or perceptual customer metrics (e.g., customer satisfaction) and observable or behavioral metrics (e.g., customer retention and lifetime value). We begin with an overview of unobservable and observable metrics, showing how they have been measured and modeled in research. We next offer nine empirical generalizations about the linkages between perceptual and behavioral metrics and their impact on financial performance. We conclude the paper with future research challenges.", "e:keyword": ["Customer satisfaction", "Service quality", "Customer lifetime value", "Customer retention", "Customer equity", "Profitability", "Firm value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0153", "e:abstract": "Branding has emerged as a top management priority in the last decade due to the growing realization that brands are one of the most valuable intangible assets that firms have. Driven in part by this intense industry interest, academic researchers have explored a number of different brand-related topics in recent years, generating scores of papers, articles, research reports, and books. This paper identifies some of the influential work in the branding area, highlighting what has been learned from an academic perspective on important topics such as brand positioning, brand integration, brand-equity measurement, brand growth, and brand management. The paper also outlines some gaps that exist in the research of branding and brand equity and formulates a series of related research questions. Choice modeling implications of the branding concept and the challenges of incorporating main and interaction effects of branding as well as the impact of competition are discussed.", "e:keyword": ["Brands", "Brand equity", "Brand extensions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0265", "e:abstract": "Questionable methods for increasing nominal wages reduce real wages (i.e., buying power) by creating inflation, shortages, lower quality, and long-term unemployment. To increase real wages (i.e., the ability to buy more), economic principles prescribe increasing productivity (i.e., greater output from less input). In contrast, marketing principles prescribes increasing the value of output (i.e., greater customer benefits) through innovation. Beyond increasing real wages, innovation spawns new occupations better matching individuals with skills and providing greater nonmonetary benefits (i.e., job satisfaction). Unfortunately, threatened entrenched incumbents often solicit protectionist legislation claiming negative externalities (e.g., short-term unemployment, lower wages, and burdens on society). Innovation does require labor to move from inferior to superior organizations (i.e., unemployment). However, protectionism only delays and dramatically aggravates the inevitable trauma associated with progress, as worker skills, firm practices, and buyer welfare fall further behind. Recent attacks demonizing Wal-Mart (e.g., the dubious Vlasic pickle claim) epitomize this situationthey are archaic vanilla protectionism, menacing both imperiled consumers and every consumer-driven business.", "e:keyword": ["Wal-Mart", "Low-prices", "Unemployment", "Wages", "Benefits", "Customer-driven orientation", "Market-based economy", "Consumer-driven organizations", "Trade unions", "Marketing principles", "Marketing theory", "Low prices", "Unemployment", "Wages", "Inflation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0220", "e:abstract": "This paper explores the benefits of letting customers escape from prepurchased service contracts by offering refunds for cancellations. We show that such a policy creates opportunities for in a capacity-constrained servicei.e., collecting cancellation fees from advance buyers who cancel, and then reselling the freed slots. The better the alternative that motivates a cancellation, the more profitable is a refund-for-cancellations policy compared with a no-refund policy that locks in customers. In contrast to previous research on money-back guarantees for durable goods, we show that offering refunds for service cancellations can be profitable (1) without charging a higher price compared with a no-refund policy, and (2) even when advance buyers would be willing to abandon the service for no refund. Also, service providers should decrease rather than increase the customer hassle cost of cancellations. Our research also suggests a new profit advantage of advance selling, i.e., capturing some of the consumer-added surplus created when customers find new alternatives (and are therefore willing to pay a fee to terminate the prepurchased contract). Finally, yield-management research typically assumes exogenous no shows by advance buyers. We suggest that offering refunds for cancellation reduces the need to reserve capacity for high-paying customers and improves capacity utilization.", "e:keyword": ["Cancellations", "Refund", "Pricing", "Yield management", "Advance selling", "Service marketing", "Capacity constraints", "Money-back guarantees"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0204", "e:abstract": "We study a channel relationship in which manufacturer(s) use independent sales representatives (rep firms), which employ salespeople to do the actual selling. We show that commission-only payments by manufacturers to rep firms lead to suboptimal outcomes for the manufacturer relative to those obtained under a vertically integrated channel. From the manufacturer's standpoint, these inefficiencies can be ameliorated through the use of sales incentives given to the rep firm's salespeople directly by the manufacturer (called spiffs). In a monopolistic environment, spiffs are shown to improve the manufacturer's profits in the face of contractual restrictions on the channel members' ability to set separate commission rates by product. For certain types of restrictions, spiffs may generate manufacturer outcomes close to the fully coordinated ones achieved under vertical integration even when compensating the rep firm through commission-only contracts. In a competitive environment, spiffs are shown to be used by a powerful manufacturer that shares a rep firm's sales efforts with the product of a weaker manufacturer (i.e., in the case of common agency). In this case, spiffs are used as a strategy to deter the weaker manufacturer from challenging the stronger manufacturer for the salesforce's valuable selling effort.", "e:keyword": ["Agency theory", "Channels of distribution", "Compensation", "Salesforce", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0238", "e:abstract": "A exists in many industries (e.g., newspaper publishing, media, software) in which a seller sells both a primary and a secondary product (e.g., a newspaper publisher sells newspapers to readers and advertising space to advertisers), and the value of the secondary product depends on the size of the user base of the primary product. This paper examines the competitive implications of asymmetric customer loyalty in such markets. In traditional markets, an advantage in customer loyalty generates a profit advantage. We show here, however, that in the presence of a cross-market network effect, a midlevel of loyalty advantage in the primary product market can lead to an overall profit disadvantage. This surprising result is derived from the interdependence of the two markets, whereby a profit in one market may be gained at the cost of the other, and by the positive relationship between a larger loyalty segment and a higher opportunity cost of price competition in the product of the primary market. Extending our model to a two-period entry game also shows that under certain conditions, the entrant with disadvantage in customer loyalty can outperform the incumbent in profit and market share. This result suggests that asymmetry in customer loyalty can be a source of first-mover advantage or disadvantage.", "e:keyword": ["Cross-market network effect", "Customer loyalty", "Competitive advantage", "First-mover advantage", "Two-sided markets", "Newspaper industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0129", "e:abstract": "This paper examines a key difference between two promotional vehicles, coupons and rebates. Whereas coupons offer deals up front, with the purchase of the product, rebates can be redeemed only after purchase. When consumers experience uncertain redemption costs, this difference translates to a difference in when uncertainty is resolved. With coupons the uncertainty is resolved before purchase; with rebates the uncertainty is resolved after purchase. As a result, we show that rebates are more efficient in surplus extraction but coupons offer more finetuned control over whom to serve. We identify the conditions under which each is optimal, and these conditions turn on the gap between low reservation price consumers' valuations and their highest redemption costs. Rebates are optimal when this gap is large; coupons tend to be optimal otherwise. Risk aversity on the part of consumers reduces the attractiveness of rebates, as does the delay between rebate redemption and rebate payment, but the latter if and only if consumers are more impatient than the seller. These observations match up well with what we know about the use of these promotional vehicles in the real world.", "e:keyword": ["Coupons", "Rebates", "Promotion", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0207", "e:abstract": "Marketing literature has long recognized that brand price elasticity need not be monotonic and symmetric, but has yet to provide generalizable market-level insights on threshold-based price elasticity, asymmetric thresholds, and the sign and magnitude of elasticity transitions. This paper introduces smooth transition regression models to study threshold-based price elasticity of the top 4 brands across 20 fast-moving consumer good categories. Threshold-based price elasticity is found for 76% of all brands: 29% reflect historical benchmark prices, 16% reflect competitive benchmark prices, and 31% reflect both types of benchmarks. The authors demonstrate asymmetry for gains versus losses on three levels: the threshold size and the sign and the magnitude of the elasticity difference. Interestingly, they observe latitude of acceptance for gains compared to the historical benchmark, but saturation effects in most other cases. Moreover, category characteristics influence the extent and the nature of threshold-based price elasticity, while individual brand characteristics impact the size of the price thresholds. From a managerial perspective, the paper illustrates the sales, revenue, and margin implications for price changes typically observed in consumer markets.", "e:keyword": ["Kinked demand curve", "Smooth-transition regression models", "Time-series analysis", "Asymmetric price thresholds", "Empirical generalizations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0216", "e:abstract": "In this paper we characterize the impact of production technology on the optimal product line design. We analyze a problem in which a manufacturer segments the market on quality attributes and offers products that are partial substitutes. Because consumers self-select from the product line, product cannibalization is an issue. In addition, the manufacturer sets a production schedule in order to balance production setups with accumulation of inventories in the presence of economies of scale. We show that simultaneous optimization of the product line design and production schedule leads to insights that differ significantly from the common intuition and assertions in the literature, which omits either the demand side or the supply side of the equation. In particular, we demonstrate that more expensive production technology always leads to lower product prices and may at the same time lead to higher quality products. Further, a less efficient production technology does not necessarily increase total production costs or reduce consumer welfare. We also demonstrate that in the presence of production technology, the demand cannibalization problem may distort product quality upward or the number of products upward, which is contrary to the standard result.", "e:keyword": ["Product line", "Segmentation", "Cannibalization", "EOQ", "Scale economies", "Marketing-manufacturing interface"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0222", "e:abstract": "Consumers aware of a new benefit will often experience uncertainty about its personal relevance or usage value. This paper shows that the decision to deliberate further to resolve this uncertainty and reach a polarized judgment of personal relevance critically depends on the posted price. In particular, a price above the consumer's initial willingness to pay might be thought provoking and enhance the perception of relevance with a certain probability. This behavioral mechanism is introduced formally and by way of an experiment with reference to the purchase of organic lettuce and fair-trade coffee. Accounting for the effect of price as a stimulus to think, a monopolistic firm should either over price (transgressive pricing) or under price (regressive pricing) in comparison to the consumer's willingness to pay. Under certain circumstances, the firm should also empower consumers with means that reduce the effort of deliberation.", "e:keyword": ["Production differentiation", "Marketing strategy", "Consumer behavior", "Pricing", "Cost of thinking", "Entry decision", "Consumer empowerment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0212", "e:abstract": "We introduce a new (point process) model of learning and forgetting, inspired by the structures of the brain, that we apply to model long-term memory for advertising and brand name recall. Recall-probability functions derived from the model are tested with classic data by Zielske [Zielske, H. A. 1959. The remembering and forgetting of advertising. 239243], as well as advertisement content and brand name recall data of a Dutch study that tracked over 40 campaigns of TV commercials. Data fits and cross-validation results indicate that the recall functions serve as a good first approximation for aggregate behavior. The shapes of optimal GRP schedules, which are obtained by maximizing a recall measure, are strongly related to the model parameters and corresponding memory processes. Comparisons with existing models in the literature indicate that a neurobiologically motivated model may give a more realistic description of memory for advertisements.", "e:keyword": ["Advertising", "Memory", "Impact", "Scheduling", "Bursting", "Dripping", "Massed and spaced learning", "Point processes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0274", "e:abstract": "The opening of Count Lev Nikolayevich (Leo) Tolstoy's novel inspired linguist, molecular physiologist and biogeographer Jared M. Diamond's eponym for the Anna Karenina principle (Diamond 1997). The principle suggests that no one property guarantees success but many guarantee failure. The Anna Karenina (TAK) bias is a logical consequence. TAK bias is more insidious than the kindred Survivor bias, which cautions that measured variables for passively observed survivors often differ from easily overlooked nonsurvivors. TAK bias, in contrast, cautions that the observed variables themselves might differ for survivors. The most revealing variables might exhibit negligible variation among survivors because survivors are necessarily alike. Perhaps variability is inversely related to the variable's importance for survival. TAK bias is more problematic for descriptive research, in contrast to normative (i.e., prescriptive) research, which seeks the true causal variables. Normative research only offers conditional aid to the decision maker on specific variables. To avoid TAK bias, we must not passively let accountants decide which variables we observe. We must actively collect data guided by predictions from deductive theory.", "e:keyword": ["Anna Karenina bias", "Survivor bias", "Descriptive research", "Inferential research", "Active data collection", "Statistical biases"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0240", "e:abstract": "In channel structures characterized by a powerful retailer (e.g., Wal-Mart, Home Depot), the dominant retailer's acceptance of a manufacturer's new product often determines the success of the new offering. Focusing on a manufacturer in such a market, we develop an approach to positioning and pricing a new product that directly incorporates the retailer's acceptance criteria into the development process. Our method also accounts for the retailer's product assortment and the competing manufacturers' potential reactions in wholesale prices. Our method merges individual-level conjoint models of preference with game-theoretic models of retailer and manufacturer behavior that are specific to the institutional setting of the focal manufacturer. The application of our approach in the context of a new power tool development project undertaken by this manufacturer also highlights the potential of our approach to other analogous institutional settings.", "e:keyword": ["New product forecasting", "Product positioning", "Distribution channel", "Conjoint model", "Game theory", "Big-box retailers", "Retailer acceptance", "Wal-Mart", "Home Depot"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0239", "e:abstract": "The growing dominance of large retailers has altered traditional channel incentives for manufacturers. In this paper, we present a theoretical model to illustrate a strategic manufacturer response to a dominant retailer. In our model, a dominant and a weak retailer compete for the sale of a single product supplied by a single manufacturer. The dominant retailer has the power to dictate the wholesale price, but the manufacturer sets the wholesale price for the weak retailer. The manufacturer also has partial ability to transfer demand between retailers. In the strategic manufacturer response, the manufacturer begins by raising the wholesale price for the weak retailer over that for the dominant retailer. This makes the weak retailer the high-margin channel. The manufacturer then transfers demand to the weak retailer by engaging in joint promotions and advertising. We then use this strategic response model to derive a testable hypothesis that may guide future research in determining the source of dominant retailers' low prices.", "e:keyword": ["Channels of distribution", "Channel power", "Retailing", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0208", "e:abstract": "Models of advertising response implicitly assume that the entire advertising budget is spent on disseminating one message. In practice, managers use different themes of advertising (for example, price advertisements versus product advertisements) and within each theme they employ different versions of an advertisement. In this study, we evaluate the dynamic effects of different themes of advertising that have been employed in a campaign. We develop a model that jointly considers the effects of wearout as well as that of forgetting in the context of an advertising campaign that employs five different advertising themes. We quantify the differential wearout effects across the different themes of advertising and examine the interaction effects between the different themes using a Bayesian dynamic linear model (DLM). Such a response model can help managers decide on the optimal allocation of resources across the portfolio of ads as well as better manage their scheduling. We develop a model to show how our response model parameters can be used to improve the effectiveness of advertising budget allocation across different themes. We find that a reallocation of resources across different themes according to our model results in a significant improvement in demand.", "e:keyword": ["Bayesian dynamic linear models", "Gibbs sampling aggregate advertising models", "Wearout effects", "Forgetting effects", "Copy effects", "Scheduling of ad copy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0214", "e:abstract": "We propose a framework to investigate consumers' brand choice and purchase incidence decisions across multiple categories, where both decisions are modeled as an outcome of a consumer's basket utility maximization. We build the model from first principles by theoretically explicating a general model of basket utility maximization and then examining the reasonable restrictions that can be placed to make the solution tractable without sacrificing its flexibility. Comparing with prior models, we show why prior multicategory purchase incidence models overemphasize the role of the cross effects of a market mix of brands in other categories on the purchase incidence decision of a given category. Additionally, we show that prior single-category models are a special case of the proposed model when further restrictions are placed on the basket utility structure. We estimate the model on household basket data for the laundry family of categories. We show (i) why prior single-category and multicategory models would systematically bias the estimates of the own- and cross-price/promotional purchase incidence elasticities; and (ii) how the market mix of each brand in each category affects the purchases across all categories, which can help retailers make promotional decisions across a portfolio of products.", "e:keyword": ["Multicategory brand choice and purchase incidence decision making", "Microeconomic theory of demand", "Basket utility maximization", "Simulated maximum likelihood"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0225", "e:abstract": "A sequence of bids in Internet auctions can be viewed as record-breaking events in which only those data points that break the current record are observed. We investigate stochastic versions of the classical record-breaking problem for which we apply Bayesian estimation to predict observed bids and bid times in Internet auctions. Our approach to addressing this type of data is through data augmentation in which we assume that participants (bidders) have dynamically changing valuations for the auctioned item, but the latent number of bidders competing in those events is unseen. We use data from notebook auctions provided by one of the largest Internet auction sites in Korea. We find significant variation in the number of latent bidders across auctions. Our other primary findings are as follows: (1) the latent bidders are significant in number relative to observed bidders, (2) the latent number of remaining bidders is considerably smaller than that of new entrants to the auction after a given bid, and (3) larger bid and time increments significantly influence the bidding participation behavior of the remaining bidders. As part of our substantive contribution, we highlight the model's ability to understand brand equity in an Internet auction context through a brand's ability to simultaneously bring in bidders, higher bid amounts, and faster bidding.", "e:keyword": ["Latent bidders", "Bidding dynamics", "Record-breaking events", "Bayesian inference", "Data augmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0227", "e:abstract": "Product-harm crises are among a firm's worst nightmares. A firm may experience (i) a loss in baseline sales, (ii) a reduced own effectiveness for its marketing instruments, (iii) an increased cross sensitivity to rival firms' marketing-mix activities, and (iv) a decreased cross impact of its marketing-mix instruments on the sales of competing, unaffected brands. We find that this quadruple jeopardy materialized in a case study of an Australian product-harm crisis faced by Kraft peanut butter. We arrive at this conclusion by using a time-varying error-correction model that quantifies the consequences of this crisis on base sales, and on own- and cross-brand short- and long-term effectiveness. The proposed modeling approach allows managers to make more informed decisions on how to regain the brands' pre-crisis performance levels.", "e:keyword": ["Brand management", "Product recalls", "Brand equity", "Marketing and public policy", "Error-correction models", "Time-varying parameters", "Time-series models", "Missing-data problems", "Gibbs sampling methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0223", "e:abstract": "While marketing activities increasingly involve personalizing product offers to individually elicited preferences, these unique specifications may not be universally important for product choice. Providing evidence of the limits of treating each customer differently, three experiments show that individuals who exhibit interdependent or collectivistic tendencies tend to be more receptive to recommendations that are not personalized to their own preferences, but instead to the collective preferences of relevant in-groups. However, we find that cultural orientation affects responses to personalized recommendations for only those products whose consumption or choice decision is subject to public scrutiny. We further demonstrate that the favorability of thoughts elicited by ads offering targeted versus personalized offers mediates the effect of cultural orientation on responses to personalization. Finally, both individualistic and collectivistic consumers respond more favorably to offers of targeted recommendations when they believe relevant others share their preferences and when their level of expertise is relatively low.", "e:keyword": ["Personalization", "Personal recommendations", "Culture"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0206", "e:abstract": "We consider the optimal two-part tariff contract between a manufacturer and a retailer. We show that retail competition (in the presence of either fixed costs or bargaining power) may lead to slotting allowances in an optimal contract, even with a monopoly manufacturer and no information asymmetry. On the other hand, slotting allowances do not arise with a monopoly retailer and no information asymmetry, whether the manufacturer is a monopoly or not. We also show that more intense retail competition, higher retail bargaining power, larger retailer fixed costs, lower marginal costs of retailing, as well as larger relative retailer size (whether coming from a location or operating advantage), have a positive impact on the incidence and the magnitude of slotting allowances. The opposing effects of the fixed and marginal operating costs on slotting allowances, as well as the impact of competition and bargaining power on profits, underscore the importance of careful definitions of these variables in empirical research.", "e:keyword": ["Slotting allowances", "Bargaining power", "Distribution channels", "Channel coordination", "Competition", "Pricing", "Retailing", "Wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1050.0116", "e:abstract": "User design offers tantalizing potential benefits to manufacturers and consumers, including a closer match of products to user preferences, which should result in a higher willingness to pay for goods and services. There are two fundamental approaches that can be taken to user design: systems and systems. With parameter-based systems, users directly specify the values of design parameters of the product. With needs-based systems, users specify the relative importance of their needs, and an optimization algorithm recommends the combination of design parameters that is likely to maximize user utility. Through an experiment in the domain of consumer laptop computers, we show that for parameter-based systems, outcomes, including measures for comfort and fit, increase with the expertise of the user. We also show that for novices, the needs-based interface results in better outcomes than the parameter-based interface.", "e:keyword": ["User design", "Product design", "Product development", "User needs", "Customer needs", "Design decisions", "Customization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0277", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0267", "e:abstract": "Even in mature markets, managers are expected to improve their brands' performance year after year. When successful, they can expect to continue executing on an established marketing strategy. However, when the results are disappointing, a change or turnaround strategy may be called for to help performance get back on track. In such cases, performance diagnostics are needed to identify turnarounds and to quantify the role of marketing policy shifts in this process. This paper proposes a framework for such a diagnosis and applies several methods to provide converging evidence for two main findings. First, contrary to prevailing beliefs, the performance of brands in mature markets is not always stable. Instead, brands systematically improve or deteriorate their performance outlook in clearly identifiable time windows that are relatively short compared to windows of stability. Second, these shifts in performance regimes are associated with the brand's marketing actions and policy shifts, as opposed to competitive marketing. Promotion-oriented marketing policy shifts are particularly potent in improving a brand's performance outlook.", "e:keyword": ["Performance improvement", "Turnaround strategy", "Marketing mix", "Advertising", "Promotion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0271", "e:abstract": "When designing price contracts, one of the major questions confronting managers is how many blocks there should be in the contract. We investigate this question in the setting of a manufacturer-retailer dyad facing a linear deterministic consumer demand. Theoretical marketing models predict that the manufacturer's profits rise dramatically when the number of blocks in the contract is increased from one to two because both channel efficiency and its share of channel profits increase. However, increasing the number of blocks to three yields no incremental profits. We test these predictions experimentally and find that increasing the number of blocks from one to two raises channel efficiency but not the manufacturer's share of profits. Surprisingly, having three blocks in the contract increases channel efficiency even further and also gives the manufacturer a slightly higher share of profits. We show that these results can be explained by a quantal response equilibrium model in which the manufacturer accounts for noisy best response due to nonpecuniary payoff components in the retailer's utility. We also show that the retailer is sensitive to the counterfactual profits it could have earned if it were charged a lower marginal price for earlier blocks in the multiple-block contract.", "e:keyword": ["Quantity discounts", "Price contracts", "Quantal response equilibrium", "Experimental economics", "Behavioral economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0270", "e:abstract": "Retailers and manufacturers believe that the mere presence of certain items in a retail assortment increases the sales volume of the whole assortment. This paper provides an empirical study of the role of every item in an assortment. Our results show that many items affect category sales over and above their own sales volume. After deconstructing the role of a stockout of individual items into three effectslost own sales, substitution to other items, and the category sales impactwe find that the category impact has the largest magnitude. Interestingly, the disproportionate impact of individual items on category sales is not restricted to top-selling items, for almost every single individual item affects category sales. It seems that variety is indeed the price of entry in retailing. Our results support recent findings that more frequently purchased categories are less adversely affected by reductions in assortment. We also find that the assortment appears to gain attractiveness when certain items are out of stock, a result that is consistent with the discussion in the literature concerning category clutter.", "e:keyword": ["Apparel retailing", "Retail assortment", "Category management", "Out-of-stocks", "Substitution", "Key items", "Hierarchical Bayes", "COM-Poisson"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0273", "e:abstract": "Following a successful idea generation exercise, a company might easily be left with hundreds of ideas generated by experts, employees, or consumers. The next step is to screen these ideas and identify those with the highest potential. In this paper we propose a practical approach to involving consumers in idea screening. Although the number of ideas may potentially be very large, it would be unreasonable to ask each consumer to evaluate more than a few ideas. This raises the challenge of efficiently selecting the ideas to be evaluated by each consumer. We describe several idea-screening algorithms that perform this selection adaptively based on the evaluations made by previous consumers. We use simulations to compare and analyze the performance of the algorithms as well as to understand their behavior. The best-performing algorithm focuses on the ideas that are the most likely to have been misclassified (as top or bottom ideas) based on the previous evaluations, and avoids discarding ideas too fast by adding random perturbations to the misclassification probabilities. We demonstrate the convergent validity of this algorithm using a field experiment, which also confirms the convergence pattern predicted by simulations.", "e:keyword": ["Innovation", "Marketing research", "Marketing surveys", "Marketing tools", "New product research", "Product development"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0261", "e:abstract": "Managers often have incentives to artificially inflate current-term earnings by cutting marketing expenditures, even if it comes at the expense of long-term profits. Because investors rely on current-term accounting measures to form expectations of future-term profits, inflating current-term results can lead to enhanced current-term stock price. We present evidence that some firms engage in this type of myopic marketing management at the time of a seasoned equity offering (SEO). In particular, a greater proportion of firms than is typical report earnings higher than normal and marketing expenditures lower than normal at the time of their SEO. Although they realize that firms might be undertaking strategies to artificially inflate current-term earnings, the financial markets are not adequately identifying and properly valuing the firms doing so. Our results indicate that myopic firms are able to temporarily inflate their stock market valuation, but in the long run, as the consequences of cutting marketing spending become manifest, they have inferior stock market performance. We propose some actions that might reduce the incentives for myopic behavior.", "e:keyword": ["Myopic marketing management", "Marketing strategy", "Marketing resource allocation", "Signal jamming", "Long-term financial performance", "Abnormal stock returns"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0241", "e:abstract": "The authors propose two variants of lexicographic preference rules. They obtain the necessary and sufficient conditions under which a linear utility function represents a standard lexicographic rule, and each of the proposed variants, over a set of discrete attributes. They then: (i) characterize the measurement properties of the parameters in the representations; (ii) propose a nonmetric procedure for inferring each lexicographic rule from pairwise comparisons of multiattribute alternatives; (iii) describe a method for distinguishing among different lexicographic rules, and between lexicographic and linear preference models; and (iv) suggest how individual lexicographic rules can be combined to describe hierarchical market structures. The authors illustrate each of these aspects using data on personal-computer preferences. They find that two-thirds of the subjects in the sample use some kind of lexicographic rule. In contrast, only one in five subjects use a standard lexicographic rule. This suggests that lexicographic rules are more widely used by consumers than one might have thought in the absence of the lexicographic variants described in the paper. The authors report a simulation assessing the ability of the proposed inference procedure to distinguish among alternative lexicographic models, and between linear-compensatory and lexicographic models.", "e:keyword": ["Lexicographic preferences", "Noncompensatory preference models", "Linear models", "Optimization techniques", "Greedy algorithm", "Approximation algorithms", "Utility theory", "Conjoint analysis", "Hierarchical clustering", "Market segmentation", "Hierarchical market structures"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0224", "e:abstract": "We model the diffusion of innovations in markets with two segments: who are more in touch with new developments and who affect another segment of whose own adoptions do not affect the influentials. This two-segment structure with asymmetric influence is consistent with several theories in sociology and diffusion research, as well as many viral or network marketing strategies. We have four main results. (1) Diffusion in a mixture of influentials and imitators can exhibit a dip or chasm between the early and later parts of the diffusion curve. (2) The proportion of adoptions stemming from influentials need not decrease monotonically, but may first decrease and then increase. (3) Erroneously specifying a mixed-influence model to a mixture process where influentials act independently from each other can generate systematic changes in the parameter values reported in earlier research. (4) Empirical analysis of 33 different data series indicates that the two-segment model fits better than the standard mixed-influence, the Gamma/Shifted Gompertz, and the Weibull-Gamma models, especially in cases where a two-segment structure is likely to exist. Also, the two-segment model fits about as well as the Karmeshu-Goswami mixed-influence model, in which the coefficients of innovation and imitation vary across potential adopters in a continuous fashion.", "e:keyword": ["Asymmetric influence", "Diffusion of innovations", "Innovation", "Market segments", "Social contagion", "Social structure"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0226", "e:abstract": "In this study, we develop a multivariate generalization of the negative binomial distribution (NBD). This new model has potential application to situations where separate NBDs are correlated, such as for page views across multiple websites. In turn, our page view model is used to predict the audience for Internet advertising campaigns. For very large Internet advertising schedules, a simple approximation to the multivariate model is also derived. In a test of nearly 3,000 Internet advertising schedules, the two new models are compared with some proprietary and nonproprietary models previously used for Internet advertising and are shown to be significantly more accurate.", "e:keyword": ["Advertising", "Internet marketing", "Media", "Probability models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0263", "e:abstract": "The cross-product term in moderated regression may be collinear with its constituent parts, making it difficult to detect main, simple, and interaction effects. The literature shows that mean-centering can reduce the covariance between the linear and the interaction terms, thereby suggesting that it reduces collinearity. We analytically prove that mean-centering neither changes the computational precision of parameters, the sampling accuracy of main effects, simple effects, interaction effects, nor the <sup>2</sup>. We also show that the determinants of the cross product matrix are identical for uncentered and mean-centered data, so the collinearity problem in the moderated regression is unchanged by mean-centering. Many empirical marketing researchers commonly mean-center their moderated regression data hoping that this will improve the precision of estimates from ill conditioned, collinear data, but unfortunately, this hope is futile. Therefore, researchers using moderated regression models should not mean-center in a specious attempt to mitigate collinearity between the linear and the interaction terms. Of course, researchers may wish to mean-center for interpretive purposes and other reasons.", "e:keyword": ["Moderated regression", "Mean-centering", "Collinearity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0293", "e:abstract": "Observing reality is especially valuable. However, without models, every situation at every time on every variable would be unpredictable. Assumptions allow models and theories to assert constancy. Assumptions distill and simplify reality by dismissing the conspicuous but irrelevant. Criticizing assumptions as unrealistic is absurd. Abstraction is the precise virtue of an assumption. For example, seldom are we prisoners facing interrogation, yet the prisoner's dilemma remains relevant. The adage A bird in the hand is worth two in the bush is relevant for more than birds. Unrealistic assumptions that deny current beliefs breed great new theories. Assumptions are analogous to the basic ingredients in a gourmet recipe. Only the final product of the recipe dictates whether the ingredients suffice. Similarly, assumptions are realistic when they produce good theories, satisfactory predictions, valuable implications, and correct recommendations. Output matters far more than input. Realism is only an issue when creatively diagnosing poorly performing models, not when judging model performance. Assumptions are the source of value in empirical analyses. If data sets were truly the source of value, empirical research studies would only greatly devalue the raw data by dramatically reducing rich observations to a few meager summary statistics or estimated parameters. Most empirical research makes a contribution by ignoring (assuming away) most information in the data. We must dramatically shift our attention far away from the hopeless pursuit and sophistry of realistic assumptions to the contribution those assumptions produce. There are scientific methods for evaluating model output (i.e., predictions, findings, implications, recommendation) on criteria such as accuracy, reliability, validity, robustness, and so on. No corresponding objective scientific methods exist for evaluating realism. Realism depends only on personal taste.", "e:keyword": ["Models", "Mathematical models", "Realistic assumptions", "Instrumentalism", "Empirical research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0253", "e:abstract": "We propose that the variety a brand offers often serves as a quality cue and thus influences which brand consumers choose. Specifically, brands that offer a greater variety of options that appear compatible and require similar skills tend to be perceived as having greater category expertise or core competency in the category, which, in turn, enhances their perceived quality and purchase likelihood. Six studies support this proposition and demonstrate that compared to brands which offer fewer products, (a) brands which offer increased compatible variety are perceived as having higher quality; (b) this effect is mediated by product variety's impact on perceived expertise; (c) the higher perceived quality produces a greater choice share of the higher variety brand, even among consumers who select options that multiple brands offer and (d) product variety also impacts post-experience perceptions of taste. The findings suggest that in addition to directly affecting brand choice share through influencing the fit with consumer preferences, product line length can also indirectly affect brand choice through influencing perceived brand quality.", "e:keyword": ["Variety", "Consumer choice", "Quality cues", "Product line length"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0205", "e:abstract": "What are the drivers of retailer pricing tactics over time? Based on multivariate time-series analysis of two rich data sets, we quantify the relative importance of competitive retailer prices, pricing history, brand demand, wholesale prices, and retailer category-management considerations as drivers of retail prices. Interestingly, competitive retailer prices account for less than 10% of the over-time variation in retail prices. Instead, pricing history, wholesale price, and brand demand are the main drivers of retail-price variation over time. Moreover, the influence of these price drivers on retailer pricing tactics is linked to retailer category margin. We find that demand-based pricing and category-management considerations are associated with higher retailer margins. In contrast, dependence on pricing history and pricing based on store traffic considerations imply lower retailer margins.", "e:keyword": ["Retail-price drivers", "Retailer profits", "Time-series models", "Generalized forecast error variance decomposition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0252", "e:abstract": "The free-riding problem occurs if the presales activities needed to sell a product can be conducted separately from the actual sale of the product. Intuitively, free riding should hurt the retailer that provides that service, but the author shows analytically that free riding benefits not only the free-riding retailer, but also the retailer that provides the service when customers are heterogeneous in terms of their opportunity costs for shopping. The service-providing retailer has a postservice advantage, because customers who have resolved their matching uncertainty through sales service incur zero marginal shopping cost if they purchase from the service-providing retailer rather than the free-riding retailer. Moreover, allowing free riding gives the free rider less incentive to compete with the service provider on price, because many customers eventually will switch to it due to their own free riding. In turn, this induced soft strategic response enables the service provider to charge a higher price and enjoy the strictly positive profit that otherwise would have been wiped away by head-to-head price competition. Therefore, allowing free riding can be regarded as a necessary mechanism that prevents an aggressive response from another retailer and reduces the intensity of price competition.", "e:keyword": ["Free riding", "Sales service", "Selling cost", "Channel conflict", "Retail competition", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0246", "e:abstract": "Consumers often have to evaluate products comprising a combination of attributes that is not expected by them, given their beliefs about how attributes normally co-vary in the product category. Such an attribute combination implies that the claimed level of a product attribute is then different from what the consumer might infer, given the level of another attribute, resulting in what we call . We develop a model to calibrate the effect of incoherence on perceptions, uncertainty, preference, and ultimately purchase. Our model can allow managers to determine consumers' acceptance for different positions in the multiattribute space, so they can optimize their product's positioning. Our model implies that a product that combines positively valued attributes might increase some elements of preference for the product, but if those attributes occur in unexpected combinations, incoherence will also increase uncertainty which in turn might lower other elements of preference. The net risk-adjusted preference for a product in our model accommodates both the benefit from the expected attribute levels and the uncertainty associated with incoherence. We derive implications from the model and provide an empirical test that supports those implications.", "e:keyword": ["Product positioning", "Product management", "New products", "Brand management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0247", "e:abstract": "In this paper we define an embedded premium (EP) as an enhancement that involves a social cause added on to a product or service. We characterize EP as a sales promotion strategy and juxtapose it with traditional approaches, such as discounts and rebates. Across three experiments, using a nationwide Internet panel and employing stated measures and model-based inference, we find that at low denominations EP is more effective than an equivalent price discount. We describe how an EP's social association may influence consumer choice quite differently than price promotions and, contrary to the asymmetric price promotion effect documented in the promotions literature, we find that EP benefits an unknown brand more than a known brand. Our hierarchical Bayes approach uncovers heterogeneity in EP effectiveness that can be explained by affinity toward the focal charity, personal motivations, and demographic markers. An identifiable segment of individuals prefer the other over self, suggesting possible EP optimization and segmentation strategies. Two such strategies, customization and coverage, are empirically tested, and the former is shown to be very effective. Our findings have broad implications for brand managers with regard to resource allocation and EP program return on investment (ROI), as well as important social welfare implications.", "e:keyword": ["Embedded premium", "Sales promotion", "Consumer choice", "Hierarchical Bayes", "ROI", "Cause-related marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0213", "e:abstract": "Greedoid languages provide a basis to infer best-fitting noncompensatory decision rules from full-rank conjoint data or partial-rank data such as consider-then-rank, consider-only, or choice data. Potential decision rules include elimination by aspects, acceptance by aspects, lexicographic by features, and a mixed-rule lexicographic by aspects (LBA) that nests the other rules. We provide a dynamic program that makes estimation practical for a moderately large numbers of aspects. We test greedoid methods with applications to SmartPhones (339 respondents, both full-rank and consider-then-rank data) and computers (201 respondents from Lenk et al. 1996). We compare LBA to two compensatory benchmarks: hierarchical Bayes ranked logit (HBRL) and LINMAP. For each benchmark, we consider an unconstrained model and a model constrained so that aspects are truly compensatory. For both data sets, LBA predicts (new task) holdouts at least as well as compensatory methods for the majority of the respondents. LBA's relative predictive ability increases (ranks and choices) if the task is full rank rather than consider then rank. LBA's relative predictive ability does not change if (1) we allow respondents to presort profiles, or (2) we increase the number of profiles in a consider-then-rank task from 16 to 32. We examine trade-offs between effort and accuracy for the type of task and the number of profiles.", "e:keyword": ["Lexicography", "Noncompensatory decision rules", "Choice heuristics", "Optimization methods in marketing", "Conjoint analysis", "Product development", "Consideration sets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0228", "e:abstract": "We report on the finalists from the 2005 Informs Society for Marketing Science (ISMS) Practice Prize Competition, representing the best examples of rigor plus relevance that our profession produces.", "e:keyword": ["Marketing science practice", "Customer lifetime value", "Optimal pricing", "Promotion planning", "One-to-one marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0249", "e:abstract": "The Customer Equity and Lifetime Management (CELM) solution is based on a decision-support system that offers marketing managers a scientific framework for the optimal planning and budgeting of targeted marketing campaigns to maximize return on marketing investments. The CELM technology combines advanced models of Markov decision processes (MDPs), Monte Carlo simulation, and portfolio optimization. MDPs are used to model customer dynamics and to find optimal marketing policies that maximize the value generated by a customer over a given time horizon. Lifetime value optimization is achieved through dynamic programming algorithms that identify which marketing actions, such as cross-selling, up-selling, and loyalty marketing campaigns, transition customers to better value and loyalty states. The CELM technology can also be used to simulate the financial impact of a given marketing policy using Monte Carlo simulation. This allows marketing managers to simulate several targeting scenarios to assess budget requirements and the expected impact of a given marketing policy. The benefits of the solution are illustrated with the Finnair case study, where CELM has been used to optimize marketing planning and budgeting for Finnair's frequent-flyer program (FFP).", "e:keyword": ["Marketing optimization", "Loyalty programs", "Markov decision processes", "Portfolio optimization", "Marketing budget allocation", "Customer equity", "Customer lifetime value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0245", "e:abstract": "We quantified the net unit and profit impact of each promotion offered in 2003 by CVS, a leading U.S. drug retail chain, and analyzed the key drivers of variation in this net impact. We used this analysis to identify the least effective promotions and conducted a controlled field test to demonstrate the impact of eliminating them before chainwide implementation. Our key findings are as follows. First, approximately 45% of the gross lift from promotions is incremental for CVS. Further, for every unit of gross lift, 0.16 unit of some other product is purchased elsewhere in the store. Still, more than 50% of promotions are not profitable because the lower promotional margin is not sufficiently offset by incremental units. Second, there is substantial variation in net profit impact across categories. Our field test shows that eliminating promotions chainwide in 15 of the worst performing categories will decrease sales by about $7.8 million but will improve profit by approximately $52.6 million. This is very impressive given that CVS front store sales in 2003 were approximately $9 billion while the net profit impact of promotions was -$25.3 million.", "e:keyword": ["Promotion profitability", "Retail promotions", "Retail promotion impact"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0217", "e:abstract": "The main objective of this report is to describe a decision-support system for dynamic retail pricing and promotion planning. Our weekly demand model incorporates price, reference price effects, seasonality, article availability information, features, and discounts. Building on previous research, we quantify demand interdependencies and integrate the resulting profit-lifting effects into the optimal pricing model. The methodology was developed and implemented at bauMax, an Austrian do-it-yourself retailer. Along with the practical requirements, an objective function was employed that can be used as a vehicle for implementing a retailer's strategy. Eight pricing rounds with thousands of different stock-keeping units have each served as a testing ground for our approach. Based on various benchmarking methods, a positive impact on profit was reported. The currently implemented marketing decision-support system increased gross profit on average by 8.1 and sales by 2.1%.", "e:keyword": ["Reference price", "Demand interdependency", "Revenue management", "Retail strategy", "Pricing research", "Dynamic pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0309", "e:abstract": "This editorial provides an overview of the editorial process at one peer-reviewed publication. The editorial starts by explaining the role of the players (the editors, the review team, the area editor). The editorial then covers each step in the review process, from how reviewers are selected to how authors should respond to different outcomes. The editorial ends by discussing citation metrics, appointments to the editorial board and copyrights. This article argues that (1) requesting more reviews yields a faster, more informative review process; (2) publishing more articles can raise citations per article; (3) for many submissions, some reviewers should evaluate procedures, whereas others should evaluate contribution; (4) reviewers should not micromanage revisions; (5) editors must, unfortunately, write overly cautious decision letters; and (6) it is important to reward reviewers with board appointments and published acknowledgments. Journals must be author-friendly to survive.", "e:keyword": ["Marketing science", "Electronic journals", "Print journals", "Citations", "Peer review", "Referees", "Research streams", "The review process"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0257", "e:abstract": "Polyhedral methods for choice-based conjoint analysis provide a means to adapt choice-based questions at the individual-respondent level and provide an alternative means to estimate partworths when there are relatively few questions per respondent, as in a Web-based questionnaire. However, these methods are deterministic and are susceptible to the propagation of response errors. They also assume, implicitly, a uniform prior on the partworths. In this paper we provide a probabilistic interpretation of polyhedral methods and propose improvements that incorporate response error and/or informative priors into individual-level question selection and estimation. Monte Carlo simulations suggest that response-error modeling and informative priors improve polyhedral question-selection methods in the domains where they were previously weak. A field experiment with over 2,200 leading-edge wine consumers in the United States, Australia, and New Zealand suggests that the new question-selection methods show promise relative to existing methods.", "e:keyword": ["Conjoint analysis", "Choice models", "Estimation and other statistical techniques", "International marketing", "Marketing research", "New-product research", "Product development", "Bayesian methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0279", "e:abstract": "Bias in the market for news is well-documented. Recent research in economics explains the phenomenon by assuming that consumers want to read (watch) news that is consistent with their tastes or prior beliefs rather than the truth. The present paper builds on this idea but recognizes that (i) besides biased consumers, there are also conscientious consumers whose sole interest is in discovering the truth, and (ii) consistent with reality, media bias is constrained by the truth. These two factors were expected to limit media bias in a competitive setting. Our results reveal the opposite. We find that media bias may increase when there are more conscientious consumers. However, this increased media bias does not necessarily hurt conscientious consumers who may be able to recover more information from multiple media outlets the more the outlets are biased. We discuss the practical implications of these findings for media positioning, media pricing, media planning, and the targeting of advertising.", "e:keyword": ["Information goods", "Complements", "Media competition", "Media positioning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0281", "e:abstract": "Reverse auctions are fast becoming the standard for many procurement activities. In the past, the majority of such auctions have been solely price based, but increasingly attributes other than price affect the auction outcome. Specifically, the buyer uses a scoring function to compare bids and the bid with the highest score wins. We investigate two mechanisms commonly used for procurement in business-to-business markets, in a setting in which buyers' welfare is affected by exogenous nonprice attributes such as the quality, service, and past relationships. Under both mechanisms, bidders bid based on price, but in the buyer-determined mechanism, the buyer is free to select the bid that maximizes her surplus while in the price-based mechanism, the buyer commits to awarding the contract to the low price bidder. We find, both in theory and in the laboratory, that the buyer-determined mechanism increases the buyer's welfare only as long as enough suppliers compete. If the number of suppliers is small and the correlation between cost and quality is low, the buyer is better off with the price-based mechanism. These findings are intended to help procurement managers make better decisions in designing procurement mechanisms for a variety of settings.", "e:keyword": ["Bidding", "Procurement", "Reverse auctions", "Multiattribute auctions", "Behavioral game theory", "Experimental economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0256", "e:abstract": "The use of adaptive designs in conjoint analysis has been shown to lead to an endogeneity bias in part-worth estimates using sampling experiments. In this paper, we re-examine the endogeneity issue in light of the likelihood principle. The likelihood principle asserts that all relevant information in the data about model parameters is contained in the likelihood function. We show that, once the data are collected, adhering to the likelihood principle leads to analysis where endogeneity becomes ignorable for estimation. The likelihood principle is implicit to Bayesian analysis, and discussion is offered for detecting and dealing with endogeneity bias in marketing.", "e:keyword": ["Likelihood principle", "Adaptive design", "Bayes theorem", "Directed acyclic graphs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0280", "e:abstract": "The common wisdom is that a retailer suffers when its wholesale supplier encroaches on the retailer's operations by selling directly to final consumers. We demonstrate that the retailer can benefit from encroachment even when encroachment admits no synergies and does not facilitate product differentiation or price discrimination. The retailer benefits because encroachment induces the encroaching supplier to reduce the wholesale price in order not to diminish unduly the retailer's demand for the manufacturer's wholesale product. The lower wholesale price and increased downstream competition mitigate double marginalization problems and promote efficiency gains that can secure Pareto improvements.", "e:keyword": ["Channels of distribution", "Encroachment", "Market entry", "Retailing and wholesaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0255", "e:abstract": "This paper demonstrates that increased product differentiation will make it more difficult to sustain collusion when it is costly to coordinate or maintain collusion. This result holds for a wide range of models, including all those commonly used to model competition between differentiated products. This contrasts with the previous theoretical literature, which shows that, in the absence of these costs, greater differentiation can help foster collusion under some common models of product differentiation but is consistent with the empirical literature, which suggests that collusion tends to occur most among homogeneous firms.", "e:keyword": ["Game theory", "Product differentiation", "Competition", "Collusion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0278", "e:abstract": "Dual process models conceptualize two systems of processing that are activated when presented with a decision task, the quick and affective System 1 and the deliberative and rule-based System 2. In this article, we explore whether the affective component of System 1 has the potential to interfere with the information integration component of System 2 by utilizing everyday consumer decision-making situations that require the integration of provided information to make optimal choices. We posit that if the provided information has an affective System 1 element, then the affective reaction serves as an input to the System 2 process of information integration. Such an affective input has the potential to cause improper information integration resulting in a biased mental representation, which in turn leads to suboptimal choices. Across three experiments, we test the interfering role of affect in information integration. Experiment 1 establishes the mediating role of the affective System 1 generating a preference for the suboptimal option and rules out the alternate account of analytical skills. Experiments 2 and 3 provide converging evidence for the proposed account that System 1 interferes with System 2 and argue against the alternate account of System 1 directly influencing choice.", "e:keyword": ["System 1 and system 2", "Dual process models", "Choice", "Affect", "Irrationality", "Two-part tariff", "Misattribution", "Information integration", "Stimulus-induced affect", "Affect and cognition", "Behavioral decision theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0260", "e:abstract": "This paper examines the effects of brand credibility, a central concept in information economicsbased approaches to brand effects and brand equity, on consumer choice and choice set formation. We investigate the mechanisms through which credibility effects materialize, namely, through perceived quality, perceived risk, and information costs saved. The credibility of a brand as a signal is defined as the believability of the product position information contained in a brand, which depends on consumer perceptions of the willingness and ability of firms to deliver what they have promised. The choice set is defined as the collection of brands that have a nonzero probability of being chosen among those actually available for choice in a given context. Furthermore, we study the impact of brand credibility on the variance of the stochastic component of utility. Not only do choice model parameters capture the impact of systematic utility differences on choice probabilities, but also the magnitude of this systematic impact is moderated by the relative importance of the stochastic utility component in preference. We term this moderation phenomenon , which we conceptualize as the decision makers' capacity to effectively discriminate between products' utilities in choice situations. We estimate a discrete choice model of brand choice set formation and preference discrimination on experimental data in two categoriesjuice and personal computersand find strong evidence for brand credibility effects and differential mechanisms through which brand credibility's impact materializes on brand choice conditional on choice set, choice set formation, and preference discrimination.", "e:keyword": ["Information economics", "Perceived quality", "Perceived risk", "Brand preference", "Branding", "Brand choice", "Choice models", "Personal computers", "Juice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0283", "e:abstract": "In communication, information, and other industries, three-part tariffs are increasingly popular. A three-part tariff is defined by an access price, an allowance, and a marginal price for any usage in excess of the allowance. Empirical nonlinear pricing studies have focused on consumer choice under two-part tariffs. We show that consumer behavior differs under three-part tariffs and assess how consumer demand uncertainty impacts tariff choice. We develop a discrete/continuous model of choice among three-part tariffs and estimate it using consumer-level data on Internet usage. Our model extends prior work in accommodating consumer switching to competitors, thereby capturing behavior in competitive industries more accurately. Our empirical work shows that demand uncertainty is a key driver of choice among three-part tariffs. Consumers' expected bill increases with the variation in their usage, steering them toward tariffs with high allowances. Consequently, demand uncertainty decreases consumer surplus and increases provider revenue. A further analysis of consumers' responsiveness to the different elements of a three-part tariff under the provider's current pricing structure reveals that prices affect a consumer's tariff choice more than her usage quantity and that the allowance plays a strong role in consumer tariff choice. Based on our results, we derive implications for pricing with three-part tariffs.", "e:keyword": ["Pricing", "Nonlinear pricing", "Discrete/continuous choice model", "Internet access", "Three-part tariffs", "Uncertainty", "Choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0268", "e:abstract": "Recent research studies have shown that, in the aggregate, survey participation is associated with increased purchase behaviors. Whereas selective customer response could be responsible for some of this correlation, we conclude that survey participation does cause changes in purchase behavior. On the basis of this conclusion, we investigate the possible differential impact of participating in a firm-sponsored satisfaction survey on (1) services purchases, (2) responsiveness to promotions, (3) interpurchase time, and (4) spending, across customers. We develop a joint model of these four customer behaviors and explicitly examine the role of customer characteristics and store-specific variables in moderating the effects of survey participation. We also study how these effects change over time. The data used for the analysis come from a longitudinal field study of customer satisfaction conducted by a U.S. automotive services firm. It contains two groups of customers. One group was administered a customer satisfaction survey and the other was not. Our results reveal a substantial positive relationship between satisfaction survey participation and all the customer behaviors studied. Assuming a causal relationship, we also find the effects of satisfaction survey participation to vary across customers and stores, and over time.", "e:keyword": ["Survey research", "Customer behavior", "Mere measurement effect", "Survey participation effect", "Hierarchical Bayes", "Conway-Maxwell-Poisson"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0338", "e:abstract": "Despite warnings against inferring causality from observed correlations or statistical dependence, some articles do. Observed correlation is neither necessary nor sufficient to infer causality as defined by the term's everyday usage. For example, a deterministic causal process creates pseudorandom numbers; yet, we observe no correlation between the numbers. Child height correlates with spelling ability because age causes both. Moreover, order is problematicwe hear train whistles before observing trains, yet trains cause whistles. Scientific methods specifically prohibit inferring causal theories from specific observations (i.e., effects) because, in part, many credible causes are perfectly consistent with available observations. Moreover, actions inferred from effects have more unintended consequences than actions based on sound deductive causal theories because causal theories predict multiple effects. However, an often overlooked but key feature of these theories is that we describe the cause with more variables than the effect. Consequently, inductive processes might appear deductive as the number of effects increases relative to the number of potential causes. For example, in real criminal trials, jurors judge whether sufficient evidence exists to infer guilt. In contrast, determining guilt in criminal mystery novels is deductive because the number of clues (i.e., effects) is large relative to the number of potential suspects (i.e., causes). We can make inferential tasks resemble deductive tasks by increasing the number of effects (i.e., variables) relative to the number of potential causes and seeking a shared cause for all observed effects. Moreover, under some conditions, the method of seeking shared causes might approach deductive reasoning when the number of causes is strictly limited. At least, the resulting number of possible causal theories is far less than the number generated from repeated observations of a single effect (i.e., variable).", "e:keyword": ["Causality", "Common cause", "Shared causes", "Deduction", "Inference", "Scientific method", "Structural equations", "Scientific method"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0254", "e:abstract": "When faced with a choice of selecting one of several available products (or possibly buying nothing), according to standard theoretical perspectives, people will choose the option with the highest costbenefit difference. However, we propose that decisions about free (zero price) products differ, in that people do not simply subtract costs from benefits but instead they perceive the benefits associated with free products as higher. We test this proposal by contrasting demand for two products across conditions that maintain the price difference between the goods, but vary the prices such that the cheaper good in the set is priced at either a low positive or zero price. In contrast with a standard costbenefit perspective, in the zero-price condition, dramatically more participants choose the cheaper option, whereas dramatically fewer participants choose the more expensive option. Thus, people appear to act as if zero pricing of a good not only decreases its cost, but also adds to its benefits. After documenting this basic effect, we propose and test several psychological antecedents of the effect, including social norms, mapping difficulty, and affect. Affect emerges as the most likely account for the effect.", "e:keyword": ["Free", "Zero", "Affect", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0286", "e:abstract": "Product quality is probably undervalued by firms because there is little consensus about appropriate measures and methods to research quality. We suggest that published ratings of a product's quality are a valid source of quality information with important strategic and financial impact. We test this thesis by an event analysis of abnormal returns to stock prices of firms whose new products are evaluated in . Quality has a strong immediate effect on abnormal returns, which is substantially higher than that for other marketing events assessed in prior studies. Moreover, there are some important asymmetries in the effect. We discuss the research, managerial, investing, and policy implications.", "e:keyword": ["Stock returns", "Quality", "Published reviews", "New products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0289", "e:abstract": "When an improvable durable good (such as packaged software) saturates the market, the seller could be tempted to release new versions too frequently, hurting her profit. A novel contractual device, which I term as a Free New Version Rights warranty (free NVR warranty), can help the seller overcome this temptation. In a two-period game-theoretic model involving a monopolist firm facing heterogeneous consumers, I derive conditions under which a rational monopolist can act suboptimally: She could face a commitment problem and offer the new version, even if doing so lowers her overall profit. Profit is hurt because when consumers expect a new version, (a) fewer consumers buy the initial version, and (b) the monopolist is forced to charge a lower price for the initial version. I show how the free NVR warranty, which requires the monopolist to offer consumers the right to receive the new version for free for a limited period, can solve her commitment problem. This is a new, surprising finding: By bundling new-version rights with the initial version, the monopolist at first appears to be denying herself future revenue. I derive conditions under which this apparently unprofitable action is optimal, which is my main contribution. When free NVR is offered, consumer surplus decreases and social surplus increases. This work extends prior literature on durable goods and the Coase conjecture to innovative durable goods with network externalities. The findings have important practical implications for firms selling new versions of innovative durable goods subject to network effects, as well as for their consumers.", "e:keyword": ["High-tech marketing", "Game theory", "Innovation", "Marketing strategy", "Product development", "Product life cycles", "Product policy", "Signaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0296", "e:abstract": "This paper examines optimal product positioning strategies of asymmetric firms in the context of retail outlet locations in the fast food industry. The relationships between profits and product differentiation reveal that both McDonald's and Burger King are better off avoiding close competition if the market area is large enough. However, in small market areas, McDonald's would prefer to be located together with Burger King rather than have the two outlets be only a slight distance apart. In contrast, Burger King's profits always increase with greater differentiation. Offsetting these incentives is the desirability of locating centrally to appeal to the most customers. The equilibrium depends on the market's size. In small markets, McDonald's locates near the center of the market, and Burger King locates to the side of the market. In larger markets, McDonald's and Burger King choose locations on opposite sides of the market, although McDonald's locates closer to the optimal central location than Burger King. I also show that the role of price competition on product positioning is fundamentally different under asymmetric competition than under symmetric competition. Price competition unambiguously induces symmetric firms toward differentiation. In contrast, with asymmetric firms, price competition shifts Burger King's incentives toward locating closer to McDonald's, even while price competition increases McDonald's desire for differentiation. As a result, equilibrium locations with asymmetric firms are approximately the same, regardless of whether prices adjust with location.", "e:keyword": ["Product positioning", "Pricing research", "Geographic competition", "Fast food"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0291", "e:abstract": "We propose and test a new approach for modeling consumer heterogeneity in conjoint estimation based on convex optimization and statistical machine learning. We develop methods both for metric and choice data. Like hierarchical Bayes (HB), our methods shrink individual-level partworth estimates towards a population mean. However, while HB samples from a posterior distribution that is influenced by exogenous parameters (the parameters of the second-stage priors), we minimize a convex loss function that depends only on endogenous parameters. As a result, the amounts of shrinkage differ between the two approaches, leading to different estimation accuracies. In our comparisons, based on simulations as well as empirical data sets, the new approach overall outperforms standard HB (i.e., with relatively diffuse second-stage priors) both with metric and choice data.", "e:keyword": ["Bayesian analysis", "Data mining", "Econometric models", "Estimation and other statistical techniques", "Hierarchical Bayes analysis", "Marketing research", "Regression and other statistical techniques"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0272", "e:abstract": "Consumer perceptions of advertising creativity are investigated in a series of studies beginning with scale development and ending with comprehensive model testing. Results demonstrate that perceptions of ad creativity are determined by the interaction between divergence and relevance, and that overall creativity mediates their effects on consumer processing and response.", "e:keyword": ["Creativity", "Divergence", "Advertising", "Relevance", "Measurement", "Latent variable models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0298", "e:abstract": "We offer a simple model of intertemporal choice to characterize how planned versus actual behaviors evolve for time-inconsistent smokers. Our results suggest that smokers' participation and cessation decisions are governed by the interplay between three effects. The leads smokers to advance their plans to quit smoking, whereas the leads them to consecutively revise their planned quitting age upwards. Consequently, the duration of smoking is effectively governed by which one of these two effects is dominant. Finally, for certain consumer segments, a causes an all or nothing type of extreme smoking behavior based on certain critical values of present-biased preferences. Our results provide some preliminary evidence that both marketing efforts by tobacco firms and public policy initiatives can have a significant influence on smoking behavior. In particular, we find that reductions in the age at which individuals start smoking may not only vastly extend their duration of smoking, but also convert potential never smokers into lifetime smokers. Finally, we estimate a hazard model using survey data from over 800 smokers to provide evidence in support of our theoretical model.", "e:keyword": ["Rational addiction", "Time-inconsistent preferences", "Smoking cessation", "Hyperbolic discounting", "Hazard models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0244", "e:abstract": "In most marketing experiments, managerial decisions are not based directly on the estimates of the parameters but rather on functions of these estimates. For example, many managerial decisions are driven by whether or not a feature is valued more than the price the consumer will be asked to pay. In other cases, some managerial decisions are weighed more heavily than others. The standard measures used to evaluate experimental designs (e.g., -efficiency or -efficiency) do not accommodate these phenomena. We propose alternative managerial efficiency criteria (-errors) that are relatively easy to implement. We explore their properties, suggest practical algorithms to decrease errors, and provide illustrative examples. Realistic examples suggest improvements of as much as 30% in managerial efficiency. We close by considering approximations for nonlinear criteria and extensions to choice-based experiments.", "e:keyword": ["Conjoint analysis", "Experimental design", "Product development", "Efficiency"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0269", "e:abstract": "This paper analyzes optimal selling strategies of a monopolist facing forward-looking patient unit-demand bidders in a sequential auction market. Such a seller faces a fundamental choice between two selling regimes: selling that involves learning about remaining demand from early prices, and selling that forgoes such learning and makes all selling decisions in the beginning of the game. A model of the game between the seller and the bidders is proposed to characterize the optimal regime choice. The model implies that the relative profitability of the two regimes depends on the expected gains from trade: when the expected gains from trade are low, commitment dominates adaptation, and vice versa.", "e:keyword": ["Auctions", "Game theory", "Durable-goods monopoly", "Optimal selling", "Commitment", "Adaptive dynamic pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0259", "e:abstract": "IIA (independence of irrelevant alternatives) axiom states that the ratio of choice probabilities of any two brands will depend only on the utilities of the brands. However, even if the utilities of brands are assumed to be fixed, their choice probabilities will be affected by the similarity between them. This study and several other previous studies show that a more preferred (higher utility) brand benefits more in a high similarity situation than a less preferred (lower utility) brand, which is called the asymmetric similarity effect or simply in this study. This study expands on the asymmetric effect that has been reported by many previous empirical studies and implied in choice modeling literature, by giving it an explicit mathematical formulation based on the analysis of the elimination-by-tree (EBT) model (Tversky and Sattath 1979). This study also provides an integrative theoretical summary showing how the asymmetric effect is related to the similarity effect, dominance effect, and IIA condition.", "e:keyword": ["Brand choice model", "Asymmetric effect", "Elimination-by-tree model", "Similarity effect", "IIA"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0242", "e:abstract": "Cross-brand pass-through implies that a retailer responds to wholesale promotional support from a target brand by changing the retail prices of competitive brands. Besanko et al. (2005) model a target brand's retail price as a function of its own and other brands' wholesale prices using 780 observations (15 price zones  52 weeks) and take the many significant coefficients for other brands' wholesale prices that they find as evidence of cross-brand pass-through. Because price zones do not react independently to wholesale prices when they set a brand's retail price, Besanko et al.'s (2005) estimation overstates the number of independent observations by a factor of 15. When we correct for this overstatement of independent observations, we find that the number of stable, significant coefficients for other brands' wholesale prices is lower than one would expect by chance. We conclude that there is no evidence of cross-brand pass-through in the 11 categories analyzed by Besanko et al. (2005).", "e:keyword": ["Pricing", "Promotion", "Retailing and wholesaling", "Channels of distribution", "Econometric models", "Dominick's", "Packaged goods", "Laundry detergent", "Price zones"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0344", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0337", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0346", "e:abstract": ", in collaboration with the Informs Society on Marketing Science (ISMS), announces a new type of authored submissionthe database report. will consider the publication of submitted databases. The same standards will apply to these detached databases as for the data employed in submitted manuscripts.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0343", "e:abstract": "This special classics edition of presents seven of our early, most highly rated articles according to the Social Sciences Citation Index (developed by the Institute of Scientific Information), published before widespread library subscriptions to and before formal and systematic inclusion in the SSCI index. This issue includes Thaler (1985), Guadagni and Little (1983), Jeuland and Shugan (1983), Anderson (1985), Hauser and Shugan (1983), McGuire and Staelin (1983), and Pasternack (1985). Among other benefits, these articles should now enjoy widespread library access, inclusion in numerous databases, availability in electronically searchable format, and formal inclusion in citation services. This introduction to the special issue also summarizes some findings from our all-time most highly cited articles in Google Scholar: Thaler (1985), Anderson and Sullivan (1993), Anderson and Weitz (1989), Guadagni and Little (1983), Griffin and Hauser (1993), Jeuland and Shugan (1983), Novak et al. (2000), Bolton (1998), Anderson (1985), and Pasternack (1985). Finally, it summarizes articles excelling on citations per year: Acquisti and Varian (2005) and Lynch and Ariely (2000). The best is yet to come.", "e:keyword": ["Classic articles", "Greatest hits", "Most-cited articles", "Highest impact", "Mental accounting", "Scanner data", "Channel strategy", "Defensive strategy", "Transaction cost analysis", "Competitive channel strategy", "Return policies"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0348", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0330", "e:abstract": "A new model of consumer behavior is developed using a hybrid of cognitive psychology and microeconomics. The development of the model starts with the mental coding of combinations of gains and losses using the prospect theory value function. Then the evaluation of purchases is modeled using the new concept of transaction utility. The household budgeting process is also incorporated to complete the characterization of mental accounting. Several implications to marketing, particularly in the area of pricing, are developed. This article was originally published in , Volume 4, Issue 3, pages 199214, in 1985.", "e:keyword": ["Mental accounting", "Consumer choice", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0345", "e:abstract": "Guadagni and Little (1983) had a surprising (to the authors) number of citations, presumably because it was the first paper to build a useful model with UPC scanner panel data. More surprising (but not to the authors) was that the model, compared to those in most academic papers, found widespread practical application. The reason for this was that a small, entrepreneurial consulting firm developed and sold applications based on the model. The paper also generated a substantial follow-on of academic literature. Examples illustrate a few of the directions in which later research went.", "e:keyword": ["Brand choice", "Multinomial logit", "UPC data", "Marketing science practice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0331", "e:abstract": "A multinomial logit model of brand choice, calibrated on 32 weeks of purchases of regular ground coffee by 100 households, shows high statistical significance for the explanatory variables of brand loyalty, size loyalty, presence/absence of store promotion, regular shelf price and promotional price cut. The model is parsimonious in that the coefficients of these variables are modeled to be the same for all coffee brand-sizes. The calibrated model predicts remarkably well the share of purchases by brand-size in a hold-out sample of 100 households over the 32-week calibration period and a subsequent 20-week forecast period. The success of the model is attributed in part to the level of detail and completeness of the household panel data employed, which has been collected through optical scanning of the Universal Product Code in supermarkets. Three short-term market response measures are calculated from the model: regular (depromoted) price elasticity of share, percent increase in share for a promotion with a median price cut, and promotional price cut elasticity of share. Response varies across brand-sizes in a systematic way with large share brand-sizes showing less response in percentage terms but greater in absolute terms. On the basis of the model a quantitative picture emerges of groups of loyal customers who are relatively insensitive to marketing actions and a pool of switchers who are quite sensitive. This article was originally published in , Volume 2, Issue 3, pages 203238, in 1983.", "e:keyword": ["Choice", "Logit", "Marketing-mix", "Scanners"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0342", "e:abstract": "Channel coordination and, more generally, coordination of activities between interdependent economic agents is even more important today than when the paper was published more than 20 years ago. One reason is the trend toward globalization and outsourcing caused, in part, by the development of increasingly complex products and services that integrate many different competencies. Coordination today involves all the aspects of the marketing mix: product design coordination, price and service coordination (the focus of Managing Channel Profits), coordination of availability in highly hybrid distribution systems to better reach an increasingly complex and fragmented market, and coordination of communication with the target market. Managing Channel Profits argues a tendency toward a lack of coordination without explicit institutional arrangements and coordination mechanisms such as quantity discounts and contracts to solve the problem. The price and service coordination ideas of Managing Channel Profits have been successfully applied for 25 years of developments in marketing of revenue- and cost-sharing arrangements between increasingly interdependent market participants.", "e:keyword": ["Coordination", "Cost sharing", "Revenue sharing", "Distribution channels", "Supply chain management", "Pricing", "Channel strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0332", "e:abstract": "A channel of distribution consists of different channel members each having his own decision variables. However, each channel member's decisions do affect the other channel members' profits and, as a consequence, actions. A lack of coordination of these decisions can lead to undesirable consequences. For example, in the simple manufacturer-retailer-consumer channel, uncoordinated and independent channel members' decisions over margins result in a higher price paid by the consumer than if those decisions were coordinated. In addition, the ensuing suboptimal volume leads to lower profits for both the manufacturer and the retailer. This paper explores the problems inherent in channel coordination. We address the following questions. What is the effect of channel coordination? What causes a lack of coordination in the channel? How difficult is it to achieve channel coordination? What mechanisms exist which can achieve channel coordination? What are the strengths and weaknesses of these mechanism? What is the role of nonprice variables (e.g., manufacturer advertising, retailer shelf-space) in coordination? Does the lack of coordination affect normative implications from in-store experimentation? Can quantity discounts be a coordination mechanism? Are some marketing practices actually disguised quantity discounts? We review the literature and present a simple formulation illustrating the roots of the coordination problem. We then derive the form of the quantity discount schedule that results in optimum channel profits. This article was originally published in , Volume 2, Issue 3, pages 239272, in 1983.", "e:keyword": ["Distribution", "Profitability", "Margins", "Coordination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0333", "e:abstract": "This descriptive study explores the reasons for integration of the personal selling function, i.e., the use of employee (direct) salespeople rather than manufacturers' representatives (reps). A hypothesized model is developed based on both transaction cost analysis and the sales force management literature. Data from 13 electronic component manufacturers covering 159 U.S. sales districts are used to estimate a logistic model of the probability of going direct in a given district. Results are shown to be stable across specification and estimation methods and to fit the data well. The transaction cost model is generally supported. The principal finding is that the greater the difficulty of evaluating a salesperson's performance, the more likely the firm to substitute surveillance for commission as a control mechanism, i.e., to use a direct sales force. Among other findings, direct sales forces are also associated with complex, hard-to-learn product lines and with districts that demand considerable nonselling activities. Several factors prove unrelated, including company size, the amount of travel a district requires, and the importance of key accounts. This article was originally published in , Volume 4, Issue 3, Pages 234254, in 1985.", "e:keyword": ["Organizational design", "Organization control", "Sales force management", "Vertical integration"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0341", "e:abstract": "For every new product and service entrant, there are usually many incumbents who must defend their positions in the market. Hence, defensive strategy is as least as critical as new-product strategy. Our 1983 article argued that defensive strategy critically depends on the distribution of buyer preferences and the position of the new entrant relative to the position of the incumbent in a multidimensional attribute space. Since the appearance of our 1983 article in , research in defensive strategy has progressed in both prescriptive and descriptive directions. Subsequent research on defensive strategy has also addressed empirical, methodological, theoretical, and substantive issues. Today, defensive strategy is more important than ever, with shorter new-product life cycles, persistent service innovation, remarkable technological change, global competition, and the invention of new channels of distributions.", "e:keyword": ["Marketing theory", "Strategy", "Defensive strategy", "Competitive strategy", "Heterogeneous buyer preferences", "Product positioning", "Multidimensional perceptual spaces", "New-product strategy", "Pricing", "Advertising", "Distribution decisions", "Marketing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0334", "e:abstract": "This paper analyzes how a firm should adjust its marketing expenditures and its price to defend its position in an existing market from attack by a competitive new product. Our focus is to provide usable managerial recommendations on the strategy of response. In particular we show that if products can be represented by their position in a multiattribute space, consumers are heterogeneous and maximize utility, and awareness advertising and distribution can be summarized by response functions, then for the profit maximizing firm: - it is optimal to decrease awareness advertising, - it is optimal to decrease the distribution budget unless the new product can be kept out of the market, - a price increase may be optimal, and - even under the optimal strategy, profits decrease as a result of the competitive new product. Furthermore, if the consumer tastes are uniformly distributed across the spectrum - a price decrease increases defensive profits, - it is optimal (at the margin) to improve product quality in the direction of the defending product's strength and - it is optimal (at the margin) to reposition by advertising in the same direction. In addition we provide practical procedures to estimate (1) the distribution of consumer tastes and (2) the position of the new product in perceptual space from sales data and knowledge of the percent of consumers who are aware of the new product and find it available. Competitive diagnostics, such as the angle of attack, are introduced to help the defending manager. This article was originally published in , Volume 2, Issue 4, pages 319360, in 1983.", "e:keyword": ["Competition", "Pricing", "Product entry", "Defensive marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0340", "e:abstract": "The goal of this brief note is to provide the reader with an appreciation of how this paper was initially conceived, to state what I think are the paper's key insights, and to outline a few of the many challenges and opportunities that still exist in studying channel management issues.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0335", "e:abstract": "This paper investigates the effect of product substitutability on Nash equilibrium distribution structures in a duopoly where each manufacturer distributes its goods through a single exclusive retailer, which may be either a franchised outlet or a factory store. Static linear demand and cost functions are assumed, and a number of rules about players' expectations of competitors' behavior are examined. It is found that for most specifications product substitutability does influence the equilibrium distribution structure. For low degrees of substitutability, each manufacturer will distribute its product through a company store; for more highly competitive goods, manufacturers will be more likely to use a decentralized distribution system. This article was originally published in , Volume 2, Issue 2, pages 161191, in 1983.", "e:keyword": ["Channel management", "Distribution", "Vertical integration", "Industry analysis", "Game", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0347", "e:abstract": "The paper discusses the genesis of the 1985 paper Optimal Pricing and Return Policies for Perishable Commodities, as well as the critical ideas presented in that research. A brief review of the literature that cited and expanded the results of the paper is also presented.", "e:keyword": ["Channel coordination", "Single-period inventory model", "Newsboy problem", "Revenue sharing", "Supply chain", "Perishable commodities"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0336", "e:abstract": "This paper considers the pricing decision faced by a producer of a commodity with a short shelf or demand life. A hierarchical model is developed, and the results of the single period inventory model are used to examine possible pricing and return policies. The paper shows that several such policies currently in effect are suboptimal. These include those where the manufacturer offers retailers full credit for all unsold goods or where no returns of unsold goods are permitted. The paper also demonstrates that a policy whereby a manufacturer offers retailers full credit for a partial return of goods may achieve channel coordination, but that the optimal return allowance will be a function of retailer demand. Therefore, such a policy cannot be optimal in a multi-retailer environment. It is proven, however, that a pricing and return policy in which a manufacturer offers retailers a partial credit for all unsold goods can achieve channel coordination in a multi-retailer environment. This article was originally published in , Volume 4, Issue 2, pages 166176, in 1985.", "e:keyword": ["Distribution", "Coordination", "Channel", "Newsboy problem", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0282", "e:abstract": "The authors test a value chain model entailing a progression of influence from retail employee job perceptions -> retail employee job performances -> customer evaluations -> customer spending and comparable store sales growth. The authors test the model using three matched samples of 1,615 retail employees, 57,656 customers, and 306 stores of a single retail chain. The authors find that three retail employee job perceptions (conscientiousness, perceived organizational justice, and organizational identification) have main and interactive effects on three dimensions of employee job performance (in-role performance, extra-role performance toward customers, and extra-role performance toward the organization). In turn, these performance dimensions exert influence on customer evaluations of the retailer (a satisfaction, purchase intent, loyalty, and word-of-mouth composite). The authors also show that employee perceptions exert a direct influence on customer evaluations, and that customer evaluations affect retail store performance (customer spending and comparable store sales growth). Finally, the authors conduct some simple simulations that show: (1) how changes in employee perceptions may raise average employee performances; (2) how changes in employee performances enhance average customer evaluations; and (3) how changes in customer evaluations raise average customer spending and comparable store sales growth. The authors then show that employee job perceptions and performances ripple thru the system to affect customer spending and store sales growth. The authors offer implications for theory and practice.", "e:keyword": ["Retail value chain", "Customer service employees", "Customer satisfaction", "Customer spending", "Sales growth"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0300", "e:abstract": "Firms frequently position themselves as specialists. An implication of specialization is that the firm has forgone alternative opportunities. In the context of effort-intensive categories, we show that a firm can signal quality to consumers by specializing. In the model, a firm must decide to provide one service offering or to market two services. By entering a single category, the firm incurs an opportunity cost of not entering the secondary profitable category, but may attain reduced costs. The net cost is the signaling cost that a high-quality type firm incurs to signal quality over a low-quality type firm. We show that in homogenous markets, a high-quality type firm signals its high-quality type by specializing in one category. When consumers are heterogeneous, the firm can signal its high-quality type by using prices alone in both the primary and secondary categories. However, specialization can be used as a secondary signal of quality in heterogeneous markets because of lower signaling costs. We also find that signaling using specialization is more likely in the presence of competition.", "e:keyword": ["Specialization", "Signaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0294", "e:abstract": "This research models the dynamics of customer relationships using typical transaction data. Our proposed model permits not only capturing the dynamics of customer relationships, but also incorporating the effect of the sequence of customer-firm encounters on the dynamics of customer relationships and the subsequent buying behavior. Our approach to modeling relationship dynamics is structurally different from existing approaches. Specifically, we construct and estimate a nonhomogeneous hidden Markov model to model the transitions among latent relationship states and effects on buying behavior. In the proposed model, the transitions between the states are a function of time-varying covariates such as customer-firm encounters that could have an enduring impact by shifting the customer to a different (unobservable) relationship state. The proposed model enables marketers to dynamically segment their customer base and to examine methods by which the firm can alter long-term buying behavior. We use a hierarchical Bayes approach to capture the unobserved heterogeneity across customers. We calibrate the model in the context of alumni relations using a longitudinal gift-giving data set. Using the proposed model, we probabilistically classify the alumni base into three relationship states and estimate the effect of alumni-university interactions, such as reunions, on the movement of alumni between these states. Additionally, we demonstrate improved prediction ability on a hold-out sample.", "e:keyword": ["Customer relationship management", "Hidden Markov models", "Dynamic choice models", "Segmentation", "Bayesian analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0284", "e:abstract": "The authors dedicate this paper in honor of the memory of their deceased co-author, Erin Anderson. In dyadic business relationships, parties can be incorrect in reading their counterparts' relational closeness. For example, they can overestimate or underestimate the counterpart's commitment to their relationship. In the business-to-business (B2B) literature, the consequences of such inaccurate perceptions have not been empirically investigated. We advance and test the proposition that the impact of misreading the other party's relational closeness depends on the direction of the error. We propose that overestimating the counterpart's relational closeness (CRC) is beneficial, while underestimating the counterpart's relational closeness is detrimental for the relationship's functioning. Using original dyadic data in the service sector, we show that most companies underestimate their CRC, in which case becoming perceptually more accurate would improve their relationships. But the opposite holds for parties that overestimate their CRC, in which case becoming perceptually more accurate would actually make the relationship deteriorate. Furthermore, we show that even in long-standing relationships, companies do not know how accurate their perceptions are, even when they believe that they correctly perceive their CRC. We discuss managerial implications of our findings and encourage future research to determine why most decision makers underestimate their CRC, which can lead to impaired functioning of B2B relationships.", "e:keyword": ["Business-to-business marketing", "Organizational research", "Channels of distribution", "Services marketing", "Key informant approach", "Measurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0299", "e:abstract": "This paper advances the literature on multicategory demand models by simultaneously handling of the household. We propose a model of and outcomes in multiple categories. Our results show that cross-category promotional spillovers are asymmetric between the two product categories of bacon and eggs. The total retail profit responds more to bacon price than to egg price. Promoting bacon is found to have a bigger impact on egg profit than the impact of egg promotion on bacon profit. We decompose (1) the total retail profits, as well as (2) the cross-category profit impact of a price promotion, into its two components, and find that (1) 23% (67%) of the total retail profit impact of a promotion on bacon (eggs) arises on account of quantity effects, and (2) 40% (33%) of the increase in egg (bacon) profit from promoting bacon (eggs) is on account of quantity effects.", "e:keyword": ["Multicategory", "Multivariate choices", "Bivariate logit", "Incidence", "Quantity", "Basket data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0295", "e:abstract": "Existing evidence suggests that preferences are affected by whether a price is presented as one all-inclusive expense or partitioned into a set of mandatory charges. To explain this phenomenon, we introduce a new mechanism whereby price partitioning affects a consumer's perception of the secondary (i.e., nonfocal) benefits derived from a transaction. Four experiments support the hypothesis that a partitioned price increases the amount of attention paid to secondary attributes tagged with distinct price components. Characteristics of the offered secondary attributes such as their perceived value, relative importance, and evaluability can therefore determine whether price partitioning stimulates or hinders demand. Beyond its descriptive and prescriptive implications, this theory contributes to the emerging notion that pricing can transform, as well as capture, the utility of an offer.", "e:keyword": ["Consumer behavior", "Pricing", "Price partitioning", "Attention", "Information processing", "Framing effects", "Multi-attribute utility"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0285", "e:abstract": "We study two main questions in this paper: (1) How do spillovers of knowledge created by manufacturers' investments in process innovation affect channel structure and effort investment incentives? (2) What are the interactions between organizational incentives to form joint ventures and strategic alliances with competitors, and coordinate decisions vertically with downstream channel members? We focus on situations where spillovers are involuntary, firms' innovative activities are nonoverlapping, and firms benefit directly from the results of competitors' innovations. Under these conditions, we find that spillovers in process knowledge increase the likelihood of observing decentralized channel structures. Surprisingly, decentralized manufacturers invest more in process innovation than perfectly coordinated manufacturers do when spillovers are large. Moreover, in industries where large spillovers exist, horizontal cooperation among manufacturers induces higher levels of process innovation investments than channel coordination does. From a public policy perspective, however, the desirability of such cooperative arrangements among competitors depends on channel structure: joint ventures among decentralized manufacturers are more likely to meet the regulators' criteria of raising effort investments than cooperation among integrated manufacturers would be. Investment incentives are best provided when firms share their process knowledge and are buffered from subsequent price competition by independent retailers.", "e:keyword": ["Channel coordination", "Process innovation", "Spillovers", "Research joint ventures"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0297", "e:abstract": "Registrars' offices at most universities face the daunting task of allocating course seats to students. Because demand exceeds supply for many courses, course allocation needs to be done equitably and efficiently. Many schools use bidding systems in which student bids are used both to infer preferences over courses and to determine student priorities for courses. However, this dual role of bids can result in course allocations not being market outcomes, and in unnecessary efficiency loss, which can potentially be avoided with the use of an appropriate market mechanism. We report the result of field and laboratory studies that compare a typical course-bidding mechanism with the alternate Gale-Shapley Pareto-dominant market mechanism. Results from the field study (conducted at the Ross School of Business, University of Michigan) suggest that using the latter could vastly improve efficiency of course allocation systems while facilitating market outcomes. Laboratory experiments with greater design control confirm the superior efficiency of the Gale-Shapley mechanism. The paper tests theory that has important practical implications because it has the potential to affect the learning experience of very large numbers of students enrolled in educational institutions.", "e:keyword": ["Auction", "Bidding", "Consumer behavior", "Experimental economics", "Field experiment", "Indivisible goods", "Market design", "Matching", "Mechanism design"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0288", "e:abstract": "We use a unique data set to estimate the price sensitivities of households in online and offline shopping channels when the same households shop across channels. We observe households that shop interchangeably at the online and the offline stores in the same grocery chain and investigate their purchase behavior in specific product categories. Although nearly 90% of households in our sample shop both at online and offline stores, we find that, across 12 vastly different product categories, these households exhibit lower price sensitivities when they shop online than when they shop offline. Our analysis accounts for observed and unobserved household heterogeneity as well as price endogeneity. The results hold for large basket-share categories and small basket-share categories, for consumer packaged goods and nonpackaged goods, for categories that are more likely to be purchased online because of their bulkiness or heaviness, and for categories that are more likely to be purchased offline because of their sensory nature. Households' price sensitivities are also closely related to demographics and inversely related to how far the households are located from the offline stores. Reasons for the lower price sensitivities in the online medium are discussed.", "e:keyword": ["Price sensitivity", "Internet", "Scanner panel data", "Logit demand model", "Endogeneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0301", "e:abstract": "This article introduces a newly discovered property of discrete-choice models, which I call the invariant proportion of substitution (IPS). Like the independence from irrelevant alternatives (IIA) property, IPS implies individual behavior that is counterintuitive in the context of choice among similar alternatives. But models that alleviate the concerns raised by IIA, such as generalized extreme value and covariance probit models, do not necessarily alleviate the concerns raised by IPS. I explore the implications of the IPS property on individual behavior in several choice contexts and discuss some models that alleviate the concerns raised by IPS.", "e:keyword": ["Econometric models", "Choice models", "Logit", "Probit", "Generalized extreme value", "Independence from irrelevant alternatives", "Individual choice behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0387", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0373", "e:abstract": "We investigate the sensitivity of cross-brand pass-through estimates to two types of pooling: across stores, and across regular price and promotional price weeks. Using the category data from Besanko, Dub, and Gupta (2005), hereafter BDG, we find consistent support across all 11 categories for the predictive power of the wholesale prices of substitute products for retail shelf prices. A Bayesian procedure is used to address the small sample issues that arise in the absence of pooling. Even though the unpooled results render our inferences for specific cross-brand pass-through magnitudes reported in BDG as imprecise, consistent with McAlister (2007), we find significant empirical support for cross-brand pass-through. We next assess the sensitivity of cross-brand pass-through estimates to pooling. This requires us to construct a much longer time series of 224 weeks for the refrigerated orange juice category, in contrast with the 52-week samples used in BDG and McAlister (2007). We find strong empirical support for the predictive power of wholesale prices of substitute products for retail shelf prices. In addition, we find evidence of nonzero own- and cross-brand pass-through elasticities for which our inferences are much more precise. These findings are robust to the separation of regular and promotional price weeks. However, the magnitudes of own-brand and cross-brand pass-through are quite different during promotional and regular price weeks. Our results clearly show that with longer data series and more robust models that can handle small sample sizes, there is evidence of cross-brand pass-through, substantiating the findings in BDG. Finally, we comment on why our results are entirely consistent with both the theoretical and empirical literatures on category pricing and retailer behavior.", "e:keyword": ["Pricing", "Promotion", "Retailing", "Channels of distribution", "Econometric models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0310", "e:abstract": "Prior behavioral research has suggested that advertising can influence a consumer's quality evaluation through informative and transformative effects. The informative effect acts directly to inform a consumer of product attributes and hence shapes her evaluations of brand quality. The transformative effect affects the consumer's evaluation of brand quality by enhancing her assessment of her subsequent consumption experience. In addition, advertising may influence a consumer's utility directly, even without providing any explicit informationthis is the persuasive effect. In this paper, we propose a framework that formally models the processes through which all three effects of advertisements impact consumers' brand evaluations and their subsequent brand choice decisions. In particular, we model source credibility, confirmatory bias, and bounded rationality on the part of consumers, by appropriately modifying the standard Bayesian learning approach. Our model conforms closely to prior behavioral literature and the experimental findings therein. In our empirical analysis, we get significant estimates of both informative and transformative effects across brands. We find interesting temporal patterns across the effects; for instance, the importance of transformative effects seem to grow over time, while that of informative effects diminishes. Finally, we conduct policy experiments to examine the impact of increased ad intensity on advertising effects, as well as the role played by consumption ambiguity.", "e:keyword": ["Advertising effects", "Informative effects", "Persuasive effects", "Structural models", "Consumer learning", "Policy experiments", "Bounded rationality", "Confirmatory bias"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0303", "e:abstract": "For marketers, television remains the most important advertising medium. This paper proposes a two-sided model of the television industry. We estimate viewer demand for programs on one side and advertiser demand for audiences on the other. The primary objective is to understand how each group's program usage influences the other group. Four main conclusions emerge. First, viewers tend to be averse to advertising. When a highly rated network decreases its advertising time by 10%, our model predicts a median audience gain of about 25% (assuming no competitive reactions). Second, we find the price elasticity of advertising demand is -2.9, substantially more price elastic than 30 years ago. Third, we compare our estimates of advertiser and viewer preferences for program characteristics to networks' observed program choices. Our results suggest that advertiser preferences influence network choices more strongly than viewer preferences. Viewers' two most preferred program genres, Action and News, account for just 16% of network program hours. Advertisers' two most preferred genres, Reality and Comedy, account for 47% of network program hours. Fourth, we perform a counterfactual experiment in which some viewers gain access to a hypothetical advertisement avoidance technology. The results suggest that ad avoidance tends to increase equilibrium advertising quantities and decrease network revenues.", "e:keyword": ["Advertising", "Broadcasting", "Demand estimation", "Empirical industrial organization", "Endogeneity", "Entertainment marketing", "Media", "Television", "Two-sided markets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0302", "e:abstract": "How can a standardized product survive in a mass-customized world? This requires understanding that consumers often experience problems predicting their future hedonic reactions to new experiences (such as custom products), leading to feelings of regret. This form of regret occurs not because the custom product differs from specifications, but because consumers miswanted the design they ordered. Our analytic model shows that regret aversion induces consumers to design custom products to reflect the attributes of the available standard products. Consequently, regret-averse consumers may choose the standard product rather than place a custom order. The number of available standard products, however, moderates both these effects. Two experiments empirically substantiate the key predictions of the analytical model: (a) the custom product's resemblance to the standard product grows with regret aversion associated with miswanting, (b) there exists a segment of regretfully loyal consumers for the standard product in a mass-customized world and it expands with regret aversion, (c) both the above effects are weakened by the presence of a second standard product, and (d) the custom product can increase its market share when the number of standard products increases.", "e:keyword": ["Customization", "Standard goods", "Preference uncertainty", "Regret"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0311", "e:abstract": "Switching marketing channels is an expensive and sticky decision. While a number of theories suggest efficiency and strategic differences between channels, there is virtually no work on combining these ideas into an empirically workable methodology to assess the impact of a channel switch. In this study, we undertake to close this gap with an empirical study of the sports drink market, featuring competing producers and heterogeneous channels. We estimate demand and cost parameters for a number of alternative models of competitive interaction and use these estimates to study the switching of Gatorade from its extant (independent wholesaler) channel to the direct store delivery (DSD) channel belonging to Pepsi. Our initial results indicate the following: Pepsi should switch Gatorade to the DSD channel (i) the switch decreases Gatorade's manufacturing cost by at least 14%, (ii) the switch increases the share of profit it can obtain by at least 13%, (iii) the switch enhances demand by the equivalent of a price cut of 4.96 for a 32-ounces package. Absent these increases, Pepsi should . Our methodology and results speak to both managers contemplating a channel switch and antitrust authorities faced with the task of evaluating the consequences of a change in vertical structure.", "e:keyword": ["Channel structure", "New empirical industrial organization", "Oligopoly competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0305", "e:abstract": "There is substantial literature documenting the presence of state-dependent utility with packaged goods data. Typically, a form of brand loyalty is detected whereby there is a higher probability of purchasing the same brand as has been purchased in the recent past. The economic significance of the measured loyalty remains an open question. We consider the category pricing problem and demonstrate that the presence of loyalty materially affects optimal pricing. The prices of higher quality products decline relative to those of lower quality when loyalty is introduced into the model. Given the well-known problems with the confounding of state dependence and consumer heterogeneity, loyalty must be measured in a model which allows for an unknown and possibly highly nonnormal distribution of heterogeneity. We implement a highly flexible model of heterogeneity using multivariate mixtures of normals in a hierarchical choice model. We use an Euler equations approach to the solution of the dynamic pricing problem which allows us to consider a very large number of consumer types.", "e:keyword": ["Dynamic pricing", "Loyalty", "State dependence", "Consumer heterogeneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0361", "e:abstract": "This paper investigates the impact of research productivity on the salaries of marketing faculty members. We examine how the number of articles published in various types of publications affects faculty members' nine-month salary using a sample of three years of information on 298 marketing professors from 33 research-oriented, public universities. Consistent with research conducted in other disciplines, we find a positive impact of publishing on salary. We estimate the individual salary impact for each of the four top-tier journals in marketing, finding that the biggest impact on salary comes from publishing in . Further, we find that publishing in Tier 1 journals in marketing has a bigger salary impact than in Tier 1 journals outside marketing. Finally, we find that faculty and department characteristics also affect salary. For example, being from a higher ranked research university and being a full professor are each associated with higher salary.", "e:keyword": ["Research", "Financial impact", "Salary", "Publication"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0306", "e:abstract": "In their purchase decisions, online customers seek to improve decision quality while limiting search efforts. In practice, many merchants have understood the importance of helping customers in the decision-making process and provide online decision aids to their visitors. In this paper, we show how preference models which are common in conjoint analysis can be leveraged to design a questionnaire-based decision aid that elicits customers' preferences based on simple demographics, product usage, and self-reported preference questions. Such a system can offer relevant recommendations quickly and with minimal customer input. We compare three algorithmscluster classification, Bayesian treed regression, and stepwise componential regressionto develop an optimal sequence of questions and predict online visitors' preferences. In an empirical study, stepwise componential regression, relying on many fewer and easier-to-answer questions, achieved predictive accuracy equivalent to a traditional conjoint approach.", "e:keyword": ["Conjoint analysis", "Recommender system", "Online decision aid", "Efficiency"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0304", "e:abstract": "This paper studies a model in which consumers search among multiple competing firms for products that match their preferences at a reasonable price. We focus on how easier search, possibly due to the adoption of search-facilitating technologies such as the Internet, influences equilibrium prices, assortments, firm profits, and consumer welfare. Conventional wisdom suggests that easier search creates a competition-intensifying effect that puts pressure on firms to lower their prices and reduce assortments. However, in our model we demonstrate that search also exhibits a market-expansion effect that encourages firms to expand their assortmenteasier search means that each firm is searched by more consumers. Because of broader assortments, consumers are more likely to find products that better match their ideal preferences, improving the efficiency of the market. In fact, we demonstrate that the market-expansion effect can even dominate the competition-intensifying effect potentially leading to higher prices, broader assortments, more profits, and expanded welfare.", "e:keyword": ["Search", "Internet", "Price competition", "Assortment", "Product variety", "Long-tail phenomenon", "Game theory", "Differentiated competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1060.0262", "e:abstract": "Firms often carry a portfolio of multiple brands within a product category to target different quality tiers in the market. Furthermore, to satisfy heterogeneous consumer preferences within each quality tier, these firms also offer several variants for each brand. A natural outcome of this practice is interbrand variant overlap that could occur across tiers or within a tier. In this paper, we show that across-tier variant overlap is likely to diminish the preference of an upper-tier brand and enhance the preference of a lower-tier brand. We also find that variant overlap within a tier is likely to increase preferences of a brand belonging to the tier. Such variant overlap effects have important brand portfolio management implications for a multibrand firm. Specifically, we demonstrate that such a firm can enhance its portfolio profit under certain conditions by pruning its variants to reduce variant overlap. Because our paper relies on aggregate data, future research should investigate variant overlap at the individual level using panel or experimental data.", "e:keyword": ["Brand portfolio management", "Variant overlap", "Product line pruning", "Multibrand firm", "Bayesian"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0323", "e:abstract": "We analyze a large-scale field test conducted with a mail-order catalog firm to investigate how customers react to premium prices for larger sizes of women's apparel. We find that customers who demand large sizes react unfavorably to paying a higher price than customers for small sizes. Further investigation suggests that these consumers perceive that the price premium is unfair. Overall, premium pricing led to a 6% to 8% decrease in gross profits.", "e:keyword": ["Product line pricing", "Price discrimination", "Fairness", "Price promotion", "Experimental economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0290", "e:abstract": "This paper designs an optimal mechanism for selling a set of commonly ranked objects. Although buyers rank these objects in the same order, the rates at which their valuations change for a less-preferred object might be different. Four stylized cases are identified according to this difference: parallel, convergent, divergent, and convergent-then-divergent. In general, the optimal mechanism cannot be interpreted as a conventional second-price auction. A reserve price is imposed for each object. Depending on which of the four stylized cases is considered, a higher-value bidder may be allocated a higher-ranked or lower-ranked object. There is also a positive probability that a higher-ranked object is not allocated while a lower-ranked one allocated. In a departure from the extant mechanism-design literature, the individual-rationality constraint for a mid-range type of bidder can be binding.", "e:keyword": ["Slot allocation", "Optimal mechanism", "Common ranking", "Auction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0312", "e:abstract": "When demand is uncertain, manufacturers and retailers often have private information on future demand, and such information asymmetry impacts strategic interaction in distribution channels. In this paper, we investigate a channel consisting of a manufacturer and a downstream retailer facing a product market characterized by short product life, uncertain demand, and price rigidity. Assuming the firms have asymmetric information about the demand volatility, we examine the potential benefits of sharing information and contracts that facilitate such cooperation. We conclude that under a wholesale price regime, information sharing might not improve channel profits when the retailer underestimates the demand volatility but the manufacturer does not. Although information sharing is always beneficial under a two-part tariff regime, it is in general not sufficient to achieve sharing, and additional contractual arrangements are necessary. The contract types we consider to facilitate sharing are profit sharing and buyback contracts.", "e:keyword": ["Channels of distribution", "Decisions under uncertainty", "Game theory", "Supply chains"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0292", "e:abstract": "One of the most controversial findings in Morgan and Rego (2006) was that two widely advocated loyalty metrics, Net Promoter and Number of Recommendations, have little or no value in predicting the financial outcomes of firms. We argue that neither measure was actually examined and that conclusions about the predictive value of these measures cannot be drawn from their analysis. A primary problem is that the measures used in Morgan and Rego (2006) do not adequately adjust for the presence of neutral word-of-mouth activity. Nevertheless, Morgan and Rego (2006) provide important information regarding other common customer metrics and firm financial outcomes. We are unaware of another longitudinal study that examines the predictive value of satisfaction and loyalty metrics in such a comprehensive way.", "e:keyword": ["Net Promoter", "Word-of-mouth", "Recommendations", "Financial performance", "Intentions", "Customer satisfaction", "Customer loyalty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0375", "e:abstract": "We examine the ability of the Net Promoter of Morgan and Rego (2006) measure constructed using behavioral word-of-mouth (WOM) data to provide insights into the Net Promoter<sup></sup> customer loyalty concept popularized by Reichheld (2003), which is indicated by a score constructed using attitudinal intention-to-recommend data. We show that despite differences in data and operationalization, the two measures are very closely correlated and behave remarkably similarly when examined relative to a third related variable, customer satisfaction.", "e:keyword": ["Word-of-mouth", "Customer loyalty", "Marketing strategy", "Net promoter", "Recommendations", "Customer satisfaction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0453", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0372", "e:abstract": "We introduce the work of the finalists in the 2006 ISMS Practice Prize Competition, representing outstanding examples of rigor plus relevance in our profession. The winner, describing a collaboration between J.D. Power and Associates and U.C. Riverside, involves a sequenced program of research to understand the effect of promotional activity in the U.S. automobile market. The other three finalists address a model to calibrate the effects of corporate brand activity across three divisions at Allstate Insurance; a method for using CLV models to focus customer contact strategies at IBM; and a segmentation strategy to protect and gain customers in the face of a new entrant for the Austrian mobile telecommunications carrier, tele.ring.", "e:keyword": ["Sales promotions", "Choice models", "Brand equity", "Customer lifetime value", "Perceptual mapping practice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0374", "e:abstract": "We develop a consumer response model to evaluate and plan pricing and promotions in durable-good markets. We discuss its implementation in the U.S. automotive industry, which spends about $45 billion each year in price promotions. The approach is based on a random effects multinomial nested logit model of product (e.g., a vehicle model, such as Hyundai Tucson), and transaction-type choice. Transaction types include combinations of acquisition types (e.g., purchase versus lease) and pricing instruments (cash rebates, reduced APR financing, lease payment discounts). We estimate the model using hierarchical Bayes methods to capture response heterogeneity at the local market level. We find key characteristics unique to durable-good markets. First, consumers are heterogeneous in both their brand and transaction-type preferences. Second, consumers differ in their overall price sensitivity as well as in their relative sensitivity to alternative pricing instruments (e.g., cash discounts, reduced monthly payments). Third, the most effective pricing programs tend to be those in which automakers offer consumers a menu of options to choose from (e.g., a choice among a cash discount, reduced interest rate financing, or a lease payment discount). We illustrate the model through an empirical application to a sample of data drawn from J.D. Power transaction records in the entry SUV segment and discuss examples of actual implementations.", "e:keyword": ["Choice models", "Nested logit", "Random coefficients", "Pricing", "Promotions", "Automobiles", "Hierarchical Bayes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0320", "e:abstract": "We develop a robust model for estimating, tracking, and managing brand equity for multicategory brands based on customer survey and financial measures. This model has two components: (1) offering value (computed from discounted cash flow analysis) and (2) relative brand importance (computed from brand choice models such as multinomial logit, heteroscedastic extreme value, and mixed logit). We apply this model to estimate the brand equity of Allstatea leading insurance companyand its leading competitor, which compete in multiple categories. The model captures the brand's spillover effects from one category to another. In addition, we identify the dimensions that drive a brand's image, examine the relationships among advertising, brand equity, and shareholder value, and build a decision support simulator for the focal brand. Our model provides reliable estimates of brand equity, and our results show that advertising has a strong long-term positive influence on brand equity, which is significantly positively related to shareholder value. The model, the brand equity estimates, and the decision support simulator are used by key executives across multiple functional areas and have enabled the company to substantially gain by reallocating its advertising resources to improve brand equity and shareholder value, and by offering better guidance to analysts and investors.", "e:keyword": ["Brand equity", "Intangible assets", "Brand management", "Choice model", "Advertising", "Shareholder value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0319", "e:abstract": "Customer management activities at firms involve making consistent decisions over time, about: (a) which customers to select for targeting, (b) determining the level of resources to be allocated to the selected customers, and (c) selecting customers to be nurtured to increase future profitability. Measurement of customer profitability and a deep understanding of the link between firm actions and customer profitability are critical for ensuring the success of the above decisions. We present the case study of how IBM used customer lifetime value (CLV) as an indicator of customer profitability and allocated marketing resources based on CLV. CLV was used as a criterion for determining the level of marketing contacts through direct mail, telesales, e-mail, and catalogs for each customer. In a pilot study implemented for about 35,000 customers, this approach led to reallocation of resources for about 14% of the customers as compared to the allocation rules used previously (which were based on past spending history). The CLV-based resource reallocation led to an increase in revenue of about $20 million (a tenfold increase) without any changes in the level of marketing investment. Overall, the successful implementation of the CLV-based approach resulted in increased productivity from marketing investments. We also discuss the organizational and implementation challenges that surrounded the adoption of CLV in this firm.", "e:keyword": ["Customer relationship management", "Customer lifetime value", "Field experiment", "Return on marketing contacts", "Missing value imputation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0307", "e:abstract": "Tele.ring is a mobile phone organization selling contracts and cell phones in the Austrian market. The market situation in 2005 was highly competitive and dynamic, resulting in relatively short tariff life cycles. Excessively long lead times made tele.ring's management feel dissatisfied with their new tariff development process. Furthermore, a new competitor had entered the market, posing a major threat, and it was unclear how to effectively safeguard tele.ring's position in the market. In cooperation with the management, we implemented and tested a new segmentation, targeting, and positioning tool, which provides managers with information on their target markets, customer preferences, competitors' strengths, and customer segments. It allows for the simultaneous visualization of these data on a single map and facilitates timely and accurate decision making. In particular, we report on the design and the implementation of a new pricing scheme, Formel 10, which became the most successful new tariff introduction in this competitive market. tele.ring's managers were very much impressed with our tool's ability to represent the market on a single map and with its capacity to allow for intuitive interpretation. In addition, the tool enhanced internal communication between its users and different stakeholders during the new tariff development process.", "e:keyword": ["Segmentation", "Targeting", "Positioning", "Preference mapping", "Pricing", "Mobile phone market"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0313", "e:abstract": "In recent years, the issue of copyright protection for intellectual properties such as computer software, music CDs, and videos has become increasingly important. It is often claimed that illegal copying of intellectual property costs companies billions of dollars in lost revenues and reduces firms' incentives to innovate. Some researchers have shown that copying can be beneficial to firms when there are strong network effects and copying expands the market. In this paper, we first examine the impact of illegal copying of software and other similar intellectual properties on firms' prices, profits, and quality choices, even when there are no network effects and the market is saturated. We show that contrary to the claims of manufacturers, there are conditions under which copying can increase firms' profits, lead to better quality products, and increase social welfare. This is because weaker copyright protection enables firms to reduce price competition by allowing price-sensitive consumers to copy. Thus, copyright protection can serve as a coordination device to reduce price competition. We also examine how equilibrium copyright enforcement is affected by network externalities. In contrast to previous research, we show that strong network effects can sometimes lead to a firm choosing higher levels of copyright protection. Our results show that in the presence of strong network effects, copyright enforcement by one firm can serve as a coordinating device to reduce price competition.", "e:keyword": ["Piracy", "Pricing", "Innovation", "Network effects", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0315", "e:abstract": "The motion picture industry is characterized by a dynamic market environment, limited shelf space and product category management, and consequently, complex channel contracts specifying the split of box office revenue between distributors and exhibitors. Although such a contracting practice creates a considerable administrative effort and channel conflict, it is not clear whether such complexity is necessary for superior channel performance. This study investigates this question by analyzing the impact of movie contract structure on movie scheduling and channel member profitability. We develop and analyze a game-theoretic model using the genetic algorithm approach and a decision support system, , to capture strategic behaviors of channel members in a complex market environment. We find that simpler two-part tariff or 50/50 split contracts perform as well as the current contracts. Thus, the complexity of the market environment need not be reflected in the complexity of the channel contracts. Channel contract structure has significant impact on channel member profitability and the exhibitor's movie-scheduling behavior. In particular, our results indicate that the flat rate contract structure represents an attractive alternative to the current practice for distributors.", "e:keyword": ["Channel contracts", "Movie industry", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0316", "e:abstract": "While retailers have sales data to forecast demand, manufacturers have a broad understanding of the market and the coming trends. It is well known that pooling such demand information within a distribution channel improves supply chain logistics. However, little is known about how information-sharing affects wholesale pricing incentives. In this paper, we investigate a channel structure where a manufacturer and two retailers have private signals of the state of the demand. Our model identifies the presence of a pricing distortion, which we term the , when a manufacturer sets price to an uninformed retailer. Because of this inference effect, the manufacturer would like to set a low wholesale price to signal to the retailer that the demand is low. On the other hand, the manufacturer would like to set a high wholesale price so that he earns the optimal margin on each unit sold. Vertical information sharing benefits the manufacturer by eliminating the distortion caused by the inference effect, which is more profound in a channel whose retailer has a noisier signal. This result implies that when there is a cost associated with transmitting information, the manufacturer may choose to share information with only the less-informed retailer rather than with both.", "e:keyword": ["Channels of distribution", "Information sharing", "Retailing", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0317", "e:abstract": "Firms often use a pool or series of advertising themes in their campaigns. Thus, for example, a firm may employ some of its advertising to promote price-related themes or messages and other of its advertising to promote product-related themes. This study examines the interdependence that can occur between pairs of themes in a pool (i.e., ), the impact of these pooling effects on the allocation of advertising expenditures, and the factors that can affect forgetting rates (or, conversely, carry-over rates) in a multitheme advertising environment. The study measures pooling, wear out, and forgetting (carry-over) effects for a campaign that uses five different advertising themes. To obtain these measures, I extend the linear Nerlove-Arrow (NA) (1962) model to a nonlinear model of advertising theme quality and goodwill and estimate the extended model using Markov chain Monte Carlo (MCMC) and particle filtering ideas. Particle filtering belongs to a class of sequential Monte Carlo (SMC) methods designed to estimate nonlinear/nonnormal state space models. Results show that forgetting (or carry-over) rates may be time varying and a function of prior goodwill (past advertising) and other advertising variables. Results show, moreover, that pooling effects can reduce theme wear out and, in turn, significantly improve advertising efficiency.", "e:keyword": ["Nonlinear state space model", "Particle filtering/smoothing", "Sequential Monte Carlo", "Sequential importance resampling", "Markov chain Monte Carlo", "Metropolis hastings", "Aggregate advertising models", "Pooling effects", "Forgetting effects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0318", "e:abstract": "This paper defines a unique type of product or service offering, termed , and analyzes a novel selling strategy, termed (PS). A probabilistic good is not a concrete product or service but an offer involving a probability of getting any one of a set of multiple distinct items. Under the probabilistic selling strategy, a multi-item seller creates probabilistic goods using the existing distinct products or services and offers such probabilistic goods as additional purchase choices. The probabilistic selling strategy allows sellers to benefit from introducing a new type of buyer uncertainty, i.e., uncertainty in product assignments. First, introducing such uncertainty enables sellers to create a virtual product or service (i.e., probabilistic good), which opens up a creative way to segment a market. We find that the probabilistic selling strategy is a general marketing tool that has the potential to benefit sellers in many different industries. Second, this paper shows that creating buyer uncertainty in product assignments is a new way for sellers to deal with their own market uncertainty. We illustrate two such benefits: (a) offering probabilistic goods can reduce the seller's information disadvantage and lessen the negative effect of demand uncertainty on profit, and (b) offering probabilistic goods can solve the mismatch between capacity and demand and enhance efficiency. Emerging technology is creating exciting (previously unfeasible) opportunities to implement PS and to obtain these many advantages.", "e:keyword": ["Probabilistic selling", "Probabilistic goods", "Opaque goods", "Pricing", "Product differentiation", "e-commerce", "Yield management", "Inventory management", "Product line", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0321", "e:abstract": "Estimates of the prices customers are willing to pay for new products or services using responses from survey questionnaires are notoriously biased on the high side. An approach to obtaining more realistic estimates is suggested here, called the exaggeration bias-corrected contingent valuation method (EBC-CVM). The method is an alternative to conventional contingent valuation methods (CVMs) that have been used in economics and, to a lesser extent, in marketing. Two experiments and one field study are presented to demonstrate the effectiveness of the method. In each case, the proposed method outperformed conventional CVMs in comparison with real choices or more realistic price estimates.", "e:keyword": ["Willingness to pay", "Contingent valuation method", "Exaggeration bias", "New product pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0324", "e:abstract": "Existing literature discusses a number of possible pioneering cost advantages and disadvantages. In this paper, we empirically test three different sources of long-term pioneering cost advantage, and and three different sources of pioneering cost disadvantage, , and . We disentangle these sources by breaking total cost of a business unit into three different componentspurchasing, production, and selling, general, and administrative (SG&A) costsand identifying conditions that intensify or reduce the effect of the proposed source. Using two samples of business units, one for consumer goods and one for industrial goods, we find support for five of the six sources of pioneering cost advantage and disadvantage in both samples, while the advantage due to preemption of ideal market space is limited to the consumer goods sample. The unconditional analysis shows a pioneering purchasing cost but even larger pioneering production and SG&A cost . The complexity of our obtained findings suggests that managers need to think carefully about their particular conditions before making assumptions about the cost and, therefore, profit implications of a pioneering strategy.", "e:keyword": ["Pioneering", "Cost effects", "Preemption", "Supply chain management", "IV-estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0325", "e:abstract": "We evaluate whether stock prices can predict the sales takeoff and the long-term survival of firms at takeoff. We find that abnormal returns are strongly positive in the year prior to takeoff, thus suggesting an important signal of the takeoff. Moreover, we find that negative abnormal returns in the year of takeoff and one year after takeoff increase the hazard of market exit by 9.5 times relative to firms without these negative abnormal returns. We discuss the implications of these findings for managers and researchers.", "e:keyword": ["Sales takeoff", "Stock market forecast", "Event study", "New product research", "Product life cycle", "Forecasting", "Product management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0326", "e:abstract": "Co-branding is often used by companies to reinforce the image of their brands. In this paper, we investigate the conditions under which a brand's image is reinforced or impaired as a result of co-branding, and the characteristics of a good partner for a firm considering co-branding for image reinforcement. We address these issues by conceptualizing attribute beliefs as two-dimensional constructs: The first dimension reflects the expected value of the attribute, while the second dimension reflects the degree of certainty about the attribute. We argue that these parameters are updated after consumers are exposed to a co-branding activity, and we develop an analytical model that incorporates these notions. An analysis of the model leads to several propositions, which we test in an experiment. Our findings indicate that it is not necessarily in a brand's best interest to choose an alliance partner that is of the highest performance possible. Moreover, we find that, while expected values of the brand attributes may improve as a result of co-branding, under certain conditions the uncertainty associated with the brands increases through the alliance. Implications for co-branding researchers and practitioners are discussed.", "e:keyword": ["Brand alliances", "Spillover effects", "Co-branding", "Image reinforcement", "Brand positioning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0450", "e:abstract": "This paper describes a new data set available to academic researchers (at the following website: http://mktsci.pubs.informs.org. These data are comprised of store sales and consumer panel data for 30 product categories. The store sales data contain 5 years of product sales, pricing, and promotion data for all items sold in 47 U.S. markets. In two U.S. markets, the store level data are supplemented with panel-level purchase data and cover the entire population of stores. Further information is available regarding store characteristics in these markets. We address several potential applications of these data, as well as the access protocol. The data set described in this paper is maintained by IRI. Any fees charged by IRI for the distribution of the data set will be used for the continual maintenance and updating of the data. Scholarships to cover IRI's fees (for those who need it) are available through the INFORMS Society for Marketing Science (ISMS). Please see the website above for further details.", "e:keyword": ["Price", "Promotion", "Product", "Distribution", "Sales"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0425", "e:abstract": "A contract with -class pricing divides a large set of goods or services into classes and assigns a single price to any element of a class. While the literature has emphasized rationales based on screening, this paper looks at the implications of costly pricing. My analysis suggests that class pricing is more likely to be used when the number of buyers is smaller, the number of versions is larger, the variance in costs is smaller, and demand ex ante differs less between versions. Under simple conditions classes should be designed to minimize the sum of squared within-class cost deviations. In bilateral trades, the most efficient game form is that in which classes are designed by the player with fewer varied gains from trade, while the traded version is chosen by the other player. Decisions are thus made by the player who cares most about them, while the opponent prescribes a set of limits.", "e:keyword": ["Pricing", "Microeconomics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0349", "e:abstract": "We model the commercial World Wide Web as a directed graph that emerges as the equilibrium of a game in which utility maximizing websites purchase (advertising) in-links from each other while also setting the price of these links. In equilibrium, higher content sites tend to purchase more advertising links (mirroring the Dorfman-Steiner rule) while selling less advertising links themselves. As such, there seems to be specialization across sites in revenue models: high content sites tend to earn revenue from the sales of content, whereas low content ones earn revenue from the sales of traffic (advertising). In an extension, we also allow sites to establish (reference) out-links to each other and find that there is a general tendency to establish reference links to sites with higher content. Finally, we explore network formation in the presence of search engines and find that the higher the proportion of people using them, the more sites have an incentive to specialize in certain content areas. Our results have interesting practical implications for search-engine optimization, the pricing of online advertising, and the choice of Internet business models. They also shed light on why Google can use the web's link structure to rank sites by content.", "e:keyword": ["Internet advertising", "Game theory", "Network formation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0314", "e:abstract": "Critics have long faulted the wide-spread practice of trade promotions as wasteful. It has been estimated that this practice adds up to $100 billion worth of inventory to the distribution system. Yet, the practice continues. In this paper, we propose a price discrimination model of trade promotions. We show that in a distribution channel characterized by a dominant retailer, a manufacturer has incentives to price discriminate between the dominant retailer and smaller independents. While offering all retailers the same pricing policy, price discrimination can be implemented through trade promotions because they induce different inventory-ordering behaviors on the part of retailers. Differences in inventory holding costs have been shown to be an important determinant of consumer promotions. Our analysis suggests that differences in holding costs are also potentially an important driver for the use of trade promotions. The implications from our model explain a number of anecdotal and/or empirically observed puzzles about how trade promotions are practiced. For example, our analysis explains why chain stores welcome trade promotions but independents do not. Our analysis outlines implications for managing trade promotions.", "e:keyword": ["Channels of distribution", "Channel power", "Trade promotion", "Forward buying"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0368", "e:abstract": "There is strong evidence that consumers learn from their brand consumption experiences and continuously update their brand purchase probabilities. However, in explaining why established competitive brands continuously compete through periodic price promotions, most of the existing theories do not attribute this phenomenon to underlying consumer learning. Yet, try it, you will like it is often the stated rationale of the practicing marketing manager in offering consumers a price promotion on a brand. This paper examines the connection between consumer learning and the offering of price promotions. The consumer model specified is Markovian in nature and encompasses in addition to consumer learning other empirical findings such as the increase in product class consumption in response to price reductions. The consumer models used in the existing game theoretic approaches to this problem are shown to be mostly special cases of the proposed model. It is demonstrated that for the commonly used price response functions the existence of consumer learning, at a level of intensity consistent with that identified in empirical works, makes it optimal for competing brands to periodically offer price promotions. Moreover, it is shown that the competing brands should promote in different periods as opposed to head to head.", "e:keyword": ["Price promotions", "Consumer learning", "Competition", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0398", "e:abstract": "Most supermarket firms choose to position themselves by offering either everyday low prices (EDLP) across several items or offering temporary price reductions (promotions) on a limited range of items. While this choice has been addressed from a theoretical perspective in both the marketing and economic literature, relatively little is known about how these decisions are made in practice, especially within a competitive environment. This paper exploits a unique store level data set consisting of every supermarket operating in the United States in 1998. For each of these stores, we observe the pricing strategy the firm has chosen to follow, as reported by the firm itself. Using a system of simultaneous discrete choice models, we estimate each store's choice of pricing strategy as a static discrete game of incomplete information. In contrast to the predictions of the theoretical literature, we find strong evidence that firms cluster by strategy by choosing actions that agree with those of its rivals. We also find a significant impact of various demographic and store/chain characteristics, providing some qualified support for several specific predictions from marketing theory.", "e:keyword": ["EDLP", "Promotional pricing", "Positioning strategies", "Supermarkets", "Discrete games"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0328", "e:abstract": "Two widely recognized components, central to the calculation of customer value, are acquisition and retention propensities. However, while extant research has incorporated such components into different types of models, limited work has investigated the kinds of associations that may exist between them. In this research, we focus on the relationship between a prospective customer's time until acquisition of a particular service and the subsequent duration for which he retains it, and examine the implications of this relationship on the value of prospects and customers. To accomplish these tasks, we use a bivariate timing model to capture the relationship between acquisition and retention. Using a split-hazard model, we link the acquisition and retention processes in two distinct yet complementary ways. First, we use the Sarmonov family of bivariate distributions to allow for correlations in the observed acquisition and retention times ; next, we allow for differences customers using latent classes for the parameters that govern the two processes. We then demonstrate how the proposed methodology can be used to calculate the discounted expected value of a subscription based on the time of acquisition, and discuss possible applications of the modeling framework to problems such as customer targeting and resource allocation.", "e:keyword": ["Customer acquisition", "Customer retention", "Customer relationship management", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0329", "e:abstract": "The authors study the takeoff of 16 new products across 31 countries (430 categories) to analyze how and why takeoff varies across products and countries. They test the effect of 12 hypothesized drivers of takeoff using a parametric hazard model. The authors find that the average time to takeoff varies substantially between developed and developing countries, between work and fun products, across cultural clusters, and over calendar time. Products take off fastest in Japan and Norway, followed by other Nordic countries, the United States, and some countries of Midwestern Europe. Takeoff is driven by culture and wealth plus product class, product vintage, and prior takeoff. Most importantly, time to takeoff is shortening over time and takeoff is converging across countries. The authors discuss the implications of these findings.", "e:keyword": ["Diffusion of innovations", "Global marketing", "Consumer innovativeness", "Marketing metrics", "New products", "Product takeoff", "Product life cycles"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0351", "e:abstract": "With $40 billion in annual gross merchandise volume, electronic auctions comprise a substantial and growing sector of the retail economy. Using unique data on Celtic coins, we estimate a structural model of buyer and seller behavior via Markov chain Monte Carlo (MCMC) with data augmentation. Results indicate that buyer valuations are affected by item, seller, and auction characteristics; buyer costs are affected by bidding behavior; and seller costs are affected by item characteristics and the number of listings. The model enables us to compute fee elasticities even though there is no variation in fees in our data. We find that commission elasticities exceed per item fee elasticities because they target high-value sellers and enhance their likelihood of listing. By targeting commission reductions to high-value sellers, auction house revenues can be increased by 3.9%. Computing customer value, we find that attrition of the largest seller would decrease fees paid to the auction house by $97. Given the seller paid $127 in fees, competitive effects offset only 24% of those fees. In contrast, competition offsets 81% of the buyer attrition effect. In both events, the auction house would overvalue its customers by neglecting competitive effects.", "e:keyword": ["Auctions", "Structural models", "Two-sided markets", "Empirical IO", "Bayesian statistics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0353", "e:abstract": "Multinational corporations (MNCs) often pursue global strategies that emphasize efficiency, flexibility, and learning, but globally developed strategies often clash with the environmental idiosyncrasies of MNC country subsidiary markets in which the strategy is actually implemented. Extant research pays little attention to the contingent efficacy of such global strategies from the perspective of MNC country subsidiary markets. We adopt the strategy-environment alignment principle and study how host country task- and institutional environments might influence the efficacy of global strategies for MNC subsidiary performance. We assess MNC subsidiary performance using subjective managerial judgments, and on the basis of recent research on human judgments, we theorize that these judgments embody information about judgment magnitude and uncertainty. A mean-variance function model simultaneously teases out the effects of the explanatory variables on the magnitude and uncertainty of MNC subsidiary performance judgments. To test the hypotheses, we analyze survey data from German and Japanese subsidiaries in the United States. The results support the use of the mean-variance function model and specific theory about the antecedents of performance judgment magnitude and uncertainty. Findings pertaining to interactions between global strategies and the facets of the local country environment reveal ways in which MNCs can adapt global strategies to navigate the complex array of country markets they face.", "e:keyword": ["Multinational corporation", "Global strategies", "Environment", "Judgment uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0352", "e:abstract": "In several marketing contexts, strategic complementarity between the actions of individual players demands that players coordinate their decisions to reach efficient outcomes. Yet coordination failure is a common occurrence. We show that the well-established psychological phenomenon of asymmetric dominance can facilitate coordination in two experiments. Thus, we demonstrate a counterintuitive result: A common bias in individual decision making can help players to coordinate their decisions to obtain efficient outcomes. Further, limited steps of thinking alone cannot account for the observed asymmetric dominance effect. The effect appears to be due to increased psychological attractiveness of the dominating strategy, with our estimates of the incremental attractiveness ranging from 3%6%. A learning analysis further clarifies that asymmetric dominance and adaptive learning can guide players to an efficient outcome.", "e:keyword": ["Strategic decision making", "Asymmetric dominance effect", "Bounded rationality", "Coordination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0327", "e:abstract": "Brand saliencethe extent to which a brand visually stands out from its competitorsis vital in competing on the shelf, yet is not easy to achieve in practice. This study proposes a methodology to determine the competitive salience of brands, based on a model of visual search and eye-movement recordings collected during a brand search experiment. We estimate brand salience at the point of purchase, based on perceptual features (color, luminance, edges) and how these are influenced by consumers' search goals. We show that the salience of brands has a pervasive effect on search performance, and is determined by two key components: The bottom-up component is due to in-store activity and package design. The top-down component is due to out-of-store marketing activities such as advertising. We show that about one-third of salience on the shelf is due to out-of-store and two-thirds due to in-store marketing. The proposed methodology for competitive salience analysis exposes the optimal visual differentiation level of a brand versus its competitors, and of each SKU versus the other SKUs of the same brand. The model of the visual search process and methodology for competitive salience analysis enable diagnostic analyses of the current levels of visual differentiation of brands and SKUs at the point of purchase, and provide directions for increasing these.", "e:keyword": ["Search goals", "Eye movements", "Hidden Markov", "Brand salience", "Visual attention"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0350", "e:abstract": "Reference groups influence product and brand evaluations, especially when the product is a publicly consumed luxury good. Marketers of such luxury goods need to carefully balance two important social forces: (1) the desire of leaders to distinguish themselves from followers and (2) the countervailing desire of followers to assimilate with leaders. In this paper, we examine the theoretical implications of these social forces for firm prices, product design, and target consumer selection. We show that the presence of reference group effects can motivate firms to add costly features, which provide limited or no functional benefit to consumers. Furthermore, reference group effects can induce product proliferation on one hand and motivate firms to offer limited editions on the other hand. We find that offering a limited edition can increase sales and profits. In some cases, reference group effects can even lead to a buying frenzy.", "e:keyword": ["Reference groups", "Luxury goods", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0363", "e:abstract": "Online reverse auctions generate real-time bidding data that could be used via appropriate statistical estimation to assist the corporate buyer's procurement decision. To this end, we develop a method, called BidAnalyzer, which estimates dynamic bidding models and selects the most appropriate of them. Specifically, we enable model estimation by addressing the problem of ; i.e., only one of suppliers' bids is realized, and the other (-1) bids remain unobserved. To address partial observability, BidAnalyzer estimates the latent price distributions of bidders by applying the Kalman filtering theory. In addition, BidAnalyzer conducts model selection by applying multiple information criteria. Using empirical data from an automotive parts auction, we illustrate the application of BidAnalyzer by estimating several dynamic bidding models to obtain empirical insights, retaining a model for forecasting, and assessing its predictive performance in out-of-sample. The resulting one-step-ahead price forecast is accurate up to 2.95% median absolute percentage error. Finally, we suggest how BidAnalyzer can serve as a device for price discovery in online reverse auctions.", "e:keyword": ["Competitive bidding", "Electronic commerce", "Internet auctions", "Kalman filtering", "Reverse auctions", "Supplier sourcing", "e-procurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0354", "e:abstract": "The two main influences leading to adoption at the individual consumer level are marketing communication and interpersonal communication. Although evidence of the effect of these two influences is abundant at the market level, there is a paucity of research documenting the simultaneous effect of both influences at the individual consumer level. Thus, the primary objective of this paper is to fill the gap in the literature by documenting the existence and magnitude of both influences at the customer level while controlling for unobserved temporal effects. The pharmaceutical industry provides an appropriate context to study this problem. It has been conjectured that adoption and usage patterns of a new drug by physicianscontagionacts as a consumption externality, as it allows a given physician to learn about the efficacy and use of the drug. In addition, pharmaceutical companies target individual physicians via marketing activities such as detailing, sampling, and direct-to-consumer advertising. Our data contain the launch of a new drug from an important drug category. We chose two unrelated markets (Manhattan and Indianapolis) for our empirical analysis. We model an individual physician's decision to adopt a new drug in a given time period as a binary choice decision. This decision is modeled as a function of temporal trends (linear and quadratic) and individual physician-level contagion and marketing activity (both individual level and market level). Our contagion measure aggregates the adoption behavior of geographically near physicians for each physician in our sample. Our results from the Manhattan market indicate that both targeted communication and contagion have an effect on the individual physician's adoption decision. A major challenge is to rule out alternative explanations for the detected contagion effect. We therefore carry out a series of tests and show that this effect persists even after we control for the effects of time, individual salespeople, other marketing instruments, local market effects, and the effects of some institutional factors. We believe that our contagion effect arises because the consumption externality is stronger for geographically close physicians. We discuss some underlying processes that are probably giving rise to the contagion effect we detected. Finally, we compute the social multiplier of marketing and find it to be about 11%. We also use the estimated parameters to compare the relative effect of contagion and targeted marketing. We find that marketing plays a large (relative) role in affecting early adoption. However, the role of contagion dominates from month 4 onward and, by month 17 (or about half the duration of our data), asymptotes to about 90% of the effect.", "e:keyword": ["New product adoption", "Social networks", "Social interactions", "Contagion", "Word of mouth", "Hierarchical Bayesian methods", "Pharmaceutical industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0355", "e:abstract": "With advances in technology, the collection of information from consumers at the time of purchase is common in many categories. This information allows a firm to straightforwardly classify consumers as either new or past consumers. This opens the door for firms to implement marketing that (a) discriminates between new and past consumers and (b) entails making offers to them that are significantly different. Our objective is to examine the competitive effects of marketing that tailors offers to consumers based on their past buying behavior. In a two-period model with two competing firms, we assume that each firm is able to commit about whether or not to implement behavior-based discrimination (BBD), i.e., to add benefits to its offer for past consumers in the second period. When the firms are identical in their ability to add value to the second-period offer, BBD generally leads to lower profits for both firms. Past customers are so valuable in the second period that BBD leads to cutthroat competition in the first period. As a result, the payoffs associated with the implementation of BBD form a prisoner's dilemma. Interestingly, when a firm has a significant advantage over its competitor (one firm has the capability to add more benefits for its past customers than the other), it can increase its profit versus the base case even when there is significant competition in the second period. Moreover, the firm at a disadvantage sometimes finds that the best response to BBD by a strong competitor is to respond with a uniform price and avoid the practice completely.", "e:keyword": ["Dynamic games", "Price discrimination", "Customer data", "Product design"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0369", "e:abstract": "Choice-based conjoint analysis is a popular marketing research technique to learn about consumers' preferences and to make market share forecasts under various scenarios for product offerings. Managers expect these forecasts to be realistic in terms of being able to replicate market shares at some prespecified or base-case scenario. Frequently, there is a discrepancy between the recovered and base-case market share. This paper presents a Bayesian decision theoretic approach to incorporating base-case market shares into conjoint analysis via the loss function. Because defining the base-case scenario typically involves a variety of management decisions, we treat the market shares as constraints on what are acceptable answers, as opposed to informative prior information. Our approach seeks to minimize the adjustment of parameters by using additive factors from a normal distribution centered at 0, with a variance as small as possible, but such that the market share constraints are satisfied. We specify an appropriate loss function, and all estimates are formally derived via minimizing the posterior expected loss. We detail algorithms that provide posterior distributions of constrained and unconstrained parameters and quantities of interest. The methods are demonstrated using discrete choice models with simulated data and data from a commercial market research study. These studies indicate that the method recovers base-case market shares without systematically distorting the preference structure from the conjoint experiment.", "e:keyword": ["Bayesian decision theory", "Conjoint analysis", "Constrained optimization", "Cross-validation", "Hierarchical Bayes", "Loss function", "Market share prediction", "Penalized maximum likelihood", "Posterior risk"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0356", "e:abstract": "This paper studies the pricing problem of a durable-goods monopolist. It finds that contrary to the existing literature, profits from selling durable goods might be higher than from leasing when the products exhibit network effects. Under the influence of network effects, there exist multiple self-fulfilling equilibria that would sustain different network sizes at the same price. By using the assumption that consumers are cautious about network growth, we find that consumption externalities among heterogeneous groups of consumers generate a discontinuous demand function, which requires a lessor to offer a low price if she wants to reach the mass market. In contrast, a seller enjoys a relative advantage in that she can build a customer base by setting a lower initial price and raise the price later in the mass market. Our finding that selling can be more profitable than leasing holds when consumers are more cautious about the prospect of the product's success, which might be the case if, for example, the technology or manufacturer is relatively unknown.", "e:keyword": ["Selling and leasing", "Penetration pricing", "Network externality", "Introductory pricing", "Product life cycles"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1070.0357", "e:abstract": "This paper investigates the relationship between prices and market concentration in the auto rental industry. We assemble an original database that includes the number of auto rental operators and other exogenous demand and cost conditions at every commercial airport in the country. The data are interesting because we observe a large variation in market structure, ranging from more than 100 monopoly and duopoly markets to several competitive airports with more than eight firms. In addition, we collect daily rental prices in each market that are regressed against the number of operating firms and other control factors. Due to potential biases in treating market participants as exogenously assigned, we employ a two-stage estimation procedure in which an equilibrium model of endogenous market structure provides correction terms for the second-stage price regression. Results show that ignoring the endogeneity of market structure severely underestimates the impact of additional competitors on prices, with the competitive interaction parameters doubling in magnitude after the correction procedure. The downward bias in the competitive parameter can have important implications for horizontal mergers, which may incorrectly appear innocuous when using a model that ignores the endogeneity of market structure. More generally, our results serve as a warning on the potential biases to a large number of applications in marketing and economics that attempt to relate outcome variables such as prices, markups, or profits to the observed market structure.", "e:keyword": ["Pricing", "Market structure", "Entry", "Horizontal mergers", "Endogeneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0358", "e:abstract": "Which marketing strategies are most effective for introducing new brands? This paper sheds light on this question by ascribing growth performance to firms' postlaunch marketing choices. We decompose the success of a new brand into its ultimate market potential and the rate at which it achieves this potential. To achieve this aim we formulate a Bayesian dynamic linear model (DLM) of repeat purchase diffusion wherein growth and market potential are directly linked to the new brand's long-term advertising, promotion, distribution, and product strategy. We perform the analysis on 225 new-brand introductions across 22 repeat-purchase product categories over five years to develop generalized findings about the correlates of new-brand success. We find that access to distribution breadth plays the greatest role in the success of a new brand, and that investments in distribution and product innovation lead to greater marginal increases in sales for new brands than either discounting, feature/display, or advertising. Moreover, distribution interacts with other strategies to enhance their effectiveness. These findings underscore the utility of extending marketing mix models of new-brand performance to include product and distribution decisions.", "e:keyword": ["Diffusion", "New products", "Marketing mix", "Dynamic linear model", "Empirical generalization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0364", "e:abstract": "We study the dissociation between two common measures of valuemonetary assessment of purchase options versus the predicted utility associated with owning or consuming those options, a disparity that is reflected in well-known judgment anomalies and that is important for interpreting market research data. We propose that a significant cause of this dissociation is the difference in how these two types of evaluations are formedeach is informed by different types of information. Thus, dissociation between these two types of measures should not be interpreted as failure to map utility onto money, as such mapping is not really attempted. We suggest that monetary assessment tends to focus on the transaction in which the purchase alternative would be acquired or forgone (e.g., how fair the transaction seems), failing to adequately reflect the purchase alternative itself (e.g., the expected pleasure of owning or consuming it), which is what informs predicted utility judgments. We illustrate the value of this idea by deriving and testing empirical predictions of disparities in the impact of different types of information and manipulations on the two types of value assessment.", "e:keyword": ["Decision-making", "Buyer behavior", "Market research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0365", "e:abstract": "In this paper, we propose and operationalize a new method for optimizing shelf arrangements. We show that there are important dependencies between the layout of the shelf and stock-keeping unit (SKU) sales and marketing effectiveness. The importance of these dependencies is further shown by the substantive profit gains we obtain with our proposed shelf optimization approach. The basis of our model is a standard sales equation that explains sales using item-specific marketing effect parameters and intercepts. In a Hierarchical Bayes (HB) fashion, we augment this model with a second layer that relates the effect parameters to shelf and SKU descriptors. We also take into account potential endogeneity of facings. After estimating the parameters of the two-level model using Bayesian methodology, we carefully investigate the dependencies of SKU sales and SKU marketing effectiveness on the shelf layout. Next, we search for the shelf arrangement that maximizes the expected total profit using simulated annealing (SA). We appear to be able to increase profits for all the stores analyzed, and our approach appears to outperform well-known rules of thumb.", "e:keyword": ["Shelf management", "Sales models", "Hierarchical Bayes", "Markov Chain Monte Carlo", "Simulated annealing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0367", "e:abstract": "We investigate biases in product valuation and usage decisions that arise when consumers consider a new generation of a product that offers an expanded set of capabilities of uncertain value. Two experiments using a novel computer game show evidence of a : Participants display a high willingness to pay for a new version of the game that offers a new set of controls, but fail to fully exercise the option to use these controls after purchase. This discrepancy is attributed to a fundamental difference in how new capabilities are valued at the time of purchase versus use. Consumer usage decisions appear to be driven by such myopic concerns as a desire to avoid short-term learning costs, whereas purchase decisions often fail to take into account the factors that drive usage, and are further inflated by global optimism in the future usefulness of new capabilities. We show that this lack of foresight can be explained by an intertemporal judgment model in which consumers attempt to value the option to use new capabilities as would be prescribed by economic theory, but are prone to in their temporal valuation of present versus future costs and benefits.", "e:keyword": ["New product adoption", "Judgment under uncertainty", "Hyperbolic discounting", "Optimism bias", "Projection bias", "Option theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0370", "e:abstract": "When a DVD title is announced prior to actual distribution, consumers can often preorder the title and receive it as soon as it is released. Alternatively, once a title becomes available (i.e., formally released), consumers can obtain it upon purchase with minimal delay. We propose an individual-level behavioral model that captures the aggregate preorder/postrelease sales of motion picture DVDs. Our model is based on an optimal stopping framework. Starting with the utility function of a forward-looking consumer, and allowing for consumer heterogeneity, we derive the aggregate preorder/postrelease sales distribution. Even under a parsimonious specification for the heterogeneity distribution, our model recovers the typically observed temporal pattern of DVD preorder and sales, a pattern which exhibits an exponentially increasing number of preorder units before the release, peaks at release, and drops exponentially afterward. Using data provided by a major Internet DVD retailer, we demonstrate a number of important managerial implications stemming from our model. We investigate the role of preorder timing through a policy experiment, estimate residual sales, and forecast post-release sales based only on preorder information. We show that our model has substantially better predictive validity than benchmark models.", "e:keyword": ["Optimal stopping", "Timing model", "Online retailing", "Motion picture"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0362", "e:abstract": "In this paper, we develop a structural model of household behavior in an environment where there is uncertainty about brand attributes and both prices and advertising signal brand quality. Four quality signaling mechanisms are at work: (1) price signals quality, (2) advertising frequency signals quality, (3) advertising content provides direct (but noisy) information about quality, and (4) use experience provides direct (but noisy) information about quality. We estimate our proposed model using scanner panel data on ketchup. If price is important as a signal of brand quality, then frequent price promotion may have the unintended consequence of reducing brand equity. We use our estimated model to measure the importance of such effects. Our results imply that price is an important quality-signaling mechanism and that frequent price cuts can have significant adverse effects on brand equity. The role of advertising frequency in signaling quality is also significant, but it is less quantitatively important than price. In the printed version of , Vol. 27, No. 6, Erdem et al. (2008) was mistakenly identified as a Research Note. It is a regular article and has been corrected here and in the online table of contents.", "e:keyword": ["Consumer choice under uncertainty", "Bayesian learning", "Signaling", "Advertising and price as signals of quality", "Brand equity", "Pricing policy", "Dynamic choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0366", "e:abstract": "This paper develops a model that extends the traditional aggregate discrete-choice-based demand model (e.g. Berry et al. 1995) to account for varying levels of product availability. In cases where not all products are available at every consumer shopping trip, the observed market share is a convolution of two factors: consumer preferences and the availability of the product in stores. Failing to account for the varying degree of availability would produce incorrect estimates of the demand parameters. The proposed model uses information on aggregate availability to simulate the potential assortments that consumers may face in a given shopping trip. The model parameters are estimated by simulating potential product assortment vectors by drawing multivariate Bernoulli vectors consistent with the observed aggregate level of availability. The model is applied to the UK chocolate confectionery market, focusing on the convenience store channel. We compare the parameter estimates to those obtained from not accounting for varying availability and analyze some of the substantive implications.", "e:keyword": ["Structural modeling", "Availability", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0385", "e:abstract": "In mature markets with competing firms, a common role for advertising is to shift consumer preferences towards the advertiser in a tug-of-war, with no effect on category demand. In this paper, we analyze the effect of such combative advertising on market power. We show that, depending on the nature of consumer response, combative advertising can reduce price competition to benefit competing firms. However, it can also lead to a procompetitive outcome where individual firms advertise to increase their own profitability, but collectively become worse off. This is because combative advertising can intensify price competition such that an advertising war leads to a price war. Similar to price competition, advertising competition can result in a prisoner's dilemma where all competing firms make less profit even when the effect of each firm's advertising is to enhance consumer preferences in its favor. Given such procompetitive effects, we further show that cost of combative advertising could be a blessing in disguisehigher unit cost of advertising resulting in lower equilibrium levels of advertising, leading to higher prices and profits. We conduct a laboratory experiment to investigate how combative advertising by competing brands influences consumer preferences. Our experimental analysis offers strong support for our conclusions.", "e:keyword": ["Advertising", "Persuasion", "Game theory", "Competitive strategy", "Prisoner's dilemma", "Preference shifts"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0390", "e:abstract": "Media firms compete in two connected markets. They face rivalry for the sale of content to consumers, and at the same time, they compete for advertisers seeking access to the attention of these consumers. We explore the implications of such two-sided competition on the actions and source of profits of media firms. One main conclusion we reach is that media firms may charge higher content prices in a duopoly than in a monopoly. This happens because competition for advertisers can reduce the return per customer impression from the ad market, making each firm less willing to underprice content to increase demand. Greater competitive intensity may thus increase content profits and decrease ad profits. These findings are in sharp contrast to those in a regular one-sided product market, in which competition typically lowers product prices and profits. We extend the framework to examine competition across different media (e.g., between magazines and cable TV) and show that firms in a duopolistic medium may benefit from more intense competition from a monopolist in another medium. We characterize the conditions for each firm in the duopoly medium to bundle more ads and earn greater total profits than the rival firm in the monopoly medium.", "e:keyword": ["Media", "Advertising", "Two-sided markets", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0382", "e:abstract": "The Bass model has been a standard for analyzing and predicting the market penetration of new products. We demonstrate the insights to be gained and predictive performance of functional data analysis (FDA), a new class of nonparametric techniques that has shown impressive results within the statistics community, on the market penetration of 760 categories drawn from 21 products and 70 countries. We propose a new model called Functional Regression and compare its performance to several models, including the Classic Bass model, Estimated Means, Last Observation Projection, a Meta-Bass model, and an Augmented Meta-Bass model for predicting eight aspects of market penetration. Results (a) validate the logic of FDA in integrating information across categories, (b) show that Augmented Functional Regression is superior to the above models, and (c) product-specific effects are more important than country-specific effects when predicting penetration of an evolving new product.", "e:keyword": ["Predicting market penetration", "Global diffusion", "Bass model", "Functional data analysis", "Functional principal components", "Generalized additive models", "Functional clustering", "Spline regression", "New products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0371", "e:abstract": "New information technologies increasingly make it possible for service providers to adaptively personalize their service, fine-tuning the service over time for each individual customer, based on observation of that customer's behavior. We propose an Adaptive Personalization System and illustrate its implementation for digital audio players, a product category with rapidly expanding sales. The proposed system automatically downloads personalized playlists of MP3 songs into a consumer's mobile digital audio device and requires little proactive user effort (i.e., no explicit indication of preferences or ratings for songs). The system works in real time and is scalable to the massive data typically encountered in personalization applications. A simulation study shows the Adaptive Personalization System to outperform benchmark approaches. We implemented the Adaptive Personalization System on Palm PDAs and tested its performance with digital audio users. For actual users, the Adaptive Personalization System provides substantial improvements over benchmark approaches both in terms of the number of songs listened to and listening duration.", "e:keyword": ["Digital audio players", "Service marketing", "Personalization", "Customization", "Collaborative filtering", "One-to-one marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0376", "e:abstract": "We propose a structural approach to measuring brand and subbrand value using observational data. Brand value is defined as the difference in equilibrium profit between the brand in question and its counterfactual unbranded equivalent on search attributes. Our model allows us to make this computation rigorously, taking into account competitors' and retailers' reactions in the real and counterfactual situations. We illustrate our method using quarterly city-level data on ready-to-eat breakfast cereals, and compare our brand value estimates with those obtained from previously used reduced-form methods. A key advantage of our methodology is that it provides estimates of the value of brands to firmsmanufacturers and retailerstaking into account the brand's value to consumers as well as its impact on firm decisions.", "e:keyword": ["Branding", "Brand equity measurement", "New empirical industrial organization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0377", "e:abstract": "An important role of informative advertising is to inform consumers of the simple fact that the shop that advertises sells a particular product. This information may help consumers to save on their search activities: instead of wandering around, a consumer can simply visit the shop that has advertised, knowing that there he can find the commodity he is looking for. The implications of this simple fact have not been studied before. Using game theoretic reasoning in a model that combines consumer search and firms' advertising we show that firms may find it optimal to advertise prices that are higher than nonadvertised prices. The important mechanism underlying this result is that advertising lowers the expected search cost for consumers. Through this analysis we provide a new insight into the role of informative advertising.", "e:keyword": ["Consumer search", "Informative advertising", "Pricing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0380", "e:abstract": "We quantify the economic value of hardware base warranties in the U.S. server market to manufacturers, channel intermediaries, and customers. We further decompose the value of a warranty into its insurance value and its price discrimination value, which are the two main rationales for warranty provision in the server market. We use structural modeling and counterfactual experiments to accomplish the empirical task. We derive our demand model from utility maximization, which accounts for a customer's risk aversion behavior and heterogeneity. We obtain our pricing model from the profit maximization behavior of manufacturers and downstream firms in indirect channels, accounting for the institutional realities in the server market. Our empirical analysis uses quarterly data from 1999 to 2004 on server wholesale prices, retail prices, and sales for direct and indirect channels in the U.S. market. We find that manufacturers and downstream firms benefit from warranty provision and from sorting across heterogeneous customers by offering a menu of warranties. Customers also benefit from manufacturer warranty provision as well as from the menu of warranties offered. The insurance value of warranties increases and the price discrimination value of warranties decreases with warranty duration.", "e:keyword": ["Server market", "Warranty", "Insurance", "Sorting", "Price discrimination", "Structural modeling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0386", "e:abstract": "Random effects or mixed logit models are often used to model differences in consumer preferences. Data from choice experiments are needed to estimate the mean vector and the variances of the multivariate heterogeneity distribution involved. In this paper, an efficient algorithm is proposed to construct semi-Bayesian -optimal mixed logit designs that take into account the uncertainty about the mean vector of the distribution. These designs are compared to locally -optimal mixed logit designs, Bayesian and locally -optimal designs for the multinomial logit model and to nearly orthogonal designs (Sawtooth (CBC)) for a wide range of parameter values. It is found that the semi-Bayesian mixed logit designs outperform the competing designs not only in terms of estimation efficiency but also in terms of prediction accuracy. In particular, it is shown that assuming large prior values for the variance parameters for constructing semi-Bayesian mixed logit designs is most robust against the misspecification of the prior mean vector. In addition, the semi-Bayesian mixed logit designs are compared to the fully Bayesian mixed logit designs, which take also into account the uncertainty about the variances in the heterogeneity distribution and which can be constructed only using prohibitively large computing power. The differences in estimation and prediction accuracy turn out to be rather small in most cases, which indicates that the semi-Bayesian approach is currently the most appropriate one if one needs to estimate mixed logit models.", "e:keyword": ["Semi-Bayesian mixed logit design", "Fully Bayesian mixed logit design", "Heterogeneity", "Prediction accuracy", "Multinomial logit design", "Model-robust design", "-optimality", "Design construction algorithm"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0388", "e:abstract": "Two models of competition between high-end and low-end products benefiting the high-end firms are presented. One is a quantity competition model, and the other is a price competition model with product differentiation. The key factor is the existence of two heterogeneous consumer groups: those who demand only high-end (name-brand) products and those who care little whether products are high or low end. We show that, under certain conditions, the profits of firms in the high-end market are larger when there are firms producing low-end products than when there are not. The existence of price-sensitive consumers who care little about product quality intensifies competition among the high-end firms. The existence of low-end firms functions as a credible threat, which induces the high-end firms not to overproduce because price-sensitive consumers buy products from the low-end firms. The result provides a new theoretical mechanism concerning the profitability and pricing of national brand firms after the entry of private labels. It has an implication for pricing and marketing strategies: Established firms should not decrease their prices after the entry of nonestablished firms.", "e:keyword": ["Marketing strategy", "Pricing research", "Product positioning", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0389", "e:abstract": "This paper seeks to quantify the long-term financial impact of negative word of mouth (NWOM), an issue that has long challenged extant research. We do so with real-world data on firm security prices. The developed time-series models innovatively uncover (1) short- and long-term effects of NWOM on cash flows, stock returns, and stock volatilities, and (2) NWOM's wear-in effects (i.e., it takes a number of months before the stock price impact of NWOM reaches the peak point) and wear-out effects (i.e., it takes several months after the peak before the stock price impact of NWOM dies out completely). In addition, the results related to endogeneity and feedback effects from the stock market are also interesting, supporting the idea that historical underperformance in stock prices may breed more harmful future buzz in a vicious cycle of NWOM. After controlling for competition, NWOM's long-term financial harm becomes more destructive in magnitude, kicks in more quickly, and haunts investors longer. Overall, these findings offer some unique implications for buzz management, time-series models quantifying the financial impact of word of mouth, and the marketing-finance interface.", "e:keyword": ["Word of mouth", "Customer experience", "Marketing strategy", "Stock price"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0384", "e:abstract": "Innovation research tends to consider only the post-commercialization period or examine a few innovations through case studies. In this study, we examine 29 radical innovations from initial concept to mass-market commercialization. We find that these innovations were developed over an average of at least 50 years and divide this long development period into four distinct stagesconceptualization, gestation, early incubation, and late incubation. We find that the duration of a stage is longer when different firms lead product development at the beginning and end of the stage. These changes in product development leaders happen frequently, e.g., 76% of firms that were first to commercialize an innovation failed to launch it in the broader market. We also find that the time-to-takeoff for a product category is significantly related to the duration of the preceding late incubation stage. In addition, we find four different ways in which radical innovations borrow from prior seemingly unrelated innovations. We report many other findings on when (duration times), by whom (product development leaders), and how (technology borrowing) radical innovations are developed.", "e:keyword": ["Innovation", "New product research", "High-tech marketing", "Product development", "Speed-to-market", "Radical innovation", "Really new products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0381", "e:abstract": "In this paper, we extend the standard Hotelling model of product differentiation to incorporate a second dimension of consumer heterogeneity that relates to the quantity of the product consumers wish to buy. This extension allows us to derive optimal nonlinear pricing rules chosen by competing sellers when offering differentiated products in the marketplace. It also permits us to assess whether sellers find it optimal to offer quantity discounts in such a setting, and the implications of such discounts on their profitability. We find that offering quantity discounts corresponds, indeed, to equilibrium behavior. The extent of discounting declines the less differentiated the products. Surprisingly, when sellers offer to consumers a choice between two different-sized packages, their profits are, at most, as high as when such a choice is unavailable. Moreover, when utilizing nonlinear pricing rules is not feasible, the profits of the sellers actually decline when they offer consumers a choice between different-sized packages. A limited empirical investigation supports the comparative statics we derive in our theoretical model.", "e:keyword": ["Price discrimination", "Game theory", "Nonlinear pricing", "Pricing research", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0488", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0459", "e:abstract": "Virtual advisors often increase sales for those customers who find such online advice to be convenient and helpful. However, other customers take a more active role in their purchase decisions and prefer more detailed data. In general, we expect that websites are more preferred and increase sales if their characteristics (e.g., more detailed data) match customers' cognitive styles (e.g., more analytic). Morphing involves automatically matching the basic look and feel of a website, not just the content, to cognitive styles. We infer cognitive styles from clickstream data with Bayesian updating. We then balance exploration (learning how morphing affects purchase probabilities) with exploitation (maximizing short-term sales) by solving a dynamic program (partially observable Markov decision process). The solution is made feasible in real time with expected Gittins indices. We apply the Bayesian updating and dynamic programming to an experimental BT Group (formerly British Telecom) website using data from 835 priming respondents. If we had perfect information on cognitive styles, the optimal morph assignments would increase purchase intentions by 21%. When cognitive styles are partially observable, dynamic programming does almost as wellpurchase intentions can increase by almost 20%. If implemented system-wide, such increases represent approximately $80 million in additional revenue.", "e:keyword": ["Internet marketing", "Cognitive styles", "Dynamic programming", "Bayesian methods", "Clickstream analysis", "Automated marketing", "Website design", "Telecommunications"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0471", "e:abstract": "Website morphing seems to be a useful technique, with applications beyond matching cognitive style.", "e:keyword": ["Website", "Morphing", "Discussion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0475", "e:abstract": "These comments are a tribute to an impressive paper and suggestions for clarification of some fairly minor issues.", "e:keyword": ["Website morphing", "Gittins index"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0476", "e:abstract": "The article under discussion illustrates the trade-off between optimization and exploration that is fundamental to statistical experimental design. In this discussion, I suggest that the research under discussion could be made even more effective by checking the fit of the model by comparing observed data to replicated data sets simulated from the fitted model.", "e:keyword": ["Discussion", "Web", "Morphing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0485", "e:abstract": "Website morphing draws on the Expected Gittins’ solution to a partially observable Markov process, on the rapid consumer-segment updating with Bayesian methods, and on matching a website’s look and feel to a visitor’s cognitive style. In each area there are exciting research opportunities including optimality in the presence of switching costs (within a visit), Bayesian updating of cognitive styles across websites, extensions to other segmentation schemes such as cultural styles, morphing of other website characteristics such as advertising, and applications to other media such as smartphones.", "e:keyword": ["Internet marketing", "Cognitive styles", "Dynamic programming", "Bayesian methods", "Clickstream analysis", "Automated marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0391", "e:abstract": "Present-biased preferences capture the idea that individuals may find immediate payoffs significantly more salient than any future payoffs, rather than simply discounting the future in a time-consistent manner. In this paper we show that consumers' present-biased preferences can generate slippage, and we explore whether this can explain firms' use of mail-in rebates. We assume that the consumer population comprises members who have various degrees of present bias. The model demonstrates that if consumers have homogeneous willingness to pay for a product (and thus rebates do not serve as a mechanism for traditional price discrimination) rebates may still profitably exploit slippage, but to do so they must generate very high slippage rates. This is, because the rebate must greatly exceed the price markup because the rebate must compensate consumers for the cost of redemption and the delay in receiving the rebate. The ability of rebate programs to take advantage of present-biased consumers is quite limited in settings where there is significant variance in the degree of present bias within the population unless the extent of consumers' present bias is highly correlated with their rebate redemption costs.", "e:keyword": ["Present-biased preferences", "Rebates", "Slippage", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0392", "e:abstract": "Product innovation is the key revenue driver in the motion picture industry. Because major studios typically launch fewer than 20 movies per year, the financial performance of a single release can have a major effect on the studio's profitability. In this paper we study how single movie releases impact the investor valuation of the studio. We analyze the change in postlaunch stock price and predict the direction and magnitude of excess returns based on the revenue expectation built up for a movie release. That expectation is set, in part, by media support; i.e., highly advertised movies are expected to draw larger audiences than others. By using an event-study methodology, we isolate the impact of a movie launch on studio stock price and track the determinants of that change. We examine a comprehensive data set comprising over 300 movies released by the largest studios. Our results indicate a clear between the marketing support received by a movie and the direction and magnitude of its excess stock return post launch. Movies with above average prelaunch advertising have lower postlaunch stock returns than films with below average advertising. Our findings also suggest that movies that are hits at the box office may result in a lowering of stock price if they had high media support because of high performance expectations built up prior to launch. Thus prelaunch advertising plays a dual role of informing consumers about a movie's arrival as well as helping investors form expectations about the studio's profit performance.", "e:keyword": ["Advertising", "Stock market valuation", "Marketing-finance interface", "Stock return modeling", "Motion pictures"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0393", "e:abstract": "We develop a testing methodology that can be used to predict the performance of e-mail marketing campaigns in real time. We propose a split-hazard model that makes use of a time transformation (a concept we call virtual time) to allow for the estimation of straightforward parametric hazard functions and generate early predictions of an individual campaign's performance (as measured by open and click propensities). We apply this pretesting methodology to 25 e-mail campaigns and find that the method is able to produce in an hour and fifteen minutes estimates that are more accurate and more reliable than those that the traditional method (doubling time) produces after 14 hours. Other benefits of our method are that we make testing independent of the time of day and we produce meaningful confidence intervals. Thus, our methodology can be used not only for testing purposes, but also for live monitoring. The testing procedure is coupled with a formal decision theoretic framework to generate a sequential testing procedure useful for the real time evaluation of campaigns.", "e:keyword": ["Database marketing", "e-mail", "Pretesting", "Advertising campaigns"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0394", "e:abstract": "We consider a problem at the interface of auctions and bundling. Our revenue-maximizing seller seeking to auction one unit each of two complements or substitutes in the best-of-three formats: the auction of the bundle, separate auctions of the individual items, and a combinatorial auction. We draw on an analytical model to address the following questions: (i) Which of the auctioning strategies is optimal under the second-price, sealed-bid format? (ii) What is the optimal strategy for the bidders? (iii) When the objects are asymmetrically valued (e.g., Super Bowl ticket versus souvenir), what is the optimal auctioning sequence under the pure components strategy? Our results suggest that separate auctions of the two objects are superior to the auction of the bundle for most substitutes and even moderate complements when there are at least four bidders. The auction of the pure bundle is better suited for strong complements or with too few bidders. When the combinatorial auction is an available option, it weakly dominates the auction of the pure bundle but has domains of inferiority relative to the separate auctions. When the objects are asymmetric in value, it is optimal to auction the higher-valued object first.", "e:keyword": ["Auctions", "Bidding", "Bundling", "Game theory", "Price discrimination", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0395", "e:abstract": "In this paper, we propose an individual-level approach to diffusion and growth models. By , we refer to the unit of analysis, which is a single consumer (instead of segments or markets) and the use of granular sales data (daily) instead of smoothed (e.g., annual) data as is more commonly used in the literature. By analyzing the high volatility of daily data, we show how changes in sales patterns can self-emerge as a direct consequence of the stochastic nature of the process. Our contention is that the fluctuations observed in more granular data are not noise, but rather consist of accurate measurement and contain valuable information. By stepping into the noise-like data and treating it as information, we generated better short-term predictions even at very early stages of the penetration process. Using a Kalman-Filter-based tracker, we demonstrate how movements can be traced and how predictions can be significantly improved. We propose that for such tasks, daily data with high volatility offer more insights than do smoothed annual data.", "e:keyword": ["Growth process", "New product", "Penetration", "Sales movements", "Takeoff", "Diffusion", "Agent base modeling", "Forecasting", "Adoption", "Innovation", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0397", "e:abstract": "Click fraud is the practice of deceptively clicking on search ads with the intention of either increasing third-party website revenues or exhausting an advertiser's budget. Search advertisers are forced to trust that search engines detect and prevent click fraud even though the engines get paid for every undetected fraudulent click. We find conditions under which it is in a search engine's interest to allow some click fraud. Under full information in a second-price auction, if % of clicks are fraudulent, advertisers will lower their bids by %, leaving the auction outcome and search engine revenues unchanged. However, if we allow for uncertainty in the amount of click fraud or change the auction type to include a click-through component, search engine revenues may rise or fall with click fraud. A decrease occurs when the keyword auction is relatively competitive because advertisers lower their budgets to hedge against downside risk. If the keyword auction is less competitive, click fraud may transfer surplus from the winning advertiser to the search engine. Our results suggest that the search advertising industry would benefit from using a neutral third party to audit search engines' click fraud detection algorithms.", "e:keyword": ["Advertising", "Auctions", "Click fraud", "Game theory", "Internet marketing", "Search advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0399", "e:abstract": "In certain product categories, large discount retailers are known to offer shallower assortments than traditional retailers. In this paper, we investigate the competitive incentives for such assortment decisions and the implications for manufacturers' distribution strategies. Our results show that if one retailer has the channel power to determine its assortment first, then it can strategically reduce its assortment by carrying only the popular variety while simultaneously inducing the rival retailer to carry both the specialty and popular varieties. The rival retailer then bears higher assortment costs, which leads to relaxed price competition for the commonly carried popular variety. We also show that when the manufacturer has relative channel power, it chooses alternatively to distribute both product varieties through both retailers. Our analysis suggests, therefore, that when a retailer becomes dominant in the distribution channel, it facilitates retail segmentation into discount shops, carrying limited product lines, and specialty shops carrying wider assortments. We also illustrate how retailer power leading to strategic assortment reduction can lead to lower consumer surplus.", "e:keyword": ["Channels of distribution", "Channel power", "Assortment", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0400", "e:abstract": "Many data sets, from different and seemingly unrelated marketing domains, all involve records of consumers' movements in a spatial configuration. Path data contain valuable information for marketing researchers because they describe how consumers interact with their environment and make dynamic choices. As data collection technologies improve and researchers continue to ask deeper questions about consumers' motivations and behaviors, sets will become more common and will play a more central role in marketing research. To guide future research in this area, we review the previous literature, propose a formal definition of a path (in a marketing context), and derive a unifying framework that allows us to classify different kinds of paths. We identify and discuss two primary dimensions (characteristics of the spatial configuration and the agent) as well as six underlying subdimensions. Based on this framework, we cover a range of important operational issues that should be taken into account as researchers begin to build formal models of path-related phenomena. We close with a brief look into the future of path-based models, and a call for researchers to address some of these emerging issues.", "e:keyword": ["Path data", "Path models", "Integrative review", "Grocery shopping", "Eye tracking", "Web browsing", "Clickstream", "Information acceleration"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0401", "e:abstract": "Many brands today introduce limited edition (LE) products as part of their product line. However, little is known about the conditions under which a brand should introduce an LE product or the competitive implications of doing so. We investigate this issue using a game theoretic model of a market where two brands compete for consumers who desire exclusivity. Our analysis shows that adding an LE product has a positive direct effect on brand profits through the increased willingness of consumers to pay for such a product, but also has a negative strategic effect by increasing price competition between brands. These effects result in different conclusions depending on the nature of brand differentiation. When brands differ in quality, we show that only the high-quality brand may gain in comparison to a scenario where there are no LE products. Although a low-quality brand may offer an LE product as a defensive strategy, its profits are lower than would be in a world without LE products because of the negative strategic effect. When we consider brands that are differentiated on a horizontal attribute such as taste, we find that the negative strategic effects cause lower equilibrium profits if both brands introduce LE products. Yet brands cannot avoid introducing LE products because they face a .", "e:keyword": ["Game theory", "Marketing strategy", "Product management", "Pricing research", "Limited edition products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0403", "e:abstract": "This paper combines different aggregate-level data sets to identify new product demand in consumer packaged goods (CPG) categories. Our approach augments market-level time-series data with widely available summaries of household purchase behavior, i.e., brand penetration and purchase set size data. We show that this augmentation is helpful in the estimation of consumer heterogeneity. For instance, observing a brand with relatively large shares and low penetration typically indicates that preferences are dispersed, with relatively few customers liking the brand a lot. Whereas the combination of share and penetration is informative about heterogeneity with realistic sample sizes, in isolation neither variable may lead to precise estimates of heterogeneity. In addition, other widely available data, e.g., category penetration, is helpful in estimating the size of the total market. Using a large Monte Carlo study, the paper demonstrates the benefits of the proposed approach in estimating model parameters, price elasticities, and brand switching. Empirically, the approach is used to evaluate the launch of a new national brand, DiGiorno, in the frozen pizza category. The new brand is inferred to be very successful at expanding the category, while avoiding cannibalization of existing company share. Using only standard information, i.e., market shares, to estimate the demand model leads, in our data, to poor estimates of the degree of consumer taste variation and of switching to a new brand.", "e:keyword": ["Demand estimation", "New products", "Random coefficients logit model", "Generalized method of moments"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0409", "e:abstract": "We model a duopoly in which ex ante identical firms must decide where to direct their innovation efforts. The firms face market uncertainty about consumers' preferences for innovation on two product attributes and technology uncertainty about the success of their research and development (R&D) investments. Firms can conduct costly market research before setting R&D strategy. We find that the value of market information to a firm depends on whether its rival is expected to obtain this information in equilibrium. Consequently, one firm may forgo market research even though its rival conducts such research and learns the true state of demand. We examine both vertical and horizontal demand structures. With vertical preferences, firms are a priori uncertain about which attribute all consumers will value more. In this case, a firm that conducts market research always attempts innovation on the attribute it discovers that consumers prefer and expends more on R&D than a rival that has not conducted market research. With horizontal preferences, distinct segments existeach caring about innovation on only one attributeand firms are a priori uncertain how many consumers each segment contains. In this case, a firm that conducts market research may follow a strategy and attempt innovation to serve the smaller segment to avoid intense price competition for the larger segment. A firm that conducts market research may therefore invest less in R&D and earn lower postlaunch profits than a rival that has forgone such research.", "e:keyword": ["New product development", "Market research", "Innovation", "Differentiation", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0504", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0430", "e:abstract": "When a firm allows the return of previously purchased merchandise, it provides customers with an option that has measurable value. Whereas the option to return merchandise leads to an increase in gross revenue, it also creates additional costs. Selecting an optimal return policy requires balancing both demand and cost implications. In this paper, we develop a structural model of a consumer's decision to purchase and return an item that nests extant choice models as a special case. The model enables a firm to both measure the value to consumers of the return option and balance the costs and benefits of different return policies. We apply the model to a sample of data provided by a mail-order catalog company. We find considerable variation in the value of returns across customers and categories. When the option value is large, there are large increases in demand. For example, the option to return women's footwear is worth an average of more than $15 per purchase to customers and increases average purchase rates by more than 50%. We illustrate how the model can be used by a retailer to optimize his return policies across categories and customers.", "e:keyword": ["Choice models", "Consumer behavior", "Decisions under uncertainty", "Direct marketing", "e-commerce", "Econometric models", "Hierarchical Bayes analysis", "Latent variable models", "Marketing operations interface", "Service quality", "Targeting", "Returns"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0410", "e:abstract": "New product launches are often accompanied by extensive marketing communication campaigns. Firms' allocation decisions for these marketing communication expenditures have two dimensionsacross consumers and over time. This allocation problem is different relative to the problem of allocation of resources for existing products. In the case of new products, consumers are uncertain about their quality and learn about the products through marketing communication. Furthermore, different consumers may have different rates of learning about product quality; i.e., there may be heterogeneous learning. Thus, consumer responsiveness to marketing communication could vary along two dimensions. For each consumer, this responsiveness would vary over time, as she learns about product quality. Across consumers, there would be differences in responsiveness in each time period. For optimal allocation of marketing communication across both consumers and time, firms would need estimates of how consumer responsiveness varies across consumers and over time. Past studies have typically focused on one of these two dimensions in which responsiveness varies. They have either looked at heterogeneity in responsiveness across agents or the variation in responsiveness over time. In the context of new products, past research has looked at how consumer learning about product quality causes responsiveness to vary over time. In this study, we build a model that allows for heterogeneous learning rates and obtain individual-level learning parameters for each consumer. We use a novel and rich panel data set that allows us to estimate these model parameters. To obtain individual-level estimates of learning, we add a hierarchical Bayesian structure to the Bayesian learning model. We exploit the natural hierarchy in the Bayesian learning process to incorporate it in the hierarchical Bayesian model. We use data augmentation, coupled with the Metropolis-Hastings algorithm, to make inferences about individual-level parameters of learning. We conduct this analysis on a unique panel data set of physicians where we observe prescription decisions and detailing (i.e., sales-force effort) at the individual physician level for a new prescription drug category. Our results show that there is significant heterogeneity across physicians in their rates of learning about the quality of new drugs. We also find that there are asymmetries in the temporal evolution of responsiveness of physicians to detailingphysicians who are more responsive to detailing in early periods are less responsive later on and vice versa. These findings have interesting implications for the targeting of detailing across physicians and over time. We find that firms could increase their revenue if they took these temporal and cross-sectional differences in responsiveness into account while deciding on allocations of detailing.", "e:keyword": ["Resource allocation", "Pharmaceutical markets", "Learning models", "Markov chain Monte Carlo methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0407", "e:abstract": "Critics often decry an earnings-focused short-term orientation of management that eschews spending on risky, long-term projects such as innovation to boost a firm's stock price. Such critics assume that stock markets react positively to announcements of immediate earnings but negatively to announcements of investments in innovation that have an uncertain long-term pay off. Contrary to this position, we argue that the market's true appreciation of innovation can be estimated by assessing the total market returns to the entire innovation project. We demonstrate this approach via the Fama-French 3-factor model (including Carhart's momentum factor) on 5,481 announcements from 69 firms in five markets and 19 technologies between 1977 and 2006. The total market returns to an innovation project are $643 million, more than 13 times the $49 million from an average innovation event. Returns to negative events are higher in absolute value than those to positive events. Returns to initiation occur 4.7 years ahead of launch. Returns to development activities are the highest and those to commercialization the lowest of all activities. Returns to new product launch are the lowest among all eight events tracked. Returns are higher for smaller firms than larger firms. Returns to the announcing firm are substantially greater than those to competitors across all stages. We discuss the implications of these results.", "e:keyword": ["Innovation", "Market returns", "Event study", "Fama-French 3-factor model", "High-tech marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0408", "e:abstract": "This study investigates the effects of turning terabytes of raw retail data into managerial insights (i.e., downstream information acquisition) in a strategic channel setting. Two effects of information acquisition are identifiedthe effect that improves retail pricing decision making in an uncertain environment, and the effect whereby the retailer voluntarily discloses the acquired private information to influence the upstream manufacturer's wholesale pricing behavior. It is shown that the efficiency effect benefits the retailer without affecting the manufacturer, while the strategic effect works to the detriment of the retailer but to the advantage of the manufacturer. Nevertheless, unobservable information acquisition can mitigate the retailer's loss and the manufacturer's benefit from the strategic effect of information disclosure. Moreover, an increasing expected information acquisition cost may benefit the retailer, when that cost is low and information acquisition is unobservable to the manufacturer. The implications of this paper can shed light on how firms interact in a channel where the downstream market is data intensive, but information gleaning is costly.", "e:keyword": ["Disclosure", "Distribution channel", "Information acquisition", "Voluntary information sharing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0411", "e:abstract": "Consumers cannot purchase a prescription drug without a prescription from a physician, yet many prescription drugs are promoted to consumers with the help of direct-to-consumer (DTC) advertising. In this paper, we propose and test a competitive model of DTC advertising. We find that the brand specificity of DTC advertising can have an inverted U-shaped relationship with detailing, DTC advertising, and profits. Furthermore, an increase in the cross-price sensitivity between competing prescription drugs is not always detrimental to firm profits. A laboratory test lends qualitative support to some of our model predictions. We also discuss potential implications of DTC advertising for generic drugs and over-the-counter drugs.", "e:keyword": ["Direct-to-consumer advertising", "Pharmaceutical marketing", "Experimental economics", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0418", "e:abstract": "Marketers disclose quality information directly to potential consumers using a variety of communication channels. This study investigates how competition may influence duopoly firms' incentive to voluntarily reveal quality information. We show that firms in competitive markets reveal less information than a monopoly firm. In addition, sequential disclosure leads to asymmetric equilibrium disclosure behavior: the disclosure leader reveals unambiguously less information than in the simultaneous disclosure case, whereas the follower ex ante reveals less (more) private information than that released by the leader or by the firms in the simultaneous case when the disclosure cost is sufficiently low (high). We also examine the equilibrium firm profits and social welfare. We demonstrate that there may be a relationship between equilibrium monopoly profits (or social welfare under both monopoly and duopoly) and the disclosure cost. Moreover, in comparison to the simultaneous disclosure case, sequential disclosure can lead to increasingly softened competition, improving both firm profitability and social welfare.", "e:keyword": ["Communication", "Competition", "Disclosure", "Information transmission", "Quality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0421", "e:abstract": "Standardizing performance expectations across different outlets within a chain, differing in their individual features, their consumers, and the nature of competition they face, can be an onerous task. We develop an integrated, nonlinear, block group-level market share model of store expectations that draws upon the existing trade area as well as store performance literatures. By incorporating and normalizing a large number of external and internal factors impacting performance, we are able to offer a means for the retailer to determine equitable standards. The model is estimated using a variation of the maximum-likelihood estimation, on a data set fashioned from several sources and aggregated at the block group and store levels. Finally, we propose a set of indices that allows us to evaluate relative performances of stores and regions given the competitive environments they face. We find that a block group-level model offers a better fit, as well as significantly richer implications, than a traditional store-level model. Results show that a significant number of stores operate well below their expected levels, an insight not obvious from the raw numbers used to report store statistics to upper management.", "e:keyword": ["Retailing", "Store performance", "Benchmarking", "Econometric models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0434", "e:abstract": "We investigate price competition between firms in markets characterized by consumer variety seeking. While previous research has addressed the effect of consumer inertia on prices, there exists no research on the effects of variety seeking on price competition. Our study fills this gap in the literature. Using a two-period duopoly framework as in Klemperer's analysis of inertial markets, we show that the noncooperative pricing equilibrium in a market with consumer variety seeking may be the same as the collusive outcome in an otherwise identical market without variety seeking. Specifically, our variety-seeking model implies tacit collusion between firms in periods, unlike the inertia model of Klemperer that implies tacit collusion between firms only in the period but implies fierce price competition in the first period. When consumers are assumed to have rational expectations about future prices, the implied first-period prices increase further, which is consistent with what Klemperer finds in an inertial market. To summarize, while our variety-seeking analyses support two key results (pertaining to second-period prices and rational expectations) previously derived for inertial markets by Klemperer, they depart from one key result (pertaining to first-period prices).", "e:keyword": ["Variety seeking", "Pricing", "Oligopoly", "Staying cost", "Attribute satiation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0423", "e:abstract": "This paper investigates the issues concerning a film producer that finances production costs not only by the conventional funding from an institutional investor, but also by Internet funding, financing through the Internet from so-called netizen investors. In Internet funding, netizen investors engage in word-of-mouth activities. Assuming that information asymmetry exists between the producer and investors, we investigate how the Internet funding size varies with the word-of-mouth effect, the monitoring effect of the institutional investor, and the bargaining power of the producer over investors. When the producer has no bargaining power, the Internet funding size is determined by balancing the word-of-mouth effect with the monitoring effect by the institutional investment. If there is no word-of-mouth effect, there may be no Internet funding, because netizen investors interpret Internet funding as an indicator of a negative profit. When the producer has high bargaining power, full Internet funding is possible if the information asymmetry of the film quality is resolved. We discuss how information asymmetry can be resolved by the monitoring of the film quality, the producer's reputation, or the insurance on investment returns. Our model helps to capture several interesting aspects of Internet funding in the Korean film industry.", "e:keyword": ["Internet fund", "Word of mouth", "Monitoring", "Bargaining power", "Film production"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0502", "e:abstract": "This research extends a Pareto/NBD model of customer-base analysis using a hierarchical Bayesian (HB) framework to suit today's customized marketing. The proposed HB model presumes three tried and tested assumptions of Pareto/NBD models: (1) a Poisson purchase process, (2) a memoryless dropout process (i.e., constant hazard rate), and (3) heterogeneity across customers, while relaxing the independence assumption of the purchase and dropout rates and incorporating customer characteristics as covariates. The model also provides useful output for CRM, such as a customer-specific lifetime and survival rate, as by-products of the MCMC estimation. Using three different types of databasesmusic CD for e-commerce, FSP data for a department store and a music CD chain, the HB model is compared against the benchmark Pareto/NBD model. The study demonstrates that recency-frequency data, in conjunction with customer behavior and characteristics, can provide important insights into direct marketing issues, such as the demographic profile of best customers and whether long-life customers spend more.", "e:keyword": ["CRM", "Direct marketing", "Customer lifetime", "Bayesian method", "MCMC"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0383", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0424", "e:abstract": "We analyze firms' decisions to invest in customer relationship management (CRM) initiatives such as acquisition and retention in a competitive context, a topic largely ignored in past CRM research. We characterize each customer by her intrinsic preference towards each firm, the contribution margin she generates for each firm, and her responsiveness to each firm's retention and acquisition efforts. We show that a firm should invest most heavily in retaining those customers that exhibit moderate responsiveness to its CRM efforts. Further, a firm should most aggressively seek to attract those customers that exhibit moderate responsiveness to their provider's CRM efforts and those that are moderately profitable for their current provider. Investing more in customers that are more responsive does not always lead to higher firm profits, because stronger competition for such customers tends to erode the effects of higher CRM efforts of an individual firm. When firms develop a customer relationship over time to generate higher contribution margin or customer responsiveness, we show that such developments may not always be desirable, because sometimes these future benefits may lead to more intense competition and hence lower profits for both firms.", "e:keyword": ["Customer relationship management", "Competitive strategy", "Game theory", "Customer lifetime value", "Customer equity", "Customer acquisition", "Customer development", "Customer retention", "Services marketing", "Relationship marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0402", "e:abstract": "We examine grocery shopping paths using the traveling salesman problem (TSP) as a normative frame of reference. We define the TSP-path for each shopper as the shortest path that connects all of his purchases. We then decompose the length of each observed path into three components: the length of the TSP-path, the additional distance because of (i.e., not following the TSP-order of category purchases), and the additional distance because of (i.e., not following the shortest point-to-point route). We explore the relationship between these deviations and different aspects of in-store shopping/purchase behavior. Among other things, our results suggest that (1) a large proportion of trip length is because of travel deviation; (2) paths that deviate substantially from the TSP solution are associated with larger shopping baskets; (3) order deviation is strongly associated with purchase behavior, while travel deviation is not; and (4) shoppers with paths closer to the TSP solution tend to buy more from frequently purchased product categories.", "e:keyword": ["Traveling salesman problem", "Grocery shopping path", "Path data optimality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0489", "e:abstract": "This article explores some aspects of wine journalism in Norwegian newspapers. Two issues are discussed: First, are wine sales influenced by wine journalists' reviews? Second, do experts agree on what makes a good wine buy? The results show that wine sales are indeed significantly influenced by the judgements of wine critics; a 10% rise in newspapers' scores in Norway was accompanied by an average increase of 16%18% in sales figures for table wines. The effect of wine reviews varied somewhat from newspaper to newspaper. It proved difficult to establish criteria for a good wine buy that are objective and independent of the person making the judgement. The journalists gave no unanimous recommendation of good wine buys to the consumers; the same wine could get good reviews in some papers and might well receive run-of-the-mill reviews in others. However, a majority of the reviewers seemed to agree in the ranking of most of the wines, even if the absolute value of the scores differed.", "e:keyword": ["Advertising", "Entertainment marketing", "Media"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0404", "e:abstract": "Price-matching guarantees (PMGs) are offers to match a competitor's price on a specific item. Such guarantees are extremely common in U.S. retail practice, and their impact has been studied in several published papers. The existing analytic literature models each retailer as a single-product seller; most work assumes that each retailer's product is identical with (or completely substitutable for) the competitor's product. In reality, competing retailers often sell multiple products, and these products do not always overlap, or may overlap partially but not totally. Furthermore, retailers that offer PMGs routinely exclude certain offerings from PMG coverage. This raises the interesting question of how product variety and product-stocking factors affect retailer decisions about offering PMGs. In this paper, we simultaneously consider three factors of retailing importance that imply results consistent with PMG use and nonuse: the ability to choose to stock the same product as, or a product differentiated from, a competitor's offering; the possibility of shelf-space limitations on the ability to stock complete variety; and the category-demand-enhancing effect of variety. These are sensible and realistic descriptive factors shaping retailers' product and pricing decisions, and because they have not been considered jointly in the prior literature on PMGs, their joint consideration helps to expand our understanding of the drivers of PMG implementation and impact. In the presence of these three factors, we examine retailers' decisions about whether to offer a PMG, what product(s) to stock, and how to price the product(s) stocked. Our results show that shelf-space limitations have an important influence on PMG provision: in particular, when retailers are shelf-space constrained, and product substitutability in the category is sufficiently large, choosing to use PMGs (which by definition also requires stocking identical products) is strictly less profitable than enduring Bertrand (price) competition but enjoying retail product differentiation. The result that PMGs can be profit-reducing relative to head-to-head retail competition is a novel one, driven in our model by the opportunity costs of stocking identical products, i.e., the inability to benefit from the demand-enhancing effects of variety and the differential (small or large) between Bertrand pricing and PMG pricing levels. We further show that under asymmetric shelf-space availability, either product variety will be severely limited or retailers will offer a different array of products. Weak substitution between products leads to the latter, with pricing between the differentiated products Bertrand and monopoly levels; strong substitution leads to the former, with pricing at the monopoly level. Our results also show that with unlimited shelf space, both competing retailers offer PMGs, stock the entire available product line, and enjoy monopoly pricing. Given our focus on product variety issues, we also relate our results to the literature on branded variants. Our results demonstrate that the nature of product variety, the availability of retail shelf space, and the category-demand-enhancing effect of variety are key market characteristics that jointly and strongly affect the optimality of PMGs and the resulting pricing and profitability characteristics of the market.", "e:keyword": ["Price-matching guarantees", "Retail competition", "Product-line assortment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0406", "e:abstract": "When researchers from different fields with different norms collaborate, the question arises of how name-ordering conventions are chosen and how they affect contribution credits. In this paper, we answer these questions by studying two disciplines that exemplify the two cornerstones of name-ordering conventions: lexicographical ordering (i.e., alphabetical ordering, endorsed in economics) and nonlexicographical ordering (i.e., ordering according to individual contributions, endorsed in psychology). Inferences about credits are unambiguous in the latter arrangement but imperfect in the former, because alphabetical listing can reflect ordering according to individual contributions by chance. We contrast the fields of economics and psychology with marketing, a discipline heavily influenced by both. Based on archival data, consisting of more than 38,000 journal articles, we show that the three fields have different ordering practices. In two empirical studies with 351 faculty and graduate student participants from all three disciplines, as well as in a computer simulation, we show that ordering practices systematically affect and shape the allocation of perceived contributions and credit. Whereas strong disciplinary norms in economics and psychology govern the allocation of contribution credits, a more heterogeneous picture emerges for marketing. This lack of strong norms has detrimental effects in terms of assigned contribution credits.", "e:keyword": ["Decision making", "Information processing", "Social norms", "Contribution credits", "Authorship"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0428", "e:abstract": "Many services are delivered to a (large) number of customers simultaneously within a confined zone (e.g., restaurants, resorts, trains, and airplanes). Under unexpected high demand, customers experience discomfort from two major sources: (a) the that arises when too many customers (i.e., sardines) compete for space and service resources, and (b) the that results from an exit cost incurred by customers who self-select to escape the unpleasant service. This paper investigates the optimal compensation and pricing policies under these two effects. We find that offering compensation to sardines can improve profit and social welfare. However, consumers do benefit when compensated for the discomfort from crowding. This paper also provides insights by exploring the impact of changes in the two effects on price and profit.", "e:keyword": ["Service quality", "Service pricing", "Customer experience", "Negative externality", "Customer discomfort management", "Compensation", "Customer satisfaction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0481", "e:abstract": "We examine the problem of how a content provider, specifically the National Academies Press (NAP), can optimally price the different forms of its productprint and PDFthat it sells online. Whereas products in the traditional product line generally tend to be substitutes, the different content product forms could range from being substitutes to being complements across customers. Thus the content provider can possibly sell bundles of the product forms, leading to additional revenue. We first discuss NAP's decision context and describe the model we proposed for developing NAP's optimal pricing policies for its different forms. We describe the choice experiment we conducted on the publisher's website that maximally uses the online interface to collect relevant data needed to estimate our model. We show how NAP embraced the results from the model for developing a new business model and how it used the insights derived from the study to set pricing policies and monitor sales performance as a function of pricing. Finally, we perform validation of the model and the implemented policies using dynamic modeling of sales data from NAP's website. The paper illustrates how e-commerce technologies can lead to the development of optimal policies using marketing models.", "e:keyword": ["Pricing", "Product line", "Digital products", "Product form", "Bundling", "Choice experiment", "Internet"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0470", "e:abstract": "In addition to retailing new vehicles, automotive manufacturers in the United States sell millions of vehicles through leasing and to fleet customers every year. The majority of these vehicles are returned to the automotive manufacturers at the end of the contracted term and must be remarketed. In 2007, about 10 million used vehicles were sold at more than 400 auctions in the United States. Large consigners face decisions every day about when, where, and at what price to offer these vehicles, which has significant financial implications for their profitability. To address the challenges of the distribution process, (), a division of J.D. Power and Associates, developed the PIN Optimal Distribution of Auction Vehicles System (ODAV), an automated decision optimization system that helps remarketers maximize profits through the most advantageous distribution of their auction vehicles. At the core of the system is a combination of three models that determine the distribution of the vehicles on a daily basis: a nearest neighbor linear regression model for short-term auction price forecasting; an autoregressive integrated moving average time-series analysis model for volume-price elasticity; and a genetic algorithm optimizer for vehicle distribution. Since its launch in 2003, PIN has been providing ODAV services on a daily basis, and to date, more than two million vehicles have been distributed through this system. In this paper, we will describe the PIN ODAV System, its implementation, and the business impact by using as an example the experience with our largest client, Chrysler Group LLC.", "e:keyword": ["Used vehicle", "Auction price", "Distribution", "Forecasting", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0477", "e:abstract": "Procter & Gamble (P&G) Asia-Pacific is interested in managing value growth. Only after fully understanding the true effects of the marketing-mix variables can P&G managers make strategic decisions answering questions such as the following: (1) Are the P&G brands in the detergent market inelastic or elastic with respect to price? How has the price elasticity changed over time? Can P&G increase the price of its brands to gain value growth? (2) What are the price, distribution, and sizing combinations needed to achieve the desirable value growth? (3) How can P&G gain market share from its competitors without cannibalizing its own brands? P&G Asia-Pacific approached us to develop a value growth framework to answer these questions. To generate the answers for the above questions, we develop a three-step weighted random coefficient estimator that captures the heterogeneity across cross sections (different stock-keeping units and states) and the endogeneity of distribution. Based on the parameter estimates, we provide strategic recommendations to P&G for a field test to validate our suggestions. We developed a simulator for P&G managers so that they can generate appropriate marketing-mix strategies for achieving the desired value growth. As a result, P&G gained over $39 million in value growth over a one-year period by implementing the recommendations from our modeling approach.", "e:keyword": ["Marketing mix", "Price elasticity", "Distribution", "Random coefficient model", "System of equations", "Sales value", "Revenue", "Sales volume"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0432", "e:abstract": "We study how market structure within a product category varies across retail formats. Building on the literature on internal market structure, we estimate a joint store and brand choice model where the loading matrix of brand attributes are allowed to be retail format specific. The approach allows us to recover brand maps for different retail formats while controlling for the short-term marketing mix activities at these stores and the self-selection of households that frequent a particular format. The model is applied to consumer panel data from two product categories, where households are observed to make purchases across three store types: high-end grocery store, traditional supermarket, and large everyday low pricing (EDLP) formats. Our results show strong correlations between the marketing mix sensitivities, store format preference, and unobserved brand attributes. These correlations translate into significant differences in market structure across retail formats and in the direction and size of preference vectors for unobservable brand attributes. We find a tight clustering of brands at the EDLP format, whereas brands are found to compete in distinct subgroups at other stores. Results show that failure to account for retail format effects can substantially bias the understanding of underlying market structure and could lead to incorrect implications in applications such as new product entry.", "e:keyword": ["Market structure", "Brand maps", "Retail formats", "Heterogeniety"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0439", "e:abstract": "In the last few decades, the measurement of marketing constructs has improved tremendously. Our discipline has also started to systematically catalogue our measurement knowledge in influential handbooks of marketing scales. However, at least two important issues remain. First, existing scales are often too long for administration in nonstudent samples or in applied studies. Second, existing (U.S.-developed) scales may contain items that are not informative about the underlying construct in particular countries, whereas relevant items tapping into local cultural expressions of the construct in question may be missing. To address these issues, we propose a new model that yields country-specific yet fully cross-nationally comparable short forms of unidimensional marketing scales. The procedure is based on hierarchical item response theory and optimal test design. The procedure is flexible in the sense that the researcher can specify various constraints on item content, scale length, and measurement precision. Because our procedure allows inclusion of country-specific (or emic) items in standardized (or etic) scales, it presents an important step toward resolving the emic-etic dilemma that has plagued international marketing research for decades.", "e:keyword": ["Measurement", "Marketing research", "Marketing surveys", "International marketing", "Scale construction", "Measurement invariance"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0440", "e:abstract": "Prior marketing literature has overlooked the role of regulatory regimes in explaining international sales growth of new products. This paper addresses this gap in the context of new pharmaceuticals (15 new molecules in 34 countries) and sheds light on the effects of regulatory regimes on new drug sales across the globe. Based on a time-varying coefficient model, we find that differences in regulation substantially contribute to cross-country variation in sales. One of the regulatory constraints investigated, i.e., manufacturer price controls, has a positive effect on drug sales. The other forms of regulation such as restrictions of physician prescription budgets and the prohibition of direct-to-consumer advertising (DTCA) tend to hurt sales. The effect of manufacturer price controls is similar for newly launched and mature drugs. By contrast, regulations on physician prescription budgets and DTCA have a differential effect for newly launched and mature drugs. Whereas the former hurts mature drugs more, the latter has a larger effect on newly launched drugs. In addition to these regulatory effects, we find that national culture, economic wealth, and lagged sales also affect drug sales. Our findings may be used as input by managers for international launch and marketing decisions. They may also be used by public policy administrators to assess the role of regulatory regimes in pharmaceutical sales growth.", "e:keyword": ["International new product growth", "Drug", "Pharmaceutical", "Regulation", "Culture", "Economics", "Time-varying effects", "Penalized splines"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0442", "e:abstract": "We propose a dynamic programming framework for retailers of frequently purchased consumer goods in which the prices affect both the profit per visit in the current period and the number of visitors (i.e., store traffic) in future periods. We show that optimal category prices in the infinite-horizon problem also maximize the closed form sum of a geometric series, allowing us to derive meaningful analytical results. Modeling the linkage between category prices and future store traffic fundamentally changes optimal pricing policy. Optimal pricing must balance current profits against future traffic; under general conditions, optimal long-run prices are uniformly lower across all categories than those that maximize current profits. This result explains the empirical generalization that category demand in grocery stores is inelastic. Parameterizing profit per visit and store traffic reveals that, as future traffic becomes more sensitive to price, retailers should increasingly lower current prices and sacrifice current profits. We also determine how the burden of drawing future traffic to the store should be distributed across categories; this is the foundation for a new taxonomy of category roles.", "e:keyword": ["Marketing", "Dynamic programming", "Pricing", "Optimization", "Retailing", "Store traffic"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0444", "e:abstract": "In this paper, we investigate the effectiveness of a firm's proactive management of customer-to-customer communication. We are particularly interested in understanding how, if at all, the firm should go about effecting meaningful word-of-mouth (WOM) communications. To tackle this problem, we collect data from two sources: (1) we implement a large-scale field test in which a national firm created word of mouth through two populations: customers and noncustomers, and (2) we collect data from an online experiment. We break our theoretical problem into two subproblems. First, we ask: What kind of WOM drives sales? Motivated by previous research, we hypothesize that for a product with a low initial awareness level, WOM that is most effective at driving sales is created by less loyal (not highly loyal) customers and occurs between acquaintances (not friends). We find support for this in the field test as well as in an experimental setting. Hence, we demonstrate the potential usefulness of exogenously created WOM: conversations are created where none would naturally have occured otherwise. Then, we ask: Which agents are most effective at creating this kind of WOM? In particular, we are interested in evaluating the effectiveness of the commonly used opinion leader designation. We find that although opinion leadership is useful in identifying potentially effective spreaders of WOM among very loyal customers, it is less useful for the sample of less loyal customers.", "e:keyword": ["Word of mouth", "Promotion", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0446", "e:abstract": "The strategy Philip Morris adopted in 1993 featured a one-time, permanent, publicly announced price cut, an event referred to as Marlboro Friday. Little is known about the impact of permanent and publicly announced price cuts on consumer brand switching behaviors for an addictive product. In the context of Marlboro Friday, we investigate (1) how consumers' brand choices are affected by a permanent price cut, (2) whether differential and dynamic effects of permanent price cuts occur for different types of consumers, and (3) the implications of publicly announced permanent price cuts on consumer brand switching behavior in the long run. We develop a dynamic structural brand choice model that allows for consumer forward-looking behavior, learning, and addiction, and investigate how consumers adjusted their brand choice behaviors before and after this permanent price cut. Using unique consumer panel data pertaining to cigarette purchases before and after the event, we provide behavioral explanations of whether and how the drastic and permanent price cut represented an effective step to encourage brand switching for an addictive product and a necessary step for Philip Morris to combat the growing market share of generic brands.", "e:keyword": ["Marlboro Friday", "Permanent price cut", "Brand switching", "Choice model", "Uncertainty", "Learning", "Price expectation", "Risk aversion", "Addiction", "Dynamic structural model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0455", "e:abstract": "This research has three objectives: (i) to compile analytical results on national brand and store brand marketing obtained from mathematical models, (ii) to assess the external validity of those results and thus the applicability of the results to practice, and (iii) to identify avenues for further research on national brand and store brand competition. A total of 44 analytical results (29 related to retailer strategies and 15 related to manufacturer strategies) are compiled from a survey of literature published between 1966 and 2006. Three criteria are then used to assess the external validity of these resultsrobustness (R), empirical support (E), and credibility (C) (collectively, REC). Each result is quantitatively assessed (scored) using these three criteria. Robustness is measured as the total number of relevant market conditions for which the result has been shown to hold. Empirical support is measured as the number of independent empirical studies in which the findings are consistent with the analytical result. Credibility is measured as the believability of the theoretical result as perceived by experienced brand managers and retail executives. Thus, the REC scoring approach represents a triangulation of perspectivesrobustness (modeler perspective), empirical support (empiricist perspective), and credibility (managerial perspective). In particular, this research serves in part as a bridge between scholars and practitioners in the context of national brand and store brand marketing.", "e:keyword": ["Private labels", "Store brands", "Competitive strategies", "Mathematical models", "Marketing generalizations"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0447", "e:abstract": "We use data on all manufacturer funding and promotion activity by a major U.S. retailer during a two-year period to compute promotion pass-through and assess its magnitude. Then, we estimate a two-tiered probit and lognormal regression model to study drivers of the large variation we observe in pass-through rates. Although our analysis is based on data from a single retailer, it provides a much more complete picture of the magnitude and variability of pass-through than has been available to date. Some key insights from our work are as follows. First, the retailer passes through more than 100% of the total manufacturer funding it receives in aggregate, but the median pass-through rate for individual manufacturers is much lower than 100%. Second, some manufacturers are promoted even without funding. This is more likely for private label and high-share manufacturers in high-lift and high-margin categories. Third, a small number of manufacturer and category characteristics explain a significant amount of the variation in pass-through. In particular, pass-through is higher for private label. It increases with the share of the manufacturer in the focal category but also with the sales of that manufacturer in other categories. Categories with high sales, high promotion lift, low concentration, and low margin get more pass-through. We corroborate some recent conclusions in the literature, e.g., that some pass-through rates are much higher than 100% and that high-share manufacturers get more pass-through. We add several new insights, e.g., on the magnitude of aggregate pass-through, on cross-pass-through across and within categories, on pass-through for private label, and on the category drivers of pass-through.", "e:keyword": ["Trade promotion", "Promotion pass-through", "Promotion response", "Retail promotions", "Private label promotions", "Cross-pass-through"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0445", "e:abstract": "We consider a marketer of components who can select one of three alternative pricing strategies: (1) a pure component strategy (i.e., the customer can only buy the components individually), (2) a pure bundling strategy (i.e., the components must be purchased together), or (3) a mixed bundling strategy (i.e., the customer may buy a component individually, or buy the bundle). We consider a market where customer knowledge of components varies and propose that a high-knowledge customer can determine with greater certainty whether a given component is useful to her. Consequently, more knowledgeable customers have higher variability in their reservation price of a component. Using an analytical model, we identify the conditions under which each of the three pricing strategies maximizes profit and show that three factors determine the optimal strategy: marginal costs of components, distribution of knowledge over the customer population, and relative sizes of customer segments where each segment is interested in the same subset of components. Managerial implications and directions for future research are discussed.", "e:keyword": ["Components", "Bundling", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0537", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0495", "e:abstract": "We investigate the association between information contained in the American Customer Satisfaction Index (ACSI) metric and future stock market performance. Some past research has provided results suggesting that the financial markets misprice customer satisfaction; i.e., firms advantaged in customer satisfaction are posited to earn positive future-period abnormal stock returns. We reexamine this relationship and find that statistically significant evidence of financial market mispricing of customer satisfaction is limited to firms in the computer and Internet sector. The results suggest that the mispricing anomaly reported in past research appears not to stem from a systemic failure of the financial markets to impound the financial implications of customer satisfaction into current stock price, but rather from abnormal returns achieved by a small group of satisfaction leaders in the computer and Internet sector over the period of study. Analyses based on unconditional risk covariates and analyses using conditional risk covariates estimated from short-window, high-frequency data support this finding.", "e:keyword": ["Marketing metrics", "Valuation", "Mispricing", "Customer satisfaction", "Financial performance", "Efficient markets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0505", "e:abstract": "According to Jacobson and Mizik [Jacobson, R., N. Mizik. 2009. The financial markets and customer satisfaction: Reexamining possible financial market mispricing of customer satisfaction. (5) 810819], excess stock portfolio returns for firms with strong customer satisfaction are small and statistically insignificant, and if there is any above-market performance at all, it is due to a small set of firms in the computer and Internet industries. But their data seem to suggest the opposite. The returns are actually both exceptionally large and significant. Using monthly data, their portfolio consisting of strong American Customer Satisfaction Index (ACSI) firms outperformed the market by 0.0053, corresponding to 6.4% cumulative risk-adjusted above-market returns on an annual basis over a 10-year perioda performance that would beat at least 99% of all large-cap U.S. stock funds tracked by Morningstar. Using a different treatment of risk, their annualized risk-adjusted return is a whopping 8.4% better than market. After eliminating computer, Internet, and utility companies, they find that the monthly risk-adjusted abnormal returns drop to 0.0045, which corresponds to an annual above-market return of 5.4%. This too is better than 99% of all actively managed stock funds in the population. Yet Jacobson and Mizik conclude that these returns are not statistically significant and that there is no evidence that stock returns from firms with strong customer satisfaction outperform the market over the long run. The failure to reject the null hypothesis is probably due to a lack of statistical power in Jacobson and Mizik's analysis. We discuss why this is likely the case and then present new data updating the results from our original article [Fornell, C., S. Mithas, F. Morgeson III, M. S. Krishnan. 2006. Customer satisfaction and stock prices: High returns, low risk. (1) 314]. The above-market returns persist and are both economically and statistically significant.", "e:keyword": ["Customer satisfaction", "Stock prices", "Stock returns", "Risk", "Market value", "Stock portfolios"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0526", "e:abstract": "A number of recent marketing studies examine the stock market's response to the release of American Customer Satisfaction Index (ACSI) scores. The broad purpose of these studies is to investigate the stock market's valuation of customer satisfaction. However, a key focus is on whether customer satisfaction information predicts long-run returns. We provide evidence on the market's pricing of ACSI information using a more comprehensive set of well-established tests from the accounting and finance literatures. We find that ACSI scores provide some incremental information on future operating income and that the market quickly responds to the release of information on large increases in satisfaction. However, we find no evidence that ACSI predicts long-run returns. These results suggest that customer satisfaction information is value relevant, but they are also consistent with Jacobson and Mizik's conclusion [Jacobson, R., N. Mizik. 2009. The financial markets and customer satisfaction: Reexamining possible financial market mispricing of customer satisfaction. (5) 810819] that mispricing of ACSI information, if present at all, is limited.", "e:keyword": ["Customer loyalty", "Econometrics", "Stock valuation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0531", "e:abstract": "We appreciate the opportunity to respond to the commentaries and additional analyses by Fornell et al. [Fornell, C., S. Mithas, F. V. Morgeson III. 2009a. The economic and statistical significance of stock returns on customer satisfaction. . (5) 820825] and Ittner et al. [Ittner, C., D. Larcker, D. Taylor. 2009. The stock market's pricing of customer satisfaction. (5) 826835]. Both studies have multiple theoretical and econometric limitations that challenge the validity of their arguments and findings (e.g., neither study allows for time-varying risk factor loadings in their assessments of mispricing although the composition of firms in their analyzed portfolios changes over time, Fornell et al. mischaracterize the efficient markets hypothesis, and Ittner et al. do not use standard panel data econometric methods and models). Generalizations about customer satisfaction, like any other construct, should be assessed by appropriate econometric methods and should withstand rigorous scrutiny. We believe an open, frank dialogue can help clear up misconceptions, air central issues, and advance better understanding of methods and analyses for assessing the financial market implications of marketing metrics such as customer satisfaction.", "e:keyword": ["Customer satisfaction", "Mispricing", "Value relevance"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0448", "e:abstract": "As high-tech markets mature, replacement purchases inevitably become the dominant proportion of sales. Despite the clear importance of product replacement, little empirical work examines the separate roles of adoption and replacement. A consumer's replacement decision is dynamic and driven by product obsolescence because these markets frequently undergo rapid improvements in quality and falling prices. The goal of this paper is to construct a model of consumer product replacement and to investigate the implications of replacement cycles for firms. To this end, I develop and estimate a dynamic model of consumer demand that explicitly accounts for the replacement decision when consumers are uncertain about future price and quality. Using a unique data set from the PC processor industry, I show how to combine aggregate data on sales and product ownership to infer replacement behavior. The results reveal substantial variation in replacement behavior over time, and this heterogeneity provides an opportunity for managers to tailor their product introduction and pricing strategies to target the consumers of a particular segment that are most likely to replace in the near future.", "e:keyword": ["Durable goods", "Replacement", "Structural estimation", "Dynamic programming", "Innovation", "Upgrades", "PC processor", "CPU", "Technology products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0449", "e:abstract": "In a market of consumers with varying willingness to pay, using product line as a discrimination tool may extract higher profits than serving all consumers with a single product. Local context effects, however, point to yet another consideration in designing product lines: how the appeal of a product changes with the context provided by other products in the choice set. I present a model of product line design that incorporates both discrimination and context management goals and offers recommendations for the variety and positioning of products. To this end, the model makes use of a framework that allows preferences to be choice set dependent. Given this framework, I study how the firm manages externalities between products created by such dependencies. The firm creates distortions above and beyond those resulting from discrimination motives alone. For example, in a vertically differentiated market for quality, quality distortions exist even for the consumers with the highest valuations. The range of quality provisions, given the number of products, is compressed as the relative importance of unfavorable comparisons among products increases. Surprisingly, this compression may even lead the firm to forego discrimination among consumers regardless of the cost of offering distinct products.", "e:keyword": ["Price discrimination", "Product positioning", "Context effects", "Choice set-dependent preferences", "Context management", "Behavioral economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0456", "e:abstract": "In this paper, we study the standardization and customization decisions of two firms in a competitive setting, along with variety, lead time, and price decisions. We incorporate consumer heterogeneity both in firm preference (or store convenience) and in product attribute preferences. We find that the equilibrium outcome depends on the cost efficiencies of the production technologies as well as the consumer sensitivity to product fit and lead time. We develop an index that signifies the relative attractiveness of the standardization and customization strategies, and the potential outcomes. We identify the strategic roles of product variety and lead time in the competition. In contrast to the previous literature, we find that increasing the variety will not intensify the price competition if there is sufficient firm differentiation. Rather, it relieves the price pressure for the firm as it satisfies consumer needs better and enables higher price premiums. We also analyze the impact of asymmetric variable costs, fixed costs, and brand reputation on the equilibrium decisions.", "e:keyword": ["Customization", "Product differentiation", "Product variety", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0457", "e:abstract": "Although previous research demonstrates the profitability of partial refund policies in a monopoly setting, there is a certain lack of ubiquity in practice about these refunds in competitive service markets. This raises the question of how a partial refund policy may work and whether it is even sustainable in a competitive environment. This study investigates how competition may influence the profitability and the equilibrium choice of refund policies. It is shown that partial refunds may endogenously change the nature of strategic interaction between service providers from local monopolies into a competition regime, which moderates the gains from exploiting the efficiency-enhancing effect of partial refunds. A whole range of pure-strategy equilibria can be obtained as a result of the interplay between the efficiency-improving and the competition-intensifying effects. When the capacity is small (large) such that the efficiency-improving (the competition-intensifying) effect is dominant, both firms in equilibrium follow identical partial (zero) refund policies. Moreover, interestingly, the symmetric firms may end up in an asymmetric equilibrium in which one firm follows a partial refund policy and the other adopts a zero refund policy.", "e:keyword": ["Advance selling", "Cancellations", "Capacity constraint", "Competition", "Contingent contracts", "Refunds", "Reservations", "Services", "Uncertainty", "Yield management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0458", "e:abstract": "In spite of the fact that many durable products are sold through dealers, the literature has largely ignored the issue of how product durability affects the interactions between a manufacturer and her dealers. We seek to fill this gap by considering a durable goods manufacturer that uses independent dealers to get her product to consumers. In contrast to much of the literature, we specifically consider the possibility that if the manufacturer sells her product, then the dealers can either sell or lease it to the final consumer. One of our more interesting findings is that, when the level of competition among dealers is high, the manufacturer prefers to use a lease-brokering arrangement in which the dealers earn a margin for brokering leases between the manufacturer and end consumers, instead of selling her product to the dealers. This complements existing results that show that when suppliers of durable goods interact directly with consumers, selling is the dominant strategy for high levels of competitive intensity.", "e:keyword": ["Channel structure", "Durability", "Time inconsistency", "Double marginalization", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0460", "e:abstract": "While creativity in advertising is a growing area of marketing research, relatively little is known about how the effects of creativity are produced. Accordingly, this research explores the basic persuasive (i.e., desire to postpone closure) and emotional (i.e., positive affect) mechanisms through which creative ads exert their influence on consumer viewing and purchase intentions. In addition, the model hypothesizes that the level of involvement with the ad moderates the desire to postpone closure effects but not the emotional impact. An overall model of the impact of ad creativity is developed and tested using structural equations analysis. Results from three experiments show the model receives good support.", "e:keyword": ["Advertising creativity", "Information processing", "Persuasion", "Resistance to persuasion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0461", "e:abstract": "The act of trading in a used car as partial payment for a new car resonates with practically all consumers. Such transactions are prevalent in many other durable goods markets, ranging from golf clubs to CT scanners. What roles do trade-ins play in these markets? What motivates the seller to set up a channel to facilitate trade-ins? Intuitively, accepting a trade-in would appear to stimulate demand for the producer's product, but facilitating the resale of these used goods that substitute for new goods might also increase cannibalization. Although such transactions involve billions of dollars, we know relatively little about this practice from the extant research literature. This paper develops an analytical model that incorporates key features of real-world durable goods markets: (i) coexistence of new and used goods markets, (ii) consumer heterogeneity with respect to quality sensitivity, (iii) firms that anticipate the cannibalization problem arising from the coexistence of new and used goods, and (iv) lemon problems in resale markets, whereby sellers of used goods are better informed than buyers about the quality of their particular item. In our analysis, a trade-in policy amounts to an intervention by the firm in the used goods market, which reduces inefficiencies arising from the lemon problem. It motivates owners to purchase new goods and reduces their proclivity to hold on to purchased goods because of the low price the latter would fetch in a lemon market. We also show that trade-in programs are more valuable for less reliable products, as well as for products that deteriorate more slowly. Our analysis shows that producers in durable goods markets should consider trade-in programs as a matter of routine. Despite cannibalization concerns arising from a more active resale market, a producer's profits will inevitably rise from introducing trade-ins, given pervasive lemon problems. We test the key predictions of the model about price and volume of trade by assembling a data set of transactions of U.S. automobile consumers and find broad support for our model.", "e:keyword": ["Durable goods", "Trade-ins", "Adverse selection", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0462", "e:abstract": "The universality of design perception and response is tested using data collected from 10 countries: Argentina, Australia, China, Germany, Great Britain, India, The Netherlands, Russia, Singapore, and the United States. A Bayesian, finite-mixture, structural equation model is developed that identifies latent logo clusters while accounting for heterogeneity in evaluations. The concomitant variable approach allows cluster probabilities to be country specific. Rather than a priori defined clusters, our procedure provides a posteriori cross-national logo clusters based on consumer response similarity. Our model reduces the 10 countries to three cross-national clusters that respond differently to logo design dimensions: the West, Asia, and Russia. The dimensions underlying design are found to be similar across countries, suggesting that elaborateness, naturalness, and harmony are universal design dimensions. Responses (affect, shared meaning, subjective familiarity, and true and false recognition) to logo design dimensions (elaborateness, naturalness, and harmony) and elements (repetition, proportion, and parallelism) are also relatively consistent, although we find minor differences across clusters. Our results suggest that managers can implement a global logo strategy, but they also can optimize logos for specific countries if desired.", "e:keyword": ["Design", "Logos", "International marketing", "Standardization", "Adaptation", "Structural equation models", "Gibbs sampling", "Concomitant variable", "Bayesian", "Mixture models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0463", "e:abstract": "In his JulyAugust 2007 editorial of , Steven Shugan argues that the realism of assumptions does not matter as long as a theory or model produces satisfactory predictions and claims further that unrealistic assumptions breed good theories. This commentary discusses the problems of his argument and presents a very different view about the realism of assumptions. Assumptions need not be realistic if the only goal of science is prediction. However, a major function of theory is also to explain and not just to predict. The role of explanation is more important in the social sciences because it is far more difficult to produce accurate predictions in the social than the natural sciences. Assumptions, especially core assumptions, often constitute the foundation of the mechanismic explanations provided by a theory. Unrealistic assumptions may lead to faulty explanations and false predictions. Contrary to Shugan's view, the realism of an assumption cannot be assessed just based on the output of a theory. It has to be tested independently of or in conjunction with the hypotheses of the theory. Also, contrary to Shugan's claim, more realistic assumptions result in better theories. As theory development advances, efforts should be directed toward making assumptions more realistic.", "e:keyword": ["Assumptions", "Explanation", "Prediction", "Mechanism", "Realism", "Research design"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0467", "e:abstract": "Remember James Boswell, ninth Laird of Auchinleck, author of the famous maxim that the road to hell is paved with good intentions? Trying to build realistic theories differs dramatically from having correct explanatory theories tested on objective criteria, e.g., verifiable prediction. Evaluating theories on whether assumptions are realistic is potentially subjective, biased, and arbitrary. A theory's domain determines whether its assumptions are sufficiently realistic and when assumptions must hold and to what degree, so testing assumptions in isolation puts an unnecessary burden on the assumptions (i.e., they must hold everywhere). For theories explaining cooperation and information exchange, predictions reveal that the prisoner's dilemma assumptions (only two prisoners, four possible outcomes, two possible actions, etc.) are sufficiently realistic. For theories explaining prisoner sentencing guidelines and probation policy, predictions might suggest otherwise. Scientific methods allow the evaluation of theories on criteria such predictive accuracy, reliability, validity, and robustnessnot based on realism. When multiple explanatory theories survive initial testing, one derives conflicting predictions. For example, a theory that people are broccoli produces correct predictions (people are mortal) and incorrect predictions (people are biennial). Tragic consequences can occur when theory adoption depends on whether assumptions are disliked, unpopular, or exclude a favorite variable. Denounce journals that reject models with insightful new implications because the assumptions are too simple or merely disliked. The term unrealistic sometimes means personally disliked.", "e:keyword": ["Models", "Theory testing", "Mathematical models", "Scientific method", "Realism", "Mathematical assumptions", "Empirical research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0468", "e:abstract": "In the response to my commentary on his 2007 editorial entitled It's the Findings, Stupid, Not the Assumptions, Steven Shugan raises a number of thought-provoking ideas. In this rejoinder, I focus on three issues that Shugan and I hold the most divergent views. First, while Shugan uses the terms realistic and realism in several different meanings, I define the realism of an assumption as the extent to which it corresponds with the real world. Second, Shugan makes a strong claim that predictions can be objectively evaluated whereas assumptions cannot. I refute his claim by arguing that testing predictions and testing assumptions follow the same research process of checking whether the proposition concerned corresponds with reality. Third, Shugan maintains that given predictive accuracy, assumptions need not be realistic. I hold an opposite view for the obvious reason that the same prediction may be generated by completely different mechanisms, which in turn are based on different assumptions. Last but not least, the example of socialist economic planning shows that unrealistic assumptions can generate dangerous theories.", "e:keyword": ["Assumptions", "Prediction", "Realism", "Mechanismic explanation", "Theory testing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0496", "e:abstract": "Eric Tsang's response makes the legitimate point that prediction and explanation can be different goals. However, his arguments also suffer from several errors in logic, most often the converse error. I do not claim that unrealistic assumptions breed good theories. I only claim that breakthrough theories usually have assumptions deemed unrealistic. Hence, unrealistic assumptions breed both good and bad theories. That is why science tests theories, not assumptions. Moreover, one can easily prove that realistic assumptions are not required. Consider situations when one of two competing theories must be correct. For example, in criminal cases, the prosecution's theory is that the accused committed the crime. The defense's theory is that the accused is innocent. One theory is correct despite the fact that both could make obviously unrealistic assumptions. For example, the prosecution might unrealistically assume that unreliable eyewitness testimony is sufficient to convict. The defense might unrealistically assume that a dubious alibi is sufficient to acquit. Juries decide on all the evidence and not each assumption. I now answer some of Eric Tsang's questions.", "e:keyword": ["Models", "Mathematical models", "Realistic assumptions", "Instrumentalism", "Empirical research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0493", "e:abstract": "In this research, we show that the interaction between territory allocation and sales force compensationtwo key drivers of sales productivitystrongly affects the firm's profitability. We analyze an agency-theoretic model that jointly considers the degree of negative or positive correlation across territory outcomes, differences in territories' sales potentials, the agency problem with risk-averse salespeople, and the availability of both own-territory compensation elements, such as commission, and elements dependent on the performance of others, such as group commissions or tournaments. We find that allocating salespeople to negatively correlated sales territories beneficially diversifies each salesperson's portfolio of sales outcomes when this allocation includes a group commission pay component, and can improve profitability even with a decrease in average territory sales performance. In a larger sales force, a balanced allocation of salespeople, coupled with a group commission, dominates an imbalanced allocation. Comparing piece-rate compensation (with or without a group commission component) to tournaments alongside the allocation problem, we find that tournaments are favored over piece-rate plans when territories are highly positively correlated, territory sales potentials are similar, and salespeople have a low disutility for effort and are not very risk-averse. A piece-rate plan conversely dominates a tournament when these conditions are reversed.", "e:keyword": ["Agency theory", "Channels of distribution", "Compensation", "Sales force"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0492", "e:abstract": "Consumers set goals to achieve a variety of objectives such as losing weight, saving for retirement, and achieving better health. A large body of literature in psychology and consumer behavior shows that goals can help consumers achieve these objectives. However, there is almost no research that examines how we should set optimal goals. The purpose of this paper is to develop a parsimonious framework that examines how goals can help performance and how we should set optimal goals. We use the literature on hyperbolic discounting to model these issues. Our results show that goals can often increase performance but can also sometimes encourage procrastination. We show that some goals are worse than having no goals, even when the goals are achieved and the consumer exerts more effort because of the goal. We also find that the presence of goals can lead to myopic consumers behaving as if they were hyperopic. Our results also show that the most difficult goals should be assigned to consumers with moderate levels of motivation and self-control problems. We also find that it is sometimes optimal to set goals that are never achieved.", "e:keyword": ["Hyperbolic discounting", "Optimal goal setting", "Behavioral economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0494", "e:abstract": "Prices for goods such as blades for razors, ink for printers, and concessions at movies are often set well above cost. Theory has shown that this could yield a profitable price discrimination strategy often termed metering. The idea is that a customer's intensity of demand for aftermarket goods (e.g., the concessions) provides a meter of how much the customer is willing to pay for the primary good (e.g., admission). If this correlation in tastes for the two goods is positive, a high price on the aftermarket good allows firms to extract a greater total price (admissions plus concessions) from higher-type customers. This paper develops a simple aggregate model of discrete-continuous demand to motivate how this correlation can be tested using simple regression techniques and readily available firm data. Model simulations illustrate that the regressions can be used to predict whether aftermarket prices should be above, below, or equal to their marginal cost. We then apply the approach to box office and concession data from a chain of Spanish theaters and find that high-priced concessions do extract more surplus from customers with a greater willingness to pay for the admission ticket.", "e:keyword": ["Metering", "Price discrimination", "Empirical industrial organization", "Entertainment", "Concession sales", "Movie theaters"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0497", "e:abstract": "The concept of one-to-one marketing is intuitively appealing, but there is little research that investigates the value of individual-level marketing relative to segment-level or mass marketing. In this paper, we investigate the financial benefits of and computational challenges involved in one-to-one marketing. The analysis uses data from an online grocery and drug retailer. Like many retailers, this firm uses multiple promotional instruments including discount coupons, free shipping offers, and a loyalty program. We investigate the impact of customizing these promotions on the two most important consumer decisions: the decision to buy from the store and expenditure. Our modeling approach accounts for two sources of heterogeneity in consumers' responsiveness to various marketing mix elements: cross-sectional differences across consumers and temporal differences within consumers based on the purchase cycle. The model parameter estimates are fed into a dynamic programming model that determines the optimal number, sequence, and timing of promotions to maximize retailer profits. A series of policy simulations show that customizing promotions leads to a significant increase in profits relative to the firm's current practice of uniform promotions. However, the effectiveness of various promotions varies because of both cross-sectional differences in consumers as well within consumer heterogeneity due to purchase cycle factors. For instance, we find that free shipping tends to be the preferred instrument for re-acquiring lapsed customers, whereas an across-the-board price cut (via a discount coupon) is the most effective tool for managing the segment of most active customers. Interestingly, we find that customizing based on within-customer temporal heterogeneity contributes more to profitability than exploiting variations across consumers. This is important because the computational burden of implementing the dynamic optimization to account for cross-sectional heterogeneity is far greater than accounting for temporal heterogeneity. Furthermore, targeting promotions based only on timing rather than the nature and magnitude of the offers across consumers alleviates the public relations risks of price discrimination. Implications for marketing managers are also discussed.", "e:keyword": ["CRM", "One-to-one marketing", "Promotions", "Dynamic programming", "Heterogeneity", "Database marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0499", "e:abstract": "Online retailing boasts two major advantages: convenience of home shopping and easy access to information. In this paper, I argue that these two features have important implications for retailers' channel and advertising decisions. Two major questions are addressed: When should a conventional bricks-and-mortar retailer adopt a multichannel strategy? When should a multichannel retailer use its website to advertise offline prices? Analysis shows that the answers hinge on the nature of the product, the retailer's costs, and the competitors' strategies as well as the competitiveness of the market. Multichannel retailing is not necessarily the best strategy for all retailers; no adoption, asymmetric adoption, and symmetric adoption of the strategy are all possible equilibria. Advertising the in-store prices online is not always optimal. Price advertising in multichannel retailing has a different effect when compared with conventional single-channel retailing. It helps coordinate the channels by shifting the sales from online to offline, which is particularly useful when margins online are relatively low. The finding that multichannel retailers can benefit from drawing consumers back to physical stores highlights the risk of boosting online sales without considering the adverse effect on the offline channel and indicates a shifting role of the Web in retailers' businesses.", "e:keyword": ["Multichannel retailing", "Internet", "Price advertising", "Distribution channel", "Channel coordination", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0513", "e:abstract": "Brands often form alliances to enhance their brand equities. In this paper, we examine the alliances between professional athletes (athlete brands) and sports teams (team brands) in the National Basketball Association (NBA). Athletes and teams match to maximize the total added value created by the brand alliance. To understand this total value, we estimate a structural two-sided matching model using a maximum score method. Using data on the free-agency contracts signed in the NBA during the four-year period from 1994 to 1997, we find that both older players and players with higher performance are more likely to match with teams with more wins. However, controlling for performance, we find that brand alliances between high brand equity players (defined as receiving enough votes to be an all-star starter) and medium brand equity teams (defined by stadium and broadcast revenues) generate the highest value. This suggests that top brands are not necessarily best off matching with other top brands. We also provide suggestive evidence that the maximum salary policy implemented in 1998 influenced matches based on brand equity spillovers more than matches based on performance complementarities.", "e:keyword": ["Branding", "Brand alliances", "Sports marketing", "Matching model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0514", "e:abstract": "The purpose of this article is to analyze how competitive forces may influence the way media firms like TV channels raise revenue. A media firm can either be financed by advertising revenue, by direct payment from the viewers (or the readers, if we consider newspapers), or by both. We show that the scope for raising revenues from consumer payment is constrained by other media firms offering close substitutes. This implies that the less differentiated the media firms' content, the larger is the fraction of their revenue coming from advertising. A media firm's scope for raising revenues from ads, on the other hand, is constrained by how many competitors it faces. We should thus expect that direct payment from the media consumers becomes more important the larger the number of competing media products.", "e:keyword": ["Advertising", "Media economics", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0519", "e:abstract": "In this paper, we study the practice of overselling in a competitive environment where late-arriving consumers value the good higher than early-arriving ones but the former's arrival is uncertain. We show that overselling is a dominant strategy for the firms. However, it can lead to a prisoners' dilemma situation in which all firms are worse off overselling. We further show that only when demand from the late consumers far exceeds the supply and there is a sufficiently high profit margin from reselling does overselling result in a Pareto-dominant outcome for the firms.", "e:keyword": ["Overselling", "Overbooking", "Pricing", "Revenue management", "Competition", "Capacity constraints"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0521", "e:abstract": "Manufacturers often have a choice of whether to advertise something positive about their own products without mentioning their rivals' products (a noncomparative ad) or whether to portray their rivals negatively in addition to promoting their own products (a comparative ad). In this paper we ask: First, if a manufacturer in a distribution channel can choose between a comparative ad and a noncomparative ad, all else being equal, which should it choose? Second, under what conditions would a manufacturer want to reinforce its advertising message at the point of sale with in-store displays, and when should the retailer allow the displays? Third, how does the possibility of in-store displays influence the manufacturer's choice of ad content? We find that a manufacturer will prefer to run comparative ads over noncomparative ads for advertising that is untargeted or that appeals primarily to the manufacturer's core consumers, and run noncomparative ads over comparative ads for advertising that appeals primarily to the rival's core consumers. We also find that in-store displays will be optimal for the manufacturer and its retailers if and only if they increase the overall joint profit of the retailer, the manufacturer, and its rival. Finally, we find that the possibility of offering in-store displays increases a manufacturer's incentive to run noncomparative ads. However, some comparative ads may be so attractive to the manufacturer that it will run them with or without retailer help. Our paper is the first to introduce a channel-based explanation for why manufacturers may or may not want to engage in comparative advertising.", "e:keyword": ["Game theory", "Channel coordination", "Comparative advertising", "Distribution channel"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0528", "e:abstract": "The existing marketing literature suggests that persuasive advertising elicits counteractions from competing manufacturers and consequently leads to wasteful cancellation of the advertising effects. Thus, persuasive advertising is widely perceived to be combative in nature. A series of previously published papers demonstrates that appropriate targeting may partially mitigate the combative nature of persuasive advertising in that either the rival manufacturer or the retailer may benefit. In this paper, we complement their results by demonstrating the possibility that every channel member may benefit from persuasive advertising, i.e., a Pareto improvement along the distribution channel, thereby leading to the conclusion that persuasive advertising need not result in channel conflict.", "e:keyword": ["Persuasive advertising", "Product substitutability", "Channel conflict", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0464", "e:abstract": "Analysts often rely on methods that presume constant stochastic variance, even though its degree can differ markedly across experimental and field settings. This reliance can lead to misestimation of effect sizes or unjustified theoretical or behavioral inferences. Classic utility-based discrete-choice theory makes sharp, testable predictions about how observed choice patterns should change when stochastic variance differs across items, brands, or conditions. We derive and examine the implications of assuming constant stochastic variance for choices made under different conditions or at different times, in particular, whether substantive effects can arise purely as artifacts. These implications are tested via an experiment designed to isolate the effects of stochastic variation in choice behavior. Results strongly suggest that the stochastic component should be carefully modeled to differ across both available brands and temporal conditions, and that its variance may be relatively greater for choices made for the future. The experimental design controls for several alternative mechanisms (e.g., flexibility seeking), and a series of related models suggest that several econometrically detectable explanations like correlated error, state dependence, and variety seeking add no explanatory power. A series of simulations argues for appropriate flexibility in discrete-choice specification when attempting to detect temporal stochastic inflation effects.", "e:keyword": ["Brand choice", "Choice models", "Decisions under uncertainty", "Decision making over time", "Econometric models", "Lab experiments", "Measurement and inference", "Probability models", "Simulation", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0474", "e:abstract": "We discuss the Salisbury and Feinberg paper [Salisbury, L. C., F. M. Feinberg. 2010. Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. (1) 117], setting their contribution in the historical context of the wider literature on the role of error variability in discrete choice models. We discuss the seminal nature of their contribution and suggest that the paper should be required reading for current and future Ph.D. students.", "e:keyword": ["Choice models", "Stochastic utility", "Variance"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0511", "e:abstract": "The implications of Salisbury and Feinberg's (2010) paper [Salisbury, L. C., F. M. Feinberg. 2010. Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. (1) 117] for the process of model development and testing in the field of intertemporal choice analysis is explored. Although supporting the overall thrust of Salisbury and Feinberg's critique of previous empirical work in the area, we also see their paper as illustrating the dangers of drawing strong inferences about the behavioral interpretation of statistical model parameters without seeking convergent empirical evidence. In particular, we are skeptical about the extent to which the reported effects of temporal distance on the estimated scale parameter, σ, are uniquely, or even primarily, due to unobserved error inflation that reflects consumer's uncertainty about future utility. This interpretation is brought into question by several lines of reasoning. Conceptually, we note that uncertainty is different from error and that, for choice data, the error inflation model is mathematically identical to a model in which the scale parameter is a deterministic function of the temporal discount rate. Empirically, a reanalysis of data from previously published experiments does not consistently support temporal error inflation, temporal convergence of choice shares, or the scale parameter as an explanation of variety seeking in choice sequences. In our opinion, the cumulative results of research on intertemporal choice require models in which the attributes of choice alternatives are differentially discounted over time. Despite these findings, we advocate that choice researchers should indeed follow Salisbury and Feinberg's advice to not assume that error variances will be unaffected by experimental manipulations, and such effects should be explicitly modeled. We also agree that uncovering effects on error variance is just the first step, and the ultimate goal should be to rigorously explain the for such effects.", "e:keyword": ["Behavioral decision theory", "Choice modeling", "Measurement and inference", "Behavioral economics", "Random utility models", "Intertemporal choice", "Psychological process models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0530", "e:abstract": "We examine the specification and interpretation of discrete-choice models used in behavioral theory testing, with a focus on separating coefficient scale from error scale, particularly over time. Numerous issues raised in the thoughtful commentaries of Louviere and Swait [Louviere, J., J. Swait. 2010. Discussion of Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. (1) 1822] and Hutchinson, Zauberman, and Meyer (HZM) [Hutchinson, J. W., G. Zauberman, R. Meyer. 2010. On the interpretation of temporal inflation parameters in stochastic models of judgment and choice. (1) 2331] are addressed, specifically the roles of response scaling, preference covariates, actual versus hypothetical consumption, immediacy, and heterogeneity, as well as key differences between the experimental setup in Salisbury and Feinberg [Salisbury, L. C., F. M. Feinberg. 2010. Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. (1) 117] and those typifying intertemporal choice and construal level theory. We strongly concur with most of the general conclusions put forth by the commentary authors, but we also emphasize a central point made in our research that may have been lost: that the temporal inflation effects observed in our empirical analysis could be attributed to stochastic effects, deterministic influences, or an amalgam; appropriate inferences depend on the nature of one's data and stimuli. We also report on further analyses of our data, as well as a meta-analysis of HZM's Table 1 that is consistent with our original findings. Implications for, and dimensions relevant to, future research on temporal stochastic inflation and its role in choice-based research are discussed.", "e:keyword": ["Brand choice", "Choice models", "Construal level theory", "Decisions under uncertainty", "Decision making over time", "Econometric models", "Intertemporal choice", "Measurement and inference", "Probability models", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0466", "e:abstract": "A common theme in marketing literature is the acquisition and retention of customers as they trade up from inexpensive introductory offerings to those of higher quality. We develop a nonhomothetic choice model to accommodate effects of advertising, professional recommendation, and other factors that facilitate the description and management of trade-up. Our model allows advertising to affect the relative superiority or inferiority of products. This allows for a wide variety of trade-up patterns beyond those obtained from a standard random utility formulation of the logit model. Our nonhomothetic model allows for advertising to affect more than just brand intercepts (perceived quality), but also the rate at which consumers are willing to trade up to higher-quality brands. Advertising effects are measured using a randomized treatment and evaluated by considering their direct implications for firm pricing and profits.", "e:keyword": ["Nonhomothetic utility", "Advertising", "Quality", "Discrete choice"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0472", "e:abstract": "This research aims to provide insights into the determinants of channel profitability and the relative power in the channel by considering consumer demand and the interactions between manufacturers and retailers in an equilibrium model. We use the Nash bargaining solution to determine wholesale prices and thus how margins are split in the channel. Equilibrium margins are a function of demand primitives and of retailer and manufacturer bargaining power. Bargaining power is itself a function of exogenous retail and manufacturer characteristics. The parties' bargaining positions are determined endogenously from the estimated substitution patterns on the demand side. The more they have to lose in a negotiation relative to an outside option, the weaker the bargaining position. We use the proposed bargaining model to investigate the role of the three main factors that have been blamed for the power shift from manufacturers to retailers in recent years (firm-size increases, store-brand introductions, and service-level differentiation). In our empirical analysis of the German market for coffee, we find that bargaining power varies among the different manufacturer-retailer pairs. This result suggests that bargaining power is not an inherent characteristic of a firm but rather depends on the negotiation partner. We are able to confirm empirically previous theoretical findings that there can be cases where the slice of the pie that goes to one of the channel members may decrease, but the overall pie increases and compensates for the smaller share of profits.", "e:keyword": ["Bargaining", "Distribution channels", "Competitive strategy", "Econometric models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0480", "e:abstract": "We examine a previously unstudied category of exchange items in which the true value is unknown to both the buyer and seller at the time of exchange but becomes known to both at a future time after the exchange. Real-world examples of such exchange items as in our study include forward contracts and fixed-fee turnkey contracts. We demonstrate that the discrepancy between the seller's willingness to accept (WTA) and buyer's willingness to pay (WTP) increases with (1) the level of uncertainty about the exchange item's value and (2) the exchange parties' level of risk aversion. In a series of studies, we manipulate and measure the level of uncertainty of the exchange item, measure the level of risk aversion of the exchange parties, and study the respective effects on decreasing the WTP while increasing the WTA.", "e:keyword": ["Exchange", "Valuation", "Willingness to pay", "Willingness to accept", "Buyer", "Seller", "Uncertainty", "Risk aversion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0482", "e:abstract": "The past few years have seen increasing interest in taking the notion of customer lifetime value (CLV) and extending it to value a customer base (with subsequent links to corporate valuation). The application of standard textbook discussions of CLV leads to calculations based on a single retention rate. However, at the cohort level, retention rates typically increase over time. It has been suggested that these observed dynamics are due, in large part, to a sorting effect in a heterogeneous population. We show that failing to recognize these dynamics yields a downward-biased estimate of the residual value of the customer base (compared to an aggregate analysis that ignores these dynamics). We also explore the implications of failing to account for retention dynamics when computing retention elasticities and find that the frequently reported values underestimate the true effect of increases in underlying retention rates in a heterogeneous world.", "e:keyword": ["Customer lifetime value", "Shifted-beta-geometric", "Retention elasticity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0483", "e:abstract": "We study the effects of retailer in-store media on distribution channel relationships. Retailers open in-store media (ISM) and allow manufacturers to advertise to shoppers. Our results suggest that ISM has an important role in coordinating a distribution channel on advertising volume and product sales, and on mitigating supplier competition. Improved channel coordination is achieved through the internalization of advertising decisions from commercial forms of media (e.g., radio, TV, newspaper). A retailer may strategically subsidize manufacturers for their advertising on ISM. This subsidy is optimal even if ISM is more effective than commercial media. With manufacturer competition, a retailer can strategically use a competitive premium to ration excessive advertising between competing suppliers in a category. When manufacturers are asymmetric with preadvertising brand awareness, a retailer has an incentive to price discriminate by charging lower prices to manufacturers whose brand awareness is higher.", "e:keyword": ["In-store media", "Advertising", "Distribution channel", "Channel coordination", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1080.0484", "e:abstract": "Referral bonuses, in which an existing customer gets an in-kind or cash reward for referring a new customer, are a popular way to stimulate word of mouth. In this paper, we examine key firm decisions about such bonuses. Others have studied referral bonus programs; a key difference is that we study the role of recommendations not just in spreading awareness (as they do) but also in providing assessments. We start with the idea that people have a variety of reasons for making product recommendations, including placing a value on a friend's outcome with a product they recommend. We apply that idea in a context of asymmetric information: A customer combines his knowledge about the product and his familiarity with friends' tastes, making him more informed than the friends. Thus, the recommendation is a signal about the value of the product to the friend. In this setting, we consistently find that the greater the concern for others' outcomes, the higher the referral bonus should be, as long as the firm cannot more efficiently motivate recommendations with a lower price. Moreover, if price is the more efficient lever, the optimal bonus is zero, and the optimal price is low. We also show that greater concern tends to reduce firm profit and, in some cases, actually reduces consumer welfare as well.", "e:keyword": ["Word of mouth", "Reward programs", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0486", "e:abstract": "This paper examines the interaction of information provision, product quality, and pricing decisions by competitive firms to explore the following question: in a competitive market where consumers face uncertainty about product quality and/or their preference for quality, which firmsthose that sell higher- or lower-quality productshave the higher incentive to provide what type of information? We find that while the higher-quality firm should always provide information resolving consumer uncertainty on product quality, the lower-quality firm under certain conditions will have the higher incentive to and will be the one to provide information resolving consumer uncertainty about their quality preferences. In the analysis, we trace the latter result to competition and to free-riding on the information provision. Specifically, in a monopoly market or when consumer free-riding is restricted by the costliness of store visits, the lower-quality firm would have a lower incentive to provide information resolving consumer preference uncertainty than otherwise. The model is also adapted to examine product returns as a possible strategy of information provision.", "e:keyword": ["Uncertainty", "Information", "Competitive strategy", "Service", "Free-riding", "Game theory", "Product returns"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0490", "e:abstract": "Choice decisions in the marketplace are often made by a collection of individuals or a group. Examples include purchase decisions involving families and organizations. A particularly unique aspect of a joint choice is that the group's preference is very likely to diverge from preferences of the individuals that constitute the group. For a marketing researcher, the biggest hurdle in measuring group preference is that it is often infeasible or cost prohibitive to collect data at the group level. Our objective in this research is to propose a novel methodology to estimate joint preference without the need to collect joint data from the group members. Our methodology makes use of both stated and inferred preference measures, and merges experimental design, statistical modeling, and utility aggregation theories to capture the psychological processes of preference revision and concession that lead to the joint preference. Results based on a study involving a cell phone purchase for 214 parent-teen dyads demonstrate predictive validity of our proposed method.", "e:keyword": ["Joint decision making", "Preference revision", "Utility aggregation", "Bayesian"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0498", "e:abstract": "Many consumer durable retailers often do not advertise their prices and instead ask consumers to call them for prices. It is easy to see that this practice increases the consumers' cost of learning the prices of products they are considering, yet firms commonly use such practices. Not advertising prices may reduce the firm's advertising costs, but the strategic effects of doing so are not clear. Our objective is to examine the strategic effects of this practice. In particular, how does making price discovery more difficult for consumers affect competing retailers' price, service decisions, and profits? We develop a model in which a manufacturer sells its product through a high-service retailer and a low-service retailer. Consumers can purchase the retail service at the high-end retailer and purchase the product at the competing low-end retailer. Therefore, the high-end retailer faces a free-riding problem. A retailer first chooses its optimal service levels. Then, it chooses its optimal price levels. Finally, a retailer decides whether to advertise its prices. The model results in four structures: (1) both retailers advertise prices, (2) only the low-service retailer advertises price, (3) only the high-service retailer advertises price, and (4) neither retailer advertises price. We find that when a retailer does not advertise its price and makes price discovery more difficult for consumers, the competition between the retailers is less intense. However, the retailer is forced to charge a lower price. In addition, if the competing retailer does advertise its prices, then the competing retailer enjoys higher profit margins. We identify conditions under which each of the above four structures is an equilibrium and show that a low-service retailer not advertising its price is a more likely outcome than a high-service retailer doing so. We then solve the manufacturer's problem and find that there are several instances when a retailer's advertising decisions are different from what the manufacturer would want. We describe the nature of this channel coordination problem and identify some solutions.", "e:keyword": ["Price advertising", "Channel coordination", "Retailing", "Free-riding"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0512", "e:abstract": "We examine two questions: Does the roundness or precision of prices bias magnitude judgments? If so, do these biased judgments affect buyer behavior? Results from five studies suggest that buyers underestimate the magnitudes of precise prices. We term this the precision effect. The first three studies are laboratory experiments designed to test the existence of the precision effect and examine the underlying psychological processes. In Study 1, we find that precise prices are judged to be smaller than round prices of similar magnitudes. For example, participants in this experiment incorrectly judged $395,425 to be smaller than $395,000. In Study 2, we show that precision is more likely to affect magnitude judgments under conditions of uncertainty. Study 3 demonstrates that manipulating prior experience with the pattern of roundness and precision in numbers can moderate the precision effect. Studies 4 and 5 examine whether the precision effect influences buyers' willingness to pay for negotiated purchases (e.g., houses). In Study 4, we conduct an experiment on a nationally representative sample of homeowners to demonstrate that participants are willing to pay more for houses when the sellers use precise (e.g., $364,578) instead of comparable round (e.g., $365,000) prices. In Study 5, we analyze data from residential real estate transactions in two separate markets and find that buyers pay higher sale prices when list prices are more precise. These empirical results enrich our understanding of the psychological processes that underlie price magnitude judgments and have substantive implications for buyer and seller behavior.", "e:keyword": ["Behavioral pricing", "Numerical cognition", "Roundness", "Precision", "Heuristics", "Biases"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0517", "e:abstract": "Paid placements on search engines reached sales of nearly $11 billion in the United States last year and represent the most rapidly growing form of online advertising today. In its classic form, a search engine sets up an auction for each search word in which competing websites bid for their sponsored links to be displayed next to the search results. We model this advertising market, focusing on two of its key characteristics: (1) the interaction between the list of search results and the list of sponsored links on the search page and (2) the inherent differences in attractiveness between sites. We find that both of these special aspects of search advertising have a significant effect on sites' bidding behavior and the equilibrium prices of sponsored links. Often, sites that are not among the most popular ones obtain the sponsored links, especially if the marginal return of sites on clicks is quickly decreasing and if consumers do not trust sponsored links. In three extensions, we also explore (1) heterogeneous valuations across bidding sites, (2) the endogenous choice of the number of sponsored links that the search engine sells, and (3) a dynamic model where websites' bidding behavior is a function of their previous positions on the sponsored list. Our results shed light on the seemingly random order of sites on search engines' list of sponsored links and their variation over time. They also provide normative insights for both buyers and sellers of search advertising.", "e:keyword": ["Internet marketing", "Position auctions", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0541", "e:abstract": "This paper develops a framework for measuring tippingthe increase in a firm's market share dominance caused by indirect network effects. Our measure compares the expected concentration in a market to the hypothetical expected concentration that would arise in the absence of indirect network effects. In practice, this measure requires a model that can predict the counterfactual market concentration under different parameter values capturing the strength of indirect network effects. We build such a model for the case of dynamic standards competition in a market characterized by the classic hardware/software paradigm. To demonstrate its applicability, we calibrate it using demand estimates and other data from the 32/64-bit generation of video game consoles, a canonical example of standards competition with indirect network effects. In our example, we find that indirect network effects can lead to a strong, economically significant increase in market concentration. We also find important roles for beliefs on both the demand side, as consumers tend to pick the product they expect to win the standards war, and on the supply side, as firms engage in penetration pricing to invest in growing their networks.", "e:keyword": ["Dynamic oligopoly", "Indirect network effects", "Tipping", "Standards war", "High technology"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0509", "e:abstract": "Packaged goods manufacturers spend in excess of $75 billion annually on trade promotions, even though their effectiveness has been hotly debated by academics and practitioners for decades. One reason for this ongoing debate is that empirical research has been limited mostly to case studies, managerial surveys, and data from one or two supermarket chains in a single market. In this paper, we assemble a unique data set containing information on prices, quantities, and promotions throughout the entire channel in a category. Our study extends the empirical literature on pass-through in three important ways. First, we investigate how pass-through varies across more than 1,000 retailers in over 30 states. Second, we study pass-through at multiple levels of the distribution channel. Third, we show how the use of accounting metrics, such as average acquisition cost, rather than transaction cost, yields biased estimates of pass-through and therefore overstates the effectiveness of trade promotions. We find that mean pass-through elasticities are 0.71, 0.59, and 0.41, for the wholesaler, retailer, and total channel, respectively. More importantly, at each level of the channel we observe large variances in pass-through estimates that we explain using various measures of cost and competition. Surprisingly, we find that market structure and competition have a relatively small impact on pass-through. We conclude by showing how the profitability of manufacturer and wholesaler deals can be improved by utilizing detailed effectiveness estimates. For example, a manufacturer using an trade promotion strategy might offer a 10% off invoice deal to all retailers on every product. This strategy would decrease manufacturer and wholesaler profits for 56% of product/store combinations, whereas retailers experience a profit boost in 96% of cases. Manufacturers and wholesalers can avoid unprofitable trade deals for specific products and retailers by utilizing estimates of pass-through, consumer price elasticity, and margins. Compared to the inclusive strategy, such a trade promotion strategy would improve deal profitability by 80% and reduce costs by 40%.", "e:keyword": ["Trade promotion", "Pass-through", "Channels", "Competition", "Measurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0516", "e:abstract": "During the summer of 2005, the three domestic U.S. automobile manufacturers offered a customer promotion that allowed customers to buy new cars using discount programs formerly offered only to employees. The initial months of the promotion were record sales months for each of the three firms, suggesting that customers thought that the prices offered during the promotion were particularly attractive. In reality, however, many customers paid prices under the employee discount pricing promotion. We propose that the promotion changed customers' beliefs about current versus future prices, convincing them to purchase during the promotion rather than delay in anticipation of future discounts. We investigate several alternative explanations for the simultaneous increase in prices and sales, including advertising, decreased financing costs, industry trends, disutility of bargaining, consumer differences, and changes in trade-in values. None of these explanations fully explains the concomitant increase in prices and sales.", "e:keyword": ["Pricing", "Sales promotions", "Natural experiments"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0506", "e:abstract": "We present a framework of durable goods purchasing behavior in related technology product categories that incorporates the following aspects unique to technology product purchases. First, it accounts for consumers' anticipation of declining prices (or increasing quality) over time. Second, the durable nature of technology products implies that even if two categories are related as complements, consumers may stagger their purchases over several periods. Third, the forward-looking consumer decision process, as well as the durable nature of technology products, implies that a consumer's purchase in one category will depend on the anticipated price and quality trajectories of all categories. As a potential aid to future researchers, we also lay out the data requirements for empirically estimating the parameters of our model and the consequences of not having various elements of these data on our ability to estimate the parameters. The data available for our empirical analysis are household-level information on -level first-time decisions in three categoriespersonal computers, digital cameras, and printers. Our results reveal a strong complementary relationship between the three categories. Policy simulations based on a temporary price decrease in any one category provide interesting insights into how consumers would modify their adoption behavior over time and also across categories as a consequence of the price decrease.", "e:keyword": ["Technology products", "Consumer adoption", "Complementary products", "Forward-looking consumers", "Econometric models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0500", "e:abstract": "Mere observation of others' choices can be informative about product quality. This paper develops an individual-level dynamic model of observational learning and applies it to a novel data set from the U.S. kidney market, where transplant candidates on a waiting list sequentially decide whether to accept a kidney offer. We find strong evidence of observational learning: patients draw negative quality inferences from earlier refusals in the queue, thus becoming more inclined towards refusal themselves. This self-reinforcing chain of inferences leads to poor kidney utilization despite the continual shortage in kidney supply. Counterfactual policy simulations show that patients would have made more efficient use of kidneys had the concerns behind earlier refusals been shared. This study yields a set of marketing implications. In particular, we show that observational learning and information sharing shape consumer choices in markedly different ways. Optimal marketing strategies should take into account how consumers learn from others.", "e:keyword": ["Observational learning", "Learning models", "Informational cascades", "Herding", "Quality inference", "Bayes' rule", "Dynamic programming", "Kidney allocation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0507", "e:abstract": "In this study we develop a method that optimally selects online media vehicles and determines the number of advertising impressions that should be purchased and then served from each chosen website. As a starting point, we apply Danaher's [Danaher, P. J. 2007. Modeling page views across multiple websites with an application to Internet reach and frequency prediction. (3) 422437] multivariate negative binomial distribution (MNBD) for predicting online media exposure distributions. The MNBD is used as a component in the broader task of media selection. Rather than simply adapting previous selection methods used in traditional media, we show that the Internet poses some unique challenges. Specifically, online banner ads and other forms of online advertising are sold by methods that differ substantially from the way other media advertising is sold. We use a nonlinear optimization algorithm to solve the optimization problem and derive the optimum online media schedule. Data from an online audience measurement firm and an advertising agency are used to illustrate the speed and accuracy of our method, which is substantially quicker than using complete enumeration.", "e:keyword": ["Advertising", "Internet marketing", "Media", "Optimization", "Probability models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0520", "e:abstract": "In a viral marketing campaign, an organization develops a marketing message and encourages customers to forward this message to their contacts. Despite its increasing popularity, there are no models yet that help marketers to predict how many customers a viral marketing campaign will reach and how marketers can influence this process through marketing activities. This paper develops such a model using the theory of branching processes. The proposed viral branching model allows customers to participate in a viral marketing campaign by (1) opening a seeding e-mail from the organization, (2) opening a viral e-mail from a friend, and (3) responding to other marketing activities such as banners and offline advertising. The model parameters are estimated using individual-level data that become available in large quantities in the early stages of viral marketing campaigns. The viral branching model is applied to an actual viral marketing campaign in which over 200,000 customers participated during a six-week period. The results show that the model quickly predicts the actual reach of the campaign. In addition, the model proves to be a valuable tool to evaluate alternative what-if scenarios.", "e:keyword": ["Branching processes", "Forecasting", "Markov processes", "Online marketing", "Viral marketing", "Word of mouth"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0518", "e:abstract": "We present a demand system for tied goods incorporating dynamics arising from the tied nature of the products and the stockpiling induced by storability and durability. We accommodate competition across tied good systems and competing downstream retail formats by endogenizing the retail format at which consumers choose to stockpile inventory. This facilitates measurement of long-run retail substitution effects and yields estimates of complementarities within, and substitution across, competing systems of tied goods. We present an empirical application to an archetypal tied goods category: razors and blades. We discuss the implications of measured effects for manufacturer pricing when selling the tied products through an oligopolistic downstream retail channel and assess the extent to which retail substitution reduces channel conflict.", "e:keyword": ["Tied goods", "Retail competition", "Dynamic discrete choice", "Durable good replacement", "Endogenous consumption", "Long-run effects", "Vertical channels", "Razor blade market"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0508", "e:abstract": "The mixed or heterogeneous multinomial logit (MIXL) model has become popular in a number of fields, especially marketing, health economics, and industrial organization. In most applications of the model, the vector of consumer utility weights on product attributes is assumed to have a multivariate normal (MVN) distribution in the population. Thus, some consumers care more about some attributes than others, and the IIA property of multinomial logit (MNL) is avoided (i.e., segments of consumers will tend to switch among the subset of brands that possess their most valued attributes). The MIXL model is also appealing because it is relatively easy to estimate. Recently, however, some researchers have argued that the MVN is a poor choice for modelling taste heterogeneity. They argue that much of the heterogeneity in attribute weights is accounted for by a pure scale effect (i.e., across consumers, all attribute weights are scaled up or down in tandem). This implies that choice behaviour is simply more random for some consumers than others (i.e., holding attribute coefficients fixed, the scale of their error term is greater). This leads to a scale heterogeneity MNL model (S-MNL). Here, we develop a generalized multinomial logit model (G-MNL) that nests S-MNL and MIXL. By estimating the S-MNL, MIXL, and G-MNL models on 10 data sets, we provide evidence on their relative performance. We find that models that account for scale heterogeneity (i.e., G-MNL or S-MNL) are preferred to MIXL by the Bayes and consistent Akaike information criteria in all 10 data sets. Accounting for scale heterogeneity enables one to account for extreme consumers who exhibit nearly lexicographic preferences, as well as consumers who exhibit very random behaviour (in a sense we formalize below).", "e:keyword": ["Choice models", "Mixture models", "Consumer heterogeneity", "Choice experiments"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0524", "e:abstract": "This paper examines demand elasticities using an integrated framework proposed by Hanemann [Hanemann, M. W. 1984. Discrete/continuous models of consumer demand. (3) 541561], which models the incidence, brand choice, and quantity decisions of a consumer as an outcome of her utility maximization subject to budget constraints. Although the Hanemann framework has been the mainstay of earlier efforts to examine these decisions jointly, empirical researchers who have used the it to study purchase behavior have often found that the quantity elasticities are around -1, regardless of the brand or category. We attempt to uncover the underlying reasons for this finding and propose approaches to get as close to the true quantity elasticities as possible. We do this by (i) analytically demonstrating how assumptions on the distribution of the brand-specific econometrician's errors imply certain restrictions that in turn force quantity elasticities to -1, (ii) discussing how these restrictions can be alleviated by considering a suitable specification of unobserved parameter heterogeneity, and (iii) using scanner data to empirically illustrate the impact of the restrictions on quantity elasticities and the relative efficacy of multiple specifications of unobserved heterogeneity in easing those restrictions. We find that the specification of unobserved heterogeneity influences estimates of quantity elasticities and that the mixture normal specification outperforms the alternatives.", "e:keyword": ["Microeconomic theory of demand", "Quantity elasticities", "Hanemann's framework", "Stochastic properties of distribution functions of econometrician's errors", "Stochastic properties of finite mixture/normal random/mixture normal specifications of heterogeneity", "Structural approach"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0525", "e:abstract": "We introduce and test a behavioral model of consumer product search that extends a baseline normative model of sequential search by incorporating nonnormative influences that are local in the sense that they reflect consumers' undue sensitivity to recently encountered alternatives. We propose two types of such local behavioral influences that, at each stage of a search process, can manifest themselves both in which of the products inspected up to that point is deemed to be the most preferred one (the product comparison decision) and whether to terminate the search at that stage (the stopping decision). The first of these influences is that consumers respond excessively to the attractiveness of the currently inspected product, at the expense of all others (focalism). The second proposed behavioral influence is that consumers overreact to the difference in attractiveness between the current product and the one encountered just prior to it (local contrast). Converging evidence from two experiments, which combine to guarantee both high internal and high external validity, provides support for the proposed behavioral influences. Our findings demonstrate that consumers' product comparison and stopping decisions in sequential product search are jointly governed by normative principles and by the proposed local behavioral influences.", "e:keyword": ["Search behavior", "Consumer product search", "Decision making", "Behavioral decision theory", "Consumer behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0529", "e:abstract": "The price-aggressive discount format, popularized by chains such as Aldi and Lidl, is very successful in most Western economies. Its success is a major source of concern for traditional supermarkets. Discounters not only have a direct effect on supermarkets' market shares, but they also exert considerable pressure to improve operational efficiency and/or to decrease prices. We use an empirical entry model to study the degree of intra- and interformat competition between discounters and supermarkets. Information on the competitive impact of new entrants is derived from the observed entry decisions of supermarkets and discounters in a large cross section of local markets, after controlling for a number of local market characteristics. In our modeling framework, we endogenize the number of retailers and allow for asymmetric intra- and interformat competitive effects in a flexible way. We apply our modeling approach to the German grocery industry, where the discount format has stabilized after two decades of continued growth. We find evidence of intense competition within both the supermarket and discounter format, although competition between supermarkets is found to be more severe. Most importantly, discounters only start to affect the profitability of conventional supermarkets from the third entrant onwards. This may explain why many retailers rush to add a discount chain to their portfolio: early entrants may benefit from the growth of the discount-prone segment without cannibalizing the profits of their more conventional supermarket stores.", "e:keyword": ["Empirical entry model", "Hard discounters", "Interformat competition", "Grocery industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0533", "e:abstract": "This paper studies a manufacturer's optimal decisions on extending its product line when the manufacturer sells through either a centralized channel or a decentralized channel. We show that a manufacturer may provide a longer product line for consumers in a decentralized channel than in a centralized channel if the market is fully covered. In addition, a manufacturer's decisions on the length of its product line may not always be optimal from a social welfare perspective in either a centralized or a decentralized channel. Under certain conditions, a decentralized channel can provide the product line length that is socially optimal, whereas a centralized channel cannot.", "e:keyword": ["Product line", "Distribution channel", "Consumer heterogeneity", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0534", "e:abstract": "Manufacturers can acquire consumer information in a sequential manner and influence downstream retail behavior through sharing the acquired information. This paper examines the interaction between a manufacturer's optimal information acquisition and sharing strategies in a vertical relationship, capturing the impacts of both the flexibility to sequentially control information collection and the flexibility in ex post voluntary sharing. We show that when information acquisition is sequential, the manufacturer may not acquire perfect information even if it is costless to do so. This self-restriction in information acquisition follows from the manufacturer's motivation to strategically influence retail behavior. When information acquisition is inflexible and constrained to be either zero or perfect information, the manufacturer acquires less (more) information under mandatory (voluntary) sharing. Moreover, voluntary sharing unambiguously leads to more information being generated, because the manufacturer has the option to strategically withhold the acquired information that turns out to be unfavorable. Finally, the conditions under which the manufacturer ex ante prefers a particular sharing format are examined.", "e:keyword": ["Information acquisition", "Information sharing", "Channel", "Disclosure", "Vertical relationship"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0535", "e:abstract": "This paper shows that when the alternatives offered to consumers span the preference space (as it would be chosen by a firm), search or evaluation costs may lead consumers not to search and not to choose if too many or too few alternatives are offered. If too many alternatives are offered, then the consumer may have to engage in many searches or evaluations to find a satisfactory fit. This may be too costly and result in the consumer avoiding making a choice altogether. If too few alternatives are offered, then the consumer may not search or choose, fearing that an acceptable choice is unlikely. These two forces result in the existence of a finite optimal number of alternatives that maximizes the probability of choice.", "e:keyword": ["Product line", "Choice", "Search", "Information overload"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0536", "e:abstract": "Panel survey data have been gaining importance in marketing. However, one challenge of estimating econometric models based on panel survey data is how to account for underreporting; that is, respondents do not report behavioral incidences that actually occur. Underreporting is especially likely to occur in a panel survey because the data-recording mechanism is often tedious, complex, and effortful. The probability of underreporting is likely to vary across respondents and also over the duration of the survey period. In this paper, we propose a model to simultaneously study reported behavioral incidences and partially observed actual behavioral incidences. We propose a Bayesian approach for estimating the proposed model. We treat those unobserved actual behavioral incidences as latent variables, and the Gibbs sampler makes it convenient to impute the nonreported consumption incidences along with making inferences on other model parameters. Our proposed method has two advantages. First, it offers a model-based approach to remove the underreporting bias in panel survey data and therefore allows marketing researchers to make accurate inferences about consumers' actual behavior. Second, the method also offers a natural way to study factors that influence respondents' propensity to underreport. Because we treat those underreported behavioral incidences as nonmissing at random, this underreporting propensity varies across respondents and over time. This understanding can help marketing researchers design the right strategy to intervene and incentivize respondents to authentically report and hence improve the quality of survey data. The proposed model and estimation approach are tested on both synthetic data and actual panel survey data on consumer-reported beverage-drinking behavior. Our analysis suggests that underreporting can significantly mask respondents' true behavior.", "e:keyword": ["Panel survey data analysis", "Measurement error", "Underreporting", "Bayesian analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0545", "e:abstract": "Used goods markets are currently important transaction channels for durable products. For some durable products, such markets first appeared when retailers started buying back used products from old customers and selling them to new ones for a profit (). The growth of electronic peer-to-peer (P2P) markets opened up a second, frictionless used goods channel where new customers can buy used products directly from old customers (). Both these markets compete with the original where retailers sell unused products procured from the manufacturer. This paper focuses on understanding the role that the sequential emergence of the above two used goods markets plays in shaping the of the manufacturer and the of the primary market retailer in the context of a decentralized, dyadic channel dealing with a renewable set of consumers. Our analysis establishes that frequent product upgrades and rising retail prices in durable product sectors of our interest are due to the emergence of the P2P used goods market and how the market interacts with the retail used goods source in altering the relative powers of the channel partners. Moreover, contrary to popular belief, we show that the initial introduction of the retail used goods channel actually discourages introduction of new versions and restrains the rise in retail prices. We also comment on how the two used goods markets affect the profits of the channel partners. We then provide empirical support for our theoretical result regarding product upgrades using data from the college textbook industry, a durable product that fits our model setup.", "e:keyword": ["Used goods markets", "Product upgrades", "Retail pricing", "Customer buybacks", "Durable products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0539", "e:abstract": "The interest in social networks among marketing scholars and practitioners has sharply increased in the last decade. One social network of which network scholars increasingly recognize the unique value is the academic collaboration (coauthor) network. We offer a comprehensive database of the collaboration network among marketing scholars over the last 40 years (available at http://mktsci.pubs.informs.org. Based on the ProQuest database, it documents the social collaboration among researchers in dozens of the leading marketing journals, enabling us to create networks of active marketing researchers. Unlike most of the published academic collaboration research, our database is dynamic and follows the evolution of the field over many years. In this paper, we describe the database and point to some basic network descriptives that lead to interesting research questions. We believe this database can be of much value to researchers interested in the evolution of social networks over time, as well as the specific evolution of the marketing discipline. The data set described in this paper is maintained by the authors and available through http://mktsci.pubs.informs.org", "e:keyword": ["Social network", "Scientific collaboration", "Egocentric networks", "Scientometrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0542", "e:abstract": "Prerelease demand forecasting is one of the most crucial yet difficult tasks facing marketers in the $60 billion motion picture industry. We propose functional shape analysis (FSA) of virtual stock markets (VSMs) to address this long-standing challenge. In VSMs, prices of a movie's stock reflect the dynamic demand expectations prior to the movie's release. Using FSA, we identify a small number of distinguishing shapes, e.g., the last-moment velocity spurt, that carry information about a movie's future demand and produce early and accurate prerelease forecasts. We find that although forecasting errors from the existing methods, e.g., those that rely on movie features, can be as high as 90.87%, our approach results in an error of only 4.73%. Because demand forecasting is especially useful for managerial decision making when provided a movie's release, we further demonstrate how our method can be used for early forecasting and compare its power against alternative approaches. We also discuss the theoretical implications of the discovered shapes that may help managers identify indicators of a potentially successful movie early and dynamically.", "e:keyword": ["Prerelease demand forecasting", "Motion pictures", "Virtual stock market", "Functional shape analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0559", "e:abstract": "This paper develops a model for the estimation and analysis of demand in the context of social interactions. Decisions made by a group of customers are modeled to be an equilibrium outcome of an empirical discrete game, such that all group members must be satisfied with chosen outcomes. The game-theoretic approach assists estimation by allowing us to account for the endogeneity of group members' decisions while also serving as a managerial tool that can simulate equilibrium outcomes for the group when the firm alters the marketing mix to the group. The model builds upon the existing literature on empirical models of discrete games by introducing a random coefficients heterogeneity distribution. Monte Carlo simulations reveal that including the heterogeneity resolves the endogenous group formation bias commonly noted in the social interactions literature. By estimating the heterogeneous equilibrium model using Bayesian hierarchical Markov chain Monte Carlo, we can also recover some parameters at the individual level to evaluate group-specific characteristics and targeted marketing strategies. To validate the model and illustrate its implications, we apply it to a data set of groups of golfers. We find significant social interaction effects, such that 65% of the median customer value is attributable to the customer and the other 35% is attributable to the customer's affect on members of his group. We also consider targeted marketing strategies and show that group-level targeting increases profit by 1%, whereas targeting within groups can increase profitability by 20%. We recognize that customer backlashes to targeting could be greater when group members receive different offers, so we suggest some alternatives that could retain some of the profitability of within group targeting while avoiding customer backlashes.", "e:keyword": ["Decision making", "Interdependent preferences", "Consumption", "Discrete choice", "Social interactions", "Targeted marketing", "Customer relationships"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0552", "e:abstract": "The phenomenon of paid search advertising has now become the most predominant form of online advertising in the marketing world. However, we have little understanding of the impact of search engine advertising on consumers' responses in the presence of organic listings of the same firms. In this paper, we model and estimate the interrelationship between organic search listings and paid search advertisements. We use a unique panel data set based on aggregate consumer response to several hundred keywords over a three-month period collected from a major nationwide retailer store chain that advertises on Google. In particular, we focus on understanding whether the presence of organic listings on a search engine is associated with a positive, a negative, or no effect on the click-through rates of paid search advertisements, and vice versa for a given firm. We first build an integrated model to estimate the relationship between different metrics such as search volume, click-through rates, conversion rates, cost per click, and keyword ranks. A hierarchical Bayesian modeling framework is used and the model is estimated using Markov chain Monte Carlo methods. Our empirical findings suggest that click-throughs on organic listings have a positive interdependence with click-throughs on paid listings, and vice versa. We also find that this positive interdependence is asymmetric such that the impact of organic clicks on increases in utility from paid clicks is 3.5 times stronger than the impact of paid clicks on increases in utility from organic clicks. Using counterfactual experiments, we show that on an average this positive interdependence leads to an increase in expected profits for the firm ranging from 4.2% to 6.15% when compared to profits in the absence of this interdependence. To further validate our empirical results, we also conduct and present the results from a controlled field experiment. This experiment shows that total click-through rates, conversions rates, and revenues in the presence of both paid and organic search listings are significantly higher than those in the absence of paid search advertisements. The results predicted by the econometric model are also corroborated in this field experiment, which suggests a causal interpretation to the positive interdependence between paid and organic search listings. Given the increased spending on search engine-based advertising, our analysis provides critical insights to managers in both traditional and Internet firms.", "e:keyword": ["Paid search advertising", "Organic search listings", "Search engines", "Click-through rates", "Conversion rates", "Electronic commerce", "Internet markets", "Monetization of user-generated content", "Hierarchical Bayesian modeling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0540", "e:abstract": "Price promotions and bundling have been two of the most widely used marketing tools in industry practice. Past literature has assumed that firms respond to price promotions by promoting a product in the same category. In this paper, we extend this literature as well as the bundling literature by considering the possibility that a firm may respond to a competitor's price promotions by also offering a cross-buying or bundling discount. Using a game-theoretic model, we show that bundle discounts can help increase profits in a competitive market by creating endogenous loyalty, thereby reducing the intensity of promotional competition. We also find that bundle discounts can be used as an effective defensive marketing tool to prevent customer defection to the competition.", "e:keyword": ["Bundling", "Competitive marketing strategy", "Game theory", "Price promotions", "Brand loyalty"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0544", "e:abstract": "With increasing fragmentation of media markets and recent advances in technology, loss of advertising effectiveness has been a great concern for marketers. For consumers, the digital video recorder (DVR) offers the possibility to fast-forward through live programming. Whereas the DVR thus benefits consumers by reducing nuisance from commercials, industry observers believe that it may diminish advertisers' profits by rendering commercials ineffective. We use a model of informative advertising to study the effect of DVR penetration on competing advertisers' strategies and profits. We find that the overall effect of DVRs depends on the trade off between loss of advertising effectiveness and reduction in competition between firms. The latter effect arises because DVR penetration may increase the ratio of partially informed to fully informed consumers. We identify conditions under which an increase in DVR penetration counterintuitively leads firms to increase advertising levels and enjoy higher profits. Interestingly, we find that greater DVR penetration is beneficial for firms when the share of DVR owners in the population is above--rather than below--a threshold level. We also study the impact of different fast-forwarding (\"zipping\") behaviors on product market competition.", "e:keyword": ["Advertising", "Game theory", "Competitive analysis", "Marketing strategy", "Advertising avoidance technologies"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0546", "e:abstract": "This paper investigates the effects of a limited consumer memory on the price competition between firms. It studies a specific aspect of memory--namely, the categorization of available price information that the consumers may need to recall for decision making. This paper analyzes competition between firms in a market with uninformed consumers who do not compare prices, informed consumers who compare prices but with limited memory, and informed consumers who have perfect memory. Consumers, aware of their memory limitations, choose how to encode the prices into categories, whereas firms take the limitations of consumers into account in choosing their pricing strategies. Two distinct types of categorization processes are investigated: (1) a symmetric one in which consumers compare only the labels of price categories from the competing firms and (2) an asymmetric one in which consumers compare the recalled price of one firm with the actual price of the other. We find that the equilibrium partition for the consumers calls for finer categorization toward the bottom of the price distribution. Thus consumers have a motivation to invest in greater memory resources in encoding lower prices to induce firms to charge more favorable prices. The interaction between the categorization strategies of the consumers and the price competition between the firms is such that small initial improvements in recall move the market outcomes quickly toward the case of perfect recall. Even with few memory categories, the expected price consumers pay and their surplus is close to the case of perfect recall. There is thus a suggestion in this model that market competition adjusts to the memory limitations of consumers.", "e:keyword": ["Bounded rationality", "Memory", "Behavioral economics", "Categorization", "Competitive strategy", "Pricing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0547", "e:abstract": "This study attempts to answer a basic customer management dilemma facing firms: when should the firm use behavior-based pricing (BBP) to discriminate between its own and competitors' customers in a competitive market? If BBP is profitable, when should the firm offer a lower price to its own customers rather than to the competitor's customers? This analysis considers two features of customer behavior up to now ignored in BBP literature: heterogeneity in customer value and changing preference (i.e., customer preferences are correlated but not fixed over time). In a model where both consumers and competing firms are forward-looking, we identify conditions when it is optimal to reward the firm's own or competitor's customers and when BBP increases or decreases profits. To the best of our knowledge, we are the first to identify conditions in which (1) it is optimal to reward one's own customers under symmetric competition and (2) BBP can increase profits with fully strategic and forward-looking consumers.", "e:keyword": ["Behavior-based pricing", "Customer heterogeneity", "Stochastic preferences", "Forward-looking customers", "Forward-looking firms", "Customer relationship management", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0550", "e:abstract": "This paper documents the existence and magnitude of contiguous word-of-mouth effects of signal quality of a video-on-demand (VOD) service on customer acquisition. We operationalize contiguous word-of-mouth effect based on geographic proximity and use behavioral data to quantify the effect. The signal quality for this VOD service is exogenously determined, objectively measured, and spatially uncorrelated. Furthermore, it is unobserved to the potential subscriber and is revealed postadoption. For a subscriber, the signal quality translates directly into the number of movies available for viewing, thus representing a part of the overall service quality. The combination of signal quality along with location and neighborhood information for each subscriber and potential subscriber allows us to resolve the typical challenges in measuring causal social network effects. We find that contiguous word of mouth affects about 8% of the subscribers with respect to their adoption behavior. However, this effect acts as a double-edged sword because it is asymmetric. We find that the effect of negative word of mouth arising from poor signal quality is more than twice as large as the effect of positive word of mouth arising from excellent signal quality. Besides contiguous word of mouth, we find that advertising and the retail environment also play a role in adoption.", "e:keyword": ["Services", "Service quality", "New product adoption", "Word of mouth", "Contagion", "Social networks", "High technology", "Hazard models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0551", "e:abstract": "Before the deregulation of digital subscriber line (DSL) services by the Federal Communications Commission (FCC) in 2005, phone companies were required to share their DSL bandwidth with independent DSL providers. Despite the large number of independent providers that entered the market, phone companies accounted for 95.3% of all DSL subscribers in 2005. A common explanation for this is based on supply-side factors such as the costs faced by these providers to lease phone lines from phone companies, as well as the price discounts offered by phone companies. In this paper, we look for a demand-side explanation for this market outcome. Analyzing consumer choices in the broadband category alone would lead us to the conclusion that consumers have a much higher preference for their local phone providers--a finding at odds with service awards received by independent DSL providers. Thus we look for a demand-side explanation that is based on the demand not just for broadband services but also for related services such as cable TV and local phone. We find evidence of strong complementarities between the consumption of broadband and of those related categories. The main source of such complementarities, in our data, is the benefits to consumers from having a single provider for multiple services. We then carry out counterfactual experiments assuming that there are no changes in the regular prices of the various services. Our results indicate that the share of phone companies in the broadband market would have been 43% smaller without complementarities stemming from such a single-provider effect, whereas shutting off the state dependence effects would have reduced their share by 30%, and shutting off the effects of price discounts on the DSL+local phone bundle would have resulted in their share declining by 21%.", "e:keyword": ["Complementarities", "Product bundle", "Broadband"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0553", "e:abstract": "Despite the economic significance of the theme park industry and the huge investments needed to set up new attractions, no marketing models exist to guide these investment decisions. This study addresses this gap in the literature by estimating a response model for theme park attendance. The model not only determines the contribution of each attraction to attendance, but also how this contribution is distributed within and across years. The model accommodates saturation effects, which imply that the impact of a new attraction is smaller if similar attractions are already present. It also captures reinforcement effects, meaning that a new attraction may reinforce the drawing power of similar extant attractions, especially when these were introduced recently. The model is calibrated on 25 years of weekly attendance data from the Efteling, a leading European theme park. Our return on investment calculations show that it is more profitable to invest in multiple smaller attractions than in one big one. This finding is in remarkable contrast with the current \"arms race\" in the industry. Furthermore, even though thrill rides tend to be more effective than theme rides, there are conditions under which one should consider to switch to the latter.", "e:keyword": ["Entertainment industry", "Theme parks", "Return on investment", "Bundling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0554", "e:abstract": "Private labels (PLs) are ubiquitous in several categories, including groceries, apparel, and appliances. However, existing empirical work has not examined the differential impact of various upstream supply arrangements for PL products or the strategic motives for PL supply. To do so requires one to model the interaction between private and national label (NL) products both upstream and downstream while accounting for strategic behavior on the part of manufacturers and retailers and retaining essential differences between NL and PL products. We build a model that satisfies these requirements and lets us answer our two research questions: First, can an NL firm profit from being an outsourced PL supplier? Second, what are the upstream and downstream impacts of different PL supply arrangements? We answer these questions by modeling private labels as homogenous products at wholesale, but as differentiated products at retail. In contrast, national label products are differentiated at both wholesale and retail levels. Using structural model estimates for fluid milk in a major metropolitan area, we conduct three counterfactual experiments. We find that both NL producers and retailers profit from adding private labels. We also find that a vertically integrated supply of PL leads to lower prices for end consumers.", "e:keyword": ["Private labels", "Structural estimation", "Distribution channels", "Counterfactuals"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0555", "e:abstract": "Many firms increasingly offer community venues to their customers to facilitate social interactions amongst them. Prior studies have shown that community participants have high engagement and loyalty toward the firm and provide useful feedback and referrals. However, it is not clear whether community participants are the firm's \"fans\" to begin with and self-select themselves into the community, or whether community participation leads to increased relational customer behaviors. In the current research, we employ data from a field experiment to help answer this question. The data come from a year-long study conducted by eBay Germany, and they reveal that a simple e-mail invitation significantly increased customer participation in the firm's community. Results also show that community participation had mixed effects on customers' likelihoods of participating in buying and selling behaviors. Community participation did not translate into increased behaviors, as would be commonly expected. Although there is no impact of participation on the number of bids placed or the revenue earned, there is a negative impact of participation on the number of listings and the amount spent. Together, these results suggest that the community participants become more selective and efficient sellers, and they also become more conservative in their spending on the items for which they bid. The results also show that customer community marketing programs may be targeted to a broader set of the firm's customers than just the fans.", "e:keyword": ["Customer community", "Online social interactions", "Customer relationship management", "Hierarchical bayes", "MCMC", "Multivariate Tobit"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0558", "e:abstract": "Health-care experts believe that increases in portion sizes served by food vendors contribute to the obesity epidemic. This paper shows that food vendors can profit handsomely by using supersizing strategies where regular portion sizes are priced sufficiently high to discourage price-conscious consumers from selecting them, and the prices for enlarging food portions are set so low that these customers are tempted to order the larger portion sizes and overeat. Setting aside the impact of obesity on health-care costs, we show that using supersizing to steer customers toward consuming excessive amounts of food can destroy value from a social perspective; thus this social value destruction trap adds another justification for pressuring food vendors to reduce supersizing for unhealthy food. As a public policy response, we consider how \"moderating policies\" may counter these effects through measures designed specifically to encourage eating in moderation by applying supersizing bans, taxes, and warnings.", "e:keyword": ["Pricing", "Supersizing", "Overeating", "Obesity", "Self-control", "Temptation", "Social responsibility", "Public policy", "Moderating policies"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0567", "e:abstract": "We develop a conceptual framework about the impact that branding activity (the audiovisual representation of brands) and consumers' focused versus dispersed attention have on consumer moment-to-moment avoidance decisions during television advertising. We formalize this framework in a dynamic probit model and estimate it with Markov chain Monte Carlo methods. Data on avoidance through zapping, along with eye tracking on 31 commercials for nearly 2,000 participants, are used to calibrate the model. New, simple metrics of attention dispersion are shown to strongly predict avoidance. Independent of this, central on-screen brand positions, but not brand size, further promote commercial avoidance. Based on the model estimation, we optimize the branding activity that is under marketing control for ads in the sample to reduce commercial avoidance. This reveals that brand pulsing--while keeping total brand exposure constant--decreases commercial avoidance significantly. Both numerical simulations and a controlled experiment using regular and edited commercials, respectively, provide evidence of the benefits of brand pulsing to ward off commercial avoidance. Implications for advertising management and theory are addressed.", "e:keyword": ["Commercial avoidance", "Branding", "Attention", "Dynamic probit model", "Optimization", "Pulsing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0560", "e:abstract": "Two-sided exchange networks (such as eBay.com) often advertise their number of users, presumably to encourage further participation. However, these networks differ markedly on how they advertise their user base. Some highlight the number of sellers, some emphasize the number of buyers, and others disclose both. We use field experiment data from a business-to-business website to examine the efficacy of these different display formats. Before each potential seller posted a listing, the website randomized whether to display the number of buyers and/or sellers, and if so, how many buyers and/or sellers to claim. We find that when information about both buyers and sellers is displayed, a large number of sellers deters further seller listings. However, this deterrence effect disappears when only the number of sellers is presented. Similarly, a large number of buyers is more likely to attract new listings when it is displayed together with the number of sellers. These results suggest the presence of indirect network externalities, whereby a seller prefers markets with many other sellers because they help attract more buyers.", "e:keyword": ["Two-sided markets", "Information disclosure", "Competition", "Inference", "Entry", "Field experiment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0557", "e:abstract": "Can negative information about a product increase sales, and if so, when? Although popular wisdom suggests that \"any publicity is good publicity,\" prior research has demonstrated only downsides to negative press. Negative reviews or word of mouth, for example, have been found to hurt product evaluation and sales. Using a combination of econometric analysis and experimental methods, we unify these perspectives to delineate contexts under which negative publicity about a product will have positive versus negative effects. Specifically, we argue that negative publicity can increase purchase likelihood and sales by increasing product awareness. Consequently, negative publicity should have differential effects on established versus unknown products. Three studies support this perspective. Whereas a negative review in the New York Times hurt sales of books by well-known authors, for example, it increased sales of books that had lower prior awareness. The studies further underscore the importance of a gap between publicity and purchase occasion and the mediating role of increased awareness in these effects.", "e:keyword": ["Negative publicity", "Awareness", "Word of mouth", "Product success"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0562", "e:abstract": "Whereas a growing body of research has examined the consumer-related implications of deceptive advertising, the stock market consequences stemming from the regulatory exposure of such infractions remain largely unexplored. In a step to address this gap, the current research examines the effect of regulatory reports of misleading ads on firm stock prices. Results from an event study, focusing on the pharmaceutical industry as the empirical context, show an average abnormal return of -0.91% associated with regulatory reports of deceptive advertising. Analysis of the abnormal returns, however, reveals that the stock market response to these reports is shaped by omission bias, in that investors penalize commission violations more than omission violations. Furthermore, firm reputation is found to moderate the penalty for commission violations. In addition, two experiments examine the effect of such violations on investor beliefs. The first helps elucidate the process mechanism underlying the observed stock market effects and the second provides insights regarding the reputation-omission bias interaction for firms committing repeat violations. Overall, our findings provide important theoretical, managerial, and public policy implications regarding the role of financial markets in regulating deceptive ad practices.", "e:keyword": ["Deceptive advertising", "Omission bias", "Firm reputation", "Event study", "Experiment", "Pharmaceutical industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0563", "e:abstract": "The pioneering Pasternack returns-policy model analyzed channel coordination with a single supplier catering to a retailer facing stochastic demand for a perishable product with a fixed price, and the model showed that giving partial returns of unsold stock to the retailer is the optimal policy for the entire supply chain. The result thus begs the question as to why manufacturers of perishable commodities widely accept full returns of unsold stock as the norm. We model the environment as one where two capacity-constrained manufacturers compete for shelf space with the same retailer, and we show that a complete-credit returns policy is in fact the only possible equilibrium of the game. Our results obviate the need for knowing the exact functional form of the demand distribution in order to compute the returns credit, as Pasternack's results would require. From a retailer's standpoint, we establish a simple procurement strategy and show that it is optimal. The same game with price-only contracting has a pure-strategy equilibrium when the supplier capacities are below a threshold value and a mixed-strategy equilibrium when the supplier capacities cross this threshold but are still so limited that no single supplier can with certainty supply all the quantity demanded.", "e:keyword": ["Channel coordination versus channel competition", "Newsvendor problem", "Vertical control", "Returns policies", "Perishable goods", "Procurement strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0564", "e:abstract": "This paper examines pairwise assortment similarities at U.S. supermarkets to understand how assortment composition and size are related to underlying factors that describe local store clientele, local competitive structure, and the retail outlets' characteristics. The top-selling items, which cumulatively make up 50% of sales, are sold at nearly every store, but other items are viewed as optional. We find that, within states, supermarkets owned by the same chain carry similar assortments and that the composition of their clientele and the presence of competing stores have effects on assortment similarity that are an order of magnitude smaller than ownership structure. In contrast, we find that, across states, supermarkets owned by the same chain do version their assortment. We explain this difference using extant work on the minimal efficient scale of supermarkets and on local demand effects. Furthermore, we investigate the distribution and role of regional brands. We find that regional brands are primarily distributed by small regional chains or independent stores. \"Value\" regional brands are primarily distributed by supermarket firms without store brands, whereas the distribution of \"premium\" regional brands is unrelated to the presence of store brands. We discuss our findings in the context of modeling assortment decisions and manufacturers designing distribution policies.", "e:keyword": ["Retailing", "Assortment", "Supermarket", "Dyadic data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0565", "e:abstract": "Market segmentation is inherently a multicriterion problem even though it has often been modeled as a single-criterion problem in the traditional marketing literature and in practice. This paper discusses the multicriterion nature of market segmentation and develops a new mathematical model that addresses this issue. A new method for market segmentation based on multiobjective evolutionary algorithms, called MMSEA, is developed. It complements existing segmentation methods by optimizing multiple objectives simultaneously, searching for globally optimal solutions, and approximating a set of Pareto-optimal solutions. We have applied and evaluated this method in two empirical studies for two firms from distinct industries: descriptive segmentation of the cell phone service market from a dual-value creation perspective and predictive segmentation of retail customers based on profit and customer sociodemographic attributes. The results provide decision makers with compelling alternatives and enhanced flexibility currently missing in existing market segmentation methods.", "e:keyword": ["Multicriterion market segmentation", "Descriptive market segmentation", "Predictive market segmentation", "Multiobjective evolutionary algorithms", "Multiobjective optimization", "Multiobjective clustering", "Pareto-optimal solution set"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0584", "e:abstract": "The ability to demonstrate the impact of marketing action on firm financial performance is crucial for evaluating, justifying, and optimizing the expenditure of a firm's marketing resources. This presents itself as a formidable task when one considers both the variety and potential influence of marketing activity. We propose a hierarchical Bayesian model of simultaneous supply and demand that allows us to formally study the financial impact of a variety of marketing activities, including those that operate on different timescales. The supply-side model provides insight into how the firm allocates resources across its various subunits. We illustrate our approach in a services context by integrating data from three independent studies conducted by a large national bank. Our model allows customer and employee satisfaction to influence firm profitability by moderating the conditional relationship between the bank's operational inputs and its proclivity to produce revenue.", "e:keyword": ["Customer satisfaction", "Employee satisfaction", "Endogeneity", "Resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0570", "e:abstract": "The U.S. pharmaceutical industry spent upwards of $18 billion on marketing drugs in 2005; detailing and drug sampling activities accounted for the bulk of this spending. To stay competitive, pharmaceutical managers need to maximize the return on these marketing investments by determining which physicians to target as well as when and how to target them. In this paper, we present a two-stage approach for dynamically allocating detailing and sampling activities across physicians to maximize long-run profitability. In the first stage, we estimate a hierarchical Bayesian, nonhomogeneous hidden Markov model to assess the short- and long-term effects of pharmaceutical marketing activities. The model captures physicians' heterogeneity and dynamics in prescription behavior. In the second stage, we formulate a partially observable Markov decision process that integrates over the posterior distribution of the hidden Markov model parameters to derive a dynamic marketing resource allocation policy across physicians. We apply the proposed approach in the context of a new drug introduction by a major pharmaceutical firm. We identify three prescription-behavior states, a high degree of physicians' dynamics, and substantial long-term effects for detailing and sampling. We find that detailing is most effective as an acquisition tool, whereas sampling is most effective as a retention tool. The optimization results suggest that the firm could increase its profits substantially while decreasing its marketing spending. Our suggested framework provides important implications for dynamically managing customers and maximizing long-run profitability.", "e:keyword": ["Pharmaceutical marketing", "Marketing resource allocation", "Long-term effect of marketing activities", "Hidden Markov model", "Bayesian estimation", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0571", "e:abstract": "This paper explores whether and how a firm should adapt its strategy in view of consumer use of prior customer ratings. Specifically, we consider optimal pricing and whether the firm should offer an unexpected frill to early customers to enhance their product experiences. We show that if price history is unobserved by consumers, a forward-looking firm should always modify its strategy from single-period optimal one, but it may be optimal to do so by lowering price, by lowering price and offering frills, or by raising price and offering frills, depending on the market growth rate. Specifically, the last strategy becomes optimal when market growth rate is high enough. The results are similar when the price history is observed by consumers, except that no deviation from single-period profit maximization choices is optimal when market growth is low enough. We also analyze whether the firm should prefer that the price information be stated in or left out of consumer reviews. In addition, in considering the effects of consumer heterogeneity, we conclude that the optimal firm's effort to affect ratings is higher when the idiosyncratic part of consumer uncertainty is larger.", "e:keyword": ["Game theory", "Product augmentation", "Customer satisfaction", "Uncertainty", "Forward-looking behavior", "Pricing", "Customer ratings"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0572", "e:abstract": "Our objective in this paper is to measure the impact (valence, volume, and variance) of national online user reviews on designated market area (DMA)-level local geographic box office performance of movies. We account for three complications with analyses that use national-level aggregate box office data: (i) aggregation across heterogeneous markets (spatial aggregation), (ii) serial correlation as a result of sequential release of movies (endogenous rollout), and (iii) serial correlation as a result of other unobserved components that could affect inferences regarding the impact of user reviews. We use daily box office ticket sales data for 148 movies released in the United States during a 16-month period (out of the 874 movies released) along with user review data from the Yahoo! Movies website. The analysis also controls for other possible box office drivers. Our identification strategy rests on our ability to identify plausible instruments for user ratings by exploiting the sequential release of movies across markets--because user reviews can only come from markets where the movie has previously been released, exogenous variables from previous markets would be appropriate instruments in subsequent markets. In contrast with previous studies that have found that the main driver of box office performance is the volume of reviews, we find that it is the valence that seems to matter and not the volume. Furthermore, ignoring the endogenous rollout decision does not seem to have a big impact on the results from our DMA-level analysis. When we carry out our analysis with aggregated national data, we obtain the same results as those from previous studies, i.e., that volume matters but not the valence. Using various market-level controls in the national data model, we attempt to identify the source of this difference. By conducting our empirical analysis at the DMA level and accounting for prerelease advertising, we can classify DMAs based on their responsiveness to firm-initiated marketing effort (advertising) and consumer-generated marketing (online word of mouth). A unique feature of our study is that it allows marketing managers to assess a DMA's responsiveness along these two dimensions. The substantive insights can help studios and distributors evaluate their future product rollout strategies. Although our empirical analysis is conducted using motion picture industry data, our approach to addressing the endogeneity of reviews is generalizable to other industry settings where products are sequentially rolled out.", "e:keyword": ["Online word of mouth", "Sequential new product release", "Endogeneity", "Instrumental variables", "Generalized method of moments", "Motion pictures"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0608", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0561", "e:abstract": "This paper presents five empirical tests of the popular modeling abstraction that assumes bids from online auctions with proxy bidding can be analyzed \"as if\" they were bids from a second-price sealed-bid auction. The tests rely on observations of the magnitudes and timings of the top two proxy bids, with the different tests stemming from different regularity assumptions about the underlying distribution of valuation signals. We apply the tests to data from three eBay markets--MP3 players, DVDs, and used cars--and we reject the sealed-bid abstraction in all three data sets. A closer examination of these rejections suggests that they are driven by less experienced bidders. This consistent rejection casts doubt on several existing theories of online auction behavior and suggests some demand estimates based on the abstraction can be biased. To assess the direction and magnitude of this bias, we propose and estimate a new model in which some bidders conform to the abstraction while other bidders bid in a reactive fashion. Because reactive bidding can be at least partially detected from the data, we are able to estimate the underlying distribution of demand and compare it to what the sealed-bid abstraction implies. We find that our proposed model fits the data better, and our demand estimates reveal a large potential downward bias were we to assume the second-price sealed-bid model instead.", "e:keyword": ["Auctions", "Online auctions", "Demand estimation", "Sealed-bid model", "eBay", "Order statistics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0581", "e:abstract": "This study explores the implications of rejecting the sealed-bid abstraction proposed by Zeithammer and Adams [Zeithammer, R., C. Adams. 2010. The sealed-bid abstraction in online auctions. Marketing Sci. 29(6) 964-987]. Using a conditional order statistic model that relies on the joint distribution of the top two proxy bids of an auction, Zeithammer and Adams show that inexperienced bidders' reactive bidding is the main cause of the rejection of the sealed-bid abstraction. Their empirical study suggests that a large percentage of bidders reactively bid, and there is weak evolutionary pressure for bidders to converge to sealed bidding. We discuss theoretical implications of this rejection and the role of bidder experience, as well as inferences about bidder learning. Tracking an inexperienced bidder's bidding behavior over time, we show that bidders learn and their bidding strategy gravitates toward rational bidding. Potential biases in bidder experience measurement and bidder learning can be assessed using a cross-sectional, time-series data set that tracks a random sample of new eBay bidders. Learning speed is faster with their complete bidding history rather than feedback ratings or winning observations only. We highlight the importance of proper measures of bidder experience and its effect on bidding strategy evolutions, both of which play important roles in clarifying bidding behavior in online auctions.", "e:keyword": ["Auctions", "Online", "Learning", "Bidding"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0593", "e:abstract": "We argue that the Zeithammer and Adams paper [Zeithammer, R., C. Adams. 2010. The sealed-bid abstraction in online auctions. Marketing Sci. 29(6) 964-987] successfully documents consistent patterns in eBay bidding data that cast doubt on the common assumption that bidders in such auctions follow a \"bid = value\" strategy. These anomalies lend support to the authors' alternative model in which some bidders bid reactively and consequently bid below their valuation most of the time. The consistency of the authors' findings as well as the ability of their alternative explanation to account for all of their test results lends great support to their thesis. However, we think that several of their empirical tests examine ancillary assumptions about bidder behavior and do not test the bid = value assumption directly. Furthermore, although their reduced-form model incorporating \"reactive\" bidders is a good first attempt at expanding the canonical framework, we worry that their counterfactual pricing analysis using the reactive model is suspect because the parameters they estimate are not structural. Overall, the Zeithammer and Adams paper is a carefully argued critique of empirical methods used to study online auctions and provides valuable ideas to improve on these methods.", "e:keyword": ["eBay", "Auction demand estimation", "Iid bidder valuation", "Optimal reserve price", "Sealed-bid assumption"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0597", "e:abstract": "This paper presents the authors' rejoinder to Zeithammer and Adams [Zeithammer, R., C. Adams. 2010. The sealed-bid abstraction in online auctions. Marketing Sci. 29(6) 964-987]. This rejoinder clarifies and qualifies conclusions of the original paper and makes suggestions for fruitful areas of future research. In particular, the original paper shows that bidding style can make a big difference in managerial decisions, but a structural model would be necessary to make confident predictions under different reserve prices. The rejoinder also clarifies the interpretation of feedback as a measure of bidder experience, and the relationship between bidder experience and bidding style.", "e:keyword": ["Auctions", "Econometrics", "Empirical IO methods", "Measurement and inference"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0574", "e:abstract": "Using aggregate product search data from Amazon.com, we jointly estimate consumer information search and online demand for consumer durable goods. To estimate the demand and search primitives, we introduce an optimal sequential search process into a model of choice and treat the observed market-level product search data as aggregations of individual-level optimal search sequences. The model builds on the dynamic programming framework by Weitzman [Weitzman, M. L. 1979. Optimal search for the best alternative. Econometrica 47(3) 641-654] and combines it with a choice model. It can accommodate highly complex demand patterns at the market level. At the individual level, the model has a number of attractive properties in estimation, including closed-form expressions for the probability distribution of alternative sets of searched goods and breaking the curse of dimensionality. Using numerical experiments, we verify the model's ability to identify the heterogeneous consumer tastes and search costs from product search data. Empirically, the model is applied to the online market for camcorders and is used to answer manufacturer questions about market structure and competition and to address policy-maker issues about the effect of selectively lowered search costs on consumer surplus outcomes. We demonstrate that the demand estimates from our search model predict the actual product sales ranks. We find that consumer search for camcorders at Amazon.com is typically limited to 10-15 choice options and that this affects estimates of own and cross elasticities. In a policy simulation, we also find that the vast majority of the households benefit from Amazon.com's product recommendations via lower search costs.", "e:keyword": ["Cost-benefit analysis", "Optimal sequential search", "Demand for durable goods", "Information economics", "Consideration sets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0575", "e:abstract": "To evaluate the success of a new product, managers need to determine how much of its new demand is due to cannibalizing the firm's other products, rather than drawing from competition or generating primary demand. We introduce a time-varying vector error-correction model to decompose the base sales of a new product into its constituent sources. The model allows managers to estimate cannibalization effects and calculate the new product's net demand, which may be considerably less than its total demand. We apply our methodology to the introduction of the Lexus RX300 using detailed car transaction data. This case is especially interesting because the Lexus RX300 was the first crossover sport utility vehicle (SUV), implying that its demand could come from both the luxury SUV and the luxury sedan categories. Because Lexus was active in both categories, there was a double cannibalization potential. We show how the contribution of the different demand sources varies over time and discuss the managerial implications for both the focal brand and its competitors.", "e:keyword": ["New product", "Cannibalization", "Aggregate response models", "Time-series models", "Missing data", "Bayesian methods", "Dynamic linear models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0576", "e:abstract": "Although advance selling and probabilistic selling differ in both motivation and implementation, we argue that they share a common characteristic--both offer consumers a choice involving buyer uncertainty. We develop a formal model to examine the general economics of purchase options that involve buyer uncertainty, explore the differences in buyer uncertainty created via these two strategies, and derive conditions under which one dominates the other. We show that the seller can address unobservable buyer heterogeneity by inducing sales involving buyer uncertainty via two different mechanisms: (1) homogenizing heterogeneous consumers and (2) separating heterogeneous consumers. Offering advance sales encourages customers to purchase while they are uncertain about their consumption states (more homogeneous), but offering probabilistic goods encourages customers to reveal their heterogeneity via self-selecting whether or not to purchase the uncertain product. The relative attractiveness of these two selling strategies depends on the degree of two types of buyer heterogeneity: (1) Max_Value-Heterogeneity, which is the variation in consumers' valuations for their preferred good, and (2) Strength-Heterogeneity, which is the variation in the strength of consumers' preferences. Neither strategy is advantageous unless the market exhibits sufficient Max_Value-Heterogeneity. However, whereas Strength-Heterogeneity can destroy the profit advantage of advance selling, a mid-range of Strength-Heterogeneity is necessary for probabilistic selling to be advantageous.", "e:keyword": ["Advance selling", "Probabilistic selling", "Demand uncertainty", "Consumer heterogeneity", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0577", "e:abstract": "Reverse pricing is a market mechanism under which a consumer's bid for a product leads to a sale if the bid exceeds a hidden acceptance threshold the seller has set in advance. The seller faces two key decisions in designing such a mechanism. First, he must decide where in the process to collect the revenue--that is, whether to commit to a minimum markup above cost (and thus define the bid-acceptance threshold given cost) and whether to set a fee for the consumer's right to bid. Second, the seller must decide whether to facilitate or hinder consumer learning about the current bid-acceptance threshold. We analyze these decisions for a profit-maximizing small intermediary retailer selling to consumers who can also purchase the product in an outside posted-price market. The optimal revenue model is to charge a fee for the right to bid and then accept all bids above cost, rather than to set a positive minimum markup above cost. Avoiding minimum markups in favor of a bidding fee is more profitable because of increased efficiency arising from more entry by consumers and higher bids by the entrants. When consumers learn about the bid-acceptance threshold before they enter the market, efficiency increases further, and generating revenue through a bidding fee can compensate the seller for his loss of information rent when the competition from the outside posted-price firm is relatively weak.", "e:keyword": ["Reverse pricing", "Name-your-own-price", "Analytical modeling", "e-commerce"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0578", "e:abstract": "Consumers often return a product to a retailer because they learn after purchase that the product does not match as well with preferences as had been expected. This is a costly issue for retailers and manufacturers--in fact, it is estimated that the U.S. electronics industry alone spent $13.8 billion dollars in 2007 to restock returned products [Lawton, C. 2008. The war on returns. Wall Street Journal (May 8) D1]. The bulk of these returns were nondefective items that simply were not what the consumer wanted. To eliminate returns and to recoup the cost of handling returns, many retailers are adopting the practice of charging restocking fees to consumers as a penalty for making returns. This paper employs an analytical model of a bilateral monopoly to examine the impact of reverse channel structure on the equilibrium return policy and profit. More specifically, we examine how the return penalty is affected by whether returns are salvaged by the manufacturer or by the retailer. Interestingly, we find that the return penalty may be more severe when returns are salvaged by a channel member who derives greater value from a returned unit. Also, the manufacturer may earn greater profit by accepting returns even if the retailer has a more efficient outlet for salvaging units.", "e:keyword": ["Channels of distribution", "Pricing", "Reverse logistics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0580", "e:abstract": "Many businesses track repeat transactions on a discrete-time basis. These include (1) companies for whom transactions can only occur at fixed regular intervals, (2) firms that frequently associate transactions with specific events (e.g., a charity that records whether supporters respond to a particular appeal), and (3) organizations that choose to utilize discrete reporting periods even though the transactions can occur at any time. Furthermore, many of these businesses operate in a noncontractual setting, so they have a difficult time differentiating between those customers who have ended their relationship with the firm versus those who are in the midst of a long hiatus between transactions. We develop a model to predict future purchasing patterns for a customer base that can be described by these structural characteristics. Our beta-geometric/beta-Bernoulli (BG/BB) model captures both of the underlying behavioral processes (i.e., customers' purchasing while \"alive\" and time until each customer permanently \"dies\"). The model is easy to implement in a standard spreadsheet environment and yields relatively simple closed-form expressions for the expected number of future transactions conditional on past observed behavior (and other quantities of managerial interest). We apply this discrete-time analog of the well-known Pareto/NBD model to a data set on donations made by the supporters of a nonprofit organization located in the midwestern United States. Our analysis demonstrates the excellent ability of the BG/BB model to describe and predict the future behavior of a customer base.", "e:keyword": ["BG/BB", "Beta-geometric", "Beta-binomial", "Customer-base analysis", "Customer lifetime value", "CLV", "RFM", "Pareto/NBD"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0582", "e:abstract": "In this research, we examine a novel mechanism of interorganizational relationship dissolution: incoherence in a partner's behavior. We propose that the discrepancy between an exchange partner's opportunistic behavior and the focal firm's expectations may create a state of incoherence and uncertainty and that this effect can be damaging to the exchange even when the partner's behavior is better than expected. Using nearly 500 longitudinal, confidential reports of industrial buyers and sellers, we find supportive evidence that (1) the net effect of the discrepancy is initially positive when behavior is better than expected but becomes rapidly negative thereafter, and (2) the net effect of the discrepancy is always negative when behavior is worse than expected. Thus, these effects will generally damage the exchange even as the partner tries to improve the relationship. This gives insight into why exchange relationships that hit a downward spiral can be difficult, if not impossible, to salvage. We also show that the dysfunctional consequences of discrepancy are mitigated through exchange structures such as the magnitude of dependence on an organizational partner, the development phase of the relationship, and the presence of bilateral idiosyncratic investments. Implications for theory and the management of interorganizational relationships are developed.", "e:keyword": ["Interorganizational relationship management and performance", "Belief discrepancy", "Incoherence", "Organizational sensemaking", "Opportunism", "Dependence", "Relationship phase", "Bilateral investments"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0585", "e:abstract": "This paper looks into the effects of information transparency on market participants in an online trading environment. We study these effects in business-to-business electronic markets with firms competing in both upstream and downstream industries. The prior literature generally assumes that either the downstream firm (buyer) or the upstream firm (seller) is a monopoly. It is not clear whether information transparency would still create value if both buyers and sellers face oligopolistic competition, where the benefits of information transparency could be competed away. To answer this question, we first develop a simple two-echelon e-market model and then extend the model to more general settings. We find that information transparency can create value for the overall e-market, yet it affects buyers and sellers very differently: one side will be hurt, depending on the competition mode (Cournot or Bertrand) in the downstream. This suggests that a manufacturer-owned, a supplier-owned, and a neutral e-market will have different preferences for information transparency. Finally, we find that information transparency can hurt consumers when the downstream industry engages in Bertrand competition. This is a surprising result given the expectation that online markets create substantial value for consumers.", "e:keyword": ["Electronic markets", "Competition", "Uncertainty", "Market microstructure", "Information transparency", "B2B marketing", "Game theory", "Analytical modeling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0600", "e:abstract": "The availability of digital channels for media distribution has raised many important questions for marketers, notably, whether digital distribution channels will cannibalize physical sales and whether legitimate digital distribution channels will dissuade consumers from using (illegitimate) digital piracy channels. We address these two questions using the removal of NBC content from Apple's iTunes store in December 2007, and its restoration in September 2008, as natural shocks to the supply of legitimate digital content, and we analyze the impact of this shock on demand through BitTorrent piracy channels and the Amazon.com DVD store. To do this we collected two large data sets from Mininova.com and Amazon.com, documenting levels of piracy and DVD sales for both NBC and other major networks' content around these events. We analyze these data in a difference-in-difference model and find that NBC's decision to remove its content from iTunes in December 2007 is causally associated with an 11.4% increase in the demand for NBC's pirated content. This is roughly equivalent to an increase of 48,000 downloads a day for NBC's content and is approximately twice as large as the total legal purchases on iTunes for the same content in the period preceding the removal. We also find evidence of a smaller, and statistically insignificant, decrease in piracy for the same content when it was restored to the iTunes store in September 2008. Finally, we see no change in demand for NBC's DVD content at Amazon.com associated with NBC's closing or reopening of its digital distribution channel on iTunes.", "e:keyword": ["Internet", "Piracy", "Digital distribution", "Distribution channels", "Cannibalization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0601", "e:abstract": "Price dispersion in simultaneous online auctions is a puzzle in light of the relatively low search costs required to find the lower price. Much of this price dispersion appears to be due to a lack of switching by bidders between auctions, which in turn could be due to inertia related to search costs. We identify some of the influencing factors through a controlled field experiment involving pairs of simultaneous auctions. Keeping the sellers and the goods sold identical between two auctions, we vary auction design features between and within pairs including shipping cost, open reserve, secret reserve price, and duration, and we provide bidders with incentives to search. We use a choice model that examines individual choice between pairs of simultaneous auctions. We find that within-pair price dispersion is substantial and that prices and auction choice by bidders are indeed related to search costs. We find strong inertia in auction choice and find that this effect significantly interacts with time left in the auction. Although individuals do not always choose a lower-priced auction, they are more likely to do so when search costs are low or search incentives are high.", "e:keyword": ["Experimental economics", "Auctions", "Field experiments", "Search"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0568", "e:abstract": "In the original version of the paper \"New Perspectives on Customer `Death' Using a Generalization of the Pareto/NBD Model\" by Kinshuk Jerath, Peter S. Fader, and Bruce G. S. Hardie (Marketing Science, Articles in Advance, May 27, 2010, DOI: 10.1287/mksc.1100.0568), some issues were brought to the attention of the journal by the authors after online publication in Articles in Advance. The accepting editor-in-chief, Steven M. Shugan, has chosen to retract the original published paper, allowing the authors the opportunity to resubmit a new paper that fully resolves those issues to the satisfaction of the authors and the journal.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0621", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1090.0491", "e:abstract": "In this research we introduce a new class of multivariate probability models to the marketing literature. Known as \"copula models,\" they have a number of attractive features. First, they permit the combination of any univariate marginal distributions that need not come from the same distributional family. Second, a particular class of copula models, called \"elliptical copula,\" has the property that they increase in complexity at a much slower rate than existing multivariate probability models as the number of dimensions increase. Third, they are very general, encompassing a number of existing multivariate models and providing a framework for generating many more. These advantages give copula models a greater potential for use in empirical analysis than existing probability models used in marketing. We exploit and extend recent developments in Bayesian estimation to propose an approach that allows reliable estimation of elliptical copula models in high dimensions. Rather than focusing on a single marketing problem, we demonstrate the versatility and accuracy of copula models with four examples to show the flexibility of the method. In every case, the copula model either handles a situation that could not be modeled previously or gives improved accuracy compared with prior models.", "e:keyword": ["Bayesian estimation", "Discrete copula", "Markov chain Monte Carlo", "Gaussian copula", "Media modeling", "Probability models", "Website page views"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0579", "e:abstract": "The likelihood for copula modeling appears when both the data and the copula representations are seen as being driven by common uniform latent variables. This perspective facilitates Bayesian inference for prediction and copula selection.", "e:keyword": ["Bayesian analysis", "Latent variable", "Likelihood"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0604", "e:abstract": "Estimating copula models using Bayesian methods presents some subtle challenges, ranging from specification of the prior to computational tractability. There is also some debate about what is the most appropriate copula to employ from those available. We address these issues here and conclude by discussing further applications of copula models in marketing.", "e:keyword": ["Bayesian estimation", "Discrete copula", "Markov chain Monte Carlo", "Gaussian copula", "Media modeling", "Probability models", "Website page views"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0586", "e:abstract": "By analyzing various alternative mixed channel structures composed of a monopoly manufacturer and online and offline outlets, we investigate how the specific channel structure and varying market conditions moderate the impact of Internet channel entry on the channel members and consumers. As an extension of Balasubramanian's model [Balasubramanian, S. 1998. Mail versus mall: A strategic analysis of competition between direct marketers and conventional retailers. Marketing Sci. 17(3) 181-195], our game-theoretic model captures the fundamental difference between two different channel types and consumer heterogeneity in preference for the Internet channel use. The equilibrium solutions indicate that Internet channel entry does not always lead to lower retail prices and enhanced consumer welfare. We also find that an independent retailer might become worse off after adding its own Internet outlet under certain market conditions. We find that the impact of the Internet channel introduction substantially varies across channel structures and market environments. We explain these varied results by proposing a framework of five key strategic forces that shape the overall impact of the Internet channel introduction.", "e:keyword": ["Channels of distribution", "Game theory", "Internet marketing", "Interchannel coordination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0588", "e:abstract": "The Internet has increased the flexibility of retailers, allowing them to operate an online arm in addition to their physical stores. The online channel offers potential benefits in selling to customer segments that value the convenience of online shopping, but it also raises new challenges. These include the higher likelihood of costly product returns when customers' ability to \"touch and feel\" products is important in determining fit. We study competing retailers that can operate dual channels (\"bricks and clicks\") and examine how pricing strategies and physical store assistance levels change as a result of the additional Internet outlet. A central result we obtain is that when differentiation among competing retailers is not too high, having an online channel can actually increase investment in store assistance levels (e.g., greater shelf display, more-qualified sales staff, floor samples) and decrease profits. Consequently, when the decision to open an Internet channel is endogenized, there can exist an asymmetric equilibrium where only one retailer elects to operate an online arm but earns lower profits than its bricks-only rival. We also characterize equilibria where firms open an online channel, even though consumers only use it for research and learning purposes but buy in stores. A number of extensions are discussed, including retail settings where firms carry multiple product categories, shipping and handling costs, and the role of store assistance in impacting consumer perceived benefits.", "e:keyword": ["Channels of distribution", "Retailing", "Internet marketing", "Product returns", "Reverse logistics", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0589", "e:abstract": "Brand preferences and marketplace demand are a reflection of the importance of underlying needs of consumers and the efficacy of product attributes for delivering value. Dog owners, for example, may look to dog foods to provide specific benefits for their pets (e.g., shiny coats) that may not be available from current offerings. An analysis of consumer wants for these consumers would reveal weak demand for product attributes resulting from low efficacy, despite the presence of strong latent interest. The challenge in identifying such unmet demand is in distinguishing it from other reasons for weak preference, such as general noninterest in the category and heterogeneous tastes. We propose a model for separating out these effects within the context of conjoint analysis, and we demonstrate its value with data from a national survey of toothpaste preferences. Implications for product development and reformulation are explored.", "e:keyword": ["Unmet demand", "Heterogeneous variable selection", "Conjoint analysis", "Bayesian hierarchical model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0590", "e:abstract": "The nature of the effect of media advertising on brand choice is investigated in two product categories in analyses that combine household scanner panel data with media exposure information. Alternative model specifications are tested in which advertising is assumed to directly affect brand utility, model error variance, and brand consideration. We find strong support for advertising effects on choice through an indirect route of consideration set formation that does not directly affect brand utility. Implications for media buying and advertising effects are explored.", "e:keyword": ["Bayesian analysis", "Threshold effects", "Determinants of utility"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0591", "e:abstract": "Are brands the \"new religion\"? Practitioners and scholars have been intrigued by the possibility, but strong theory and empirical evidence supporting the existence of a relationship between brands and religion is scarce. In what follows, we argue and demonstrate that religiosity is indeed related to \"brand reliance,\" i.e., the degree to which consumers prefer branded goods over unbranded goods or goods without a well-known national brand. We theorize that brands and religiosity may serve as substitutes for one another because both allow individuals to express their feelings of self-worth. We provide support for this substitution hypothesis with U.S. state-level data (field study) as well as individual-level data where religiosity is experimentally primed (study 1) or measured as a chronic individual difference (study 2). Importantly, studies 1 and 2 demonstrate that the relationship between religiosity and brand reliance only exists in product categories in which brands enable consumers to express themselves (e.g., clothes). Moreover, studies 3 and 4 demonstrate that the expression of self-worth is an important factor underlying the negative relationship.", "e:keyword": ["Brands", "Brand reliance", "Brand choice", "Religion", "Self-expression", "Self-worth"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0592", "e:abstract": "The use of a durable good is limited by both its physical life and usable life. For example, an electric-car battery can last for five years (physical life) or 100,000 miles (usable life), whichever comes first. We propose a framework for examining how a profit-maximizing firm might choose the usable life, physical life, and selling price of a durable good. The proposed framework considers differences in usage rates and product valuations by consumers and allows for the effects of technological constraints and product obsolescence on a product's usable and physical lives. Our main result characterizes a relationship between optimal price, cost elasticities, and opportunity costs associated with relaxing upper bounds on usable and physical lives. We describe conditions under which either usable life or physical life, or both, obtains its maximum possible values; examine why a firm might devote effort to relaxing nonbinding constraints on usable life or physical life; consider when price cuts might be accompanied with product improvements; and examine how a firm might be able to cross-subsidize product improvements.", "e:keyword": ["Product life", "Product design", "Technology development", "Durable goods", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0602", "e:abstract": "The idea of hierarchical, sequential, or intermediate effects has long been posited in textbooks and academic literature. Hierarchical effects occur when relationships among variables are mediated through other variables. Challenges in studying hierarchical effects in marketing include the large number of items present in most commercial studies and the presence of heterogeneous relationships among the variables. Existing approaches have dealt with the large number of variables by employing a factor structure representation of the data and have used standard mixture distributions for representing different response segments. In this paper, we propose a Bayesian model for the analysis of hierarchical data using the actual response items and incorporating heterogeneity that better reflects consumer stages in a decision process. Cross-sectional data from a national brand-tracking study are used to illustrate our model, where we find empirical support for a hierarchical relationship among media recall, brand beliefs, and intended actions. We find these effects to be insignificant when measured with standard models and aggregate analyses. The proposed model is useful for understanding the influence of variables that lead to intermediate as opposed to direct effects on brand choice.", "e:keyword": ["Hierarchical Bayes", "Mediation analysis", "Structural heterogeneity", "Variable selection"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0603", "e:abstract": "Firms in several markets attract consumers by offering discounts in other unrelated markets. This promotion strategy, which we call \"cross-market discounts,\" has been successfully adopted in the last few years by many grocery retailers in partnership with gasoline retailers across North America, Europe, and Australia. In this paper, we use an analytical model to investigate the major forces driving the profitability of this novel promotion strategy. We consider a generalized scenario in which purchases in a source market lead to price discounts redeemable in a target market. Our analysis shows that this strategy can be a revenue driver by simultaneously increasing prices as well as sales in the source market, even though we assume the demand curve to be downward sloping in price. Moreover, it distributes additional consumption (motivated by the discount) in two markets, and under diminishing marginal returns from consumption, this can simultaneously increase firm profits and consumer welfare more effectively than traditional nonlinear pricing strategies. Our study provides many other interesting insights as well, and our key results are in accordance with anecdotal evidence obtained from managers and industry publications.", "e:keyword": ["Fuelperks!", "Retail promotions", "Nonlinear pricing", "Competition", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0606", "e:abstract": "In certain categories, an important element of competition is the use of previews to signal information to potential consumers about product attributes. For example, the front page of a newspaper provides a preview to potential newspaper buyers before they purchase the product. In this context, a news provider can provide previews that are highly informative about the content of the news product. Conversely, a news provider can utilize a preview that is relatively uninformative. We examine the incentives that firms have to adopt different preview strategies in a context where they do not have complete control of product positioning. Our analysis shows that preview strategy can be a useful source of differentiation. However, when a firm adopts a strategy of providing informative previews, it confers a positive externality on a competitor that utilizes uninformative previews. This reinforces the incentive of the competitor to use uninformative previews and explains why the market landscape in news provision is often characterized by asymmetric competition.", "e:keyword": ["Product positioning", "Preview design", "Information goods", "Information revelation", "Product differentiation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0607", "e:abstract": "\"Behavior-based personalization\" has gained popularity in recent years, whereby businesses offer personalized products based on consumers' purchase histories. This paper highlights two perils of behavior-based personalization in competitive markets. First, although purchase histories reveal consumer preferences, competitive exploitation of such information damages differentiation, similar to the classic finding that behavior-based price discrimination intensifies price competition. With endogenous product design, there is yet a second peril. It emerges when forward-looking firms try to avoid the first peril by suppressing the information value of purchase histories. Ideally, if a market leader serves all consumers on day 1, purchase histories contain no information about consumer preferences. However, knowing that their rivals are willing to accommodate a market leader, firms are more likely to offer a mainstream design at day 1, which jeopardizes differentiation. Based on this understanding, I investigate how the perils of behavior-based personalization change under alternative market conditions, such as firms' better knowledge about their own customers, consumer loyalty and inertia, consumer self-selection, and the need for classic designs.", "e:keyword": ["Behavior-based personalization", "Behavior-based price discrimination", "Revealed preference", "Segmentation", "Targeting", "Competition", "Customer relationship management", "Endogenous information generation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0566", "e:abstract": "We study how opinion leadership and social contagion within social networks affect the adoption of a new product. In contrast to earlier studies, we find evidence of contagion operating over network ties, even after controlling for marketing effort and arbitrary systemwide changes. More importantly, we also find that the amount of contagion is moderated by both the recipients' perception of their opinion leadership and the sources' volume of product usage. The other key finding is that sociometric and self-reported measures of leadership are weakly correlated and associated with different kinds of adoption-related behaviors, which suggests that they probably capture different constructs. We discuss the implications of these novel findings for diffusion theory and research and for marketing practice.", "e:keyword": ["Diffusion of innovations", "Opinion leadership", "Social contagion", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0595", "e:abstract": "No abstract available.", "e:keyword": ["Social networks", "Influence", "Diffusion of innovation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0596", "e:abstract": "Isuggest five broad directions for future research on social influence and opinion leadership that could, if appropriately addressed, dramatically improve how we conceptualize and manage social contagions in a variety of domains.", "e:keyword": ["Social networks", "Peer influence", "Behavioral contagion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0605", "e:abstract": "No abstract available.", "e:keyword": ["Diffusion of innovations", "Opinion leadership", "Social contagion", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0614", "e:abstract": "Building on the commentaries on our work, we make additional suggestions for future research on social contagion and new product diffusion. In particular, we note that social contagion may occur for many reasons and that investigating how various personal or group characteristics moderate the amount of influence some customers exert or the extent to which others are sensitive to potential influence can provide insights into the social mechanism(s) at work.", "e:keyword": ["Diffusion of innovations", "Opinion leadership", "Social contagion", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0615", "e:abstract": "We show both analytically and through Monte Carlo simulations that applying standard hazard models to right-truncated data, i.e., data from which all right-censored observations are omitted, induces spurious positive duration dependence and hence can trick researchers into believing to have found evidence of social contagion when there is none. Truncation also tends to deflate the effect of time-invariant covariates. These results imply that not accounting for right truncation can lead managers to rely too much on word of mouth in generating new product adoption and to poorly identify the customers most likely to adopt early. Not accounting for right truncation can also lead to suboptimal pricing decisions and to erroneous assessments of variations in customer lifetime value. We assess the effectiveness of four possible solutions to the problem and find that only using an analytically corrected likelihood function protects one against truncation artifacts inflating coefficients of contagion and attenuating coefficients of time-invariant covariates.", "e:keyword": ["Hazard models", "Duration dependence", "New product diffusion", "Social contagion"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0609", "e:abstract": "Facebook and Google offer hybrid advertising auctions that allow advertisers to bid on a per-impression or a per-click basis for the same advertising space. This paper studies the properties of equilibrium and considers how to increase efficiency in this new auction format. Rational expectations require the publisher to consider past bid types to prevent revenue losses to strategic advertiser behavior. The equilibrium results contradict publisher statements and suggest that, conditional on setting rational expectations, publishers should consider offering multiple bid types to advertisers. For a special case of the model, we provide a payment scheme that achieves the socially optimal allocation of advertisers to slots and maximizes publisher revenues within the class of socially optimal payment schemes. When this special case does not hold, no payment scheme will always achieve the social optimum.", "e:keyword": ["Advertising", "Auctions", "Internet marketing", "Search advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0620", "e:abstract": "We present a dynamic factor-analytic choice model to capture evolution of brand positions in latent attribute space. Our dynamic model allows researchers to investigate brand positioning in new categories or mature categories affected by structural change such as entry. We argue that even for mature categories not affected by structural change, the assumption of stable attributes may be untenable. We allow for evolution in attributes by modeling individual-level time-specific attributes as arising from dynamic means. The dynamic attribute means are modeled as a Bayesian dynamic linear model (DLM). The DLM is nested within a factor-analytic choice model. Our approach makes efficient use of the data by leveraging estimates from previous and future periods to estimate current period attributes. We demonstrate the robustness of our model with data that simulate a variety of dynamic scenarios, including stationary behavior. We show that misspecified attribute dynamics induce temporal heteroskedasticty and correlation between the preference weights and the error term. Applying the model to a panel data set on household purchases in the malt beverage category, we find considerable evidence for dynamics in the latent brand attributes. From a managerial perspective, we find advertising expenditures help explain variation in the dynamic attribute means.", "e:keyword": ["Choice modeling", "Bayesian estimation", "Dynamic models", "Factor-analytic models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0622", "e:abstract": "In this paper, I study profitability of the name-your-own-price channel (NYOP) in the presence of risk-averse buyers. First, I provide conditions that guarantee that for the monopolistic seller the NYOP is more profitable than the posted price. Second, I consider a more competitive framework where buyers with rejected bids have access to an alternative option. I show that if under the posted-price scenario there are unserved customers with low valuations, then NYOP is more profitable than the posted price. Finally, I study whether adding the posted-price option to the NYOP will further increase the seller's profit and show that for the decreasing absolute risk-aversion utility and a monopolistic seller it does not. In the presence of an alternative option, the answer depends on whether buyers consider the posted-price option and the alternative option to be close substitutes or not. Adding the posted-price option will increase the profit in the former case and will not in the latter.", "e:keyword": ["Pricing", "Bidding", "Name-your-own-price", "Reverse auctions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0624", "e:abstract": "Experimental and survey-based research suggests that consumers often rely on their intuition and cognitive shortcuts to make decisions. Intuition and cognitive shortcuts can lead to suboptimal decisions and, especially in high-stakes decisions, to legitimate welfare concerns. In this paper, we propose an extension of a Bayesian learning model that allows us to quantify the impact of salience--the fact that some pieces of information are easier to retrieve from memory than others--on physician learning. We show, using data on actual prescriptions for real patients, that physicians' belief formation is strongly influenced by salience effects. Feedback from switching patients--the ones the physician decided to switch to a clinically equivalent treatment--receives considerably more weight than feedback from other patients. In the category we study, salience effects slowed down physicians' speed of learning and the adoption of a new treatment, which raises welfare concerns. For managers, our findings suggest that firms that are able to eliminate, or at least reduce, salience effects to a greater extent than their competitors can speed up the adoption of new treatments. We explore the implications of these results and suggest alternative applications of our model that are relevant for policy makers and managers.", "e:keyword": ["Consumer learning", "Quasi-Bayesian learning models", "Behavioral modeling", "Medical decision making", "Physician learning", "New drug adoption"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0629", "e:abstract": "Existing research on choice designs focuses exclusively on compensatory models that assume that all available alternatives are considered in the choice process. In this paper, we develop a method to construct efficient designs for a two-stage, consider-then-choose model that involves a noncompensatory screening process at the first stage and a compensatory choice process at the second stage. The method applies to both conjunctive and disjunctive screening rules. Under certain conditions, the method also applies to the subset conjunctive and disjunctions of conjunctions screening rules. Based on the local design criterion, we conduct a comparative study of compensatory and conjunctive designs--the former are optimized for a compensatory model and the latter for a two-stage model that uses conjunctive screening in its first stage. We find that conjunctive designs have higher level overlap than compensatory designs. This occurs because level overlap helps pinpoint screening behavior. Higher overlap of conjunctive designs is also accompanied by lower orthogonality, less level balance, and more utility balance. We find that compensatory designs have a significant loss of design efficiency when the true model involves conjunctive screening at the consideration stage. These designs also have much less power than conjunctive designs in identifying a true consider-then-choose process with conjunctive screening. In contrast, when the true model is compensatory, the efficiency loss from using a conjunctive design is lower. Also, conjunctive designs have about the same power as compensatory designs in identifying a true compensatory choice process. Our findings make a strong case for the use of conjunctive designs when there is prior evidence to support respondent screening.", "e:keyword": ["Experimental design", "Conjoint choice designs", "D-optimality", "Consider-then-choose model", "Noncompensatory screening rules"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0617", "e:abstract": "The failure of firms in the face of technological change has been a topic of intense research and debate, spawning the theory (among others) of disruptive technologies. However, the theory suffers from circular definitions, inadequate empirical evidence, and lack of a predictive model. We develop a new schema to address these limitations. The schema generates seven hypotheses and a testable model relating to platform technologies. We test this model and hypotheses with data on 36 technologies from seven markets. Contrary to extant theory, technologies that adopt a lower attack (\"potentially disruptive technologies\") (1) are introduced as frequently by incumbents as by entrants, (2) are not cheaper than older technologies, and (3) rarely disrupt firms; and (4) both entrants and lower attacks significantly reduce the hazard of disruption. Moreover, technology disruption is not permanent because of multiple crossings in technology performance and numerous rival technologies coexisting without one disrupting the other. The proposed predictive model of disruption shows good out-of-sample predictive accuracy. We discuss the implications of these findings.", "e:keyword": ["Technology disruption", "Firm disruption", "Demand disruption", "Correlated hazards", "Prediction of disruption"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0613", "e:abstract": "Many firms have introduced Internet-based customer self-service applications such as online payments or brokerage services. Despite high initial sign-up rates, not all customers actually shift their dealings online. We investigate whether the multistage nature of the adoption process (an \"adoption funnel\") for such technologies can explain this low take-up. We use exogenous variation in events that possibly interrupt adoption, in the form of vacations and public holidays in different German states, to identify the effect on regular usage of being interrupted earlier in the adoption process. We find that interruptions in the early stages of the adoption process reduce a customer's probability of using the technology regularly. Our results suggest significant cost-saving opportunities from eliminating interruptions in the adoption funnel.", "e:keyword": ["Online banking", "Technology adoption", "Adoption process", "Online security", "Self-service technology"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0618", "e:abstract": "We study how multiattribute product choices are affected by peer influence. We propose a two-stage conjoint-based approach to examine three behavioral mechanisms of peer influence. We find that when faced with information on peer choices, consumers update their attribute preferences in a Bayesian manner. This suggests that greater uncertainty in the attribute preferences of a focal consumer and lesser uncertainty in preferences of peers both lead to greater preference revision. Greater number of peers is associated with greater preference revision, although the extent of preference revision diminishes with increasing number of peers. Furthermore, to address the significant time and costs associated with collecting sociometric data, we estimate the accuracy of predicted consumer choices when peer influence data are unavailable. Online social network membership and frequency of peer interactions provide better proxies than more common demographic similarity measures. These findings have key implications, especially for word-of-mouth marketing.", "e:keyword": ["Preference revision", "Bayesian updating", "Attribute preference uncertainty", "Social networks", "Social influence", "Conjoint analysis", "Bayesian estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0583", "e:abstract": "We use data from a large-scale field experiment to explore what influences the effectiveness of online advertising. We find that matching an ad to website content and increasing an ad's obtrusiveness independently increase purchase intent. However, in combination, these two strategies are ineffective. Ads that match both website content and are obtrusive do worse at increasing purchase intent than ads that do only one or the other. This failure appears to be related to privacy concerns: the negative effect of combining targeting with obtrusiveness is strongest for people who refuse to give their income and for categories where privacy matters most. Our results suggest a possible explanation for the growing bifurcation in Internet advertising between highly targeted plain text ads and more visually striking but less targeted ads.", "e:keyword": ["e-commerce", "Privacy", "Advertising", "Targeting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0594", "e:abstract": "In a very intriguing and groundbreaking study, Goldfarb and Tucker [Goldfarb, A., C. Tucker. 2011. Online display advertising: Targeting and obtrusiveness. Marketing Sci. 30(3) 389-404] show that online advertising targeting and obtrusiveness boost purchase intent independently, but not jointly. The authors rule out recall as an explanatory mechanism and provide preliminary evidence that the effect may be driven by privacy concerns. We comment on the substantive importance of this finding by discussing the psychological and economic implications of the effect.", "e:keyword": ["Online advertising", "Internet targeting", "Pop-up ads", "Obtrusiveness", "Reactance", "Privacy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0599", "e:abstract": "In \"Online Display Advertising: Targeting and Obtrusiveness,\" Avi Goldfarb and Catherine Tucker present an empirical investigation and discussion of consumers' reactions to obtrusive and targeted Internet advertisements. They find, among other things, that obtrusive advertisements when combined with targeting tend to generate no greater effectiveness in consumer response than either method used alone. Perhaps one of the most interesting findings in their results involves the authors' focus on privacy salience and its connection to advertisement presentation. This brief discussion of their work (1) highlights the importance of further research in line with the well-crafted privacy preference inquiries of Goldfarb and Tucker's study and (2) complements the authors' discussion regarding possible public policy implications with a legal discussion elaborating on existing legal grounds for liability and restrictions on advertising consumers perceive as privacy invasive.", "e:keyword": ["Law", "Policy", "Privacy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0634", "e:abstract": "The commentaries on our work suggest several broader implications of our findings as well as a concern that we understate the size of the effect. In this rejoinder, we discuss our views on the regulatory implications, the implications for firm strategies, and the implications for our understanding of the underlying behavioral processes. We also acknowledge that our original calculation of $464 million in cost savings for industry is conservative. We conclude with a call for \"privacy engineering\" research that combines computer science tools with an understanding of consumer behavior and economics to improve marketing and economic outcomes while safeguarding consumer privacy.", "e:keyword": ["e-commerce", "Privacy", "Advertising", "Targeting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0633", "e:abstract": "Automotive sales forecasts traditionally focus on predictors such as advertising, brand preference, life cycle position, retail price, and technological sophistication. The quality of the cars' design is, however, an often-neglected variable in such models. We show that incorporating objective measures of design prototypicality and design complexity in sales forecasting models improves their prediction by up to 19%. To this end, we professionally photographed the frontal designs of 28 popular models, morphed the images, and created objective prototypicality (car-to-morph Euclidian proximity) and complexity (size of a compressed image file) scores for each car. Results show that prototypical but complex car designs feel surprisingly fluent to process, and that this form of surprising fluency evokes positive gut reactions that become associated with the design and positively impact car sales. It is important to note that the effect holds for both economy (functionality oriented) and premium (identity oriented) cars, as well as when the above-mentioned traditional forecasting variables are considered. These findings are counter to a common intuition that consumers like unusual-complex designs that reflect their individuality or prototypical-simple designs that are functional.", "e:keyword": ["Automobile sales", "Product design", "Processing fluency", "Visual prototypicality", "Visual complexity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0638", "e:abstract": "This paper studies the tendency to use negative ads. For this purpose, we focus on an interesting industry (political campaigns) and an intriguing empirical regularity (the tendency to \"go negative\" is higher in close races). We present a model of electoral competition in which ads inform voters either of the good traits of the candidate or of the bad traits of his opponent. We find that in equilibrium, the proportion of negative ads depends on both voters' knowledge and the candidate's budget. Furthermore, for an interesting subset of the parameter space, negativity increases in both knowledge and budget. Using data on the elections for the U.S. House of Representative in 2000, 2002, and 2004, we examine the model and its implications. Using nonstructural estimation, we find that negativity indeed increases in both voters' knowledge and the candidate's budget. Furthermore, we also find that knowledge and budget mediate the effect of closeness on negativity. Using structural estimation, we reinforce these findings. Specifically, we find that the model's parameters are within the subset of the parameter space discussed above. Thus, the evidence implies that the model is not only helpful in identifying variables that were ignored by previous studies (i.e., knowledge and budget) but also in explaining an intriguing empirical regularity.", "e:keyword": ["Political marketing", "Advertising", "Analytical models", "Bayesian estimation", "Cross-sectional analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0626", "e:abstract": "Sponsored search advertising is ascendant--Forrester Research reports expenditures rose 28% in 2007 to $8.1 billion and will continue to rise at a 26% compound annual growth rate [VanBoskirk, S. 2007. U.S. interactive marketing forecast, 2007 to 2012. Forrester Research (October 10)], approaching half the level of television advertising and making sponsored search one of the major advertising trends to affect the marketing landscape. Yet little empirical research exists to explore how the interaction of various agents (searchers, advertisers, and the search engine) in keyword markets affects consumer welfare and firm profits. The dynamic structural model we propose serves as a foundation to explore these outcomes. We fit this model to a proprietary data set provided by an anonymous search engine. These data include consumer search and clicking behavior, advertiser bidding behavior, and search engine information such as keyword pricing and website design. With respect to advertisers, we find evidence of dynamic bidding behavior. Advertiser value for clicks on their links averages about 26 cents. Given the typical $22 retail price of the software products advertised on the considered search engine, this implies a conversion rate (sales per click) of about 1.2%, well within common estimates of 1%-2% [Narcisse, E. 2007. Magid: Casual free to pay conversion rate too low. GameDaily.com (September 20)]. With respect to consumers, we find that frequent clickers place a greater emphasis on the position of the sponsored advertising link. We further find that about 10% of consumers do 90% of the clicks. We then conduct several policy simulations to illustrate the effects of changes in search engine policy. First, we find the search engine obtains revenue gains of 1% by sharing individual-level information with advertisers and enabling them to vary their bids by consumer segment. This also improves advertiser revenue by 6% and consumer welfare by 1.6%. Second, we find that a switch from a first- to second-price auction results in truth telling (advertiser bids rise to advertiser valuations). However, the second-price auction has little impact on search engine profits. Third, consumer search tools lead to a platform revenue increase of 2.9% and an increase of consumer welfare by 3.8%. However, these tools, by reducing advertising exposures, lower advertiser profits by 2.1%.", "e:keyword": ["Sponsored search advertising", "Two-sided market", "Dynamic game", "Structural models", "Empirical IO", "Customization", "Auctions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0643", "e:abstract": "In many categories consumers display cyclical buying: they repeatedly purchase in the category for several periods, followed by several periods of not buying. We believe that the cyclicality is a manifestation of cross-category substitution by the consumer, caused by \"variety-seeking\" tendencies as well as by the firm's marketing activities in all relevant categories. We propose a Markov regime-switching random coefficient logit model to represent these behaviors as stochastic switching between high and low category purchase tendencies. The main feature of the proposed model is that it divides the stream of purchase decisions of a consumer into distinct regimes with different parameter values that characterize high versus low purchase tendencies. In an empirical application of the model to purchases of yogurt-buying households, we find that as many as 38.3% households display cyclicality between high and low yogurt-purchasing tendencies. Predictions from our proposed model track observed yogurt purchases of households over time closely, and the model also fits better than two benchmark models. Alternating between high and low purchase tendencies may correspond with changing levels of consumer inventory in a substitute category. If one ignores this phenomenon, a correlation between yogurt inventory and the error term in utility arises, leading to biased estimates. Also, we show that cyclicality in buying has a key implication for a firm's price promotion strategies: a price reduction that is offered to a household during its high purchasing tendency period will result in greater increases in sales than one that is offered during its low purchasing period. This opens up a new dimension for enhancing the effectiveness of promotions--customized timing of price reductions.", "e:keyword": ["Random coefficients", "Logit model", "Endogeneity", "Heterogeneity", "Simulated maximum likelihood", "Brand choice", "Scanner data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0636", "e:abstract": "A choice model based on direct utility maximization subject to an arbitrary number of constraints is developed and applied to conjoint data. The model can accommodate both corner and interior solutions, and it provides insights into the proportion of respondents bound by each constraint. Application to volumetric choice data reveals that the majority of respondents make choices consistent with price and quantity restrictions. Estimates based on a single monetary-constraint choice model are shown to lead to biased estimates of the monetary value of attribute levels.", "e:keyword": ["Multiple constraints", "Choice model", "Corner and interior solutions", "Quantity restriction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0630", "e:abstract": "The dramatic impact of the current crisis on performance of businesses across sectors and economies has been headlining the business press for the past several months. Extant reconciliations of these patterns in the popular press rely on ad hoc reasoning. Using historical data on currency crisis episodes across the world, we show that the impact of the crisis on a firm's business is best understood by focusing on the impact of the crisis on the behavior of consumers. Our analyses show that consumer behavior in a crisis is characterized by consumption smoothing at various levels--intertemporal, intercategory, and intracategory. These behavioral adjustments result in significant reallocation of consumption expenditures. More importantly, the smoothing decisions because of a crisis are distinct and independent of the impact of changes in income and prices that accompany a crisis. Interestingly, there is marked variation in the patterns of consumption smoothing across different types of economies. Taken together, these results have important and interesting implications for managers, policy makers, and academics.", "e:keyword": ["Consumer behavior", "Consumption smoothing", "Crisis", "Econometrics", "Marketing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0640", "e:abstract": "Under the sociological theory of homophily, people who are similar to one another are more likely to interact with one another. Marketers often have access to data on interactions among customers from which, with homophily as a guiding principle, inferences could be made about the underlying similarities. However, larger networks face a quadratic explosion in the number of potential interactions that need to be modeled. This scalability problem renders probability models of social interactions computationally infeasible for all but the smallest networks. In this paper, we develop a probabilistic framework for modeling customer interactions that is both grounded in the theory of homophily and is flexible enough to account for random variation in who interacts with whom. In particular, we present a novel Bayesian nonparametric approach, using Dirichlet processes, to moderate the scalability problems that marketing researchers encounter when working with networked data. We find that this framework is a powerful way to draw insights into latent similarities of customers, and we discuss how marketers can apply these insights to segmentation and targeting activities.", "e:keyword": ["Social networks", "Nonparametric Bayes", "Dirichlet processes", "Word of mouth", "Homophily", "Probability models", "Bayesian networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0632", "e:abstract": "The World Wide Web contains a vast corpus of consumer-generated content that holds invaluable insights for improving the product and service offerings of firms. Yet the typical method for extracting diagnostic information from online content--text mining--has limitations. As a starting point, we propose analyzing a sample of comments before initiating text mining. Using a combination of real data and simulations, we demonstrate that a sampling procedure that selects respondents whose comments contain a large amount of information is superior to the two most popular sampling methods--simple random sampling and stratified random sampling---in gaining insights from the data. In addition, we derive a method that determines the probability of observing diagnostic information repeated a specific number of times in the population, which will enable managers to base sample size decisions on the trade-off between obtaining additional diagnostic information and the added expense of a larger sample. We provide an illustration of one of the methods using a real data set from a website containing qualitative comments about staying at a hotel and demonstrate how sampling qualitative comments can be a useful first step in text mining.", "e:keyword": ["Consumer-generated media", "Consumer-generated content", "Customer feedback on the Web", "Text mining", "Qualitative comments", "Large-scale qualitative data sets", "Sampling open-ended questions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0611", "e:abstract": "Using the Bayes factor estimated by harmonic mean [Newton, M. A., A. E. Raftery. 1994. Approximate Bayesian inference by the weighted likelihood bootstrap. J. Roy. Statist. Soc. Ser. B. 56(1) 3-48] to compare models with and without cross-brand pass-through, Dub and Gupta [Dub, J.-P., S. Gupta. 2008. Cross-brand pass-through in supermarket pricing. Marketing Sci. 27(3) 324-333] found that, in the refrigerated orange juice category, a model with cross-brand pass-through was selected 68% of the time. However, Lenk [Lenk, P. J. 2009. Simulation pseudo-bias correction to the harmonic mean estimator of integrated likelihoods. J. Comput. Graph. Statist. 18(1) 941-960] has demonstrated that the infinite variance harmonic mean estimator often exhibits simulation pseudo-bias in favor of more complex models. We replicate the results of Dub and Gupta in the refrigerated orange juice category and then show that any of three more stable finite variance estimators select the model with cross-brand pass-through less than 1% of the time. Relaxing the assumption that model errors are distributed normally eliminates all instances in which the cross-brand pass-through model is selected. In 10 additional categories, the harmonic-mean-estimated Bayes factor selects the model with cross-brand pass-through 69% of the time, whereas a finite variance estimator of the Bayes factor selects the model with cross-brand pass-through only 5% of the time. Applying arguments in McAlister [McAlister, L. 2007. Cross-brand pass-through: Fact or artifact? Marketing Sci. 26(6) 876-898], these 5% of cases can be attributed to capitalization on chance. We conclude that Dub and Gupta should not be interpreted as providing evidence of cross-brand pass-through.", "e:keyword": ["Cross-brand pass-through", "Promotion", "Retail and wholesale price", "Scanner data", "Bayes factor", "Model comparison", "Bayesian nonparametric"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0627", "e:abstract": "Previous research on marketing budget decisions has shown that profit improvement from better allocation across products or regions is much higher than from improving the overall budget. However, despite its high managerial relevance, contributions by marketing scholars are rare.</p> <p>In this paper, we introduce an innovative and feasible solution to the dynamic marketing budget allocation problem for multiproduct, multicountry firms. Specifically, our decision support model allows determining near-optimal marketing budgets at the country-product-marketing-activity level in an Excel-supported environment each year. The model accounts for marketing dynamics and a product's growth potential as well as for trade-offs with respect to marketing effectiveness and profit contribution. The model has been successfully implemented at Bayer, one the world's largest pharmaceutical and chemical firms. The profit improvement potential is more than 50% and worth nearly euro 500 million in incremental discounted cash flows.", "e:keyword": ["Resource allocation", "Optimization", "Sales force", "Panel data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0619", "e:abstract": "This paper describes the use of a marketing science model by Jetstar, a subsidiary of Australia's leading airline, Qantas, to effectively and profitably compete in the low-cost carrier marketplace. We trace the evolution of the Jetstar strategy from a baseline calibration of its initial position, to its efforts to attain price competitiveness and service parity, followed by its highly focused, cost-effective service delivery strategy. We develop a hierarchical model with parameters estimated at the individual level. This allows us to study not only how service design and pricing initiatives shift the perceived performance of Jetstar relative to its competitors but also how the airline can move market preferences toward areas in which it has competitive advantage. The contribution of the research is substantial. The Jetstar market share went from 14.0% to 18.1% during the first five quarterly waves of the research, and profits went from $79 million in 2006-2007, before the study was commissioned, to $124 million in 2008-2009.", "e:keyword": ["Airlines", "Consumer choice", "Bayesian estimation", "Service management", "Marketing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0610", "e:abstract": "A variable annuity is a popular product for investing retirement income. However, thousands of similar-looking variable annuity products are being offered by hundreds of financial service companies. In such a scenario, how can Prudential achieve meaningful product differentiation to increase the sales of its variable annuities? The solution led to the development and implementation of the \"Emotion Quotient\" (EQ) Tool. The EQ Tool enabled Prudential to redefine its marketing and sales approach along a proactive (as opposed to responsive) market orientation paradigm. This was accomplished by first using the EQ Tool to uncover and quantify the prevalence of certain emotions (such as fear and regret) in the prospective consumer and then pitching relevant variable annuity product(s) that could mitigate the specific behavioral risk corresponding to the prevalent emotion(s). This approach, which was backed by extensive research (as described in this study), enabled Prudential to gain over $450 million lift in variable annuity sales and contributed to consumer welfare by promoting awareness of behavioral risk to investors who are within five years of their retirement. This research study illustrates how industry can collaborate with academia to successfully apply marketing science to solve real-world business problems.", "e:keyword": ["Behavioral economics", "Emotions", "Structural equation model", "Proactive market orientation", "Implicit consumer needs", "Product positioning", "Marketing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0612", "e:abstract": "Inofec, a small- to medium-sized enterprise in the business-to-business sector, desired a more analytic approach to allocate marketing resources across communication activities and channels. We developed a conceptual framework and econometric model to empirically investigate (1) the marketing communication effects on off-line and online purchase funnel metrics and (2) the magnitude and timing of the profit impact of firm-initiated and customer-initiated contacts. We find evidence of many cross-channel effects, in particular, off-line marketing effects on online funnel metrics and online funnel metrics on off-line purchases. Moreover, marketing communication activities directly affect both early and later purchase funnel stages (website visits, online and off-line information, and quote requests). Finally, we find that online customer-initiated contacts have substantially higher profit impact than off-line firm-initiated contacts. Shifting marketing budgets toward these activities in a field experiment yielded net profit increases 14 times larger than those for the status quo allocation.", "e:keyword": ["Profit impact of marketing activities", "Off-line and online", "Persistence modeling", "Field experiment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0645", "e:abstract": "We study the bidding strategies of vertically differentiated firms that bid for sponsored search advertisement positions for a keyword at a search engine. We explicitly model how consumers navigate and click on sponsored links based on their knowledge and beliefs about firm qualities. Our model yields several interesting insights; a main counterintuitive result we focus on is the \"position paradox.\" The paradox is that a superior firm may bid lower than an inferior firm and obtain a position below it, yet it still obtains more clicks than the inferior firm. Under a pay-per-impression mechanism, the inferior firm wants to be at the top where more consumers click on its link, whereas the superior firm is better off by placing its link at a lower position because it pays a smaller advertising fee, but some consumers will still reach it in search of the higher-quality firm. Under a pay-per-click mechanism, the inferior firm has an even stronger incentive to be at the top because now it only has to pay for the consumers who do not know the firms' reputations and, therefore, can bid more aggressively. Interestingly, as the quality premium for the superior firm increases, and/or if more consumers know the identity of the superior firm, the incentive for the inferior firm to be at the top may increase. Contrary to conventional belief, we find that the search engine may have the incentive to overweight the inferior firm's bid and strategically create the position paradox to increase overall clicks by consumers. To validate our model, we analyze a data set from a popular Korean search engine firm and find that (i) a large proportion of auction outcomes in the data show the position paradox, and (ii) sharp predictions from our model are validated in the data.", "e:keyword": ["Sponsored search advertising", "Search cost", "Vertical differentiation", "Bidding strategy", "Pay per impression", "Pay per click"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0641", "e:abstract": "The critical role of research and development (R&D) and advertising in the marketing strategy of the firm is well established. This paper conceptually and empirically examines why and how much the effectiveness of these two marketing instruments differs between times of economic expansions versus periods of economic contractions--and whether these results depend on the cyclicality of the industry in question. We consider a key marketing metric (market share) and a key financial metric (firm profit). Our empirical setting is 1,175 U.S. firms across a time period spanning over three decades. We find that R&D and advertising contribute to firm performance but that their effectiveness is not constant across the business cycle. Increasing advertising share in contractions has a stronger effect on profit and market share than increasing advertising share in expansions. Likewise, investments in R&D in contractions lead to higher gains in market share and profit than R&D investments in expansions, albeit only in subsequent years. If in contractions the firm faces tight budget constraints and has to choose between either maintaining R&D or advertising, our simulation results show that maintaining R&D is associated with better company performance. We find that advertising effectiveness, in general, and in contractions, in particular, is systematically moderated by the degree of cyclicality of the industry in which the firm operates. In relatively stable industries, advertising effects are small or even nonsignificant, and they do not go beyond the year the firm advertises. However, in highly cyclical industries, advertising effects are long-lasting, its total effect being 50% larger (market share) and 200% larger (profits) than in industries of average cyclicality. The effect of industry cyclicality on advertising effectiveness is especially pronounced in contractions. Collectively, these findings provide valuable and actionable insights into how firms should respond to contractions in order to grow profits and market share.", "e:keyword": ["Economic contractions", "R&D", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0635", "e:abstract": "Many online shoppers initially acquired through paid search advertising later return to the same website directly. These so-called \"direct type-in\" visits can be an important indirect effect of paid search. Because visitors come to sites via different keywords and can vary in their propensity to make return visits, traffic at the keyword level is likely to be heterogeneous with respect to how much direct type-in visitation is generated.</p> <p>Estimating this indirect effect, especially at the keyword level, is difficult. First, standard paid search data are aggregated across consumers. Second, there are typically far more keywords than available observations. Third, data across keywords may be highly correlated. To address these issues, the authors propose a hierarchical Bayesian elastic net model that allows the textual attributes of keywords to be incorporated.</p> <p>The authors apply the model to a keyword-level data set from a major commercial website in the automotive industry. The results show a significant indirect effect of paid search that clearly differs across keywords. The estimated indirect effect is large enough that it could recover a substantial part of the cost of the paid search advertising. Results from textual attribute analysis suggest that branded and broader search terms are associated with higher levels of subsequent direct type-in visitation.", "e:keyword": ["Internet", "Paid search", "Bayesian methods", "Elastic net"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0651", "e:abstract": "What the firm should say in an advertising message, the choice of <i>content</i>, is a critical managerial decision. Here, we focus on a particular aspect of the advertising content choice: an attribute-focused appeal versus an appeal with no direct information on product attributes. We make two assumptions that capture the reality of the advertising context. First, we assume that the bandwidth of advertising is limited: a firm can only communicate about a limited number of attributes. Second, we assume that consumers are active: they can choose to engage in a costly search to obtain additional product-related information. In this setting, we show that there exists an equilibrium where the high-quality firm chooses to produce messages devoid of any attribute information in order to invite the consumer to engage in search, which is likely to uncover positive information about the product. Whereas most of the previous literature has focused on the decision to advertise as a signal of quality, we show that message content, coupled with consumer search, can also serve as a credible signal of quality. In an extension, we show that our results are robust to endogenizing the firm's decision on the amount of advertising spending.", "e:keyword": ["Advertising", "Advertising content", "Attribute", "Nonattribute-focused advertising", "Uninformative advertising", "Quality signal", "Consumer search"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0644", "e:abstract": "Marketing expenditures in the form of pricing, product development, promotion, and channel development are made to maximize profits. A challenge in evaluating the effectiveness of these expenditures is that decisions such as whether to lower prices or run promotions are made based on managers' knowledge of how sensitive consumers are to these marketing activities. Although marketing control variables are explanatory of sales, they are often set in anticipation of a market response, which reflects strategic behavior on the part of a firm. A challenge in developing a model of strategic behavior is that the process by which marketing expenditures are made is often not directly observable. We propose tests for comparing supply-side model formulations in which input variables are strategically determined. In these models, the joint likelihood of demand (y) and supply (x) can be factored into a conditional factor of demand given supply and into a marginal factor of supply. We illustrate our approach using data from a services company that operates in multiple geographic regions.", "e:keyword": ["Structural models", "Bayesian analysis", "Bayes factors", "Model selection"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0642", "e:abstract": "Interpersonal communications have long been recognized as an influential source of information for consumers. Internet-based media have facilitated information exchange among firms and consumers, as well as observability and measurement of such exchanges. However, much of the research addressing online communication focuses on ratings collected from online forums. In this paper, we look beyond ratings to a more comprehensive view of online communications. We consider the sales effect of the volume of positive, negative, and neutral online communications captured by Web crawler technology and classified by automated sentiment analysis. Our modeling approach captures two key features of our data, dynamics and endogeneity. In terms of dynamics, we model daily measures of online communications about a firm and its products as contributing to a latent demand-generating stock variable. To account for the endogeneity, we extend the latent instrumental variable technique to account for dynamic endogenous regressors. Our results demonstrate a significant effect of positive, negative, and neutral online communications on daily sales performance. Failure to account for endogeneity results in a severe attenuation of the estimated effects. From a managerial perspective, we demonstrate the importance of accounting for communication valence as well as the impact of shocks to positive, negative, and neutral online communications.", "e:keyword": ["Word of mouth", "Bayesian estimation", "Endogeneity", "Dynamics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0648", "e:abstract": "In marketing applications, it is common that some key covariates in a regression model, such as marketing mix variables or consumer profiles, are subject to missingness. The convenient method that excludes the consumers with missingness in <i>any</i> covariate can result in a substantial loss of efficiency and may lead to strong selection bias in the estimation of consumer preferences and sensitivities. To solve these problems, we propose a new Bayesian distribution-free approach, which can ensure that no customer is left behind in the analysis as a result of missing covariates. In this way, all customers are being considered in devising managerial policies. The proposed approach allows for flexible modeling of a joint distribution of multidimensional interrelated covariates that can contain both continuous and discrete variables. At the same time, it minimizes the impact of distributional assumptions involved in covariate modeling because the method does not require researchers to specify parametric distributions for covariates and can automatically generate suitable distributions for missing covariates. We have developed an efficient Markov chain Monte Carlo algorithm for inference. Besides robustness and flexibility, the proposed approach reduces modeling and computational efforts associated with missing covariates and therefore makes the missing covariate problems easier to handle. We evaluate the performance of the proposed method using extensive simulation studies. We then illustrate the method in two real data examples in which missing covariates occur: a mixed multinomial logit discrete-choice model in a ketchup data set and a hierarchical probit purchase incidence model in a retail store data set. These analyses demonstrate that the proposed method overcomes several important limitations of existing approaches for solving missing covariate problems and offers opportunities to make better managerial decisions with the current available marketing databases. Although our applications focus on consumer-level data, the proposed method is general and can be applied to other marketing applications where other types of marketing players are the units of analysis.", "e:keyword": ["CRM", "Hierarchical Bayesian", "Individual marketing", "Marketing mix variable", "MCMC", "Missing covariates"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1100.0631", "e:abstract": "Online retailing provides an opportunity for new pricing options that are not feasible in traditional retail settings. This paper proposes an interactive, dynamic pricing strategy from the perspective of customized bundling to derive savings for customers while maximizing profits for electronic retailers (\"e-tailers\"). Given product costs, posted prices, shipping fees, and customers' reservation prices, we propose a nonlinear mixed-integer programming model to increase e-tailers' profits by sequentially pricing customized bundles. The model is flexible in terms of the number and variety of products customers may choose to incorporate during the various stages of their online shopping. Our computational study suggests that the proposed model not only attracts more customers to purchase the discounted bundle but also noticeably increases profits for e-tailers. This online dynamic bundle pricing model is robust under various bundle sizes and scenarios. It improves e-tailer profit and customer savings the most when facing divergent views about product values, lower budgets, and higher cost ratios.", "e:keyword": ["e-tailing", "Online retailing", "Bundling", "Customized bundle", "Multistage dynamic pricing", "Nonlinear mixed-integer programming", "Customer budget", "Reservation price"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0656", "e:abstract": "While millions of products are sold on its retail platform, Amazon.com itself stocks and sells only a very small fraction of them. Most of these products are sold by third-party sellers who pay Amazon a fee for each unit sold. Empirical evidence clearly suggests that Amazon tends to sell high-demand products and leave long-tail products for independent sellers to offer. We investigate how a platform owner such as Amazon, facing ex ante demand uncertainty, may strategically learn from these sellers' early sales which of the \"mid-tail\" products are worthwhile for its direct selling and which are best left for others to sell. The platform owner's \"cherry-picking\" of the successful products, however, gives an independent seller the incentive to mask any high demand by lowering his sales with a reduced service level (unobserved by the platform owner).</p> <p>We analyze this strategic interaction between a platform owner and an independent seller using a game-theoretic model with two types of sellers--one with high demand and one with low demand. We show that it may not always be optimal for the platform owner to identify the seller's demand. Interestingly, the platform owner may be <i>worse off</i> by retaining its option to sell the independent seller's product, whereas both types of sellers may <i>benefit</i> from the platform owner's threat of entry. The platform owner's entry option may reduce consumer surplus in the early period, although it increases consumer surplus in the later period. We also investigate how consumer reviews influence the market outcome.", "e:keyword": ["Platform", "Retailing", "Long-tail products", "Signaling", "Asymmetric information", "Pooling equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0664", "e:abstract": "Recent business research points to the fortune awaiting to be tapped in low-end markets. In this paper, we investigate how the size of the low-end market influences a firm's profits and the pioneering firm's quality choice. As low-valuation consumers increase in a market, on average, consumers' willingness to pay decreases. This may lead us to expect firms' profits to decrease as the size of the low-end market increases. Our analysis shows that, if the size of the low-end market is below a threshold, an increase in the size of the low-end market may actually dampen price competition and improve profits, as firms can then strategically choose their quality levels such that their products are more differentiated. Conventional wisdom also suggests that the pioneering firm will offer a higher-quality product and earn more profits compared with the later entrant. In contrast to this notion of quality advantage, our analysis identifies circumstances in which a pioneer can offer a lower-quality product and yet earn more profits. An experimental test lends support for some of our model's predictions. We further extend the model to consider markets with multiple firms, firms with multiple products, and consumers with limited purchasing power.", "e:keyword": ["Low-end markets", "Quality leadership", "Pioneering advantage", "Vertical differentiation", "Experimental economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0647", "e:abstract": "We develop a two-stage consumer-level model of paid search advertising response based on standard aggregated data provided to advertisers by major search engines such as Google or Bing. The proposed model uses behavioral primitives in accord with utility maximization and allows recovering parameters of the heterogeneity distribution in consumer preferences. The model is estimated on a novel paid search data set that includes information on the ad copy. To that end, we develop an original framework to analyze composition and design attributes of paid search ads. Our results allow us to correctly evaluate the effects of specific ad properties on ad performance, taking consumer heterogeneity into account. Another benefit of our approach is allowing recovery of preference correlation across the click-through and conversion stage. Based on the estimated correlation between price- and position-sensitivity, we propose a novel contextual targeting scheme in which a coupon is offered to a consumer depending on the position in which the paid search ad was displayed. Our analysis shows that total revenues from conversion can be increased using this targeting scheme while keeping cost constant.", "e:keyword": ["Internet", "Paid search advertising", "Aggregate data", "Choice modeling", "Bayesian methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0660", "e:abstract": "We develop and test an active-machine-learning method to select questions adaptively when consumers use heuristic decision rules. The method tailors priors to each consumer based on a \"configurator.\" Subsequent questions maximize information about the decision heuristics (minimize expected posterior entropy). To update posteriors after each question, we approximate the posterior with a variational distribution and use belief propagation (iterative loops of Bayes updating). The method runs sufficiently fast to select new queries in under a second and provides significantly and substantially more information per question than existing methods based on random, market-based, or orthogonal-design questions.</p> <p>Synthetic data experiments demonstrate that adaptive questions provide close-to-optimal information and outperform existing methods even when there are response errors or \"bad\" priors. The basic algorithm focuses on conjunctive or disjunctive rules, but we demonstrate generalizations to more complex heuristics and to the use of previous-respondent data to improve consumer-specific priors. We illustrate the algorithm empirically in a Web-based survey conducted by an American automotive manufacturer to study vehicle consideration (872 respondents, 53 feature levels). Adaptive questions outperform market-based questions when estimating heuristic decision rules. Heuristic decision rules predict validation decisions better than compensatory rules.", "e:keyword": ["Active learning", "Adaptive questions", "Belief propagation", "Conjunctive models", "Consideration sets", "Consumer heuristics", "Decision heuristics", "Disjunctions of conjunctions", "Lexicographic models", "Variational Bayes estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0655", "e:abstract": "Past research in marketing and psychology suggests that pricing structure may influence consumers' perception of value. In the context of two commonly used pricing schemes, pay-per-use and two-part tariff, we evaluate the impact of pricing structure on consumer preferences for access services. To this end, we develop a utility-based model of consumer retention and usage of a new service. A notable feature of the model is its ability to capture the pricing structure effect and measure its impact on consumer retention, usage, and pricing policy.</p> <p>Using data from a pricing field experiment for a new telecommunication service, we find that consumers derive lower utility from consumption under a two-part tariff than pay-per-use pricing, resulting in lower retention of customers and lower usage of the service. Specifically, our demand analysis shows that a two-part tariff structure leads to an average decline of 10.5% in the annual retention rate and an average decrease of 38.7% in yearly usage relative to pay-per-use pricing after controlling for income effects. Despite the higher customer churn and lower usage, we find that the two-part tariff is still the profit-maximizing pricing structure. However, our results show that if firms ignore the pricing structure (or access fee) effect, then they would overcharge customers for the access fee and undercharge them for the per-minute price. Translated in terms of profitability, the failure to account for the access fee effect leads to a reduction of 11% in firm profit.", "e:keyword": ["Nonlinear pricing", "Tariff structure", "Discrete/continuous choice models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0658", "e:abstract": "Our main objective in this paper is to measure the value of customers acquired from Google search advertising accounting for two factors that have been overlooked in the conventional method widely adopted in the industry: (1) the spillover effect of search advertising on customer acquisition and sales in off-line channels and (2) the lifetime value of acquired customers. By merging Web traffic and sales data from a small-sized U.S. firm, we create an individual customer-level panel that tracks all repeated purchases, both online and off-line, and tracks whether or not these purchases were referred from Google search advertising.</p> <p>To estimate the customer lifetime value, we apply the methodology in the customer relationship management literature by developing an integrated model of customer lifetime, transaction rate, and gross profit margin, allowing for individual heterogeneity and a full correlation of the three processes. Results show that customers acquired through Google search advertising in our data have a higher transaction rate than customers acquired from other channels. After accounting for future purchases and spillover to off-line channels, the calculated value of new customers using our approach is much higher than the value obtained using conventional method. The approach used in our study provides a practical framework for firms to evaluate the long-term profit impact of their search advertising investment in a multichannel setting.", "e:keyword": ["Customer lifetime value", "Multiple-channel shopping", "Customer acquisition", "Sponsored search advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0649", "e:abstract": "We analyze the impacts of social learning (SL) on the dynamic pricing and consumer adoption of durable goods in a two-period monopoly. Consumers can make either early, uninformed purchases or late but potentially informed purchases as a result of social learning. Several results are derived. First, we identify the market conditions under which ex ante homogeneous consumers may choose to purchase at different times. Second, equilibrium adoption may demonstrate inertia (where all adopt late) or frenzy (where all adopt early). In particular, adoption inertia appears when SL intensity is reasonably high but may vanish when SL intensity exceeds a certain threshold. Third, firm profits and social welfare first weakly decrease in SL intensity and may then jump up by a lump-sum amount at the threshold SL intensity level mentioned above. Last, we show that the firm potentially benefits from informative advertising or investing to cultivate more social learning.", "e:keyword": ["Durable goods", "Dynamic pricing", "Social learning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0654", "e:abstract": "Several researchers have proposed models of buyer behavior in noncontractual settings that assume that customers are \"alive\" for some period of time and then become permanently inactive. The best-known such model is the Pareto/NBD, which assumes that customer attrition (dropout or \"death\") can occur at any point in calendar time. A recent alternative model, the BG/NBD, assumes that customer attrition follows a Bernoulli \"coin-flipping\" process that occurs in \"transaction time\" (i.e., after every purchase occasion). Although the modification results in a model that is much easier to implement, it means that heavy buyers have more opportunities to \"die.\"</p> <p>In this paper, we develop a model with a discrete-time dropout process tied to calendar time. Specifically, we assume that every customer periodically \"flips a coin\" to determine whether she \"drops out\" or continues as a customer. For the component of purchasing while alive, we maintain the assumptions of the Pareto/NBD and BG/NBD models. This <i>periodic death opportunity</i> (PDO) model allows us to take a closer look at how assumptions about customer death influence model fit and various metrics typically used by managers to characterize a cohort of customers. When the time period after which each customer makes her dropout decision (which we call <i>period length</i>) is very small, we show analytically that the PDO model reduces to the Pareto/NBD. When the period length is longer than the calibration period, the dropout process is \"shut off,\" and the PDO model collapses to the negative binomial distribution (NBD) model. By systematically varying the period length between these limits, we can explore the full spectrum of models between the \"continuous-time-death\" Pareto/NBD and the nave \"no-death\" NBD.</p> <p>In covering this spectrum, the PDO model performs at least as well as either of these models; our empirical analysis demonstrates the superior performance of the PDO model on two data sets. We also show that the different models provide significantly different estimates of both purchasing-related and death-related metrics for both data sets, and these differences can be quite dramatic for the death-related metrics. As more researchers and managers make managerial judgments that directly relate to the death process, we assert that the model employed to generate these metrics should be chosen carefully.", "e:keyword": ["Customer-base analysis", "Pareto/NBD", "BG/NBD", "Customer attrition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0665", "e:abstract": "Customer retention and customer churn are key metrics of interest to marketers, but little attention has been placed on linking the different reasons for which customers churn to their value to a contractual service provider. In this paper, we put forth a hierarchical competing-risk model to jointly model when customers choose to terminate their service and why. Some of these reasons for churn can be influenced by the firm (e.g., service problems or price-value trade-offs), but others are uncontrollable (e.g., customer relocation and death). Using this framework, we demonstrate that the impact of a firm's efforts to reduce customer churn for controllable reasons is mitigated by the prevalence of uncontrollable ones, resulting in a \"damper effect\" on the return from a firm's retention marketing efforts. We use data from a provider of land-based telecommunication services to demonstrate how the competing-risk model can be used to derive a measure of the incremental customer value that a firm can expect to accrue through its efforts to delay churn, taking this damper effect into account. In addition to varying across customers based on geodemographic information, the magnitude of the damper effect depends on a customer's tenure to date. We discuss how our framework can be used to tailor the firm's retention strategy to individual customers, both in terms of which customers to target and when retention efforts should be deployed.", "e:keyword": ["Customer lifetime value", "Retention marketing", "Bayesian estimation", "Marketing ROI", "Customer equity management", "Customer base analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0652", "e:abstract": "We study a duopoly model where consumers are heterogeneous with respect to their willingness to pay for two product characteristics and marginal costs are increasing with the quality level chosen on each attribute. We show that although firms seek to manage competition through product positioning, their differentiation strategies critically depend on how costly it is to provide higher quality. When the cost of providing quality is not too high, firms use only one attribute to differentiate their products: they maximally differentiate on one dimension and minimally differentiate on the other (a Max-Min equilibrium). Furthermore, they always differentiate along the dimension with the greater attribute range. As for the dimension with the smaller range and along which they agglomerate, firms either choose the highest quality level or the lowest quality level possible, depending on whether the marginal costs of quality provision are low or intermediate, respectively. However, for larger quality provision costs, firms exploit both dimensions to differentiate their products. In particular, we characterize a maximal differentiation equilibrium in which one firm chooses the highest quality level on both attributes while its rival offers the lowest quality level on both attributes (a Max-Max equilibrium). We discuss the managerial implications of our findings and explain how they enrich and qualify previous results reported in the literature on two-dimensional differentiation models.", "e:keyword": ["Product positioning", "Multiattribute products", "Differentiation", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0661", "e:abstract": "Recent research has empirically characterized the buyer-seller relationship as dynamically evolving from one discrete state to another. Conventional wisdom would suggest that a customer in a higher relationship state that has a higher transaction value would also have greater lifetime value to the firm. However, recent evidence suggests that higher relationship states can be ephemeral. Hence, the link between transaction value and lifetime value is not obvious. In this study, we seek to understand, within a specific empirical context, (i) the relationship between a customer's transaction value and that customer's lifetime value and (ii) the relationship between the lifetime value of a customer and the optimal level of marketing activity that needs to be directed at that customer. To this end, we develop a trivariate Tobit hidden Markov model that allows for (a) transitions among relationship states, (b) possible synergies between the various products that the supplier firm offers, (c) endogeneity in marketing activity, (d) heterogeneity in model parameters, and (e) the presence of the no-purchase option. Our results reinforce recent findings by Schweidel et al. [Schweidel, D. A., E. T. Bradlow, P. S. Fader. 2011. Portfolio dynamics for customers of a multiservice provider. <i>Management Sci.</i> <b>57</b>(3) 471-486] that higher relationship states can be short-lived. Importantly for the supplier firm, a customer in the highest relationship state in a given period does not yield the highest lifetime value to the firm. Hence, the relationship between transaction value (i.e., relationship state) and lifetime value can be nonmonotonic. At the same time, we also find a nonmonotonic relationship between the optimal expenditures that should be directed at a customer and that customer's lifetime value; i.e., the optimal level of marketing contacts is not the highest for customers with the highest lifetime value. Furthermore, we find that the optimal marketing expenditures for myopic agents are 14%-33% lower than the corresponding values for forward-looking agents. Therefore, not accounting for the long-term effects of marketing contacts would lead to suboptimal marketing budgets. Moreover, a comparison with the current marketing expenditures suggests that the current practice is closer to the myopic policy than to the forward-looking one.", "e:keyword": ["Business-to-business marketing", "Hidden Markov model", "Optimal resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0677", "e:abstract": "This foreword and the subsequent four invited articles were commissioned by Eric T. Bradlow while Editor-in-Chief of <i>Marketing Science</i>. The foreword was written in four parts; each part covers a different aspect of the Workshop on Quantitative Marketing and Structural Econometrics. The workshop was cosponsored by Columbia Business School, Duke University, the University of California at Los Angeles, and the INFORMS Society for Marketing Science and was held at the Fuqua School of Business at Duke University in August 2010. The introductory section, written by Bradlow, covers why he commissioned these articles in the first place. In his section, Jean-Pierre Dub discusses \"going from good to great\" in the structural econometrics area as applied to marketing problems. A section jointly written by Brett R. Gordon and Raphael Thomadsen (both co-organizers of the workshop) discusses the workshop itself and some important thoughts for those people doing \"structural econometrics in the trenches.\" Finally, co-workshop organizer Richard Staelin's section provides some perspective on both the workshop and structural econometrics as they relate to analytical models and empirical work for quantitative marketing researchers.", "e:keyword": ["Academic workshop", "Empirical I/O", "Structural econometrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0681", "e:abstract": "What can be learned about marketing phenomena from descriptive, structural, and experimental empirical models? Is structure implicit in a descriptive empirical model? What is a \"reduced-form model?\" What is a natural experiment, and what can one infer from a study that uses experimental data? Having clear answers to these questions can improve empirical dialog. This paper defines descriptive, structural, and experimental empirical work, provides examples, discusses their similarities and differences, and comments on their strengths and weaknesses. An important theme is that the marketing question and the data available should determine the methods used, and not the other way around. Most of the examples discussed reference linear models that are widely employed in the marketing literature. Many of the points, however, extend to the development and interpretation of cutting-edge nonlinear, dynamic, or nonparametric models used in marketing.", "e:keyword": ["Structural models", "Experiments", "Predictive models", "Instruments", "Causal effects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0650", "e:abstract": "In this note I overview the data selection and procurement process in the context of structural models. Data selection for structural models presents unique challenges because data and structure often substitute and because it is imperative to consider what information identifies causal effects of interest.</p> <p>I further discuss three types of field data on which to build empirical models: (i) data that are proprietary to firms, (ii) data that can come from the public domain, or (iii) data that can be purchased from private research firms, and I discuss the benefits and limits of each. I then detail a process for obtaining proprietary data and the potential pitfalls inherent in the process.", "e:keyword": ["Structural models", "Data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0674", "e:abstract": "Marketing researchers have used models of consumer demand to forecast future sales, to describe and test theories of behavior, and to measure the response to marketing interventions. The basic framework typically starts from microfoundations of expected utility theory to obtain an econometric system that describes consumers' choices over available options, and to thus characterize product demand. The basic framework has been augmented significantly to account for quantity choices, to accommodate purchases of several products on a single purchase occasion (multiple discreteness and multicategory purchases), and to allow for asymmetric switching between brands across different price tiers. These extensions have enabled researchers to bring the analysis to bear on several related marketing phenomena of interest. This paper has three main objectives. The first objective is to articulate the main <i>goals of demand analysis</i>--forecasting, measurement, and testing--and to highlight several considerations associated with these goals. Our second objective is to describe the main <i>building blocks of individual-level demand models</i>. We discuss approaches built on direct and indirect utility specifications of demand systems, and we review extensions that have appeared in the marketing literature. The third objective is to explore a few <i>emerging directions in demand analysis</i>, including considering demand-side dynamics, combining purchase data with primary information, and using semiparametric and nonparametric approaches. We hope researchers new to this literature will take away a broader perspective on these models and see the potential for new directions in future research.", "e:keyword": ["Demand models", "Econometrics", "Utility functions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0675", "e:abstract": "This paper provides a critical review of the methods for estimating static discrete games and their relevance for quantitative marketing. We discuss the various modeling approaches, alternative assumptions, and relevant trade-offs involved in taking these empirical methods to data. We consider games of both complete and incomplete information, examine the primary methods for dealing with the coherency problems introduced by multiplicity of equilibria, and provide concrete examples from the literature. We illustrate the mechanics of estimation using a real-world example and provide the computer code and data set with which to replicate our results.", "e:keyword": ["Discrete choice", "Games estimation", "Structural models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0668", "e:abstract": "Digital rights management (DRM) is an important yet controversial issue in the information goods markets. Although DRM is supposed to help copyright owners by protecting digital content from illegal copying or distribution, it is controversial because DRM imposes restrictions on even legal users, and there are many industry practitioners who believe that the industry would be better off without DRM. In this paper, we model consumers' utilities and their incentives to purchase legal products versus pirate illegal ones. This allows us to endogenize the level of piracy and understand how it is influenced by the presence or absence of DRM. Our analysis suggests that, counterintuitively, download piracy might decrease when the firm allows legal DRM-free downloads. Furthermore, we find that a decrease in piracy does not guarantee an increase in firm profits and that that copyright owners do not always benefit from making it harder to copy music illegally. By analyzing the competition among the traditional retailer, the digital retailer, and pirated sources of information goods, we get a better understanding of the competitive forces in the market and provide insights into the role of digital rights management.", "e:keyword": ["Game theory", "Competitive analysis", "Piracy", "Entertainment marketing", "Digital music"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0667", "e:abstract": "Whereas literature in marketing shows that individuals often use noncompensatory decision rules, existing research on dyadic choice is based on compensatory models. In this paper we present a dyadic consider-then-choose model that investigates both compensatory and noncompensatory aspects of the joint decision process. The intersection of individual consideration sets at the dyad level gives rise to dyadic decision processes (DDPs) where dyad members are in concordance or discordance about alternatives to consider. We empirically investigate the implications of different DDPs on outcomes such as decision efficiency and dyadic welfare. The methodological approach merges choice experiments with Bayesian statistical models to uncover nuances of the dyadic choice process. Data were collected using a multiphase nationwide study of 265 husband-and-wife dyads. Results across three categories indicate that both concordant and discordant dyads exist. Among concordant dyads, the noncompensatory dyads make quicker decisions that result in higher dyadic welfare. Among discordant dyads, those that restrict their consideration set make quicker decisions that result in higher welfare than those that expand their consideration set. These findings have important implications for buyers looking to maximize dyadic welfare when making joint choices and for sellers making pricing and new product design decisions.", "e:keyword": ["Group decision making", "Group welfare", "Joint choice", "Choice heuristics", "Hierarchical Bayes", "Consideration sets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0679", "e:abstract": "How should forward-looking managers plan advertising if they envision a product-harm crisis in the future? To address this question, we propose a dynamic model of brand advertising in which, at each instant, a nonzero probability exists for the occurrence of a crisis event that damages the brand's baseline sales and may enhance or erode marketing effectiveness when the crisis occurs. Because managers do not know when the crisis will occur, its random time of occurrence induces a stochastic control problem, which we solve analytically in closed form. More importantly, the envisioning of a possible crisis alters managers' rate of time preference: anticipation enhances impatience. That is, forward-looking managers discount the present--even when the crisis has not occurred--more than they would in the absence of crisis. Building on this insight, we then derive the optimal feedback advertising strategies and assess the effects of crisis likelihood and damage rate. We discover the crossover interaction: the optimal precrisis advertising decreases, but the postcrisis advertising increases as the crisis likelihood (or damage rate) increases. In addition, we develop a new continuous-time estimation method to simultaneously estimate sales dynamics and feedback strategies using discrete-time data. Applying the method to market data from the Ford Explorer's rollover recall, we furnish evidence to support the proposed model. We detect compensatory effects in parametric shift: ad effectiveness increases, but carryover effect decreases (or vice versa). We also characterize the crisis occurrence distribution that shows that Ford Explorer should anticipate a crisis in 2.1 years and within 6.3 years at the 95% confidence level. Finally, we find a remarkable correspondence between the observed and optimal advertising decisions.", "e:keyword": ["Product-harm crisis", "Optimal advertising", "Stochastic optimal control", "Random stopping time problem", "Kalman filter", "Ford Explorer rollover"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0669", "e:abstract": "Commercial open source software (COSS) products--privately developed software based on publicly available source code--represent a rapidly growing, multibillion-dollar market. A unique aspect of competition in the COSS market is that many open source licenses require firms to make certain enhancements public, creating an incentive for firms to free ride on the contributions of others. This practice raises a number of puzzling issues. First, why should a firm further develop a product if competitors can freely appropriate these contributions? Second, how does a market based on free riding produce high-quality products? Third, from a public policy perspective, does the mandatory sharing of enhancements raise or lower consumer surplus and industry profits?</p> <p>We develop a two-sided model of competition between COSS firms to address these issues. Our model consists of (1) two firms competing in a vertically differentiated market, in which product quality is a mix of public and private components, and (2) a market for developers that firms hire after observing signals of their contributions to open source. We demonstrate that free-riding behavior is supported in equilibrium, that a mandatory sharing setting can result in high-quality products, and that free riding can actually increase profits and consumer surplus.", "e:keyword": ["Open source software", "Product strategy", "Signaling", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0670", "e:abstract": "We discuss how regression discontinuity designs arise naturally in settings where firms target marketing activity at consumers, and we illustrate how this aspect may be exploited for econometric inference of causal effects of marketing effort. Our main insight is to use commonly observed discontinuities and kinks in the heuristics by which firms target such marketing activity to consumers for nonparametric identification. Such kinks, along with continuity restrictions that are typically satisfied in marketing and industrial organization applications, are sufficient for identification of local treatment effects. We review the theory of regression discontinuity estimation in the context of targeting and explore its applicability to several marketing settings. We discuss identifiability of causal marketing effects using the design and show that consideration of an underlying model of strategic consumer behavior reveals how identification hinges on model features such as the specification and value of structural parameters as well as belief structures. We emphasize the role of selection for identification. We present two empirical applications: the first measures the effect of casino e-mail promotions targeted to customers based on ranges of their expected profitability, and the second measures the effect of direct mail targeted by a business-to-consumer company to zip codes based on cutoffs of expected response. In both cases, we illustrate that exploiting the regression discontinuity design reveals negative effects of the marketing campaigns that would not have been uncovered using other approaches. Our results are nonparametric, easy to compute, and control for the endogeneity induced by the targeting rule.", "e:keyword": ["Regression discontinuity", "Nonparametric identification", "Treatment effects", "Targeted marketing", "Selection", "Endogeneity", "Casinos", "Direct mail"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0671", "e:abstract": "Although prior literature has examined reactions to drastic negative news, we examine the situation in which decision makers receive contradictory information about products and they have to decide whether to persist with or abandon product usage. We investigate physician reactions to conflicting information concerning the cardiovascular risk of Avandia, a diabetes drug. We examine how beliefs about both drug <i>effectiveness</i> and drug <i>safety</i> are updated and speculate that experience, expertise, and self-efficacy impact how such information is integrated with current quality beliefs. Unlike previous Bayesian learning models, we consider that some signals, such as positive and negative news releases and the firm's marketing effort, may be biased in that they provide an opinionated point of view. The results show interesting differences in how physician types (specialists, hospital-based primary care physicians, heavy and light prescribers) update their beliefs and the information sources they use to do so. We find evidence that safety issues about Avandia resulted in spillover concern to close competitor Actos. The results have implication for determining who should be targeted and what vehicles should be used if a firm is faced with a situation where consumers are in a quandary because of receiving conflicting messages.", "e:keyword": ["Contradictory information", "Learning models", "Pharmaceutical markets", "Endogeneity", "Hierarchical Bayes models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0666", "e:abstract": "Market response models based on field-generated data need to address potential endogeneity in the regressors to obtain consistent parameter estimates. Another requirement is that market response models predict well in a holdout sample. With both requirements combined, it may seem reasonable to subject an endogeneity-corrected model to a holdout prediction task, and this is quite common in the academic marketing literature. One may be inclined to expect that the consistent parameter estimates obtained via instrumental variables (IV) estimation predict better than the biased ordinary least squares (OLS) estimates. This paper shows that this expectation is incorrect. That is, if the holdout sample is similar to the estimation sample so that the regressors are endogenous in both samples, holdout sample validation favors regression estimates that are <i>not corrected</i> for endogeneity (i.e., OLS) over estimates that <i>are corrected</i> for endogeneity (i.e., IV estimation). We also discuss ways in which holdout samples may be used sensibly in the presence of endogeneity. A key takeaway is that if consistent parameter estimates are the primary model objective, the model should be validated with an exogenous (rather than endogenous) holdout sample.", "e:keyword": ["Model validation", "Instrumental variables", "IV estimation", "Endogeneity", "Exogeneity", "Predictive model", "Descriptive model", "Holdout sample", "Prediction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0697", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0659", "e:abstract": "The goal of this paper is to study the behavior of consumers, dealers, and manufacturers in the car sector and present an approach that can be used by managers and policy makers to investigate the impact of significant demand shocks on profits, prices, and dealer networks. More specifically, we investigate consumer demand, substitution patterns, and price decisions across different cars and dealer locations to identify dealerships with low margins or high fixed costs and measure the value of closing down dealerships for manufacturers. We apply our model empirically to the San Diego area using a transactional data set with information about the locations of dealers and consumers, as well as manufacturer and retail prices. We find strong consumer disutility for travel and find that dealers have local demand areas that are shared with a small set of competitors. We show that a reduction of market demand by 30% over two years, similar to the economic crisis of 2008-2009, results in an annual drop in prices of approximately 11%. We discuss this price drop in the context of the 2009 federal policy measure known as the Car Allowance Rebate System program. We compare predictions and actual dealership closings in the General Motors and Chrysler dealer networks as an application of our approach.", "e:keyword": ["Automobile industry", "Spatial competition", "Models of demand and supply"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0657", "e:abstract": "Consumers are often unable to resist the temptation of overconsuming certain products such as cookies, crackers, soft drinks, alcohol, etc. To control their consumption, some consumers buy small packages or abstain from purchasing the product altogether. Other consumers, however, still purchase large packages and overconsume. From a strategic perspective, firms have the option of introducing small packages or only offering large packages. We use the literature on hyperbolic discounting to model consumers' self-control problems and examine conditions under which firms will offer small packages to help consumers combat their self-control problem, and how this offering in turn affects prices, profits, consumer, and social welfare. Our results show that introducing small packages can increase firms' profits only when a small fraction of consumers have overconsumption problems or when small packages can bring in new customers. Additionally, we find that competition can sometimes reduce the incentives for firms to introduce small packages. This is particularly true when a large fraction of consumers is attracted to small packages. We also find that firms' profits can sometimes decrease if they produce healthier alternatives of their goods. Our analysis of consumer welfare reveals that small packages enhance consumer and social welfare, even though they sometimes increase the consumption of vice goods.", "e:keyword": ["Game theory", "Hyperbolic discounting", "Behavioral economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0673", "e:abstract": "Most ads in practice receive no more than a single eye fixation. This study investigates the limits of what ads can communicate under such adverse exposure conditions. We find that consumers already know at maximum levels of accuracy and with high degree of certainty whether something is an ad or is editorial material after an exposure of less than 100 milliseconds and--if the ad is typical--which product is being advertised. Even after an extremely coarse visual presentation of 100 milliseconds, the product and brand in typical ads are identified well above chance levels, with atypical ads doing slightly better at the brand level. We propose a new metric that quantifies how effectively individual ads communicate their gist in adverse exposure conditions and that predicts the immediate interest that ads draw. Bayesian mediation analyses show that because of their better gist performance, typical ads rather than atypical ones raise immediate interest after very brief exposures. These findings challenge some of the received knowledge in advertising theory and practice, and they reveal the immediate communication benefits of typical ads.", "e:keyword": ["Advertising", "Gist", "Clutter", "Typicality", "Eye fixation", "Bayesian models", "Mixed-outcome models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0689", "e:abstract": "There exists a dichotomy in the communication strategies of fashion firms--some firms purposefully cloak information on the tastefulness of their products, whereas others openly flaunt their tasteful or \"it\" products. This divide in communication strategies cannot be explained by existing wealth signaling models of fashion. In this paper, we offer a model of fashion that explains the above dichotomy. We model fashion as a social device that plays the dual role of allowing people to both fit in with their peers and differentiate themselves by signaling their good taste or access to information. In this context, we show that a fashion firm faces an interesting dilemma--if it restricts information, then only sophisticated consumers buy its products and use them to signal their taste. Cloaking thus preserves the signaling value of its products but reduces the number of social interactions enabled by them. In contrast, flaunting undermines the signaling value of its products but increases the interactions enabled by them. Given these trade-offs, we derive the conditions under which cloaking occurs. We also show that, in equilibrium, the most tasteful product endogenously emerges as the fashion hit or \"it\" product.", "e:keyword": ["Advertising", "Analytic models", "Game theory", "Marketing strategy", "Fashion", "Social interactions", "Signaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0678", "e:abstract": "Households incur transaction costs when choosing among off-line stores for grocery purchases. They may incur additional transaction costs when buying groceries online versus off-line. We integrate the various transaction costs into a channel choice framework and empirically quantify the relative transaction costs when households choose between the online and off-line channels of the same grocery chain. The key challenges in quantifying these costs are (i) the complexity of channel choice decision and (ii) that several of the costs depend on the items a household expects to buy in the store, and unobserved factors that influence channel choice also likely influence the items purchased. We use the unique features of our empirical context to address the first issue and the plausibly exogenous approach in a hierarchical Bayesian framework to account for the endogeneity of the channel choice drivers. We find that transaction costs for grocery shopping can be sizable and play an important role in the choice between online and off-line channels. We provide monetary metrics for several types of transaction costs, such as travel time and transportation costs, in-store shopping time, item-picking costs, basket-carrying costs, quality inspection costs, and inconvenience costs. We find considerable household heterogeneity in these costs and characterize their distributions. We discuss the implications of our findings for the retailer's channel strategy.", "e:keyword": ["Channel choice", "Online grocery shopping", "Transaction costs", "Plausibly exogenous", "Hierarchical Bayesian", "Green shopping"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0680", "e:abstract": "In recent years there has been a growing stream of literature in marketing and economics that models consumers as Bayesian learners. Such learning behavior is often embedded within a discrete choice framework that is then calibrated on scanner panel data. At the same time, it is now accepted wisdom that disentangling preference heterogeneity and state dependence is critical in any attempt to understand either construct. We posit that this confounding between state dependence and heterogeneity often carries through to Bayesian learning models. That is, the failure to adequately account for preference heterogeneity may result in over- or underestimation of the learning process because this heterogeneity is also reflected in the initial conditions. Using a unique data set that contains stated preferences (survey) and actual purchase data (scanner panel) for the same group of consumers, we attempt to untangle the effects of preference heterogeneity and state dependence, where the latter arises from Bayesian learning. Our results are striking and suggest that measured brand beliefs can predict choices quite well and, moreover, that in the absence of such measured preference information, the Bayesian learning behavior for consumer packaged goods is vastly overstated. The inclusion of preference information significantly reduces evidence for aggregate-level learning and substantially changes the nature of individual-level learning. Using individual-level outcomes, we illustrate why the lack of preference information leads to faulty inferences.", "e:keyword": ["Bayesian learning", "Brand choice", "Preferences", "State dependence", "Markov chain Monte Carlo"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0672", "e:abstract": "We develop and test an incentive-compatible Conjoint Poker (CP) game. The preference data collected in the context of this game are comparable to incentive-compatible choice-based conjoint (CBC) analysis data. We develop a statistical efficiency measure and an algorithm to construct efficient CP designs. We compare incentive-compatible CP to incentive-compatible CBC in a series of three experiments (one online study and two eye-tracking studies). Our results suggest that CP induces respondents to consider more of the profile-related information presented to them compared with CBC.", "e:keyword": ["Conjoint analysis", "Product", "Measurement and inference", "Experimental economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0683", "e:abstract": "Service providers and their customers are sometimes victims of failures caused by exogenous factors such as unexpected bad weather, power outages, or labor strikes. When such no-fault failures occur in confined zones, service providers may confine customers against their will if making arrangements for them to leave is very costly. Such confinements, however, can result in severe pain and suffering, and customer complaints put regulators under pressure to pass a customer bill of rights that allows captive customers to abort failed services. This paper shows that service providers are better off preempting such laws by voluntarily allowing customers to escape the service under failure. Moreover, service providers can profit by targeting compensation to customers based on whether they use or leave the service under failure.", "e:keyword": ["Service failure", "Customer bill of rights", "Advanced selling", "Targeted compensation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0688", "e:abstract": "This paper evaluates the joint impact of exclusive channels and revenue sharing on suppliers and retailers in a hybrid duopoly common retailer and exclusive channel model. The model bridges the gap in the literature on hybrid multichannel supply chains with bilateral complementary products and services with or without revenue sharing. The analysis indicates that, without revenue sharing, the suppliers are reluctant to form exclusive deals with the retailers; thus, no equilibrium results. With revenue sharing from the retailers to the suppliers, it can be an equilibrium strategy for the suppliers and retailers to form exclusive deals. Bargaining solutions are provided to determine the revenue sharing rates. Our additional results suggest forming exclusive deals becomes less desirable for the suppliers if revenue sharing is also in place under nonexclusivity. In our extended discussion, we also study the impact of channel asymmetry, an alternative model with fencing, composite package competition, and enhanced price-dependent revenue sharing.", "e:keyword": ["Exclusive channels", "Channel competition", "Revenue sharing", "Complementary goods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0682", "e:abstract": "This study examines whether user-generated content (UGC) is related to stock market performance, which metric of UGC has the strongest relationship, and what the dynamics of the relationship are. We aggregate UGC from multiple websites over a four-year period across 6 markets and 15 firms. We derive multiple metrics of UGC and use multivariate time-series models to assess the relationship between UGC and stock market performance.</p> <p>Volume of chatter significantly leads abnormal returns by a few days (supported by Granger causality tests). Of all the metrics of UGC, volume of chatter has the strongest positive effect on abnormal returns and trading volume. The effect of negative and positive metrics of UGC on abnormal returns is asymmetric. Whereas negative UGC has a significant negative effect on abnormal returns with a short \"wear-in\" and long \"wear-out,\" positive UGC has no significant effect on these metrics. The volume of chatter and negative chatter have a significant positive effect on trading volume. Idiosyncratic risk increases significantly with negative information in UGC. Positive information does not have much influence on the risk of the firm. An increase in off-line advertising significantly increases the volume of chatter and decreases negative chatter. These results have important implications for managers and investors.", "e:keyword": ["User-generated content", "Stock returns", "Online word of mouth", "Vector autoregression", "Computational text processing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0687", "e:abstract": "We estimate the joint impact of the frequency reward and customer tier components of a loyalty program on customer behavior and resultant sales. We provide an integrated analysis of a loyalty program incorporating customers' purchase and cash-in decisions, points pressure and rewarded behavior effects, heterogeneity, and forward-looking behavior. We focus on four key research questions: (1) How important is it to combine both components in one model? (2) Does points pressure exist in the context of a two-component loyalty program? (3) How is the market segmented in its response to the combined program? (4) Do the programs complement each other in terms of the incremental sales they produce?</p> <p>Our most basic message is that the frequency reward and customer tier components of loyalty programs should be modeled jointly rather than in separate models. We find strong evidence for points pressure for both the customer tier and frequency reward components using both model-based and model-free evidence. We find a two-segment solution revealing a \"service-oriented\" segment that highly values cash-ins for room upgrades and staying in \"luxury\" hotels, and a \"price-oriented\" segment that is more price sensitive and highly values the frequency reward aspects of the loyalty program. Furthermore, we find that both components generate incremental sales. Also, there was slight synergy between the programs but not a huge amount. Overall, each component contributes to increased revenues and does not interfere with the other.", "e:keyword": ["Loyalty program", "Customer tier programs", "Frequency reward", "Database marketing", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0701", "e:abstract": "When social influence plays a key role in the diffusion of new product, the value of a customer often goes beyond her own product purchase. We posit that a customer's value (CV) comes not only from her purchase value (PV) but also from her influence value (IV) (i.e., CV = PV + IV). Therefore, a customer's value can be far greater than her purchase value if she exerts a considerable influence on others. Building on a two-segment influential-imitator asymmetric influence model, we develop a model framework to derive closed-form expressions for PV, IV, and CV by customer segment as well as time of adoption, and we examine their comparative statics with respect to the diffusion parameters. A key parameter of our model framework is the social apportioning parameter, delta, which determines the credit a customer receives by influencing other potential adopters. We develop an endogenous method for determining delta as a function of the new product diffusion parameters. Our model framework allows us to investigate how a firm might accelerate product purchases by providing introductory discount offers to a targeted group of potential adopters at product launch. We find that purchase acceleration frequently leads to a significant increase in total customer value.", "e:keyword": ["New product diffusion", "Purchase value", "Influence value", "Purchase acceleration"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0695", "e:abstract": "We investigate a business-to-business context and ask <i>when</i> and <i>why</i> a firm should announce a \"reference program\" that commits the firm to facilitating the flow of information about the efficacy of its products from early adopters to potential late adopters. We model a monopolist manufacturer with a new innovation that can be sold to two potential customers. We demonstrate here two benefits of a reference program that relate not to an increase in later adopters' willingness to pay but to an increase in the willingness to pay of the early adopters themselves. The impact on the early adopters' willingness to pay arises in two ways as a result of their observation of the firm's commitment to information transmission. First, in a model of symmetric uncertainty, we show that the announcement of a reference program facilitates dynamic pricing by the manufacturer in the sense that it allows the firm to provide temporary exclusive use of the technology to one of the customers. This creates more value, which the manufacturer can extract via a higher price. In this way, a reference program can serve as a partial substitute for an exclusive-use contract. In a model with asymmetric information, we demonstrate that under certain conditions, the firm is able to use the reference program as a signal--again, to the early adopting customer--that its technology is of high quality. However, such a signal requires significant discounts to early adopters to ensure separation. As a result, a pooling equilibrium dominates in which the manufacturer fosters references regardless of its quality. Finally, by allowing the firms' private information to be stochastic, we show that separation may be a dominant outcome.", "e:keyword": ["Word of mouth", "Business-to-business marketing", "Game theory", "Signaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0691", "e:abstract": "When a firm can recognize its previous customers, it may use information about their past purchases to price discriminate. We study a model with a monopolist and a continuum of heterogeneous consumers, where consumers have the ability to maintain their anonymity and avoid being identified as past customers, possibly at a cost. When consumers can freely maintain their anonymity, they all individually choose to do so, which results in the highest profit for the monopolist. Increasing the cost of anonymity can benefit consumers but only up to a point, after which the effect is reversed. We show that if the monopolist or an independent third party controls the cost of anonymity, it often works to the detriment of consumers.", "e:keyword": ["Anonymity", "Customer recognition", "Price discrimination", "Identity management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0693", "e:abstract": "This study develops and estimates a dynamic model of consumer choice behavior in markets for seasonal goods, where products are sold over a finite season and availability is limited. In these markets, retailers often use dynamic markdown policies in which an initial retail price is announced at the beginning of the season and the price is subsequently marked down as the season progresses. Strategic consumers face a trade-off between purchasing early in the season, when prices are higher but goods are available, and purchasing later, when prices are lower but the stockout risk is higher. If the good starts providing utility as soon as it is purchased (e.g., apparel), consumers purchasing earlier in the season can also get more use from the product compared to those purchasing later.</p> <p>Our structural model incorporates three features essential for modeling the demand for seasonal goods: changing prices, limited availability, and possible dependence of total consumption utility on the time of purchase. In this model, heterogeneous consumers have expectations about future prices and product availability, and they strategically time their purchases. We estimate the model using aggregate sales and inventory data from a fashion goods retailer.</p> <p>The results indicate that, in the fashion goods context, ignoring consumers' expectations about future availability or the change in total consumption utility over the season can lead to biased demand estimates. We find that strategic consumers delay their purchases to take advantage of markdowns and that these strategic delays hurt the retailer's revenues. Retailer revenues facing strategic consumers are 9% lower than they would have been facing myopic consumers. Limited availability, on the other hand, reduces the extent of strategic delays by motivating consumers to purchase earlier. We find that the impact of strategic delays on retailer revenues would have been as high as 35% if there were no stockout risk. By means of counterfactual experiments, we show that the highest retailer profits are achieved by offering small markdowns early in the season. On the other hand, given current markdown percentages, the retailer can improve profits by carrying less stock as consumers accelerate purchases and purchase at higher prices when they anticipate scarcity in future periods. As long as the reduction in availability is not great, the profit gain from earlier higher-priced sales can overcome the loss resulting from the reduction in overall sales.", "e:keyword": ["Pricing research", "Choice models", "Forward-looking behavior", "Limited availability", "Short life cycle", "Fashion", "Retailing", "Revenue management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0704", "e:abstract": "We use laboratory experiments to examine the relative performance of the English auction (EA) and the first-price sealed-bid auction (FPA) when procuring a commodity. The mean and variance of prices are lower in the FPA than in the EA. Bids and prices in the EA agree with game-theoretic predictions, but they do not agree in the FPA. To resolve these deviations found in the FPA, we introduce a mixture model with three bidding rules: constant absolute markup, constant percentage markup, and strategic best response. A dynamic specification in which bidders can switch strategies as they gain experience is estimated as a hidden Markov model. Initially, about three quarters of the subjects are strategic bidders, but over time, the number of strategic bidders falls to below 65%. There is a corresponding growth in those who use the constant absolute markup rule.", "e:keyword": ["Procurement auction", "Experiment", "Hidden Markov model", "Decision rules of thumb"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0702", "e:abstract": "Quantity discount pricing is a common practice used by business-to-business and business-to-consumer companies. A key characteristic of quantity discount pricing is that the marginal price declines with higher purchase quantities. In this paper, we propose a choice-based conjoint model for estimating consumer-level willingness to pay (WTP) for varying quantities of a product and for designing optimal quantity discount pricing schemes. Our model can handle large quantity values and produces WTP estimates that are positive and increasing in quantity at a diminishing rate. In particular, we propose a tractable WTP function that depends on both product attributes and product quantity and that captures diminishing marginal WTP. We show how such a function embeds standard WTP functions in the quantity discount literature as special cases. We also demonstrate how to use the model to estimate the consumer value potential, which is the product of the premium a consumer is willing to pay and her volume potential. Finally, we propose a parsimonious experimental design approach for implementation.</p> <p>We illustrate the model using data from a conjoint study of online movie rental services. The empirical results show that the proposed model has good fit and predictive validity. In addition, we find that marginal WTP in this category decays rapidly with quantity. We also find that the standard choice-based conjoint model results in anomalous WTP distributions with negative WTP values and nondiminishing marginal willingness-to-pay curves. Finally, we identify four segments of consumers that differ in terms of magnitude of WTP and volume potential, and we derive optimal quantity discount schemes for a monopolist and a new entrant in a competitive market.", "e:keyword": ["Quantity discounts", "Willingness to pay", "Choice models", "Mixed logit", "Conjoint analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0696", "e:abstract": "We propose a method to include seasonality in any diffusion model that has a closed-form solution. The resulting diffusion model captures seasonality in a way that naturally matches the original diffusion model's pattern. The method assumes that additional sales at seasonal peaks are drawn from previous or future periods. This implies that the seasonal pattern does not influence the underlying diffusion pattern. The model is compared with alternative approaches through simulations and empirical examples. As alternatives, we consider the standard Generalized Bass Model (GBM) and the basic Bass Model, which ignores seasonality. One of the main findings is that modeling seasonality in a GBM generates good predictions but gives biased estimates. In particular, the market potential parameter is underestimated. Ignoring seasonality in cases where data of the entire diffusion period are available gives unbiased parameter estimates in most relevant scenarios. However, ignoring seasonality leads to biased parameter estimates and predictions when only part of the diffusion period is available. We demonstrate that our model gives correct estimates and predictions even if the full diffusion process is not yet available.", "e:keyword": ["Diffusion models", "Seasonality", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0715", "e:abstract": "The growth of the \"social\" Web has resulted in the enormous growth of what is referred to as user-generated content, or UGC. UGC takes the form of product reviews, descriptions of product usage, \"homemade advertising,\" blogs, and other consumer-initiated contributions. Following a research competition cosponsored by the Marketing Science Institute and the Wharton Interactive Media Initiative (now known as the Wharton Customer Analytics Initiative), a call for papers for a special issue of <i>Marketing Science</i> resulted in 69 submissions. Of these, eight papers were accepted, covering a range of issues such as how and why people make UGC contributions, the impact of UGC contributions, and new methods for analyzing UGC data.", "e:keyword": ["User-generated content"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0662", "e:abstract": "Whereas recent research has demonstrated the impact of online product ratings and reviews on product sales, we still have a limited understanding of the individual's decision to contribute these opinions. In this research, we empirically model the individual's decision to provide a product rating and investigate factors that influence this decision. Specifically, we consider how previously posted ratings may affect an individual's posting behavior in terms of <i>whether</i> to contribute (incidence) and <i>what</i> to contribute (evaluation), and we identify <i>selection effects</i> that influence the incidence decision and <i>adjustment effects</i> that influence the evaluation decision.</p> <p>Across individuals, our results show that positive ratings environments increase posting incidence, whereas negative ratings environments discourage posting. Our results also indicate important differences across individuals in how they respond to previously posted ratings, with less frequent posters exhibiting bandwagon behavior and more active customers revealing differentiation behavior. These dynamics affect the evolution of online product opinions. Through simulations, we illustrate how the evolution of posted product opinions is shaped by the underlying customer base and show that customer bases with the same median opinion may evolve in substantially different ways because of the presence of a core group of \"activists\" posting increasingly negative opinions.", "e:keyword": ["Online social media", "Word of mouth", "Social dynamics", "Bayesian estimation", "Internet marketing", "User-generated content"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0684", "e:abstract": "User-generated content is increasingly created through the collaborative efforts of multiple individuals. In this paper, we argue that the value of collaborative user-generated content is a function both of the direct efforts of its contributors and of its embeddedness in the content-contributor network that creates it. An analysis of Wikipedia's WikiProject Medicine reveals a curvilinear relationship between the number of distinct contributors to user-generated content and viewership. A two-mode social network analysis demonstrates that the embeddedness of the content in the content-contributor network is positively related to viewership. Specifically, locally central content--characterized by greater intensity of work by contributors to multiple content sources--is associated with increased viewership. Globally central content--characterized by shorter paths to the other collaborative content in the overall network--also generates greater viewership. However, within these overall effects, there is considerable heterogeneity in how network characteristics relate to viewership. In addition, network effects are stronger for newer collaborative user-generated content. These findings have implications for fostering collaborative user-generated content.", "e:keyword": ["User-generated content", "Information value", "Wiki", "Social network analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0685", "e:abstract": "We measure the value of promotional activities and referrals by content creators to an online platform of user-generated content. To do so, we develop a modeling approach that explains individual-level choices of visiting the platform, creating, and purchasing content as a function of consumer characteristics and marketing activities, allowing for the possibility of interdependence of decisions within and across users. Empirically, we apply our model to Hewlett-Packard's (HP) print-on-demand service of user-created magazines, named MagCloud. We use two distinct data sets to show the applicability of our approach: an aggregate-level data set from Google Analytics, which is a widely available source of data to managers, and an individual-level data set from HP. Our results compare content creator activities, which include referrals and word-of-mouth efforts, with firm-based actions, such as price promotions and public relations. We show that price promotions have strong effects but are limited to the purchase decisions, whereas content creator referrals and public relations efforts have broader effects that impact all consumer decisions at the platform. We provide recommendations as to the level of a firm's investments when \"free\" promotional activities by content creators exist. These free marketing campaigns are likely to have a substantial presence in most online services of user-generated content.", "e:keyword": ["Demand modeling", "User-generated content", "Online marketing", "Two-sided markets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0639", "e:abstract": "The success of any user-generated content website depends crucially on its asset of content contributors. How firms should invest in the acquisition and retention of content contributors represents a novel question that is particularly important for these websites. We develop a vector autoregressive (VAR) model to measure the financial values of the retention and acquisition of both contributors and content consumers. In our empirical application to a customer-to-customer marketplace, we find that contributor (seller) acquisition has the largest financial value because of their strong network effects on content consumers (buyers) and other contributors. However, the wear-in of contributors' financial values takes longer because the network effects need time to be fully realized. Our simulation-based studies (i) shed light on the value implications of \"enhancing network effects\" and (ii) quantify the revenue contributions of marketing newsletter campaigns. Our results indicate that enhancing network effects in complementary ways can further increase the marginal benefits of acquisition and retention. We also find that simply tracking click-throughs may vastly underestimate the values of marketing newsletters--in our case, by more than a factor of 5--which may lead to suboptimal marketing effort allocation.", "e:keyword": ["UGC", "Content contributors", "VAR", "Lifetime value", "Acquisition", "Retention", "C2C marketplace", "Network effects"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0653", "e:abstract": "We investigate the evolution of online ratings over time and sequence. We first establish that there exist two distinct dynamic processes, one as a function of the amount of time a book has been available for review and another as a function of the sequence of reviews themselves. We find that, once we control for calendar date, the residual average temporal pattern is increasing. This is counter to existing findings that suggest that without this calendar-date control, the pattern is decreasing. With respect to sequential dynamics, we find that ratings decrease: the <i>n</i>th rating is, on average, lower than the <i>n</i>-1th when controlling for time, reviewer effects, and book effects. We test and find some support for existing theories for this decline based on motivation. We then offer two additional explanations for this \"order effect.\" We find support for the idea that one's ability to assess the diagnosticity of previous reviews decreases: when previous reviewers are very different, more reviews may thus lead to more purchase errors and lower ratings.", "e:keyword": ["Word of mouth", "Online reviews", "Networks", "Internet marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0690", "e:abstract": "Volunteer users employ collaborative Internet technologies to develop open source products, a form of user-generated content, where time to product release is a crucial measure of project success. The open source community features two separate but related subcommunities: developer users who contribute time and effort to develop products and end users who act as collaborative testers and provide feedback. We develop hypotheses concerning how the location of the project's founders in the social network of developer users, the interplay of developer users and end users, and project and product characteristics affect time to product release. We use data on 817 development projects from SourceForge, a large open source community forum, to calibrate a split hazard model to test the hypotheses. That model supports the two-community conceptualization and most of the related hypotheses. The results have theoretical and managerial implications; for example, a pivotal position of founders in the developer user community can reduce time to product release by up to 31 and projects in which users are more engaged can experience an 11 time to product release compared with those projects in which they are not.", "e:keyword": ["User-generated products", "Product development", "Social networks", "Open source", "Innovation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0700", "e:abstract": "User-generated content on social media platforms and product search engines is changing the way consumers shop for goods online. However, current product search engines fail to effectively leverage information created across diverse social media platforms. Moreover, current ranking algorithms in these product search engines tend to induce consumers to focus on one single product characteristic dimension (e.g., price, star rating). This approach largely ignores consumers' multidimensional preferences for products. In this paper, we propose to generate a ranking system that recommends products that provide, on average, the best value for the consumer's money. The key idea is that products that provide a higher surplus should be ranked higher on the screen in response to consumer queries. We use a unique data set of U.S. hotel reservations made over a three-month period through Travelocity, which we supplement with data from various social media sources using techniques from text mining, image classification, social geotagging, human annotations, and geomapping. We propose a random coefficient hybrid structural model, taking into consideration the two sources of consumer heterogeneity the different travel occasions and different hotel characteristics introduce. Based on the estimates from the model, we infer the economic impact of various location and service characteristics of hotels. We then propose a new hotel ranking system based on the average utility gain a consumer receives from staying in a particular hotel. By doing so, we can provide customers with the \"best-value\" hotels early on. Our user studies, using ranking comparisons from several thousand users, validate the superiority of our ranking system relative to existing systems on several travel search engines. On a broader note, this paper illustrates how social media can be mined and incorporated into a demand estimation model in order to generate a new ranking system in product search engines. We thus highlight the tight linkages between user behavior on social media and search engines. Our interdisciplinary approach provides several insights for using machine learning techniques in economics and marketing research.", "e:keyword": ["User-generated content", "Social media", "Search engines", "Hotels", "Ranking system", "Structural models", "Text mining", "Crowdsourcing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0713", "e:abstract": "Web 2.0 provides gathering places for Internet users in blogs, forums, and chat rooms. These gathering places leave footprints in the form of colossal amounts of data regarding consumers' thoughts, beliefs, experiences, and even interactions. In this paper, we propose an approach for firms to explore online user-generated content and \"listen\" to what customers write about their and their competitors' products. Our objective is to convert the user-generated content to market structures and competitive landscape insights. The difficulty in obtaining such market-structure insights from online user-generated content is that consumers' postings are often not easy to syndicate. To address these issues, we employ a text-mining approach and combine it with semantic network analysis tools. We demonstrate this approach using two cases--sedan cars and diabetes drugs--generating market-structure perceptual maps and meaningful insights without interviewing a single consumer. We compare a market structure based on user-generated content data with a market structure derived from more traditional sales and survey-based data to establish validity and highlight meaningful differences.", "e:keyword": ["Text mining", "User-generated content", "Market structure", "Marketing research"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0719", "e:abstract": "This paper develops and calibrates a simple yet comprehensive set of models for the evolution of binary attribute importance weights, based on a cue-goal association framework. We argue that the utility a consumer ascribes to an attribute comes from its association with the achievement of a goal. We investigate how associations may be represented and then track back the relationship of these associations to the utility function. We explain why we believe this to be an important problem before providing an overview of the extensive literature on learning models. This literature identifies key phenomena and provides a foundation for our modeling of binary attribute importance learning, which can test for three departures from \"rational\" learning--bias, existence of priors, and the unequal weighting of sample observations (order effects). We apply our models in a laboratory setting under a number of different relationship strengths, and we find that, in our application, consumers' learning about attribute-goal associations exhibits bias and the effects of prior beliefs when the sample realizations occur with and without noise, and order effects when the sample realizations occur with noise. We provide an example of how our models can be extended to learning about more than one attribute.", "e:keyword": ["Consumer utility", "Preference dynamics", "Associative learning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0718", "e:abstract": "We propose a new statistical instrument-free method to tackle the endogeneity problem. The proposed method models the joint distribution of the endogenous regressor and the error term in the structural equation of interest (the structural error) using a copula method, and it makes inferences on the model parameters by maximizing the likelihood derived from the joint distribution. Similar to the \"exclusion restriction\" in instrumental variable methods, extant instrument-free methods require the assumption that the unobserved instruments are exogenous, a requirement that is difficult to meet. The proposed method does not require such an assumption. Other benefits of the proposed method are that it allows the modeling of discrete endogenous regressors and offers a new solution to the slope endogeneity problem. In addition to linear models, the method is applicable to the popular random coefficient logit model with either aggregate-level or individual-level data. We demonstrate the performance of the proposed method via a series of simulation studies and an empirical example.", "e:keyword": ["Endogeneity", "Copula method", "Instrumental variables", "Two-stage least squares", "Linear regression model", "Logit model", "Random coefficient"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0705", "e:abstract": "This paper analyses firms' decisions to provide connectivity to their customers. We distinguish between intraconnectivity--the ability of one firm's customers to connect to each other--and interconnectivity--the ability of one firm's customers to connect with another firm's customers. The profitability implications of allowing connectivity are not a straightforward consequence of the consumer value of connectivity, because connectivity affects not only the customer value but also the intensity of competition by creating or changing network externality. We find that if sales are driven by brand switching rather than by category expansion, a firm may find it optimal not to provide intraconnectivity even if providing it is not costly and may find it optimal to provide interconnectivity even at a cost exceeding the consumer value of connectivity. On the other hand, if category expansion is possible, providing intraconnectivity may be profitable. In this case, either the equilibrium intraconnectivity provision may be asymmetric or both firms may find it (individually) optimal to provide intraconnectivity. Under certain conditions in the latter case, the firms' choice of intraconnectivity is a prisoner's dilemma game.", "e:keyword": ["Network externality", "Product design", "Pricing", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0706", "e:abstract": "Social sharing of information goods--wherein a single good is purchased and shared through a network of acquaintances such as friends or coworkers--is a significant concern for the providers of these goods. The effect of social sharing on firm pricing and profits depends critically on two elements: the structure of the underlying consumer network and the mechanism used by groups to decide whether to purchase at a given price. We examine the effect of social sharing under different network structures (decentralized, centralized, and complete), which reflect a range of market conditions. Moreover, we draw from the mechanism design literature to examine several approaches to group decision making. Our results suggest that a firm can benefit from increased social sharing if the level of sharing is already high, enabling a pricing strategy targeted primarily at sharing groups rather than individuals. However, the point at which sharing becomes marginally beneficial for a firm depends on both the distribution of group sizes (which derives from the network structure) and the group decision mechanism. Additional insights are obtained when we extend the model to capture homophily in group formation and the potential that a subset of consumers will never share for ethical reasons.", "e:keyword": ["Information goods", "Social networks", "Pricing", "Graph theory", "Cost sharing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0707", "e:abstract": "Each year in the postsecondary education industry, schools offer admission to nearly 3 million new students and scholarships totaling nearly $100 billion. This is a large, understudied targeted marketing and price discrimination problem. This problem falls into a broader class of configuration utility problems (CUPs), which typically require an approach tailored to exploit the particular setting. This paper provides such an approach for the admission and scholarship decisions problem. The approach accounts for the key distinguishing feature of this industry--schools value the average features of the matriculating students such as percent female, percent from different regions of the world, average test scores, and average grade point average. Thus, as in any CUP, the value of one object (i.e., student) cannot be separated from the composition of all of the objects (other students in the enrolling class). This goal of achieving a class with a desirable set of average characteristics greatly complicates the optimization problem and does not allow the application of standard approaches. We develop a new approach that solves this more complex optimization problem using an empirical system to estimate each student's choice and the focal school's utility function. We test the approach in a field study of an MBA scholarship process and implement adjusted scholarship decisions. Using a holdout sample, we provide evidence that the methodology can lead to improvements over current management decisions. Finally, by comparing our solution to what management would do on its own, we provide insight into how to improve management decisions in this setting.", "e:keyword": ["Choice sets", "College choice", "Utility on averages", "Statistical approximation", "Nonconvex optimization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0712", "e:abstract": "The question as to the optimality of advertising pulsing has attracted many researchers over the last half-century. In this paper we specify a market share model in which there are two advertising-setting firms as well as a no-purchase option. The framework is that of a first-order Markov process with three states. The objective of both firms is to maximize profits. We are able to demonstrate, for a diminishing returns advertising function, that the optimal advertising strategy is pulsing. The frequency of the advertising pulse is shown to depend on the magnitude of the market share retention rate (state dependence); the higher it is, the less frequent the advertising. We further find that the optimal advertising budgets do not remain the same when the frequency of pulsing changes. Finally, we show that it is optimal for both firms to advertise in phase.", "e:keyword": ["Advertising", "Pulsing", "Dynamic models", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0716", "e:abstract": "We study a model of film distribution and consumption. The studio can release two goods, a theatrical version and a video version, and has to decide on its versioning and sequencing strategy. In contrast with the previous literature, we allow for the possibility that some consumers may watch both versions. This simple extension leads to novel results. It now becomes optimal to introduce versioning if the goods are not too substitute for one another, even when production costs are zero (pure information goods). We also demonstrate that the simultaneous release of the versions (\"day-and-date\" strategy) can be optimal when the studio is integrated with the exhibition and distribution channels. In contrast, a sequential release (\"video window\" strategy) is typically the outcome when the studio negotiates with independent distributors and exhibitors.", "e:keyword": ["Product segmentation", "Versioning", "Sequencing", "Distribution channels", "Movie industry", "Information goods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0717", "e:abstract": "Consumer new product adoption and preference evolution or learning may be influenced by intrinsic or internal factors (e.g., usage experiences, personal characteristics), external influences (e.g., social effects, media), and marketing activities of the firm. Moreover, the preference evolution in a certain category can spill over to other categories; i.e., consumers can exhibit cross-category learning. In this paper, we develop a multicategory framework to analyze the role of the above elements in the formation and evolution of consumer preferences across categories. We analyze these elements by employing multiple data sets, i.e., by combining revealed preference data (from scanner panel), stated data (from surveys measuring consumer lifestyle variables and demographics), and external influences (e.g., media mentions) in a completely heterogeneous framework while considering other facets of the learning process. By jointly estimating the model for organic purchases in six distinct food categories, we also explore the role of category differences. Results show that consumer new product adoption and learning is indeed impacted significantly and to various degrees by the aforementioned factors. We show how, by selectively encouraging purchases under various scenarios, firms can accelerate the learning process, not only for the focal category but also for other categories, thereby realizing considerable incremental profits. These results can be used by both manufacturers and retailers for more efficient allocation of marketing budgets across (new) products.", "e:keyword": ["Consumer new product adoption", "Multicategory models", "Sources of information", "Consumer learning", "Organic products", "Food products marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0711", "e:abstract": "We show how networks modify the diffusion curve by affecting its symmetry. We demonstrate that a network's degree distribution has a significant impact on the contagion properties of the subsequent adoption process, and we propose a method for uncovering the degree distribution of the adopter network underlying the dissemination process, based exclusively on limited early-stage penetration data. In this paper we propose and empirically validate a unified network-based growth model that links network structure and penetration patterns. Specifically, using external sources of information, we confirm that each network degree distribution identified by the model matches the actual social network that is underlying the dissemination process. We also show empirically that the same method can be used to forecast adoption using an estimation of the degree distribution and the diffusion parameters at an early stage (15%) of the penetration process. We confirm that these forecasts are significantly superior to those of three benchmark models of diffusion.</p> <p>Our empirical analysis indicates that under heavily right-skewed degree distribution conditions (such as scale-free networks), the majority of adopters (in some cases, up to 75%) join the process after the sales peak. This strong asymmetry is a result of the unique interaction between the dissemination process and the degree distribution of its underlying network.", "e:keyword": ["Social networks", "Diffusion of innovation", "Diffusion models", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1110.0692", "e:abstract": "This paper investigates how firms responded to standardized nutrition labels on food products required by the Nutrition Labeling and Education Act (NLEA). Using a longitudinal quasi-experimental design, we test our predictions using two large-scale samples that span 30 product categories. Results indicate that the NLEA reduced brand nutritional quality relative to a control group of products not regulated by the NLEA. At the same time, among regulated products, brand taste increased. Although this reduction in nutrition represents an unintended consequence of regulation, there were a set of category, firm, and brand conditions under which the NLEA produced a positive effect on brand nutritional quality. We find that firms were more likely to improve brand nutrition when firm risk or firm power is low. Lower risk occurs when the firm is introducing a new brand rather than changing an existing brand, and weaker power in a category is reflected by lower market share in a category. Furthermore, firms competing in low-health categories (e.g., potato chips) or small-portion categories (e.g., peanut butter) improved nutrition more than firms competing in high-health categories (e.g., bread) or large-portion categories (e.g., frozen dinners). Recommendations for firm strategy and the design of consumer information policy are examined in light of these surprising firm responses.", "e:keyword": ["Nutrition labels", "Nutrition", "Taste", "Firm strategy", "Public policy", "Quality information"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0723", "e:abstract": "This paper asks whether brand extension can serve as a signal of product quality given that it costs less than a new brand. (Existing literature has assumed either that brand extension is cost-neutral or that it costs more.) I show that it can as a perfect Bayesian equilibrium, but the argument is unconvincing. For one thing, the separating equilibrium is not unique; a pooling equilibrium also exists in which brand extension signals nothing. For another, the separating equilibrium relies on off-equilibrium beliefs that are poorly motivated in the model. I propose a refinement of the perfect Bayesian equilibrium that resolves both issues. Empirical off-equilibrium beliefs require that consumers' off-equilibrium beliefs be justifiable on the basis of their prior beliefs and product performance observations. With empirical off-equilibrium beliefs, two necessary conditions for brand extension to signal product quality are identified: (i) consumers must perceive old and new products of the firm to be positively correlated in quality, and (ii) at least some consumers must identify with brands and not the firm behind the brands. Even with these conditions in place, the signaling argument is fragile: firm observability of past performance diminishes brand extension's signaling capability; an arbitrarily small probability of failure for good products eliminates it. My results suggest that, going forward, the case for brand extension must rest on foundations other than signaling product quality.", "e:keyword": ["Brand extension", "Signaling", "Product quality", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0730", "e:abstract": "This research examines how individual differences and institutional practices influence consumer bidding in auctions. Bidders may be motivated by different goals, e.g., <i>thrill</i> (of winning the item, with minimal attention to what they pay for it) versus <i>prudence</i> (winning the item at a price at or below its perceived value). Also, innate or auctioneer-induced differences may exist in the <i>precision</i> and <i>salience</i> of bidder cognitions about the item's value. We report two studies on how these motivational and cognitive factors influence bids in descending and ascending auctions, respectively. Each study also manipulated a situational variable (<i>wait time</i> at each price step). The two auctions realized different average prices for the same item set. Average bids were higher in the descending (versus ascending) auction in several study conditions. In both auction formats, bidders primed with thrill (versus prudence) bid higher, but more precise and/or salient values attenuated this goal effect. Among other results, in the descending auction, longer wait times elicited higher bids from bidders primed with thrill (but not prudence). In the ascending auction, longer wait times produced lower bids for bidders primed with prudence (but not thrill). These findings on consumer bidding behavior have practical implications for auction design.", "e:keyword": ["Auctions", "Bidding", "Goals", "Pricing", "Value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0731", "e:abstract": "Firms in a variety of industries offer add-on products to consumers who have previously purchased a base product. We posit that consumers, in making their decisions as to whether to purchase add-ons that complement the base products, find a greater need for the value offered by the add-ons when the \"unrecovered\" value (i.e., price paid minus the benefits obtained so far) associated with the base products is higher. We conduct experiments that test the proposed hypothesis and examine the strategic implications of such consumer decision making to a firm that sells base product add-on pairs.</p> <p>Consistent with our hypothesis, the experiments show that a consumer's unrecovered value associated with the base product is positively correlated to his likelihood of purchasing the add-on. Formal modeling of this bias shows that firms may find penetration pricing strategies (such as loss leader pricing) suboptimal. Furthermore, the identified bias leads the firm to spend more resources toward enhancing both the base product and the add-on quality, especially so when the add-on will be offered before the consumer has a chance to extensively use the base product. Finally, the effect of competition in the base product market is also considered.", "e:keyword": ["Consumer behavior", "Pricing", "Behavioral decision theory", "Lab experiments", "Mental accounting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0724", "e:abstract": "Firms that sell via a direct channel <i>and</i> via indirect channels have to decide whether to allow third-party sellers to use trademarked brand names of products in their advertising. This question has been particularly controversial for advertising on search engines. In June 2009, Google started allowing any third-party reseller of a product to use a trademark such as \"DoubleTree\" in the text of its ad, even if the reseller did not have the trademark holder's permission. We study the effects of this change empirically within the hotel industry. We find some evidence that allowing third-party sellers to use a trademark in their online search advertising weakly reduced the likelihood of a consumer clicking on a trademark holder's paid search ads. However, the decrease in paid clicks was outweighed by a large increase in consumers clicking on the unpaid links to the hotelier's website within the main search results. Our evidence shows that when a third-party seller focuses on a trademarked brand in its ads, the ads become less distinct, and customers are more likely to ignore the advertised offers and buy from the direct channel.", "e:keyword": ["Trademarks", "Online search", "Search advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0722", "e:abstract": "In recent years academic research has focused on understanding and modeling the survey response process. This paper examines an understudied systematic response tendency in surveys: the extent to which observed responses are subject to state dependence, i.e., response carryover from one item to another independent of specific item content. We develop a statistical model that simultaneously accounts for state dependence, item content, and scale usage heterogeneity. The paper explores how state dependence varies by response category, item characteristics, item sequence, respondent characteristics, and whether it becomes stronger as the survey progresses. Two empirical applications provide evidence of substantial and significant state dependence. We find that the degree of state dependence depends on item characteristics and item sequence, and it varies across individuals and countries. The article demonstrates that ignoring state dependence may affect reliability and predictive validity, and it provides recommendations for survey researchers.", "e:keyword": ["State dependence", "Response process", "Surveys", "Scale usage", "Response styles", "Survey response models", "Item response theory", "Validity", "Reliability"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0714", "e:abstract": "It is well known that individuals often fail to exert proper self-control. In organizational settings, this can lead to reduced productivity and profits. We use the literature on present-biased preferences to model employees' self-control problems and examine how firms can design compensation plans to reduce the negative consequences of their employees' self-control problems. Our results suggest that firms can mitigate self-control problems by delaying payment to the employees. This can be achieved by using multiperiod quotas (such as annual quotas) to compensate employees for their cumulative performance. Although such plans are prevalent in the market, there is little theoretical research that shows when multiperiod quota plans can be optimal. The paper provides one potential explanation for the widespread use of such quota plans. Interestingly, we find that such plans may be optimal despite the fact that they encourage more procrastination. We also find that such plans lead to higher effort by the employees and can sometimes improve the welfare of not only the firm but also the employees.", "e:keyword": ["Behavioral economics", "Game theory", "Present-biased preferences", "Compensation design"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0732", "e:abstract": "Although the assumption of utility-maximizing consumers has been challenged for decades, empirical applications of alternative choice rules are still very new. We add to this growing body of literature by proposing a model based on the idea of a \"satisficing\" decision maker. In contrast to previous models (including recent models implementing alternative choice rules), satisficing depends on the order in which alternatives are evaluated. We therefore conduct a visual conjoint experiment to collect search and choice data. We model search and product evaluation jointly and allow for interdependence between them. The choice rule incorporates a conjunctive rule for the evaluations and, contrary to most previous models, does not rely on compensatory trade-offs at all. The results strongly support the proposed model. For instance, we find that search is indeed influenced by product evaluations. More importantly, the model results strongly support the satisficing stopping rule. Finally, we perform a holdout prediction task and find that the proposed model outperforms a standard multinomial logit model.", "e:keyword": ["Noncompensatory choice", "Eye tracking", "Visual conjoint experiment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0727", "e:abstract": "Social interaction (peer) effects are recognized as a potentially important factor in the diffusion of new products. In the case of environmentally friendly goods or technologies, both marketers and policy makers are interested in the presence of causal peer effects as social spillovers can be used to expedite adoption. We provide a methodology for the simple, straightforward identification of peer effects with sufficiently rich data, avoiding the biases that occur with traditional fixed effects estimation when using the past installed base of consumers in the reference group. We study the diffusion of solar photovoltaic panels in California and find that at the average number of owner-occupied homes in a zip code, an additional installation increases the probability of an adoption in the zip code by 0.78 percentage points. Our results provide valuable guidance to marketers designing strategies to increase referrals and reduce customer acquisition costs. They also provide insights into the diffusion process of environmentally friendly technologies.", "e:keyword": ["Social contagion", "Diffusion models", "Empirical IO methods", "Probability models", "Word of mouth", "Endogeneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0734", "e:abstract": "This paper analyzes the competitive role of retail shopping experience in markets with consumer search costs. We examine how a retailer's advantage in providing consumer shopping experience affects its equilibrium pricing and price advertising strategies. We find that if the consumer valuation of a shopping experience is sufficiently low, its effect on retailer strategy is similar to that of quality, and the retailer with the advantage in shopping experience then deploys higher levels of price advertising. On the other hand, when the shopping experience is valuable enough for consumers, it acts akin to price advertising in that it makes it optimal for the retailer with the advantage in shopping experience to eschew price advertising. The optimal competitive investments in consumer shopping experience can be higher than that of a monopoly. The profit impact of shopping experience for a retailer depends on the level of shopping experience: for low levels, the profit impact depends on the difference in the levels between the retailers, but for high enough levels, it depends only on whether the retailer's shopping experience level is higher than that of its competitor. In this case, even small differences in shopping experience levels can result in large differences in equilibrium profits.", "e:keyword": ["Shopping experience", "Retail competition", "Search costs", "Price advertising", "Store atmospherics", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0737", "e:abstract": "We consider how public firms influence their stock market valuations by timing the introduction of innovative new products. Our focus is on <i>innovation ratchet strategy</i>--firms timing the introduction of innovations in order to demonstrate an improvement in the number of introductions over time. We document that public firms use an innovation ratchet strategy more often than do private firms and that the stock market rewards public firms for doing so. These rewards from the stock market, however, come at the expense of performance in product markets. Specifically, because firms using an innovation ratchet strategy delay some product introductions, they have significantly lower sales growth in the year they ratchet. Finally, we identify firm and market characteristics that influence the likelihood that a public firm will engage in an innovation ratchet strategy.", "e:keyword": ["Innovation", "Timing", "Stock market", "Ratchet", "Revenue"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0738", "e:abstract": "Word-of-mouth (WOM) plays an increasingly important role in shaping consumers' attitudes and buying behaviors. Prior work in marketing has mainly focused on the aggregate impact of WOM on product sales as well as the generation of WOM. Very little attention has been paid to the consumption or usage of WOM. In this paper, utilizing a unique data set that collects information from the automobile category on whether a consumer generates WOM to others and uses WOM for making purchase decisions, we build a discrete-choice model to study consumer WOM generation and WOM consumption decisions simultaneously and empirically answer questions that have not been explored previously. We are particularly interested in studying the key drivers of WOM generation/consumption and the synergy effect between the two WOM-related activities. We apply the proposed model to survey data collected on the automobile category. We find a strong synergy between WOM generation and WOM consumption. Although some consumers view WOM generation and WOM consumption as complementary to each other, others tend to perceive the two activities as competing with each other. We also find that consumer product experience and media exposure are positively correlated with their propensity to generate WOM. However, their effect on WOM consumption is mixed. Our empirical analysis also provides evidence of unobserved heterogeneity in the way consumer WOM activities are related to consumer product experience. Overall, these findings lead to important managerial implications on targeting for effective use of WOM as a marketing tool.", "e:keyword": ["Word-of-mouth", "Communication", "Discrete-choice model", "Probit model", "Finite mixture model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0739", "e:abstract": "Competition is intense among rival technologies, and success depends on predicting their future trajectory of performance. To resolve this challenge, managers often follow popular heuristics, generalizations, or \"laws\" such as Moore's law. We propose a model, Step And Wait (SAW), for predicting the path of technological innovation, and we compare its performance against eight models for 25 technologies and 804 technologies-years across six markets. The estimates of the model provide four important results. First, Moore's law and Kryder's law do not generalize across markets; neither holds for all technologies even in a single market. Second, SAW produces superior predictions over traditional methods, such as the Bass model or Gompertz law, and can form predictions for a completely new technology by incorporating information from other categories on time-varying covariates. Third, analysis of the model parameters suggests that (i) recent technologies improve at a faster rate than old technologies; (ii) as the number of competitors increases, performance improves in smaller steps and longer waits; (iii) later entrants and technologies that have a number of prior steps tend to have smaller steps and shorter waits; but (iv) technologies with a long average wait time continue to have large steps. Fourth, technologies cluster in their performance by market.", "e:keyword": ["Technology evolution", "Innovation", "SAW model", "Moore's law", "Kryder's law", "Bass model", "Technological prediction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0740", "e:abstract": "Contextual advertising entails the display of relevant ads based on the content that consumers view, exploiting the potential that consumers' content preferences are indicative of their product preferences. This paper studies the strategic aspects of such advertising, considering an intermediary who has access to a content base, sells advertising space to advertisers who compete in the product market, and provides the targeting technology. The results show that contextual targeting impacts advertiser profit in two ways: First, advertising through relevant content topics helps advertisers reach consumers with a strong preference for their product. Second, heterogeneity in consumers' <i>content</i> preferences can be leveraged to reduce <i>product</i> market competition, especially when competition is intense. The intermediary has incentives to strategically design its targeting technology, sometimes at the cost of the advertisers. When product market competition is moderate, the intermediary offers accurate targeting such that the consumers see the most relevant ads. When competition is high, the intermediary lowers the targeting accuracy such that the consumers see less relevant ads. Doing so intensifies competition and encourages advertisers to bid for multiple content topics in order to prevent their competitors from reaching consumers. In some cases, this may lead to an asymmetric equilibrium where one advertiser bids high even for the content topic that is more relevant to its competitor.", "e:keyword": ["Online advertising", "Display advertising", "Targeting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0736", "e:abstract": "This paper studies optimal product line design when consumers need to incur costly deliberation to uncover their valuations for quality. To induce deliberation, a firm must maintain quality dispersion and cut the price of the high-end product so that consumers are motivated to deliberate in the hope that high-end consumption fits their needs. To prevent deliberation, the firm may have to offer downgraded quality at a low price so that an impulsive purchase will not appear too wasteful. Whether the firm should induce deliberation depends on how much surplus it creates by aligning the supply of quality with heterogeneous demand for quality and how much surplus it captures during this process. Interestingly, equilibrium firm profit, consumer surplus, and social welfare can all increase with the cost of deliberation. We extend the model to accommodate consumers' heterogeneous prior beliefs of their valuations for quality. We also discuss how market research could benefit from taking into account the endogeneity of consumer deliberation.", "e:keyword": ["Consumer deliberation", "Product line design", "Price discrimination", "Information acquisition", "Agency theory", "Preference construction"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0726", "e:abstract": "This paper describes two new data sets available to academic researchers (at <ext-link ext-link-type=\"uri\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"http://www.informs.org/Community/ISMS\">http://www.informs.org/Community/ISMS</ext-link>). The first is a panel data set containing the transactions of 19,936 households made over the period from December 1998 to November 2004 at a major U.S. consumer electronics retailer. There are a total of 173,262 transactions, including purchases and returns of products as well as extended warranties. There are 16 product categories and 292 subcategories, ranging from big-ticket items such as televisions to small-ticket items such as CDs and batteries. The second data set features a field experiment for a Christmas promotion that took place in December 2003 in the form of a direct mailing sent to a randomly selected group of households at the end of November 2003. We describe the data and the potential research issues that can be studied using these two durable goods data sets.", "e:keyword": ["Retailer", "Durable goods", "Panel data", "Product adoption", "Holiday promotion", "Sales forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0725", "e:abstract": "This paper studies product bundling in a distribution channel where a downstream retailer combines component goods produced by separate manufacturers acting independently. Past literature offers deep insights about bundling by a single firm whose unit costs are not impacted by choice of selling strategy. But when the retailer bundles goods from separate manufacturers, unit costs for the bundler (retailer) are, being the prices set by the manufacturers, no longer exogenous. This alters the economic balance with respect to bundling. I show that channel conflicts weaken the case for bundling. Although bundling is better than component selling for the integrated firm, it is no longer so in the decentralized channel. The culprit is a combination of vertical channel conflict (incentive misalignment with respect to bundle versus component sales) and horizontal conflict (each manufacturer wants a higher share of profits from bundle sales), with the latter playing a dominant role. They cause manufacturers to overprice component goods, weakening the retailer's incentives to bundle. The competitive interplay between firms when one (retailer) merges the prices of several (manufacturers) leads to lower profits for all. Price coordination between the firms could partially restore the role of bundling and improve the firms' profits as well as consumer surplus.", "e:keyword": ["Product bundling", "Channel conflicts", "Composite goods", "Double marginalization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0764", "e:abstract": "This paper considers the history of keywords used in <i>Marketing Science</i> to develop insights on the evolution of marketing science. Several findings emerge. First, \"pricing\" and \"game theory\" are the most ubiquitous words. More generally, the three C's and four P's predominate, suggesting that keywords and common practical frameworks align. Various trends exist. Some words, like \"pricing,\" remain popular over time. Others, like \"game theory\" and \"hierarchical Bayes,\" have become more popular. Finally, some words are superseded by others, like \"diffusion\" by \"social networking.\" Second, the overall rate of new keyword introductions has increased, but the likelihood they will remain in use has decreased. This suggests a maturation of the discipline or a long-tail effect. Third, a correspondence analysis indicates three distinct eras of marketing modeling, comporting roughly with each of the past three decades. These eras are driven by the emergence of new data and business problems, suggesting a fluid field responsive to practical problems. Fourth, we consider author publication survival rates, which increase up to six papers and then decline, possibly as a result of changes in ability or motivation. Fifth, survival rates vary with the recency and nature of words. We conclude by discussing the implications for additional journal space and the utility of standardized classification codes.", "e:keyword": ["Keywords", "History", "Marketing", "Data visualization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0745", "e:abstract": "Presidential elections provide both an important context in which to study advertising and a setting that mitigates the challenges of dynamics and endogeneity. We use the 2000 and 2004 general elections to analyze the effect of market-level advertising on county-level vote shares. The results indicate significant positive effects of advertising exposures. Both instrumental variables and fixed effects alter the ad coefficient. Advertising elasticities are smaller than are typical for branded goods yet significant enough to shift election outcomes. For example, if advertising were set to zero and all other factors held constant, three states' electoral votes would have changed parties in 2000. Given the narrow margin of victory in 2000, this shift would have resulted in a different president.", "e:keyword": ["Advertising", "Politics", "Instrumental variables", "Presidential elections"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0751", "e:abstract": "The extensive adoption of uniform pricing for branded variants is a puzzling phenomenon, considering that firms may improve profitability through price discrimination. In this paper, we incorporate consumers' concerns of peer-induced price fairness into a model of price competition and show that a uniform price for branded variants may emerge in equilibrium. Interestingly, we find that uniform pricing induced by consumers' concerns of fairness can actually help mitigate price competition and hence increase firms' profits if the demand of the product category is expandable. Furthermore, an individual firm may not have an incentive to unilaterally mitigate consumers' concerns of price fairness to its own branded variants, which suggests the long-run sustainability of the uniform pricing strategy. As a result, fairness concerns from consumers provide a natural mechanism for firms to commit to uniform pricing and enhance their profits.", "e:keyword": ["Pricing", "Peer-induced fairness", "Price fairness", "Behavioral economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0742", "e:abstract": "One of the characteristics of the fashion marketplace is the unpredictability and apparent randomness of fashion hits. Another one is the information asymmetry among consumers. In this paper, we consider fashion as a means consumers use to signal belonging to a higher social rank and propose an analytical model of fashion hits in the presence of competition and consumers who can coordinate on which product to use. We show that, consistent with the observed market phenomenon, in equilibrium, consumer coordination involves randomization between products chosen, i.e., in randomness of fashion hits. Analyzing optimal consumer choice, we find that whenever low-type consumer demand for a product is positive, a price increase results in a higher probability of high-type consumers choosing this product but lower low-type consumer demand. We also show that although high-type consumers may prefer (higher) prices that would lead to complete separation of the high- and the low-type consumers through product use, in equilibrium, firms always price as to attract positive demand from low-type consumers. The equilibrium price and profits turn out to be nonmonotonic in the low-type consumer valuation of being recognized as belonging to a higher social rank. Equilibrium profits first increase and then decrease in this valuation.", "e:keyword": ["Game theory", "Status goods", "Uncertainty", "Consumer signalling", "Price competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0749", "e:abstract": "Recognizing that initial public offerings (IPOs) represent the debut of private firms on the public stage, this study investigates how pre-IPO customer and competitor orientations (CCOs) affect IPO outcomes. Building on information economics, we propose that CCOs influence investors' sentiments toward an IPO and that both IPO-specific variables (which influence the credibility of CCO information) and facets of the organizational institutional and task environments (which influence the appropriateness of CCO information) moderate this influence. We test the framework using data collected from computer-aided text analysis, expert coders, and secondary sources for 543 IPOs across 43 industries between 2000 and 2004. A Bayesian shrinkage model, which accounts for industry-specific effects and uses latent instrumental variables to address CCO endogeneity, shows that CCOs positively influence IPO outcomes. Furthermore, (1) underwriter reputation and venture funding positively moderate the effects of CCOs; (2) technological and market turbulence positively and institutional complexity negatively moderate the effect of customer orientation; and (3) technological turbulence, competitive intensity, and institutional complexity positively moderate the effect of competitor orientation. Also, accounting for endogeneity using latent instrumental variables substantially improves the predictive validity of the model, relative to alternative model specifications.", "e:keyword": ["Initial public offering", "Customer orientation", "Competitor orientation", "Institutional environment", "Information economics", "Cheap talk", "Costly state falsification", "Latent instrumental variables", "Hierarchical Bayesian analysis", "Content analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0757", "e:abstract": "Patients increasingly request their physicians to prescribe specific brands of pharmaceutical drugs. A popular belief is that requests are triggered by direct-to-consumer advertising (DTCA). We examine the relationship between DTCA, patient requests, and prescriptions for statins. We find that although the effect of requests on prescriptions is significantly positive, the mean effect of DTCA on patient requests is negative, yet very small. More interestingly, both effects show substantial heterogeneity across physicians, which we uncover using a hierarchical Bayes estimation procedure. We find that specialists receive more requests than primary care physicians but translate them less into prescriptions. In addition, we find that the sociodemographic profile of the area a physician practices in moderates the effects of DTCA on requests and of requests on prescriptions. For instance, physicians from areas with a higher proportion of minorities (i.e., blacks and Hispanics) receive more requests that are less triggered by DTCA and are accomodated less frequently than physicians from areas with a lower proportion of minorities. Our results challenge managers to revisit the role of DTCA in stimulating patient requests. At the same time, they may trigger public policy concerns regarding physicians' accommodation of patient requests and the inequalities they may induce.", "e:keyword": ["Pharmaceutical marketing", "Advertising", "Requests", "Prescriptions", "Sociodemographics", "Minorities", "Race/origin", "DTCA"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0720", "e:abstract": "Firms that serve a large market with many diverse consumer types use discriminatory or nonlinear pricing to extract higher revenue, inducing consumers to separate by self-selecting from a large number of tariff options. But the extent of price discrimination must often be tempered by the high costs of devising and managing discriminatory tariffs, including costs of supporting consumers in understanding and making selection from a complex menu of choices. These tariff design trade-offs occur in many industries where firms face many consumer types and each consumer picks the number of units to consume over time. Examples include wireless communication services, other telecom and information technology products, legal plans, fitness clubs, automobile clubs, parking, healthcare plans, and many services and utilities. This paper evaluates alternative ways to price discriminate while accounting for both revenues and tariff management costs. The revenue-maximizing menu of quantity-price bundles can be very (or infinitely) large and hence not practical. Instead, two-part tariffs (2PTs), which charge a fixed entry fee and a per-unit fee, can extract a large fraction of the optimal revenue with a small menu of choices, and they become more attractive once the costs of tariff management are factored in. We show that three-part tariffs (3PTs), which use an additional instrument, the \"free allowance,\" are an even more efficient way to price discriminate. A relatively small menu of 3PTs can be more profitable than a menu of 2PTs of any size. This 3PT menu can be designed with less information about consumer preferences relative to the menu of two-part tariffs, which, in order to segment customers optimally, needs fine-grained information about preferences. Our analysis reveals a counterintuitive insight that more-complex tariffs need not always be more profitable; it matters whether the complexity is from many choices or more pricing instruments.", "e:keyword": ["Price discrimination", "Multipart tariffs", "Nonlinear pricing", "Tariff costs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0748", "e:abstract": "This article examines cross-price promotional effects in a dynamic context. Among other things, we investigate whether previously established findings hold when consumer and competitive dynamics are taken into account. Five main influential effects (asymmetric price effect, neighborhood price effect, asymmetric share effect, neighborhood share effect, and private label versus national brand asymmetry) appear jointly in the second layer of a pooled HB-VEC-VARX model, together with brand- and category-specific variables. This study tests the relative importance of these key factors across three scenarios: with no market dynamics, when only consumer dynamics are considered, and when competitive reactions are also taken into account. The results confirm all five influential effects, even if they are jointly estimated, and consumer and competitive dynamics are taken into account. National brand/private label asymmetry has the strongest influence on the cross-price promotional effects and becomes significantly stronger when consumer and competitive dynamics are taken into account. Dynamic consumer responses and competitive reactions both affect cross-brand price elasticities, and contrary to expectations, competitive reactions accumulate rather than diminish cross-price elasticities. Preemptive switching does occur; i.e., a brand's promotion in period <i>t</i> hurts a competitor's sales in subsequent periods. Our findings are based on an extensive data set. To attain generalizable results, we analyze 33 categories in five stores--that is, 165 store/category combinations.", "e:keyword": ["Cross-price elasticity", "Price promotions", "Asymmetry", "Dynamic effects", "Competitive reactions", "Hierarchical Bayes"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0755", "e:abstract": "We propose a structural model to study the effect of online product reviews on consumer purchases of experiential products. Such purchases are characterized by limited repeat purchase behavior of the same product item (such as a book title) but significant past usage experience with other products of the same type (such as books of the same genre). To cope with the uncertainty in quality of the product item, we posit that consumers may learn from their experience with the same type of product and others' experiences with the product item. We model the review credibility as the precision with which product reviews reflect the consumer's own product evaluation. The higher the precision, the more credible the information obtained from product reviews for the consumer, and the larger the effect of reviews on the consumer's choice probabilities. We extend the Bayesian learning framework to model consumer learning on both product quality and review credibility. We apply the model to a panel data set of 1,919 book purchases by 243 consumers. We find that consumers learn more from online reviews of book titles than from their own experience with other books of the same genre. In the counterfactual analysis, we illustrate the profit impact of product reviews and how it varies with the number of reviews. We also study the phenomenon of fake reviews. We find that fake reviews increase consumer uncertainty. The effects of more positive reviews and more numerous reviews on consumer choice are smaller on online retailing platforms that have fake product reviews.", "e:keyword": ["Learning models", "Choice models", "Product reviews"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0754", "e:abstract": "We reanalyze endogenous sample selection in the context of customer scoring, targeting, and influencing decisions. Scoring relies on ordered lists of probabilities that customers act in a way that contributes revenues, e.g., purchase something from the firm. Targeting identifies constrained sets of covariate patterns associated with high probabilities of these acts. Influencing aims at changing the probabilities that individual customers act accordingly through marketing activities. We show that successful targeting and influencing decisions require inference that controls for endogenous selection, whereas scoring can proceed relatively successfully based on simpler models that provide (local) approximations, capitalizing on spurious effects of observed covariates. To facilitate the type of inference required for targeting and influencing, we develop a prior that frees the analyst from having to specify (often arbitrary) exclusion restrictions for model identification a priori or to explicitly compare all possible models. We cover exclusions of observed as well as unobserved covariates that may cause the successive selections to be dependent. We automatically infer the dependence structure among selection stages using Markov chain Monte Carlo-based variable selection, before identifying the scale of latent variables. The adaptive parsimony achieved through our prior is particularly helpful in applications where the number of successive selections exceeds two, a relevant but underresearched situation.", "e:keyword": ["Bayesian estimation", "Targeting", "Cross-sectional analysis", "Scoring", "Causal reasoning", "Variable selection", "Sample selection"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0735", "e:abstract": "This paper reports on a large-scale implementation of marketing science models to solve the bidding problem in search engine advertising. In cooperation with the online marketing agency SoQuero, we developed a fully automated bidding decision support system, PROSAD (PRofit Optimizing Search engine ADvertising; see http://www.prosad.de), and implemented it through the agency's bid management software. The PROSAD system maximizes an advertiser's profit per keyword without the need for human intervention. A closed-form solution for the optimized bid and a newly developed “costs-per-profit” heuristic enable advertisers to submit good bids even when there is significant noise in the data. A field experiment demonstrates that PROSAD can increase the return on investment by 21 percentage points and improve the yearly profit potential for SoQuero and its clients by €2.7 million.", "e:keyword": ["Decision support system", "Optimized bidding", "Search engine advertising", "Online advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0743", "e:abstract": "Physicians may learn about prescription drug effectiveness directly from the firm via detailing or from patient experience. Patient-mediated learning is aided by the use of free drug samples. The effective use of samples is hampered by a lack of understanding of its exact return on investment implications. We seek to fill this gap by incorporating the physician's sample allocation behavior in the firm's decision making. We uncover the following implications for firms as well as policy makers. First, we find that the optimal sampling level for a drug category is a nonmonotonic function of patient payment ability and the price of the drug. Second, an increase in the cost of samples can lead to an increase in sampling and a decrease in detailing when the physician's propensity to provide sample subsidies is high. Third, when future market growth is expected to be high (early stage product life cycle and/or chronic drugs) and sampling efficiency is low, the use of sampling is profitable for the firm but leads to lower market coverage than when sampling is disallowed.", "e:keyword": ["Pharmaceutical marketing", "Sampling", "Detailing", "Patient payment ability"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0744", "e:abstract": "The recent article by Bandyopadhyay and Paul [Bandyopadhyay S, Paul AA (2010) Equilibrium returns policies in the presence of supplier competition. <i>Marketing Sci.</i> 29(5):846–857] searched for an explanation for the phenomenon the authors termed the “Pasternack paradox,” i.e., why full-credit return policies, which were considered suboptimal from the perspective of channel coordination, are prevalent in practice. The authors argued that the underlying reason is that it is the competition between suppliers rather than the coordination among channel members that dominates business practice. We show that their model actually fails to generate the claimed results. Counterexamples are given. Alternative explanations are therefore needed for the seemingly suboptimal business practice.", "e:keyword": ["Returns policy", "Channel competition", "Channel coordination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0746", "e:abstract": "The purpose of this paper is to describe the implementation of a category management tool known as Category Optimizer™ at Foster's Wine Estates Americas for one of its brands, the Beringer California Collection. Foster's was facing a common management problem: harnessing its portfolio of Beringer California Collection wines to increase profitability, improve its competitive position, and defend against a disruptive new entrant in the U.S. wine market called Yellow Tail. Category Optimizer combines the parsimony of an internal market structure with the advances that have been made in <i>assortment planning in operations research</i>, <i>assortment and stock-keeping-unit--level modeling</i>, <i>mixed logits</i>, and the <i>marketing literature on the perceptions of variety of assortment</i> to develop and estimate a model on readily available store scanner data. The model subsequently uses these results to inform strategic and tactical decision making. This approach led to recommendations that initially seemed counterintuitive; the normal response would be for Foster's to consider lowering prices to maintain share and volume, a strategy not inconsistent with many of the recommendations of past models. However, considering the additional degrees of freedom that a product range offered for defense, we demonstrated that a combination of price increases together with the introduction of a volume-flanker product in a new channel would improve profits, increase revenue, and protect and enhance market share. These were successfully implemented in early 2008, earning rich dividends for the company; increasing profitability by 70%, revenue by 3%, and earnings before interest and taxes by 8.5%; and having a positive impact on its brand ranking. In fact, in 2008, it debuted as sixth among the international wine brands. It also managed to play an important role in deposing Yellow Tail, the market share leader, from its dominant position. We conclude the paper by providing examples of other companies where this approach has also been successfully implemented and by discussing some avenues for future research.", "e:keyword": ["Disruptive innovations", "Category management", "Internal market structure", "Assortment", "Perception of variety", "Brand portfolio"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0750", "e:abstract": "A widely debated question in recent years by both strategy theorists and antitrust practitioners is what role product differentiation between firms plays in their ability to sustain a collusive agreement in order to reduce the strength of competition and gain higher profits. This paper addresses the following question: What happens to the “product differentiation--collusion sustainability” relationship when setting up and maintaining an agreement is costly? We show that introducing collusion costs into the discussion has relevant implications. Indeed, sufficiently high collusion costs modify the underlying market structure, thus altering the product differentiation--collusion sustainability relationship with respect to the case where collusion costs are absent or low. In particular, if the gains from collusion are increasing (decreasing) with the degree of product differentiation, the relationship between product differentiation and collusion sustainability is always positive (negative), whereas if the gains from collusion are inverted U-shaped, the relationship is inverted U-shaped too. These results stress the importance of considering those markets where the coordination between firms is sufficiently costly as structurally different from those markets where coordination has no costs for firms.", "e:keyword": ["Product differentiation", "Competition", "Collusion", "Collusion costs"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0752", "e:abstract": "In many markets, a handset vendor and a service provider may enter into a tie-in for a handset to be available exclusively through the service provider. We examine when and why a service provider and a handset vendor may find this arrangement mutually profitable. We find that an exclusive handset arrangement (EHA) may serve a dual strategic purpose. By restricting its handsets to one service provider, a handset vendor may be able to induce a rival handset vendor to compete less aggressively. At the same time, the service provider may be able to essentially raise a rival service provider's handset costs by limiting the handsets available to the rival. Interestingly, the handset vendor's market share may be higher when its handset is sold exclusively than when it is not. Our results might explain why EHAs seem more attractive in some markets than in others, why some service providers have exclusive arrangements even for handset models that do not seem popular, and how some handset vendors enjoy high market shares despite having many exclusive models. Furthermore, an EHA may lower the handset vendor's incentives to improve handset quality, supporting concerns raised by proponents of wireless network neutrality.", "e:keyword": ["Competitive strategy", "Distribution channel", "Exclusive arrangements", "Game theory", "Raising rival's costs", "Wireless network neutrality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0753", "e:abstract": "Until recently, brand identities were built by firms via brand image advertising. However, the flourishing consumer communication weakened the firms' grip on their brands. The interaction between advertising and consumer communications and their joint impact on brand identity is the focal point of this paper. We present a model in which consumer preference for functional attributes may correlate with the identity they desire to project of themselves. This correlation is known to the firm but not to the consumers. Both the firm and the consumers can communicate their desired brand identity, although the actual brand identity is determined endogeneously by the composition of consumers who purchase it (i.e., what types of people consume the brand). We find that sometimes the firm can strengthen the identity of its brand by refraining from advertising. This result is based on the following intermediate finding: advertising can diminish the endogeneous informativeness of consumer communications by making it one-sided. Furthermore, it turns out that refraining from brand image advertising may be optimal for the firm when the product is especially well positioned to create a strong identity---i.e., when consumer preferences for functional and self-expressive attributes are highly correlated.", "e:keyword": ["Advertising", "Communication", "Word of mouth", "Branding", "Brand image"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0756", "e:abstract": "From 2003 to 2012, the ISMS-MSI Practice Prize/Award competition has documented 25 impactful projects, with associated papers appearing in <i>Marketing Science</i>. This article reviews these papers and projects, examines their influence on the relevant organizations, and provides a perspective on the diffusion and impact of marketing science models within the organizations. We base our analysis on three sources of data---the articles, authors' responses to a survey, and in-depth interviews with the authors. We draw some conclusions about how marketing science models can create more impact without losing academic rigor while maintaining strong relevance to practice. We find that the application and diffusion of marketing science models are not restricted to the well-known choice models, conjoint analysis, mapping, and promotional analysis---there are very effective applications across a wide range of managerial problems using an array of marketing science techniques. There is no one successful approach, and although some factors are correlated with impactful marketing science models, there are a number of pathways by which a project can add value to its client organization. Simpler, easier-to-use models that offer robust and improved results can have a stronger impact than academically sophisticated models can. Organizational buy-in is critical and can be achieved through recognizing high-level champions, holding in-house presentations and dialogues, doing pilot assignments, involving multidepartment personnel, and speaking the same language as the influential executives. And we find that intermediaries often, but not always, play a key role in the transportability and diffusion of models across organizations. Although these applications are impressive and reflect profitable academic--practitioner partnerships, changes in the knowledge base and reward systems for academics, intermediaries, and practitioners are required for marketing science approaches to realize their potential impact on a much larger scale than the highly selective sample that we have been able to analyze.", "e:keyword": ["Marketing models", "Decision making", "Marketing analytics", "Implementation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0759", "e:abstract": "There is a growing trend among consumers to serially consume small, incomplete “chunks” of multiple media types---television, radio, Internet, and print---within a short time period. We refer to this behavior as <i>media multiplexing</i> and note that key challenges for integrated marketing communications media planners are (1) predicting which media or combination of media their target audience is likely to consume at any given time and (2) understanding potential substitutions and complementarities in their joint consumption. We propose a forecasting model that incorporates media-multiplexing behavior of both traditional and new media, their interdependencies, and consumer heterogeneity, and we calibrate the model using a rich database of individual-specific media activity diaries. The results suggest that accounting for media synergies within a single utility specification significantly improves model forecasts. We also introduce a utility function that directly models cross-channel media complementarities via interactive effects of the satiation parameters of own and joint consumption of various media types. Finally, our individual-level analyses generate unique insights on consumer-level media switching, multiplexing, and individual heterogeneity often ignored in aggregate data.", "e:keyword": ["Integrated marketing communications", "Media planning", "Multichannel management", "Multimedia consumption", "Substitution and complementarities", "Interactive media", "Internet advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0760", "e:abstract": "In this paper we quantify the economic worth of celebrity endorsements by studying the sales of endorsed products. We do so with the use of two unique data sets consisting of monthly golf ball sales and professional golfer (celebrity) rankings. In particular, we examine the impact Tiger Woods had on sales of Nike golf balls. Our identification of the causal effect of a celebrity is grounded in the celebrity's random performance over time. Using two different approaches, reduced form and structural, we find that there are substantial celebrity endorsement effects. From our structural model, we determine that endorsements not only induce consumers to switch brands, a business stealing effect, but also have a primary demand effect. We determine that from 2000 to 2010, the Nike golf ball division reaped an additional profit of $103 million through the acquisition of 9.9 million in sales from Tiger Woods' endorsement effect. Moreover, having Tiger Woods' endorsement led to a price premium of roughly 2.5%. As a result, approximately 57% of Nike's investment in Woods' $181 million endorsement deal was recovered <i>just</i> in U.S. golf ball sales alone.", "e:keyword": ["Endorsements", "Competitive analysis", "Entertainment marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0762", "e:abstract": "A firm may want to preannounce its plans to develop a new product in order to stimulate future demand. But given that such communications can affect rivals' incentives to develop the same new product, a firm may decide to preannounce untruthfully in order to deter competitors. We examine an incumbent's preannouncement strategy when there is uncertainty regarding the commercial viability of a new product opportunity and a threat of rival entry. Each firm has a private assessment of the market potential for the new product. Two competitive incentives arise for the incumbent in terms of discouraging rival entry: it can use preemptive communication or it can remain silent and instill a pessimistic market potential outlook. We find that an incumbent prefers to follow a <i>vaporware strategy</i>---i.e., declares plans to pursue a new product opportunity even when it may have no development intentions---when its market forecasting capabilities are weak and the demand-side benefits from preannouncing are small. By contrast, when the incumbent has strong market forecasting capabilities and the demand-side benefits are small, the incumbent adopts a <i>suddenware strategy</i>---i.e., remains silent about its new product plans even when it actually plans to develop the new product. Finally, when its market forecasting capabilities are strong and the demand-side benefits are large, the incumbent prefers to engage in a <i>trueware strategy</i>---i.e., truthfully preannounces development plans. We show that an interplay between competition-related and demand-related considerations is what allows trueware to emerge as an equilibrium in the absence of any ex post cost to engaging in vaporware. In an extension, we let the incumbent's actual development plans leak out and allow the entrant to wait and learn those plans prior to setting a research and development level. We identify conditions for the entrant to postpone development despite the risk of being late to market, as well as conditions for the entrant to commence development immediately despite not knowing the incumbent's plans based on the observed preannouncement strategy.", "e:keyword": ["New product preannouncements", "Market uncertainty", "Competitive signaling", "New product development"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0765", "e:abstract": "A setting is considered where consumers keep track of the extent to which brands care about them, which is modeled as altruism of brands toward their target consumers. Consumers who purchase an experience good of high quality reasonably deduce that the supplier of this good is relatively altruistic toward them, and they are therefore more keen to purchase a brand extension that is also directed at them. As a result, the success of brand extensions depends on the overlap between the customers of the original product and the target customers of the extension product. The quality and demand for a brand extension can be higher if the brand is perceived as caring only for its most quality-conscious consumers rather than for all possible buyers of the good.", "e:keyword": ["Quality provision", "Firm objectives", "Brand extensions", "Price signaling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0766", "e:abstract": "We examine multilateral bargaining in vertical supply relationships that involve an upstream manufacturer who sells through two competing retailers. In these relationships the negotiations are interdependent, and bargaining externality may arise across the retailers. In addition, the timing by which the manufacturer negotiates with the retailers becomes important. In simultaneous bargaining the retailers negotiate without knowing if an agreement has been reached in the other retail channel, whereas in sequential bargaining the retailer in the second negotiation is able to observe whether an agreement was reached in the first negotiation. We show that simultaneous bargaining is optimal for the manufacturer when the retail prices (and profitability) are similar, and sequential bargaining is preferred when the dispersion in the retail prices is sufficiently large. As a result of ex post renegotiations, the manufacturer may strategically stock out the less profitable retailer who charged a relatively low retail price and exclusively supply only the retailer who charged a relatively high retail price and maintained high channel profitability. Moreover, ex post multilateral bargaining can buffer downstream competition and thus lead to positive retail profits even in markets that are close to perfect competition.", "e:keyword": ["Multilateral bargaining", "Bargaining timing", "Bargaining externality", "Vertical relationships", "Retail competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0767", "e:abstract": "I introduce the work of the finalists in the 2011--2012 Gary L. Lilien ISMS-MSI Practice Prize Competition, representing once again the best combinations of rigor and relevance produced by marketing scientists. The winning paper is by a team who developed an innovative approach to measuring the impact of a social media campaign for an Indian premium ice cream retailer, Hokey Pokey. The other two finalists are from a team that developed a category management tool for a leading wine brand, Beringer, and a team that developed a system called PROSAD, which determines optimal bids to maximize an advertiser's profit per search engine advertising keyword.", "e:keyword": ["Social media", "Category management", "Search engine advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0768", "e:abstract": "Hokey Pokey, a popular “super premium” ice cream retailer, has over a dozen outlets based in India. Hokey Pokey offers “customized mix-in” flavors and realizes the importance of social media platforms to connect with its target consumers and create an engaging brand experience. However, with a limited marketing budget, the retailer needed to measure the success of its social media marketing efforts and create an optimized strategy. To accomplish this, we proposed and implemented a methodology to measure social media return on investment (ROI) and a customer's word-of-mouth (WOM) value by first creating a unique metric to measure the net influence wielded by a user in a social network, customer influence effect (CIE), and then predicting the user's ability to generate the spread of viral information. We then link WOM to the actual sales that it generates through a second metric, customer influence value (CIV), and we implement a strategy at Hokey Pokey to measure these metrics and identify their individual drivers. Finally, we refine our strategy to increase CIE and CIV, thereby impacting the profit. Our research shows that social media can be used to generate growth in sales, ROI, and positive word of mouth and can spread brand knowledge further.", "e:keyword": ["Social media marketing campaign", "ROI", "Customer influence effect", "Customer influence value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.1120.0770", "e:abstract": "The market structure of platform competition is critically important to managers and policy makers. Network effects in these markets predict concentrated industry structures, whereas competitive effects and differentiation suggest the opposite. Standard theory offers little guidance---full rationality models have multiple equilibria with wildly varying market concentration. We relax full rationality in favor of a boundedly rational cognitive hierarchy model. Even small departures from full rationality allow sharp predictions---there is a unique equilibrium in every case. When participants single-home and platforms are vertically differentiated, a single dominant platform emerges. Multihoming can give rise to a strong--weak market structure: one platform is accessed by all, and the other is used as a backup by some agents. Horizontal differentiation, in contrast, leads to fragmentation. Differentiation, rather than competitive effects, mainly determines market structure.", "e:keyword": ["Platform competition", "Bounded rationality", "Cognitive hierarchy", "Vertical and horizontal differentiation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0772", "e:abstract": "I consider a cheap-talk model in which a firm has a chance to communicate its product quality to consumers. The model describes how advertising can be both informative to consumers and profitable for the firm through its content in a vertically differentiated market. I find that advertising content may be effective in inducing search even if incentives for misrepresentation exist. In particular, a firm with an undesirable (low-quality) product is able to attract consumers who would have not incurred a search cost had they known its true quality. In this case, a semiseparating equilibrium occurs where the lowest firm types pool upward in order to increase the expected product quality while simultaneously signaling that the product is affordable. Although consumers always benefit from truth in advertising, total welfare may decrease if an undesirable firm is required to reveal its type. Finally, I show that the extent to which misrepresentation can take place increases with the cost of advertising coverage.", "e:keyword": ["Analytic models", "Advertising", "Targeting", "Marketing strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0773", "e:abstract": "We empirically study the motivations of users to contribute content to social media in the context of the popular microblogging site Twitter. We focus on noncommercial users who do not benefit financially from their contributions. Previous literature suggests that there are two main types of utility that motivate these users to post content: intrinsic utility and image-related utility. We leverage the fact that these two types of utility give rise to different predictions as to whether users should increase their contributions when their number of followers increases. To address the issue that the number of followers is endogenous, we conducted a field experiment in which we exogenously added followers (or follow requests, in the case of protected accounts) to a set of users over a period of time and compared their posting activities to those of a control group. We estimated each treated user's utility function using a dynamic discrete choice model. Although our results are consistent with both types of utility being at play, our model suggests that image-related utility is larger for most users. We discuss the implications of our findings for the evolution of Twitter and the type of value firms may derive from such platforms in the future.", "e:keyword": ["Social media", "Field experiments", "Dynamic discrete choice models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0774", "e:abstract": "The predominant perception on commonality strategy in product line design is that it entails a trade-off decision for a firm between cost savings and product differentiation. Adopting the commonality strategy may lower a firm's manufacturing costs, but it blurs the distinction between products targeting different consumer segments and makes consumer switching between products more likely such that cannibalization is always intensified. We show that this view in the literature is based on a crucial assumption that the quality valuation of one consumer segment is greater than that of another segment for all product attributes; i.e., one segment's preference structure dominates the other segment's preference structure. In this paper we consider the case of a nondominating preference structure where each segment has an attribute it values more than the other segment does. Interestingly, we show that the effect of commonality strategy is more diverse in this nondominating preference structure and that commonality can actually relieve cannibalization in the product line design. This finding gives rise to a previously unrecognized opportunity for firms to redesign their product lines to improve profits.", "e:keyword": ["Marketing/operations interface", "Product line design", "Commonality", "Nondominating preference structure", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0775", "e:abstract": "We focus on <i>destination</i> categories, so named because they have the greatest impact on where households choose to shop and, more generally, on how category positioning affects which store a household chooses. We propose a reduced-form model-based analytical approach to identify categories that fill the destination role. Our approach determines which categories are most important to shoppers' store choice decisions and helps determine in which categories the retailer provides superior value. In addition, our approach allows us to understand the impact of the retailer's long-run merchandising policy decisions on the value it provides. Previous store choice research considered the effects of pricing, assortment and other merchandising decisions at the store level but did not focus on the effect of specific categories on store choice. This focus leads us to formulate a model that can (1) measure and explain the differential impact that specific categories have on shoppers' store choice decisions and (2) measure the relative value of retailers' category offerings, partitioning that value into the component resulting from retailer merchandising and the component that is nonmerchandising related. The model form captures differences in category value across stores (i.e., the store's category positioning) by specifying a spatial model for the store choice and category incidence intercepts. Our spatial model recognizes that stores position their offering vis-à-vis the category ideal based on long-run category merchandising decisions and that not all categories have the same importance in store choice decisions. We explore these issues for five retailers in the Charlotte, North Carolina market. We find that (1) category impact on store choice is highly skewed; (2) although categories with higher sales generally have a higher impact on store choice decisions, there are exceptions; (3) impact on store choice decisions does not vary systematically by the type of category (e.g., perishable versus dry grocery); and (4) our measure of category impact on store choice, although correlated with the category development index <i>between</i> retailers, is superior in that it provides a basis for comparing category impact <i>within</i> a retailer and how relative category value, based on long-run merchandising decisions, attracts shoppers to a store.", "e:keyword": ["Category positioning", "Category and store choice modeling", "Spatial modeling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0776", "e:abstract": "Laboratory and field experiments show that when choosing among a set of objects, consumers could be subjected to context-dependent preferences and evaluate options by considering both the absolute utilities and their relative standing in the choice set. Using this premise we construct a game-theoretic model of competition between two firms and investigate how a firm's decision to differentiate or imitate is affected when consumers' preferences are context-dependent. We consider two horizontally differentiated firms where some consumers own the product of one firm and the rest own the other firm's product. One firm upgrades its existing product by adding a new feature. In the absence of any cost or capability constraints, to protect its competitive position, the other firm would prefer to upgrade its product by adding a differentiated new feature. We show that if consumers' preferences are context-dependent and the new feature is of an incremental type, the second mover prefers to imitate the first mover by adding the same feature even when there is no cost disadvantage to differentiate itself. This happens because context-dependent preferences cause consumers to dislike brands that are very differentiated from one another. Thus, if the second mover mimics the first mover, both firms can charge higher prices for their upgraded products (i.e., imitation leads to higher prices). This outcome, in turn, leads the first mover to pick a new feature such that it would induce the second mover to imitate. Therefore, our analysis shows that the need for context management leads not just to imitation but also to accommodating imitation.", "e:keyword": ["Context-dependent preferences", "Imitation", "Competition", "New product entry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0778", "e:abstract": "This study examines the strategic implications of retailer shelf layout decisions in a market characterized by consumer fit uncertainty. A retailer can display competing products in the same location, allowing consumers to inspect various products all at once or in distant locations, which induces consumers to inspect one product first and then decide whether to incur the travel cost to inspect another product. We consider a model in which two competing manufacturers distribute two horizontally differentiated products through a common retailer. Our analysis shows when the two manufacturers offer products of the same fit probabilities, the retailer obtains a greater profit by displaying competing products in distant locations if the products' fit probabilities are not too high; otherwise, the retailer is better off displaying competing products in the same location. When manufacturers offer products of differentiated fit probabilities, a retailer is more likely to benefit from displaying competing products in distant locations with an increased fit difference between products. Finally, a retailer is more likely to benefit from displaying competing products in distant locations when facing less competition from other retailers.", "e:keyword": ["Consumer fit search", "Shelf layout", "Distribution channel", "Retailing", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0779", "e:abstract": "The canonical design of customer satisfaction surveys asks for global satisfaction with a product or service and for evaluations of its distinct attributes. Users of these surveys are often interested in the relationship between global satisfaction and attributes; regression analysis is commonly used to measure the conditional associations. Regression analysis is only appropriate when the global satisfaction measure results from the attribute evaluations and is not appropriate when the covariance of the items lie in a low-dimensional subspace, such as in a factor model. Potential reasons for low-dimensional responses are that responses may be haloed from overall satisfaction and there may be an unintended lack of item specificity. In this paper we develop a Bayesian mixture model that facilitates the empirical distinction between regression models and relatively much lower-dimensional factor models. The model uses the dimensionality of the covariance among items in a survey as the primary classification criterion while accounting for the heterogeneous usage of rating scales. We apply the model to four different customer satisfaction surveys that evaluate hospitals, an academic program, smartphones, and theme parks, respectively. We show that correctly assessing the heterogeneous dimensionality of responses is critical for meaningful inferences by comparing our results to those from regression models.", "e:keyword": ["Bayesian estimation", "Surveys", "Information processing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0781", "e:abstract": "When defection is unobserved, latent attrition models provide useful insights about customer behavior and accurate forecasts of customer value. Yet extant models ignore direct marketing efforts. Response models incorporate the effects of direct marketing, but because they ignore latent attrition, they may lead firms to waste resources on inactive customers. We propose a parsimonious model that allows direct marketing to impact three relevant behaviors in latent attrition models---the frequency with which customers conduct transactions, the size of the transactions, and the duration for which customers remain active. Our model also accounts for how the organization targets its direct marketing across individuals and over time. Using donation data from a nonprofit organization, we find that direct marketing increases donation incidence for active donors. However, our analysis also shows that direct marketing has the potential to shorten the length of a donor's relationship. We find that our proposed model offers superior predictive performance compared with models that ignore the impact of direct marketing activity or latent attrition. We demonstrate the managerial applicability of our modeling approach by estimating the impact of direct marketing on donation behavior and identifying those donors most likely to conduct transactions in the future.", "e:keyword": ["Latent attrition", "Customer relationship management", "Simultaneity", "Direct marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0782", "e:abstract": "A symmetric complements refer to goods where one good is more dependent on the other, yet consumers receive enhanced utility from consuming both. Examples include garden hoses and sprinklers, chips and dip, and routine versus personalized services where the former has a broader base for utility generation and the latter is more dependent on the other's presence. Measuring asymmetric effects is difficult when all that is observed are the purchase quantities present in a consumer's market basket. We propose a direct utility model with a latent decision sequence for measuring asymmetric effects that allows us to capture differential responses to cross-category purchases and inventories. Scanner panel data of milk and cereal purchases are used to investigate the presence of asymmetric complementarity, and implications are explored through counterfactual analyses involving cross-price elasticities and spillover effects of merchandising variables.", "e:keyword": ["Utility theory", "Choice modeling", "Bayesian estimation", "Indivisible demand"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0783", "e:abstract": "This paper examines the impact of search engine optimization (SEO) on the competition between advertisers for organic and sponsored search results. The results show that a positive level of search engine optimization may improve the search engine's ranking quality and thus the satisfaction of its visitors. In the absence of sponsored links, the organic ranking is improved by SEO if and only if the quality provided by a website is sufficiently positively correlated with its valuation for consumers. In the presence of sponsored links, the results are accentuated and hold regardless of the correlation. When sponsored links serve as a second chance to acquire clicks from the search engine, low-quality websites have a reduced incentive to invest in SEO, giving an advantage to their high-quality counterparts. As a result of the high expected quality on the organic side, consumers begin their search with an organic click. Although SEO can improve consumer welfare and the payoff of high-quality sites, we find that the search engine's revenues are typically lower when advertisers spend more on SEO and thus less on sponsored links. Modeling the impact of the minimum bid set by the search engine reveals an inverse U-shaped relationship between the minimum bid and search engine profits, suggesting an optimal minimum bid that is decreasing in the level of SEO activity.", "e:keyword": ["Search engine marketing", "Electronic commerce", "Marketing contests"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0784", "e:abstract": "Many durable products with relatively short selling seasons have been using returns policies between manufacturers and retailers as the contractual protocol for some time. Recently, these sectors have witnessed the growing popularity of peer-to-peer Web-based used goods markets as important transaction channels between buyers and sellers. Given that these two issues are critically linked from both supply and demand perspectives, in this paper we study the role that consumer valuation of used products plays in shaping a manufacturer's incentive to offer a returns policy option to a retailer when used goods might be devalued compared to new ones as a result of physical deterioration (or obsolescence). We do so through a two-period dyadic channel framework where the retailer faces uncertain demand for a durable product from a renewable set of customers who are impatient but forward looking. The manufacturer, on the other hand, needs to decide whether or not to offer a returns contract to the retailer. We first characterize the necessary and sufficient condition under which a returns contract is the equilibrium strategy as well as the corresponding channel decisions. Further analysis of this condition reveals that a higher consumer valuation of used products increases the likelihood of a returns contract being the equilibrium strategy. This result seems to be robust except when the potential demands for the two periods are quite deterministic and uncorrelated. However, it contradicts the burgeoning managerial trend to replace returns contracts with price-only ones in sectors where used goods are valued relatively highly by the consumers. We also discuss how used goods markets affect the equilibrium channel decisions as well as how demand uncertainty and logistics costs associated with returns influence the equilibrium contracting strategy.", "e:keyword": ["Returns policy", "Used goods", "Stochastic demand", "Decentralized channel", "Demand correlation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0785", "e:abstract": "This paper studies the strategic interaction between firms producing strictly complementary products. With strict complements, a consumer derives positive utility only when both products are used together. We show that value-capture and value-creation problems arise when such products are developed and sold by separate firms (“nonintegrated” producers). Although the firms tend to price higher for given quality levels, their provision of quality is so low that, in equilibrium, prices are set well below what an integrated monopolist would choose. When one firm can mandate a royalty fee from the complementor producer (as often occurs in arrangements between hardware and software makers), we find that the value-capture problem is mitigated to some extent and consumer surplus rises. However, because royalty fees greatly reduce the incentives of the firm paying them to invest in quality, the arrangement exacerbates the value-creation problem and leads to even lower total quality. Surprisingly, this result can reverse with competition. Specifically, when the firm charging the royalty fee faces a vertically differentiated competitor, the value-creation problem is greatly reduced---opening the door for the possibility of a Pareto-improving outcome in which all firms and consumers benefit. It is worth noting that this outcome cannot be achieved by giving firms the option of introducing a line of product variants; competition serves as a necessary “commitment” ingredient.", "e:keyword": ["Complementary goods", "Product quality", "Royalty fees", "Competition", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0786", "e:abstract": "As firms become more customer-centric, concepts such as customer equity come to the fore. Any serious attempt to quantify customer equity requires modeling techniques that can provide accurate multiperiod forecasts of customer behavior. Although a number of researchers have explored the problem of modeling customer churn in contractual settings, there is surprisingly limited research on the modeling of usage while under contract. The present work contributes to the existing literature by developing an integrated model of usage and retention in contractual settings. The proposed method fully leverages the interdependencies between these two behaviors even when they occur on different time scales (or “clocks”), as is typically the case in most contractual/subscription-based business settings. We propose a model in which usage and renewal are modeled simultaneously by assuming that both behaviors reflect a common latent variable that evolves over time. We capture the dynamics in the latent variable using a hidden Markov model with a heterogeneous transition matrix and allow for unobserved heterogeneity in the associated usage process to capture time-invariant differences across customers. The model is validated using data from an organization in which an annual membership is required to gain the right to buy its products and services. We show that the proposed model outperforms a set of benchmark models on several important dimensions. Furthermore, the model provides several insights that can be useful for managers. For example, we show how our model can be used to dynamically segment the customer base and identify the most common “paths to death” (i.e., stages that customers go through before churn).", "e:keyword": ["Churn", "Retention", "Contractual settings", "Access services", "Hidden Markov models", "RFM", "Latent variable models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0787", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0788", "e:abstract": "Nearly a quarter of all products purchased in U.S. supermarkets and drug stores are store brands (SBs). Although the presence of SBs benefits both consumers and retailers, it is a threat to the dominance of the incumbent national brand manufacturers (NBMs). When considering the potential threat of an SB, an NBM generally pursues one of three strategies: accommodate, displace, or buffer. Under the <i>accommodation</i> strategy, the NBM repositions the products in his existing product line. Under the <i>displacement</i> strategy, the NBM elects to supply the SB to preempt the entry of the SB supplier. Under the <i>buffering</i> strategy, the NBM adds a <i>defender</i> product, which competes with his own product offering and the new SB. Using a game-theoretic model, we consider a market where consumers are heterogeneous in their valuation of product quality and analyze an NBM's response to an SB threat. We focus on two important drivers: the NBM's ability to differentiate on the quality dimensions and his cost advantage over the outside supplier of SB. To completely characterize the NBM's response, we consider two regimes. In the first regime, the NBM is a monopolist producer. In the second regime, the retailer has the added option of procuring an SB product from an independent, nonstrategic SB manufacturer. By comparing the results from both regimes, we develop a descriptive theory that clarifies the incentives of the NBM to accommodate, displace, or buffer. In doing this, we determine how the NBM's whole product portfolio should be designed, i.e., the positioning (quality levels) and prices of all its offerings.", "e:keyword": ["Product line design", "Store brands", "Distribution channels", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0790", "e:abstract": "The information processing literature provides a wealth of laboratory evidence on the effects that the choice task and individual characteristics have on the extent to which consumers engage in alternative-based versus attribute-based information processing. Less attention has been paid to studying how the processing pattern at the point of purchase is associated with a consumer's propensity to buy in shopping settings. To understand this relationship, we formulate a discrete choice model and perform formal model comparisons to distinguish among several possible dependence structures. We consider models involving an existing measure of information processing, PATTERN; a latent variable version of this measure; and several new refinements and generalizations. Analysis of a unique data set of 895 shoppers on a popular electronics website supports the latent variable specification and provides validation for several hypotheses and modeling components. We find a positive relationship between alternative-based processing and purchase, as well as a tendency of shoppers in the lower price category to engage in alternative-based processing. The results also support the case for joint modeling and estimation. These findings can be useful for future work in information processing and suggest that likely buyers can be identified while engaged in information processing prior to purchase commitment, an important first step in targeting decisions.", "e:keyword": ["Information processing", "Discrete choice", "Markov chain Monte Carlo", "Digital strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0791", "e:abstract": "Co-creation, the participation of customers in the design and production of goods and services, has been gaining popularity in recent years. In this research we incorporate firm pricing into the joint production process allowing us to study (1) production externalities between firm and customers, (2) production externalities among customers, and (3) optimal pricing by firms.We show that given a choice, a monopoly firm will opt for co-creation with customers rather than deal with passive price-taking consumers. Furthermore, the firm will increase the effort it devotes to co-creation as the number of potential co-creating customers increases.We show that the profit of a firm facing a centralized pattern of externalities among customers (with an expert, or lead user, in the center) can be higher than its profit when facing a decentralized pattern of externalities among customers and clearly dominates its profit when customers do not have any cross externalities. Thus, we provide a different justification for the use of lead users, one that depends on their network centrality and not on having lower cost, more information, or greater ability than the firm. Because the decentralized pattern has more links than the centralized pattern, our results demonstrate the importance of the pattern of links between customers, and not just their number, in determining the profitability of co-creation.Furthermore, we find that the lead user's externality spillover to other connected users, her neighbors, acts as a force multiplier on the efforts exerted by all participants in equilibrium. Specifically, a higher spillover from the lead user increases the efforts of the firm, the neighbors, and the lead user herself, and this may lead to beneficial outcomes for all.Finally, we show that in co-creation environments, a monopolist firm may benefit by committing to a single price rather than exercising price discrimination. This is because the pricing structure affects customers' incentive to invest effort in the innovation-production stage.", "e:keyword": ["Co-creation", "Customization", "Networks", "Externalities", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0793", "e:abstract": "When we think of colas, Coca-Cola first comes to mind. Products such as Cola-Cola, Tide laundry detergent, and Chapstick lip balm are the prototypical products in their respective categories. For more than three decades, research in consumer psychology has accumulated evidence on how prototypicality influences memory, shapes the composition of consideration set, and affects purchase decision. Yet there is no research on how it changes the competitive behavior of firms in a horizontally differentiated market. For example, some prototypical products are priced lower than other products in their category, whereas in certain other categories the prototypical product is priced higher. We propose a novel model of spatial competition, where the prototypicality of a product influences the probability of the product being included in consumers' consideration sets without affecting its valuation. Using the model, we examine theoretically the impact of prototypicality on the pricing decisions of competing firms. Our analysis shows that when consumer valuations are low, the prototypical product is priced lower than a nonprototypical product and earns more profits. However, when consumer valuations are high, the rank order of the prices of the prototypical product and a nonprototypical product is reversed, but not the order of profits. We subject these predictions to an empirical test. The experimental results lend support for the qualitative predictions of the model.", "e:keyword": ["Prototypical product", "Pricing", "Competition", "Experimental economics", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0794", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0795", "e:abstract": "I measure the spillover effect of intercollegiate athletics on the quantity and quality of applicants to institutions of higher education in the United States—an effect popularly known as the “Flutie effect.” I treat athletic success as a stock of goodwill that decays over time, similar to that of advertising. A major challenge is that privacy laws prevent us from observing information about the applicant pool. I overcome this challenge by using order statistic distribution to infer applicant quality from information on enrolled students. Using a flexible random-coefficients aggregate discrete choice model that accommodates heterogeneity in preferences for school quality and athletic success, as well as an extensive set of school fixed effects to control for unobserved quality in athletics and academics, I estimate the impact of athletic success on applicant quality and quantity. Overall, athletic success has a significant, long-term goodwill effect on future applications and quality. However, students with lower-than-average SAT scores tend to have a stronger preference for athletic success, whereas students with higher SAT scores have a greater preference for academic quality. Furthermore, the decay rate of athletics' goodwill is significant only for students with lower SAT scores, suggesting that the goodwill created by intercollegiate athletics resides more extensively with lower-scoring students than with their higher-scoring counterparts. But, surprisingly, athletic success impacts applications even among academically stronger students.", "e:keyword": ["Advertising", "Choice modeling", "Entertainment marketing", "Heterogeneity", "Panel data", "Structural modeling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0796", "e:abstract": "Social learning can occur when information is transferred from existing customers to potential customers. It is especially important when the information that is conveyed pertains to <i>experience attributes</i>, i.e., attributes of products that cannot be fully verified prior to the first purchase. Experience attributes are prevalent and salient when consumers shop through catalogs, on home shopping networks, and over the Internet. Firms therefore employ creative and sometimes costly methods to help consumers resolve uncertainty; we argue that uncertainty can be partially resolved through social learning processes that occur naturally and emanate from local neighborhood characteristics. Using data from Bonobos, a leading U.S. online fashion retailer, we find not only that local social learning facilitates customer trial but also that the effect is economically important because about half of all trials were partially attributable to it. Merging data from the Social Capital Community Benchmark Survey, we find that neighborhood social capital, i.e., the propensity for neighbors to trust each other and communicate with each other, enhances the social learning process and makes it more efficient. Social capital <i>does not</i> operate on trials directly; rather, it improves the learning process and therefore indirectly drives sales when what is communicated is favorable.", "e:keyword": ["Bayesian learning", "Experience attributes", "Poisson model", "Social capital", "Social learning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0797", "e:abstract": "How do firms develop marketing strategy when consumers seek to satisfy both quality and status-related considerations? We develop an analytical model to study this issue, examining both pricing and product management decisions in markets for conspicuous durable goods. Our analysis yields many interesting and nontrivial insights. First, we demonstrate that high intrinsic quality indirectly generates exclusivity via pricing effects; in turn, this exclusivity generates considerable social payoffs where consumers value status. This insight reverses the direction of causality in the existing literature, wherein only status considerations matter and mere price increases may enhance consumer utility. Second, our dynamic model indicates that where consumers prioritize status benefits, producers incur substantial price depreciation in equilibrium. Third, we examine the product management strategies used by firms to preserve early adopter exclusivity. Finally, we discuss the boundary conditions of our results as well as our results' implications for managerial and policy issues.", "e:keyword": ["Conspicuous consumption", "Status", "Durable goods", "Game theory", "Dynamic pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0798", "e:abstract": "Should a firm favor a weaker or stronger employee in a contest? Despite a widespread emphasis on rewarding the best employees, managers continue to tolerate and even favor poor performers. Contest theory reveals that evenly matched contests are the most intense, which implies that a contest designer can maximize each player's effort by artificially boosting the underdog's chances. We apply this type of “handicapping” to a two-period repeated contest between employees, in which the only information available about their abilities is their performance in the first period. In this setting, employees are strategic and forward looking, such that they fully anticipate the potential impact of the first-period contest result on the second-period contest and thus adjust their behaviors accordingly. The manager also incorporates these strategic behaviors of employees when determining an optimal handicapping policy. If employees' abilities are sufficiently different, favoring the first-period loser in the second period increases the total effort over both periods. However, if abilities are sufficiently similar, we find the opposite result occurs: total effort increases the most in response to a handicapping strategy of favoring the first-period winner.", "e:keyword": ["Game theory", "Contests", "Handicap", "Incentives", "Ratcheting", "Moral hazard"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0800", "e:abstract": "Retailers face the problem of finding the assortment that maximizes category profit. This is a challenging task because the number of potential assortments is very large when there are many stock-keeping units (SKUs) to choose from. Moreover, SKU sales can be cannibalized by other SKUs in the assortment, and the more similar SKUs are, the more this happens. This paper develops an implementable and scalable assortment optimization method that allows for theory-based substitution patterns and optimizes real-life, large-scale assortments at the store level. We achieve this by adopting an attribute-based approach to capture preferences, substitution patterns, and cross-marketing mix effects. To solve the optimization problem, we propose new very large neighborhood search heuristics. We apply our methodology to store-level scanner data on liquid laundry detergent. The optimal assortments are expected to enhance retailer profit considerably (37.3%), and this profit increases even more (to 43.7%) when SKU prices are optimized simultaneously.", "e:keyword": ["Retail assortments", "Optimization", "Product attributes", "Substitution", "Similarity", "Endogeneity", "Heuristics", "Micromarketing", "Pricing", "Hierarchical Bayes", "Gibbs sampling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0801", "e:abstract": "Although store brands (SBs) are becoming increasingly important across the world, their success varies dramatically across consumer packaged goods categories and countries. The purpose of this paper is to provide insight into how such differences in SB success originate. Using a unique data set that combines scanner data for a three- to five-year period with consumer survey data (<i>n</i> = 20,987) for scores of food, household care, and personal care categories from 23 countries around the world, we identify cross-national regularities as to the role of nine manufacturer and retailer factors in explaining SB market share. For each manufacturer and retailer factor, we determine whether it can be part of a global integration strategy, whether it can be included in a local adaptation strategy, or whether it is a candidate for worldwide learning. Our findings have important implications for national brand manufacturers and retailers.", "e:keyword": ["Store brands", "National brands", "Global strategies", "Worldwide learning", "Empirical generalizations", "International marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0802", "e:abstract": "Online advertising campaigns often consist of multiple ads, each with different creative content. We consider how various creatives in a campaign differentially affect behavior given the targeted individual's ad impression history, as characterized by the timing and mix of previously seen ad creatives. Specifically, we examine the impact that each ad impression has on visiting and conversion behavior at the advertised brand's website. We accommodate both observed and unobserved individual heterogeneity and take into account correlations among the rates of ad impressions, website visits, and conversions. We also allow for the accumulation and decay of advertising effects, as well as ad wearout and restoration effects. Our results highlight the importance of accommodating both the existence of multiple ad creatives in an ad campaign and the impact of an individual's ad impression history. Simulation results suggest that online advertisers can increase the number of website visits and conversions by varying the creative content shown to an individual according to that person's history of previous ad impressions. For our data, we show a 12.7% increase in the expected number of visits and a 13.8% increase in the expected number of conversions.", "e:keyword": ["Online advertising", "Advertising response modeling", "Online visit and conversion rates", "Bayesian models", "Targeting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0803", "e:abstract": "Researchers and practitioners devote substantial effort to targeting banner advertisements to consumers, but they focus less effort on how to communicate with consumers once targeted. Morphing enables a website to learn, automatically and near optimally, which banner advertisements to serve to consumers to maximize click-through rates, brand consideration, and purchase likelihood. Banners are matched to consumers based on posterior probabilities of latent segment membership, which are identified from consumers' clickstreams.This paper describes the first large-sample random-assignment field test of banner morphing---more than 100,000 consumers viewed more than 450,000 banners on CNET.com. On relevant Web pages, CNET's click-through rates almost doubled relative to control banners. We supplement the CNET field test with an experiment on an automotive information-and-recommendation website. The automotive experiment replaces automated learning with a longitudinal design that implements morph-to-segment matching. Banners matched to cognitive styles, as well as the stage of the consumer's buying process and body-type preference, significantly increase click-through rates, brand consideration, and purchase likelihood relative to a control. The CNET field test and automotive experiment demonstrate that matching banners to cognitive-style segments is feasible and provides significant benefits above and beyond traditional targeting. Improved banner effectiveness has strategic implications for allocations of budgets among media.", "e:keyword": ["Online advertising", "Banner advertising", "Behavioral targeting", "Context matching", "Website morphing", "Cognitive styles", "Field experiments", "Electronic marketing", "Dynamic programming", "Bandit problems", "Strategic optimization of marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0804", "e:abstract": "This paper examines the phenomenon of profit-increasing consumer exit and the related phenomenon of profit-decreasing consumer entry. We demonstrate that firms can be better off in shrinking markets and worse off in growing markets, even in the absence of competitive entry or exit. Specifically, firms may benefit if a segment of consumers who are relatively indifferent about consuming any product in the category leave the market. Profits can increase for all firms even if the exiting consumers have strong preferences for only one of the products in the market. In shrinking markets, it is reasonable to assume that the people who are likely to exit the market first are people who are “least committed” to the category. In particular, people who are the least satisfied with the existing offers are the most likely to change their behavior by finding an alternative or adopting a new technology. Similarly, in growing markets, consumers who enter the market late are generally the least committed to the category. Such exiting can relax the competitive pressure between firms and lead to increased profitability. Our findings provide an explanation for profit growth that has been observed in product industries exhibiting slow and predictable declines over time, including vacuum tubes, cigarettes, and soft drinks.", "e:keyword": ["Competitive analysis", "Analytic models", "Game theory", "Market evolution"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0805", "e:abstract": "Learning models extend the traditional discrete choice framework by postulating that consumers have incomplete information about product attributes and that they learn about these attributes over time. In this survey we describe the literature on learning models that has developed over the past 20 years, using the model of Erdem and Keane as a unifying framework [Erdem T, Keane M (1996) Decision-making under uncertainty: Capturing dynamic brand choice processes in turbulent consumer goods markets. <i>Marketing Sci.</i> 15(1):1--20]. We describe how subsequent work has extended their modeling framework and applied learning models to a wide range of different products and markets. We argue that learning models have contributed greatly to our understanding of consumer behavior---in particular, in enhancing our understanding of brand loyalty and long-run advertising effects. We also discuss the limitations of existing learning models and potential extensions. One key challenge is to disentangle learning as a source of dynamics from other key mechanisms that may generate choice dynamics (inventories, habit persistence, etc.). Another is to enhance identification of learning models by collecting and using direct measures of signals, perceptions, and expectations.", "e:keyword": ["Learning models", "Choice modeling", "Dynamic programming", "Structural models", "Brand equity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0806", "e:abstract": "We examine whether cobranding---the practice of using two established brand names on the same product---increases the market value of parent firms. Using data from the consumer packaged goods industry, we document that the average stock market reaction to the announcement of cobranded new products is approximately +1.0%. We hypothesize that this reaction is significantly higher than it would have been if these same products were single branded, and we find evidence consistent with this hypothesis. We also examine the determinants of this stock market reaction. We find that the consistency between the two brand images, the innovativeness of the product, and the exclusivity of the cobranding relationship significantly increase the market reaction to cobranding announcements. Our findings provide important managerial guidelines for enhancing firm value through cobranding partnerships.", "e:keyword": ["Cobranding", "New products", "Stock market reaction", "Marketing alliances", "Propensity score matching"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0807", "e:abstract": "When a television advertisement causes viewers to switch channels, it reduces the audience available to subsequent advertisers. This audience loss is not reflected in the advertisement price, resulting in an audience externality. The present article analyzes the television network's problem of how to select, order, and price advertisements in a break of endogenous length in order to correct audience externalities. It proposes the Audience Value Maximization Algorithm (AVMA), which considers many possible advertisement orderings within a dynamic programming framework with a strategy-proof pricing mechanism. Two data sets are used to estimate heterogeneity in viewer-switching probabilities and advertiser willingness-to-pay parameters in order to evaluate the algorithm's performance. A series of simulations shows that AVMA typically maximizes audience value to advertisers, increases network revenue relative to several alternatives, and runs quickly enough to implement.", "e:keyword": ["Advertising", "Advertising avoidance", "Media", "Television", "Pricing", "Externalities"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0809", "e:abstract": "Online freelance marketplaces are websites that match buyers of electronically deliverable services with freelancers. Although freelancing has grown in recent years, it faces the classic “information asymmetry” problem---buyers face uncertainty over seller quality. Typically, these markets use reputation systems to alleviate this issue, but the effectiveness of these systems is open to debate. We present a dynamic structural framework to estimate the returns to seller reputations in freelance sites. In our model, a buyer decides in each period whether to choose a bid from her current set of bids, cancel the auction, or wait for more bids. In the process, she trades off sellers' price, reputation, and other attributes, as well as the costs of waiting and canceling. Our framework addresses <i>dynamic selection</i>, which can lead to underestimation of reputation, through two types of persistent unobserved heterogeneities: bid arrival rates and buyers' unobserved preference for bids. We apply our framework to data from a leading freelance firm. We find that buyers are forward looking, that they place significant weight on seller reputation, and that not controlling for dynamics and selection can bias reputation estimates. Using counterfactual simulations, we infer the dollar value of seller reputations and provide guidelines to managers of freelance firms.", "e:keyword": ["Structural models", "Dynamic programming", "Empirical IO methods", "Auctions", "Online reputation systems", "Mixture models", "Dynamic selection", "Freelance marketplaces"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0810", "e:abstract": "Several key questions in bundling have not been empirically examined in marketing: Is mixed bundling more effective than pure bundling or pure components? Does correlation in consumer valuations make bundling more or less effective? Does bundling serve as a complement or substitute to network effects? To address these questions, we develop a consumer-choice model from microfoundations to capture the essentials of our setting, the handheld video game market. We provide a framework to understand the dynamic, long-term effects of bundling on demand. The primary explanation for the profitability of bundling relies on homogenization of consumer valuations for the bundle, allowing the firm to extract more surplus. We find that bundling can be effective through a novel and previously unexamined mechanism of <i>dynamic consumer segmentation</i>, which operates independent of the homogenization effect, and can in fact be stronger when the homogenization effect is weaker. We also find that bundles are treated as separate products (distinct from component products) by consumers. Sales of both hardware and software components decrease in the absence of bundling, and consumers who had previously purchased bundles might delay purchases, resulting in lower revenues. We also find that mixed bundling dominates pure bundling and pure components in terms of both hardware and software revenues. Investigating the link between bundling and indirect network effects, we find that they act as substitute strategies, with a lower relative effectiveness for bundling when network effects are stronger.", "e:keyword": ["Complementary goods", "Product strategy", "Bundling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0811", "e:abstract": "This paper investigates how individuals' product choices are influenced by the product choices of their connected others and how the influence mechanism may differ for fashion- versus technology-related products. We conduct a novel field experiment to induce and observe choice interdependence in a closed social network. In our experiment, we conceptualize individuals' choices to be driven by multiattribute utilities, and we measure their initial attribute preferences prior to observing their choice interdependence and collecting network information. These design elements help alleviate concerns in identifying social interaction effects from other confounds. Given that we have complete information on choices and their sequence, we use a discrete-time Markov chain model. Nonetheless, we also use a Markov random field (MRF) model as an alternative when the information on choice sequence is missing. We find significant social interaction effects. Our findings show that whereas experts exert asymmetrically greater influence on a technology-related product, popular individuals exert greater influence on a fashion-related product. In addition, we find choices made by early decision makers to be more influential than choices made later for the technology-related product. Finally, using the MRF with snapshot data can also provide good out-of-sample predictions for a technology-related product.", "e:keyword": ["Social interactions", "Social network", "Social influence", "Homophily", "Conjoint experiment", "Markov random field"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0812", "e:abstract": "Paid search has become the mainstream platform for online advertising, further intensifying competition between advertisers. The main objective of this research is twofold. On the one hand, we want to understand, in the context of paid-search advertising, the effects of competition (measured by the number of ads on the paid-search listings) on click volume and the cost per click (CPC) of paid-search ads. On the other hand, we are interested in understanding the determinants of competition, that is, how various demand and supply factors affect the entry probability of firms and, consequently, the total number of entrants for a keyword. We regard each keyword as a market and build an integrative model consisting of three key components: (i) the realized click volume of each entrant as a function of the baseline click volume and the decay factor; (ii) the vector of realized CPCs of those entrants as a function of the decay factor and the order statistics of the value per click at an equilibrium condition; and (iii) the number of entrants, the product of the number of potential entrants multiplied by the entry probability; the entry probability is determined by the expected revenue (a function of expected click volume, CPC, and value per click) and the entry cost at the equilibrium condition of an incomplete information game. The proposed modeling framework entails several econometric challenges. To cope with these challenges, we develop a Bayesian estimation approach to make model inferences. Our proposed model is applied to a data set of 1,597 keywords associated with digital camera/video and their accessories with full information on competition. Our empirical analysis indicates that the number of competing ads has a significant impact on the baseline click volume, decay factor, and value per click. These findings help paid-search advertisers assess the impact of competition on their entry decisions and advertising profitability. In the counterfactual analysis, we investigate the profit implication of two polices for the paid-search host: raising the decay factor by encouraging consumers to engage in more in-depth search/click-through and providing coupons to advertisers.", "e:keyword": ["Paid-search advertising", "Competition", "Internet marketing", "Bayesian estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0813", "e:abstract": "Market growth is fundamental to marketing. Frank Bass's seminal diffusion theory explains growth in new product markets. We develop an analogous theory for established markets exhibiting sporadic growth or intermittent declines.Our theory suggests that market participants repeatedly take successful and unsuccessful actions that cause them to change or to mutate in myriad and often unpredictable ways. The environment sorts these mutations, determining winners and losers. Abundant mutations often cause different market participants to become winners, displacing past winners. Abundant mutations also often cause market growth because the natural selection mechanism leaves more surviving favorable mutations. So one nonobvious falsifiable implication of our theory is that displacement precedes growth and stability precedes decline. Another is that risk taking, diversity of opinions, and experimentation should precede growth.We develop a metric for measuring displacement. Using multiple publicly available data sets (one including sales for top firms for 55 years and another including sales for all automobile models for 25 years), we find that our metric provides a practical way to measure the rate of mutation and confirm our theory's predictions. Our easily replicated tests show that our displacement metric can predict intermittent market growth or decline in very different contexts without the need for exogenous idiosyncratic explanations. Moreover, other alternative covariates (trends, lagged growth, new product entry, macroeconomic indicators, etc.) are unable to predict growth or decline.", "e:keyword": ["Market growth", "Growth theory", "Natural selection", "Market evolution", "Forecasting", "Competitive analysis", "Market decline", "Malthusian competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0814", "e:abstract": "An examination of brand prices in several categories reveals that the distribution of prices is multimodal, with firms offering shallow and deep discounts. Another interesting feature of these distributions is that they may have holes in the interior of the support. These pricing distributions do not occur in extant theoretical models of price promotions. We develop a dynamic model of competition in which some price-sensitive consumers stockpile during periods of deep discounts. A game-theoretic analysis of our model generates a multimodal pricing distribution with a hole in the interior of the support. Consumer stockpiling in our model also gives rise to negative serial correlation in prices. This is consistent with our empirical observation of the pricing distribution of several brands across multiple categories in the IRI marketing data set.We generate several interesting insights into firms' optimal promotional strategies and their interplay with the clientele mix, market structure, and other market factors. We find that, in equilibrium, stockpiling by price-sensitive consumers neither harms nor benefits firms when they adopt equilibrium strategies. Interestingly, when price-sensitive consumers stockpile, even increased consumption as a result of stockpiling does not lead to higher profits for firms.", "e:keyword": ["Promotional strategies", "Consumer stockpiling", "Endogenous stockpiling threshold", "Interior modes", "Multiple modes", "Hole in distribution", "Negative serial correlation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0815", "e:abstract": "We estimate a dynamic structural model of sales force response to a bonus-based compensation plan. This paper provides substantive insight into how different elements of the compensation plan enhance productivity. We find evidence that (1) bonuses enhance productivity across all segments; (2) overachievement commissions help sustain the high productivity of the best performers, even after attaining quotas; and (3) quarterly bonuses help improve performance of the weak performers by serving as pacers to keep the sales force on track in achieving its annual sales quotas. The paper also introduces two main methodological innovations to the marketing literature: First, we implement empirically the method proposed by Arcidiacono and Miller [Arcidiacono P, Miller RA (2011) Conditional choice probability estimation of dynamic discrete choice models with unobserved heterogeneity. <i>Econometrica</i> 79(6):1823--1867] to accommodate unobserved latent-class heterogeneity using a computationally light two-step estimator. Second, we illustrate how discount factors can be <i>estimated</i> in a dynamic structural model using field data through a combination of (1) an exclusion restriction separating current and future payoff and (2) a finite-horizon model in which there is no forward-looking behavior in the last period.", "e:keyword": ["Sales force compensation", "Bonuses", "Quotas", "Dynamic structural models", "Two-step estimation", "Discount factors"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0816", "e:abstract": "A manufacturer will often limit competition among downstream partners by authorizing only a select group of retailers to carry its product. However, it is not uncommon for authorized retailers to create an additional competitor by diverting units to an unauthorized seller. This paper presents an analytical model that demonstrates how diversion from authorized retailers to an unauthorized direct competitor can occur under circumstances not considered by the prior literature. In fact, diversion can represent a prisoner's dilemma whereby retailers diminish their own profit by selling to the unauthorized direct seller. The authorized retailer's profit loss actually increases as the per-unit diversion costs incurred by the authorized retailer decrease. The model also shows that the unauthorized direct seller earns greater profit by strategically procuring a unilaterally constraining quantity, even though this procurement strategy results in an equivalent increase in the quantity sold by the retailers. Combined, the results identify a new reason for diversion and its consequences for retailers and the unauthorized direct seller.", "e:keyword": ["Pricing", "Gray markets", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0817", "e:abstract": "With the availability of social network data, it has become possible to relate the behavior of individuals to that of their acquaintances on a large scale. Although the similarity of connected individuals is well established, it is unclear whether behavioral predictions based on social data are more accurate than those arising from current marketing practices. We employ a communications network of over 100 million people to forecast highly diverse behaviors, from patronizing an off-line department store to responding to advertising to joining a recreational league. Across all domains, we find that social data are informative in identifying individuals who are most likely to undertake various actions, and moreover, such data improve on both demographic and behavioral models. There are, however, limits to the utility of social data.In particular, when rich transactional data were available, social data did little to improve prediction.", "e:keyword": ["Social networks", "Targeting", "Electronic commerce", "Homophily", "Product", "Computational social science"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0818", "e:abstract": "Firms constantly grapple with the question of whether to make, buy, or ally for innovations. The literature has not, to our knowledge, analyzed the choice of and payoff from these alternate routes to innovation for the same firm. To address this issue, we collect, code, and analyze the choice of and payoff from 3,522 announcements of make, buy, and ally for 192 firms across 108 industries over five years.We find that announcements to make or ally generate positive and higher payoffs than announcements to buy, which generate negative payoffs. Nevertheless, firms continue to buy for two reasons. First, firms seem to have no memory of the payoff from buy, even though they have a memory of the payoff from make. Second, firms tend to buy when they lack commercializations, even though this strategy does not always seem to pay off. These results suggest that firms see buy as a signal to investors that they have a solution for what may be a deep strategic problem. Nevertheless, the negative returns to a buy can be mitigated if the acquirer is experienced, and the target is related and offers high customer benefit. We offer explanations for and implications of the results.", "e:keyword": ["Innovation", "Announcements", "Make", "Buy", "Ally", "Content analysis", "Event study", "Stock market returns"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0820", "e:abstract": "We study the relative importance of online word of mouth and advertising on firm performance over time since product introduction. The current research separates the <i>volume</i> of consumer-generated online word of mouth (OWOM) from its <i>valence</i>, which has three dimensions---attribute, emotion, and recommendation oriented. Firm-initiated advertising content is also classified as attribute or emotion advertising. We also shed light on the role played by advertising content on generating the different types of OWOM conversations. We use a dynamic hierarchical linear model (DHLM) for our analysis. The proposed model is compared with a dynamic linear model, vector autoregressive/system of equations model, and a generalized Bass model. Our estimation accounts for potential endogeneity in the key measures. Among the different OWOM measures, only the valence of recommendation OWOM is found to have a direct impact on sales; i.e., not all OWOM is the same. This impact increases over time. In contrast, the impact of attribute advertising and emotion advertising decreases over time. Also, consistent with prior research, we observe that rational messages (i.e., attribute-oriented advertising) wears out a bit faster than emotion-oriented advertising. Moreover, the volume of OWOM does not have a significant impact on sales. This suggests that, in our data, “what people say” is more important than “how much people say.” Next, we find that recommendation OWOM valence is driven primarily by the valence of attribute OWOM when the product is new and driven by the valence of emotion OWOM when the product is more mature. Our brand-level results help us classify brands as consumer driven or firm driven, depending on the relative importance of the OWOM and advertising measures, respectively.", "e:keyword": ["Dynamic hierarchical model", "Endogeneity", "Online word of mouth", "Attributes", "Emotions", "Recommendations", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0821", "e:abstract": "Our Tablet Computer data set, collected from various websites, contains market dynamics related to 2,163 products, characteristics of 794 products, more than 40,000 consumer-generated product reviews, and information about 39,278 reviewers. The market dynamic information was collected weekly for 24 weeks starting February 1, 2012. Our Tablet Computer data set comprises four tables: the Market Dynamics of Products, Product Characteristic Information, Consumer-Generated Product Reviews, and Reviewer Information tables. In turn, it offers three unique properties. First, it contains both structured product information and unstructured product reviews. Second, it comprises product characteristic information and market dynamic information. Third, this data set integrates user-generated content with manufacturer-provided content. This integrated data set (available at <ext-link ext-link-type=\"uri\"  href=\"http://pubsonline.informs.org/page/mksc/online-databases\">http://pubsonline.informs.org/page/mksc/online-databases</ext-link>) is valuable for both academics and practitioners who conduct research related to marketing, information systems, computer science, and other fields using digital data readily available through the Internet.", "e:keyword": ["Market dynamics", "Online product reviews", "Tablet computers", "User-generated content"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0822", "e:abstract": "It has been argued that retailers lack both the resources and capabilities to maximize category performance. Retailers may seek category management (CM) advice from a manufacturer, referred to as a category captain (CC). A CC's recommendations affect all brands in the category, not just her own. Despite an increase in the number of CC collaborations, retailers are still concerned about manufacturer opportunism and militant behavior by manufacturers not selected as CCs, whereas government agencies are worried about anticompetitive behavior that could harm consumers. The Federal Trade Commission recommends strictly enforced information firewalls within a CC's organization as a best-practice guideline. In this study we develop an empirical model and use policy simulations to quantify the impact of CC arrangements with information firewalls on retailers, manufacturers, and consumers. We show how these effects could be influenced by the (de)activation of vertical and horizontal information firewalls within the CC's organization.", "e:keyword": ["Category captains", "Information firewalls", "Public policy", "Retail pricing", "Price coordination", "Consumer welfare", "Competition", "Category management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0823", "e:abstract": "Firms and organizations often need to collect and analyze sensitive consumer data. A common problem encountered in such evidence-based research is that they cannot collect all essential information from one sample, and they may need to link nonoverlapping data items across independent samples. We propose an automated nonparametric data fusion solution to this problem. The proposed methods are not restricted to specific types of variables and distributions. They require no prior knowledge about how data at hand may behave differently from standard theoretical distributions, and they automate the process of generating suitable distributions that match data, therefore making our methods particularly useful for linking data with complex distributional shapes. In addition, these methods have strong theoretical support; permit highly efficient direct fusion to relate a mixture of continuous, semicontinuous, and discrete variables; and enable nonparametric identification of entire distributions of fusion variables, including higher moments and tail percentiles. These novel and promising features overcome important limitations of existing methods and have the potential to increase fusion effectiveness. We apply the proposed methods to overcome data constraints in a study of counterfeiting. By combining data sets from multiple sources, data fusion provides a feasible approach to studying the relationship between counterfeit purchases and various marketing elements, such as consumers' purchase motivations, behaviors, and attitudes; brand marketing channels; promotions; and advertisements. Therefore, data fusion sheds light on counterfeit purchase behaviors and suggests ways to counter counterfeits that would not be available if these data sets were analyzed separately.", "e:keyword": ["Counterfeit", "CRM", "Database marketing", "Nonparametric method", "Sensitive data", "Underground economics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0824", "e:abstract": "In this paper, we study the entry and expansion decisions of McDonald's and KFC in China using an originally assembled data set on the two chains' expansion in the China market from their initial entry up to year 2007. We analyze how the presence of a rival affects each firm's strategies. The results indicate that a rival's presence has a net positive effect on a chain's expansion decision. We focus on testing two possible explanations for a positive rival impact: market learning and demand expansion. First, we derive a set of theoretical predictions on how a chain's optimal expansion decision would react to its rival's expansion patterns when market learning versus demand expansion is the driving force of the rival's positive influence. The empirical analysis based on these predictions consistently suggests that market learning is more likely to explain the positive effect of KFC on McDonald's and that demand expansion is more plausible with McDonald's positive spillover on KFC. In other words, the results are consistent with the presence of KFC signaling market demand potential and growth to McDonald's and the presence of McDonald's helping to cultivate consumer taste and generate demand for Western fast food, which benefits KFC.", "e:keyword": ["Entry", "Market learning", "Demand expansion", "Emerging market", "Fast-food chains"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0825", "e:abstract": "When managers and researchers encounter a data set, they typically ask two key questions: (1) Which model (from a candidate set) should I use? And (2) if I use a particular model, when is it going to likely work well for my business goal? This research addresses those two questions and provides a rule, i.e., a decision tree, for data analysts to portend the “winning model” <i>before</i> having to fit any of them for longitudinal incidence data. We characterize data sets based on managerially relevant (and easy-to-compute) summary statistics, and we use classification techniques from machine learning to provide a decision tree that recommends when to use which model. By doing the “legwork” of obtaining this decision tree for model selection, we provide a time-saving tool to analysts. We illustrate this method for a common marketing problem (i.e., forecasting repeat purchasing incidence for a cohort of new customers) and demonstrate the method's ability to discriminate among an integrated family of a hidden Markov model (HMM) and its constrained variants. We observe a strong ability for data set characteristics to guide the choice of the most appropriate model, and we observe that some model features (e.g., the “back-and-forth” migration between latent states) are more important to accommodate than are others (e.g., the inclusion of an “off” state with no activity). We also demonstrate the method's broad potential by providing a general “recipe” for researchers to replicate this kind of model classification task in other managerial contexts (outside of repeat purchasing incidence data and the HMM framework).", "e:keyword": ["Model selection", "Machine learning", "Data science", "Business intelligence", "Hidden Markov models", "Classification tree", "Random forest", "Posterior predictive model checking", "Hierarchical Bayesian methods", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0826", "e:abstract": "Sellers often make claims about product strengths without providing evidence. Even though such claims are mere puffery, we show that they can be credible because talking up any one strength comes at the implicit trade-off of not talking up another potential strength. Puffery pulls in some buyers who value product attributes that are talked up or emphasized while pushing away other buyers who infer that the attributes they value are relative weaknesses. When the initial probability of making a sale is low, there are more potential buyers to pull in than to push away, so puffery is persuasive overall. This persuasiveness requires that buyers have some privacy about their preferences so that the seller does not completely pander to them. More generally, the results show how comparative cheap talk by an expert to a decision maker can be credible and persuasive in standard discrete choice models used throughout marketing, economics, and other disciplines.", "e:keyword": ["Cheap talk", "Sales talk", "Comparative advertising", "Negative advertising", "Unique selling point", "Targeting", "Privacy", "Pandering"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0827", "e:abstract": "I develop a dynamic investment game with a “memoryless” research and development process in which an incumbent and an entrant can invest in a new technology, and the entrant can also invest in the old technology. I show that an increase in the probability of successfully implementing a technology can cause the incumbent to <i>reduce</i> its investment. Under certain conditions, if the success probability is high, the incumbent allows the entrant to win the new technology so that firms reach an equilibrium in which they use different technologies, and threats of retaliation prevent attacks; but if the success probability is low, such an equilibrium cannot be sustained, and both firms eventually implement both technologies.", "e:keyword": ["New product development", "Defensive strategy", "Markov perfect equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0828", "e:abstract": "This study examines the dynamics of online buzz over time before product release. Employing functional data analysis, we treat the curve of prerelease buzz evolution trajectory as the unit of analysis and find that the shape of the curve significantly adds power in predicting new product performance compared with using product characteristics and firm advertising alone. Moreover, daily prerelease buzz evolution data enable accurate sales forecasting long before product release, which allows sufficient time for managers to adjust product design and/or marketing strategy. For example, the forecasting accuracy using an early buzz evolution curve ending on the 61st day before product release is not only higher than that using accumulated buzz volume until then but also higher than that using the total volume of all buzz up until product release. Beyond the sales outcome, we find that prerelease buzz is quickly reflected in firm stock returns before product release and reduces the absolute amount of postrelease stock price correction. The model accounts for endogeneity, and the results are robust after controlling for buzz sentiment. We also explore the factors influencing prerelease buzz evolution patterns, thus generating insights into how to manage prerelease buzz dynamics to enhance new product performance.", "e:keyword": ["Prerelease buzz dynamics", "Evolution pattern", "Functional data analysis", "Forecasting", "New product sales", "Stock market value"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0829", "e:abstract": "Disaggregate demand in the marketplace exists on a grid determined by the package sizes offered by manufacturers and retailers. Although consumers may want to purchase a continuous-valued amount of a product, realized purchases are constrained by available packages. This constraint might not be problematic for high-volume demand, but it is potentially troubling when demand is small. Despite the prevalence of packaging constraints on choice, economic models of choice have been slow to deal with their effects on parameter estimates and policy implications. In this paper we propose a general framework for dealing with indivisible demand in economic models of choice, and we show how to estimate model parameters using Bayesian methods. Analyses of simulated data and a scanner-panel data set of yogurt purchases indicate that ignoring packaging constraints can bias parameter estimates and measures of model fit, which results in the inaccurate measures of metrics such as price elasticity and compensating value. We also show that a portion of nonpurchase in the data (e.g., 2.27% for Yoplait Original) reflects the restriction of indivisibility, not the lack of preference. The importance of demand indivisibility is also highlighted by the counterfactual study where the removal of the smallest package size (i.e., 4 oz) mainly results in nonpurchase in the yogurt category instead of switching to larger package sizes.", "e:keyword": ["Direct utility model", "Bayesian error augmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0830", "e:abstract": "This special section is the result of an effort by several scholars to move marketing academic research toward greater practical relevance. This initiative, called Theory + Practice in Marketing (TPM), started with a conference at Columbia Business School in 2011, and the five papers published in this special section were presented at the second TPM conference held at Harvard Business School in 2012.", "e:keyword": ["Theory and practice", "Store brands", "Banner ads", "Market growth", "Retail price", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0831", "e:abstract": "We study how peers impact worker productivity growth among salespeople in the cosmetics department of a department store. We first exploit a shift assignment policy that creates exogenous variation in salespersons' peers each week to identify and quantify sources of worker learning. We find that peer-based learning is more important than learning-by-doing for individuals, and there is no evidence of forgetting. Working with high-ability peers substantially increases the long-term productivity growth of new salespeople. We then examine possible mechanisms behind peer-based learning by exploiting the multiple colocated firms in our setting that sell products with different task difficulties and compensate their sales forces using either team-based or individual-based compensation systems. The variation in incentives to compete and cooperate within and across firm boundaries, combined with variation in sales difficulty for different product classes, allows us to suggest two mechanisms behind peer-based learning: observing successful sales techniques of peers and direct teaching. Our paper advocates the importance of learning from one another in the workplace and suggests that individual peer-based learning is a foundation of both organizational learning curves and knowledge spillovers across firms.", "e:keyword": ["Sales management", "Sales force", "Personal selling", "Retailing", "Social interactions", "Knowledge transfer", "Learning models", "Social learning", "Peer-based learning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0832", "e:abstract": "Customer base analysis is a key element in customer valuation and can provide guidance for decisions such as resource allocation. Yet extant models often focus on a single activity, such as purchases from a retailer or donations to a nonprofit organization. These models do not consider other ways that an individual may engage with an organization, such as purchasing in multiple brands or contributing user-generated content. In this research, we propose a framework to generalize extant models for customer base analysis to multiple activities.Using the data from a website that allows users to purchase digital content and/or post digital content at no charge, we develop a flexible “buy ‘til you die” model to empirically examine how the two activities are related. Compared with benchmarks, our model more accurately forecasts the future behavior for both types of activities. In addition to finding evidence of coincidence between the activities while customers are “alive,” we find that the latent attrition processes are related. This suggests that conducting one type of activity is informative of whether customers are still alive to conduct another type of activity and, consequently, affects inferences of customer value.", "e:keyword": ["Customer base analysis", "Latent changepoint model", "Multivariate choice model", "Multivariate “buy ‘til you die” model", "Digital content"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0833", "e:abstract": "We identify the conditions under which a problem of optimal advance selling strategy can be mathematically transformed into a problem of optimal bundle pricing. These conditions are as follows: (i) consumers and sellers have common priors on the probability of each state being realized in the future, (ii) consumers are risk-neutral, (iii) sellers can commit to spot prices, and (iv) consumers and sellers discount the future at the same rate. The result allows both researchers and practitioners to extend and/or apply the findings from the vast literature on bundling to advance selling problems, and vice versa. We highlight several insights that are particularly relevant, such as the importance of the dependence of consumer valuations across states on the profitability of advance selling in the base case of two states as well as in the cases of more than two states or with possible competition in some of the states.", "e:keyword": ["Advance selling discounts", "Bundling", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0834", "e:abstract": "In search advertising, brand names are often purchased as keywords by the brand owner or a competitor. We aim to understand the strategic benefits and costs of a firm buying its own brand name or a competitor's brand name as a keyword. We model the effect of search advertising to depend on the presence or absence of a competitor's advertisement on the same results page. We find that the quality difference between the brand owner and the competitor moderates the purchase decision of both firms. Interestingly, in some cases, a firm may buy its own brand name only to defend itself from the competitor's threat. It is also possible that the brand owner, by buying its own branded keyword, precludes the competitor from buying the same keyword. Our result also implies that the practice of bidding on the competitor's brand name creates a prisoner's dilemma, and thus both firms may be worse off, but the search engine captures the lost profits. We also discuss the difference in our results when the search is for a generic keyword instead of a branded keyword. Finally, we find some empirical support for our theory from the observation of actual purchase patterns on Google AdWords.", "e:keyword": ["Search advertising", "Branded keywords", "Brand advertising", "Keyword selection", "Analytical model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0835", "e:abstract": "Researchers often collect continuous consumer feedback (moment-to-moment, or MTM, data) to understand how consumers respond to a variety of experiences (e.g., viewing a TV show, undergoing a colonoscopy). Analyzing how MTM judgments are integrated into overall evaluations allows researchers to determine how the structure of an experience influences consumers' post-experience satisfaction. However, this analysis is challenging because of the functional nature of MTM data. As such, previous research has typically been limited to identifying the influence of heuristics, such as relying on the average intensity, peak, and ending.We develop a Bayesian functional linear model to study how the different “moments” in the MTM data contribute to the overall judgment. Our approach incorporates a (temporally) weighted average of MTM data as well as specific “patterns” such as peak and trough, thus nesting previous approaches such as the “peak-end” rule as special cases. We apply our methodology to analyze data on TV show pilots collected by CBS. Our results reveal several interesting empirical findings. First, the last quintile of a TV show is weighted about four times as much as each of the first four quintiles. Second, patterns such as peak and trough do not play substantial roles in driving overall evaluations for TV shows. Finally, the last quintile is more important for procedural dramas than for serial dramas. We discuss the managerial implications of our results and other potential applications of our general methodology.", "e:keyword": ["Moment-to-moment data", "Functional data analysis", "Bayesian functional linear model", "TV show pilot testing", "Peak-end rule"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0836", "e:abstract": "The nature of marketing science is changing in a systematic, predictable, and irrevocable way. As information technology enables ubiquitous customer communication and big customer data, the fundamental nature of the firm's connection to the customer changes: better, more personalized service can be offered, from which service relationships are deepened, and consequently, more profitable customers grow the influence of service within the goods sector and expand the service sector in the economy. Marketing is becoming more personalized, and marketing science techniques that exploit customer heterogeneity are becoming more important. Information technology improvements also guarantee the increasing importance and usage of computationally intensive data processing and “big data.” Most importantly, these trends have already lasted for more than a century, and they will become even more pronounced in the coming years as a result of the monotonic nature of technology improvement. These changes imply a transformation of marketing science in both the topics to be emphasized and the methods to be employed. Increasingly, and inevitably, all of marketing will come to resemble to a greater degree the formerly specialized area of service marketing, only with an increased emphasis on marketing analytics.", "e:keyword": ["Service", "Customer lifetime value", "Customer loyalty", "Customer relationship management", "Customization", "Information technology", "Service productivity", "Customer equity", "Big data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0837", "e:abstract": "Human faces are used extensively in print advertisements. In prior literature, researchers have studied spokespersons in general, but few have studied faces explicitly. This paper aims to answer three questions that are important to both researchers and practitioners: (1) Do faces affect how a viewer reacts to an advertisement on the metrics that advertisers care about? (2) If faces do have an effect, is it large enough to warrant careful selection of faces when constructing print advertisements? (3) If faces do have an effect and the effect is large, what facial features elicit such differential reactions on these metrics, and are such reactions different across individuals and/or product categories? Relying on the eigenface method, a holistic approach widely used in the computer science field for face recognition, we conducted an empirical study to answer these three questions. The results show that different faces do have an effect on people's attitude toward the advertisement, attitude toward the brand, and purchase intention and that the effect is nontrivial. Multiple segments were identified and substantial differences were found among people's reactions to the faces in the ads across those segments. We also found that the effect of faces interacts with product categories and is mediated by various facial traits such as attractiveness, trustworthiness, and competence. Implications and directions for future research are discussed.", "e:keyword": ["Face", "Facial features", "Advertising effectiveness", "Eigenface"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0838", "e:abstract": "Traditional advertising, such as TV and print advertising, primarily builds awareness of a firm's product among consumers, whereas sponsored search advertising on a search engine can target consumers closer to making a purchase because they reveal their interest by searching for a relevant keyword. Increased consumer targetability in sponsored search advertising induces a firm to “poach” a competing firm's consumers by directly advertising on the competing firm's keywords; in other words, the poaching firm tries to obtain more than its “fair share” of sales through sponsored search advertising by free riding on the market created by the firm being poached. Using a game theory model with firms of different advertising budgets, we study the phenomenon of poaching, its impact on how firms allocate their advertising budgets to traditional and sponsored search advertising, and the search engine's policy on poaching. We find that, as budget asymmetry increases, the smaller-budget firm poaches more on the keywords of the larger-budget firm. This may induce the larger-budget firm to allocate more of its budget to traditional advertising, which, in turn, hurts the search engine's advertising revenues. Therefore, paradoxically, even though poaching increases competition in sponsored search advertising, the search engine can benefit from limiting the extent of poaching. This explains why major search engines use “ad relevance” measures to handicap poaching on trademarked keywords.", "e:keyword": ["Online advertising", "Paid search", "Poaching", "Keyword relevance score", "Competitive strategy", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0840", "e:abstract": "In a dynamic model with overlapping generations of consumers, we study duopolistic competition when firms can price discriminate, at each period, between their previous customers and the consumers that they have never served. Long-term contracts are not enforceable. In (Markov-perfect) equilibrium, one firm charges higher prices to its past customers than to its new customers, as past customers have revealed their strong preferences for the firm; the other firm, however, rewards its previous customers by charging lower prices to them than to its new customers. This loyalty reward strategy comes from the interplay between the firms’ usual incentive to extract surplus from consumers with revealed strong preferences and their incentives to acquire information and to recognize their young loyal customers. The result also relies on the firms’ inability a priori to tell different generations apart. It is the outcome of the unique equilibrium of a simplified two-period (or <i>T</i>-period) version of the game and holds with forward-looking consumers who are impatient enough.", "e:keyword": ["Marketing strategy", "Pricing", "Customer relationship", "Customer loyalty", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0841", "e:abstract": "Marketing managers often use consumer attitude metrics such as awareness, consideration, and preference as performance indicators because they represent their brand's health and are readily connected to marketing activity. However, this does not mean that financially focused executives know how such metrics translate into sales performance, which would allow them to make beneficial marketing mix decisions. We propose four criteria---potential, responsiveness, stickiness, and sales conversion---that determine the connection between marketing actions, attitudinal metrics, and sales outcomes.We test our approach with a rich data set of four-weekly marketing actions, attitude metrics, and sales for several consumer brands in four categories over a seven-year period. The results quantify how marketing actions affect sales performance through their differential impact on attitudinal metrics, as captured by our proposed criteria. We find that marketing--attitude and attitude--sales relationships are predominantly stable over time but differ substantially across brands and product categories. We also establish that combining marketing and attitudinal metrics criteria improves the prediction of brand sales performance, often substantially so. Based on these insights, we provide specific recommendations on improving the marketing mix for different brands, and we validate them in a holdout sample. For managers and researchers alike, our criteria offer a verifiable explanation for differences in marketing elasticities and an actionable connection between marketing and financial performance metrics.", "e:keyword": ["Consumer attitude metrics", "Responsiveness", "Potential", "Stickiness", "Sales conversion", "Hierarchical linear model", "Cross-effects model", "Empirical generalizations", "Dynamic programming model", "Optimal marketing resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0842", "e:abstract": "We model the multifaceted impact of pricing decisions in business-to-business (B2B) relationships that are governed by trust. We show how a seller can develop optimal intertemporal targeted pricing strategies to maximize profits over time while taking into consideration the impact of pricing decisions on short-term profit margin, reference price formation, and long-term relationships. Our modeling framework uses a hierarchical Bayesian approach to weave together a multivariate nonhomogeneous hidden Markov model, buyer heterogeneity, and control functions to facilitate targeting, capture the evolution of trust, and control for price endogeneity. We estimate our model on longitudinal transactions data from a retailer in the industrial consumables domain. We find that buyers in our data set can be best represented by two latent states of trust toward the seller---a “vigilant” state that is characterized by heightened price sensitivity and a cautious approach to ordering and a “relaxed” state with purchase behaviors that are consistent with high relational trust. The seller's pricing decisions can transition buyers between these two states. An optimal dynamic and targeted pricing strategy based on our model suggests a 52% improvement in profitability compared with the status quo. Furthermore, a counterfactual analysis examines the seller's optimal pricing policy under fluctuating commodity prices.", "e:keyword": ["Business-to-business marketing", "Pricing", "Customer relationship management", "Hidden Markov models", "Channel relationships"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2013.0843", "e:abstract": "This is an abridged version of an evaluation report for <i>Marketing Science</i>, which was commissioned by the INFORMS Publications Committee as part of its periodic review of every INFORMS journal. The coauthors listed here comprised the task force that conducted the research project and strategic analysis described below.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0844", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0845", "e:abstract": "There are few marketing studies of social learning about new technologies in low-income countries. This paper examines how learning through opinion leaders and social networks influences demand for nontraditional cookstoves—a technology with important health and environmental consequences for developing country populations. We conduct marketing interventions in rural Bangladesh to assess how stove adoption decisions respond to (a) learning the adoption choices of locally identified “opinion leaders” and (b) learning about stove attributes and performance through social networks. We find that households generally draw <i>negative</i> inferences about stoves through social learning and that social learning is more important for stoves with less evident benefits. In an institutional environment where consumers are distrustful of new products and brands, consumers appear to rely on their networks more to learn about negative product attributes. Overall, our findings imply that external information and marketing campaigns can induce initial adoption and experiential learning about unfamiliar technologies, but sustained use ultimately requires that new technologies match local preferences.", "e:keyword": ["Technology adoption", "Cookstoves", "Bangladesh", "Opinion leaders", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0847", "e:abstract": "A well-established phenomenon of consumer buying behavior is that consumers evaluate prices relative to a reference point and exhibit loss aversion; i.e., their propensity to buy is more negatively affected by prices above the reference point than it is positively affected by prices below the reference point. The objective of this paper is to analytically examine how the competitive strategy and profitability of firms are affected by the presence of consumer loss aversion in the price dimension. Although we assume that consumer loss aversion increases consumer propensity to search for lower prices, we find that it does not necessarily lead to lower prices or profits when firms compete over multiple periods and when the consumer reference price in subsequent periods is affected by current prices. Specifically, consumer loss aversion could lead to higher prices and profits when consumer valuation is sufficiently high relative to search costs and the proportion of consumers with positive search costs is in an intermediate range. We also show that when forward-looking firms incorporate the negative effect of price promotions on future profits, the equilibrium range of price promotions may actually increase.", "e:keyword": ["Game theory", "Price competition", "Price promotion", "Loss aversion", "Reference price"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0848", "e:abstract": "The customer relationship management allocation in marketing budgets is potentially misleading when it uses individual customer lifetime value estimations from historical data. Planned marketing interventions would change the purchasing behavior of different customers, and history-based decisions would thus be suboptimal. To cope with this inherent endogeneity, we model the optimal allocation of the marketing mix by accounting simultaneously for mass interventions and direct marketing interventions for each customer. This is a large stochastic dynamic problem that, in general, is computationally rather intractable as a result of the “curse of dimensionality.” We present an algorithm to derive the optimal marketing policies (how the firm should allocate its marketing resources) and the expected present value of those decisions, which maximize the long-term profitability of firms. This allows the firm to value customers/segments and helps the firm to target those that maximize long-term profitability given the optimal marketing resources allocation. We apply the proposed approach in the context of a kitchen appliance manufacturer. The results identify the most effective marketing policies and the endogenous customer values. It is in this context that we also dynamically identify the most profitable customer and the short- and long-term effects of marketing activities on each customer.", "e:keyword": ["CRM", "Marketing resource allocation", "Long-term effect of marketing activities", "Stochastic dynamic programming", "Dynamic panel-data models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0850", "e:abstract": "In many service markets such as consulting, auto repair, financial planning, and healthcare, the service provider may have more information about the customer’s problem than the customer, and different customers may impose different costs on the service provider. In principle, the service provider should ethically care about the customer’s welfare, but it is possible that a provider may maximize only its own profit. Moreover, the customer may not know ex ante whether the provider is ethical or purely self-interested. We develop a game-theoretic model to investigate pricing strategies and the market outcome in service markets where the provider has two-dimensional private information about her own type (whether ethical or self-interested) and about the customer’s condition (whether serious or minor). We show that in a less ethical market, a self-interested provider will charge different prices based on the customer’s condition, whereas an ethical provider will charge the same price for both conditions. In contrast, in a more ethical market, both the self-interested and the ethical provider will charge the same uniform price to both types of customers. Interestingly, both market efficiency and the customer’s ex ante expected surplus might be lower in a more ethical market than in a less ethical one.", "e:keyword": ["Social preference", "Signaling", "Credence goods", "Pricing", "Behavioral economics", "Asymmetric information"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0851", "e:abstract": "Probabilistic or opaque selling, whereby a seller hides the exact identity of a product until after the buyer makes a payment, has been used in practice and received considerable attention in the literature. Under what conditions, and why, is probabilistic selling attractive to firms? The extant literature has offered the following explanations: to price discriminate heterogeneous consumers, to reduce supply–demand mismatches, and to soften price competition. In this paper, we provide a new explanation: to exploit consumer bounded rationality in the sense of anecdotal reasoning. We build a simple model where the firm is a monopoly, consumers are homogeneous, and there is no demand uncertainty or capacity constraint. This model allows us to isolate the impact of consumer bounded rationality on the adoption of opaque selling. We find that although it is never optimal to use opaque selling when consumers have rational expectations, it can be optimal when consumers are boundedly rational. We show that opaque selling may soften price competition and increase the industry profits as a result of consumer bounded rationality. Our findings underscore the importance of consumer bounded rationality and show that opaque selling might be even more attractive than previously thought.", "e:keyword": ["Probabilistic goods", "Opaque selling", "Pricing", "Anecdotal reasoning", "Law of small numbers", "Bounded rationality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0852", "e:abstract": "Past research has established that just surveying individuals or measuring consumers' intentions can influence their subsequent behaviors. Building on self-generated validity theory and extant studies on the survey participation effect, we examine the behavioral phenomenon in a setting where consumers repeatedly participate in brand-specific surveys of all competing brands in a product category. We also investigate the existence and magnitude of the survey participation effect at the individual decision maker level while accounting for marketing communication efforts of the focal and competing brands. We test our proposed individual-consumer-level model using unique behavioral panel data with survey participation and marketing communication information. Our results suggest that the survey participation effect exists in a competitive marketplace setting where consumers' intentions toward a focal brand and all the competing brands are measured. We find evidence of a <i>backlash effect</i> wherein survey participation and marketing communication work against each other. We also find that consumers' participation in surveys of competing brands does not positively spill over to their choice of the focal brand. Based on our results, we suggest important implications for coordination between marketing communication efforts and marketing research activities.", "e:keyword": ["Survey participation effect", "Mere-measurement effect", "Marketing communication", "Sales response models", "Multivariate Poisson-lognormal model", "Physician prescription behavior", "Detailing", "Bayesian hierarchical models"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0853", "e:abstract": "The potential demand in a new industry evolves over time. Demand is initially low, but advertising by the industry’s early entrants can speed up demand growth. However, there is intrinsic uncertainty of the demand level in each period and uncertainty of the demand evolution path, which can be affected by the underlying economic environment. We construct a dynamic model that features the stochastically and endogenously expanding demand of a new industry, and we investigate the optimal entry and exit behavior of firms as the industry evolves. We find that firms’ incentive to enter early depends critically on the cost that early entrants have to pay in developing the market. When the cost is high and the benefit spills over to potential entrants, firms have an incentive to wait, and the probability of entry can increase with the number of incumbents under certain circumstances. Firms’ entry strategy is also influenced by the transition of economic states. Firms are more likely to enter under a state that shows the prospect of demand taking off soon. We also find that, in the early stage of an industry, higher demand uncertainty can induce faster entry.", "e:keyword": ["Firm diffusion", "Entry", "Evolving demand", "Uncertainty", "Dynamic model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0854", "e:abstract": "The presence of positive entertainment (e.g., visual imagery, upbeat music, humor) in TV advertisements can make them more attractive and persuasive. However, little is known about the downside of too much entertainment. This research focuses on why, when, and how much to entertain consumers in TV advertisements. We collected data in a large scale field study using 82 ads with various levels of entertainment shown to 178 consumers in their homes and workplaces. Using a novel web-based face tracking system, we continuously measure consumers' smile responses, viewing interest, and purchase intent. A simultaneous Bayesian hierarchical model is estimated to assess how different levels of entertainment affect purchases by endogenizing viewing interest. We find that entertainment has an inverted U-shape relationship to purchase intent. Importantly, we separate entertainment into that which comes before the brand versus that which comes after, and find that the latter is positively associated with purchase intent while the former is not.", "e:keyword": ["Hierarchical Bayes", "Advertising", "Entertainment", "Facial tracking", "Smile measurement"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0855", "e:abstract": "This editorial acknowledges notable contributions of many individuals to <i>Marketing Science</i> during 2013.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0856", "e:abstract": "In business-to-business settings a company's sales force often spends considerable time lobbying internally for authorization to charge lower prices. These internal lobbying activities are time consuming, and divert attention from other tasks, such as interacting with customers. We explain why internal lobbying activities serve an important role. They help the firm elicit truthful reporting of demand information from the sales force. As a result, it may be profitable for the firm to require lobbying (and make the requirement onerous), even though lobbying is a nonproductive activity that creates an additional administrative burden and imposes a deadweight loss.", "e:keyword": ["Lobbying", "Influence activities", "Sales force management", "Pricing", "Agency theory", "Incentives", "Information", "Marketing-sales interface"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0857", "e:abstract": "We investigate how the tendency to adopt a new product independently of social influence, the recipients' susceptibility to such influence, and the sources' strength of influence vary with social status. Leveraging insights from social psychology and sociology about middle-status anxiety and conformity, we propose that for products that potential adopters expect to boost their status, both the tendency to adopt independently from others and the susceptibility to contagion is higher for middle-status than for low- and high-status customers. Applying a nested case-control design to the adoption of commercial kits used in genetic engineering, we find evidence that status affects (i) how early or late one adopts regardless of social influence, (ii) how susceptible one is to such influence operating through social ties, and (iii) how influential one's own behavior is in triggering adoption by others. The inverse-U patterns in (i) and (ii) are consistent with middle-status anxiety and conformity. The findings have implications for how to use status to better understand adoption and contagion mechanisms, and for targeting customers when launching new products.", "e:keyword": ["Hazard model", "Nested case-control design", "New product adoption", "Social contagion", "Social networks", "Social status"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0859", "e:abstract": "We investigate whether partners in a brand alliance should be similar or dissimilar in brand image to foster favorable perceptions of brand fit. Using a Bayesian nonlinear structural equation model and evaluations of 1,200 brand alliances, we find that the conceptual coherence in brand personality profiles predicts attitudes towards a brand alliance. More specifically, we find that similarity in Sophistication and Ruggedness and moderate dissimilarity in Sincerity and Competence result in more favorable brand alliance evaluations. Overall, we find that similarity effects are more pronounced than dissimilarity effects. Implications for brand alliance strategies and marketing managers are discussed.", "e:keyword": ["Brand alliances", "Brand personality", "Brand management", "Nonlinear structural equation models", "Bayesian analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0860", "e:abstract": "Marketing is a field that is rich in data. Our data is of high quality, often at a highly disaggregate level, and there is considerable variation in the key variables for which estimates of effects on outcomes such as sales and profits are desired. The recognition that, in some general sense, marketing variables are set by firms on the basis of information not always observable by the researcher has led to concerns regarding endogeneity and widespread pressure to implement instrumental variables methods in marketing problems. The instruments used in our empirical literature are rarely valid and the IV methods used can have poor sampling properties, including substantial finite sample bias and large sampling errors. Given the problems with IV methods, a convincing argument must be made that there is a first order endogeneity problem and that we have strong and valid instruments before these methods should be used. If strong and valid instruments are not available, then researchers need to look toward supplementing the information available to them. For example, if there are concerns about unobservable advertising or promotional variables, then the researcher is much better off measuring these variables rather than using instruments (such as lagged marketing variables) that are clearly invalid. Ultimately, only randomized variation in marketing variables (with proper implementation and large samples) can be argued to be a valid instrument without further assumptions.", "e:keyword": ["Endogeneity", "Instrumental variables", "Pricing", "Promotion", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0861", "e:abstract": "Brands stand at the core of marketing. They are central to positioning, marketing communications, word of mouth, customer relationships, and firm profits. Brands have been studied from multiple perspectives using a variety of measures and scales. We offer a data set that contains 136 different measures of the brand characteristics for almost 700 of the top U.S. national brands across 16 categories measured by 2010. These measures cover a broad range of characteristics including brand personality, satisfaction, age, attributes related to Rogers' innovation scheme such as complexity, and the four brand equity pillars of Young and Rubicam's BrandAsset Valuator. The data were collected from a combination of sources including an original survey on 4,769 subjects. In addition, we provide quarterly data on the variables available from the BrandAsset Valuator for two and a half years between 2008 and 2010. These data can be used as a building block in research that aims to explore the antecedents of brand perceptions or connect brand characteristics with market and financial outcomes. This paper describes the data and some relevant research questions.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2014.0861\">http://dx.doi.org/10.1287/mksc.2014.0861</ext-link>.", "e:keyword": ["Brands", "Brand characteristics", "Brand personality", "Complexity", "Differentiation", "Esteem"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0862", "e:abstract": "Past studies have overlooked the joint effects of economic and customer experience factors on service purchase behaviors. Furthermore, service firms tend to make substantial investments in enhancing customer experience, mitigating the negative effects of service failures through recovery efforts and increasing overall customer satisfaction. Yet, largely due to a paucity of data, we know little about how the state of the economy influences the way in which customers use past service experiences to make future purchase decisions. We hypothesize that the state of the economy moderates the effects of customer experience factors on customers’ service purchase behaviors. In addition, we examine how personal income influences the degree to which the aggregate economy influences service purchase decisions. We test the proposed model using panel survey and transaction data from an international airline carrier. Our findings demonstrate that, contrary to wisdom in the popular press, customer experience matters more when the economy is doing better, not worse. Furthermore, lower income consumers are more sensitive to changes in the economy than higher income consumers. We validate the hypothesized model using a controlled experiment and establish that aggregate measures of the economy can be used to predict individual perceptions and purchase intentions.", "e:keyword": ["The economy", "Customer satisfaction", "Service failure", "Service recovery", "Personal income", "Customer experience", "Service purchase behavior"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0863", "e:abstract": "Competitor orientation, i.e., the focus on beating the competition rather than maximizing profits, seems to thrive in business situations despite being, by definition, suboptimal for profit-maximizing firms. Our research explains how a competitor orientation can persist and even thrive in equilibrium in markets that reward only profits. We apply evolutionary game theory to business markets where reputation matters. We use three games that represent classic interactions in business marketing: Chicken (to illustrate competition for product adoption), the Battle of the Sexes (channel negotiations), and the Prisoners' Dilemma (pricing battles).Initial populations are assumed to have both profit-maximizing managers and competitor-oriented managers (i.e., those who gain additional utility from beating others). We demonstrate that a competitor orientation can survive in equilibrium despite selection that is based solely on profits. Using Chicken, we show that a competitor orientation thrives and can even overrun the population. We use the Battle of the Sexes to show that a competitor orientation will overrun one population in a two-sided negotiation (e.g., all retailers in a retailer/manufacturer dyad). Last, using the Prisoners' Dilemma, we show that competitor orientation is not selected against. We conclude that evolutionary profit-driven selection pressures cannot be assumed to eliminate nonprofit-maximizing behavior even when selection is based purely on profitability.", "e:keyword": ["Evolutionary game theory", "Competitor orientation", "Chicken", "Prisoners' dilemma", "Battle of the sexes", "Behavioral economics", "Analytical model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0864", "e:abstract": "Recent years have seen a considerable rise in the use of product placement in television shows. Taking advantage of second-by-second product placement, advertising, and audience tuning data, this research explores the impact of such product placement on the extent to which viewers tune away from downstream advertisements. Motivated by the behavioral priming literature, we examine how this impact relates to the brand- and category-match between product placement and subsequent advertising, as well as the temporal distance between them. Our analysis suggests that the coveted first position of a commercial break holds a greater audience when preceded by product placement from the same brand. This indicates a positive synergy between the two activities that can reduce audience decline by more than 10%. Product placements by other brands, however, can actually exacerbate audience loss, thus interfering with the reach of advertisements by competitors. Significantly, these changes in audience size are not temporary, but are retained across the remaining commercials in the same break. We discuss the managerial implications of these findings and directions for future research in the rapidly changing media landscape.", "e:keyword": ["Product placement", "Advertising", "Brand strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0867", "e:abstract": "In this paper, we untangle the searchable and experiential dimensions of quality responses to entry by counterfeiters in emerging markets with weak intellectual property rights. Our theoretical framework analyzes market equilibria under competition from counterfeiting as well as under monopoly branding. A key theoretical prediction is that emerging markets can be self-corrective with respect to counterfeiting issues in the following sense: First, counterfeiters can earn positive profits by pooling with authentic brands only when consumers have good faith in the market (i.e., they believe there is low probability that any product is a counterfeit). When the proportion of counterfeits in the market exceeds a cutoff value, brands invest in self-differentiation from the competitive-fringe counterfeiters. Second, to attain a separating equilibrium with counterfeiters, branded incumbents upgrade the searchable quality (e.g., appearance) of their products more and improve the experiential quality (e.g., functionality) less compared with monopoly equilibrium. However, in the pooling equilibrium with sporadic counterfeits, authentic firms instead may invest in experiential quality to attract more of the expert consumers who are well versed in quality. This prediction uncovers the nature of product differentiation in the searchable dimension and helps with analyzing real-world innovation strategies employed by authentic firms in response to entries by counterfeit entities. In addition, welfare analysis hints at a nonlinear relationship between social welfare and intellectual property enforcement.", "e:keyword": ["Counterfeit", "Emerging markets", "Searchable quality", "Experiential quality", "Signaling", "Two-dimensional vertical differentiation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0868", "e:abstract": "There is substantial academic interest in modeling consumer experiential learning. However, (approximately) optimal solutions to forward-looking experiential learning problems are complex, limiting their behavioral plausibility and empirical feasibility. We propose that consumers use cognitively simple heuristic strategies. We explore one viable heuristic—index strategies—and demonstrate that they are intuitive, tractable, and plausible. Index strategies are much simpler for consumers to use but provide close-to-optimal utility. They also avoid exponential growth in computational complexity, enabling researchers to study learning models in more complex situations.Well-defined index strategies depend on a structural property called indexability. We prove the indexability of a canonical forward-looking experiential learning model in which consumers learn brand quality while facing random utility shocks. Following an index strategy, consumers develop an index for each brand separately and choose the brand with the highest index. Using synthetic data, we demonstrate that an index strategy achieves nearly optimal utility at substantially lower computational costs. Using IRI data for diapers, we find that an index strategy performs as well as an approximately optimal solution and better than myopic learning. We extend the analysis to incorporate risk aversion, other cognitively simple heuristics, heterogeneous foresight, and an alternative specification of brands.", "e:keyword": ["Forward-looking experiential learning", "Index strategies", "Structural models", "Cognitive simplicity", "Heuristics", "Multi-armed bandit problems", "Restless bandit problems", "Indexability"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0869", "e:abstract": "Conventional wisdom suggests that when firms face a negative externality like gray marketing (i.e., the selling of branded goods outside of the manufacturer’s authorized channels), an effective strategy to reduce the negative impact is to centralize decision making. Nevertheless, in industries with significant gray marketing, we observe many firms with decentralized decision making. Our study assesses whether decentralized decision making can be optimal when a manufacturer faces gray market distribution. We consider a market where a focal firm competes with an existing competitor that produces a differentiated product and a gray marketer that sources an identical product from a lower-priced foreign market. We find that decentralization is optimal under quantity-based competition, provided the gray market is relatively uncompetitive and the level of competitive intensity between the focal firm and the competitor is high. Decentralization leads a firm to make aggressive production decisions, which leads to lower prices, yet it also leads to higher market share for the firm compared to centralization. When the level of competitive intensity between a firm and its competitor is high, the gain in market share more than offsets the loss due to lower prices. As a result, the focal firm is better off decentralizing its operations independent of (a) whether the competitor operates in the foreign market, and (b) the competitor’s organizational structure. This finding contradicts the belief that centralized decision making is always optimal when authorized manufacturers attempt to limit the negative impact of gray markets. The findings also provide insight to understand why firms might employ decentralized decision making in industries where gray markets are active.", "e:keyword": ["Gray markets", "Diversion", "Foreign market entry", "Decision rights"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0870", "e:abstract": "We examine the nature of best-worst data for modeling consumer preferences and predicting their choices. We show that contrary to the assumption of widely used models, the best and worst responses do not originate from the same data generating process. We propose a sequential evaluation model and show that people are likely to engage in a two-step evaluation process and are more likely to select the worst alternative first before selecting the best. We find that later choices have systematically larger coefficients as compared to earlier choices. We also find the presence of an elicitation effect that leads to larger coefficients when respondents are asked to select the worst alternative, meaning that respondents are surer about what they like least than what they like most. Finally, we investigate global inference retrieval in choice tasks, which can be represented by the central limit theorem and normally distributed errors, versus episodic retrieval represented by extreme value errors. We find that both specifications of the error term are plausible and advise using the proposed sequential logit model for practical reasons. We apply our model to data from a national survey investigating the concerns associated with hair care. We find that accounting for the sequential evaluation in the best-worst tasks and the presence of the scaling effects leads to different managerial implications compared to the results from currently used models.", "e:keyword": ["Bayesian estimation", "Choice modeling", "Mathematical psychology"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0871", "e:abstract": "This paper investigates a determinant of location choice for multistore retailing firms: the trade-off between the business-stealing effect and the cost-saving effect from clustering their own stores. I present an empirical model of network choice by two multistore firms. I use lattice-theoretical results to address the computational burden of solving for an equilibrium in store networks. The framework integrates the static entry game of complete information with post-entry outcome data while using simulations to correct for the selection of entrants. I present an application of the model to the case of the convenience store industry in Okinawa Island, Japan, using unique cross-sectional data on store networks and revenues. I use parameter estimates to examine the impact of a hypothetical horizontal merger on store configurations, costs, and profits. Results suggest a retailer's trade-off between cost savings and lost revenues from clustering its stores is positive across markets and negative within a market. I find an acquirer of a hypothetical merger of two multistore firms would decrease its number of stores in suburbs but increase its number in the city center.", "e:keyword": ["Entry", "Chain", "Supermodular games", "Convenience store", "Merger", "Retail competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0872", "e:abstract": "This study investigates how prior usage experience with various decision aids available in an Internet shopping environment contributes to online purchase behavior evolution. Four types of decision aids are examined: those for (1) nutritional needs, (2) brand preference, (3) economic needs, and (4) personalized shopping lists. We construct and estimate nonhomogeneous hidden Markov models of store- and category-level purchase decisions, in which parameters vary over time across hidden states as driven by usage experience with different decision aids. We find that consumers evolve through distinct behavioral states over time, and the evolution is attributable to their prior usage experience with various decision aids. Moreover, the impact varies by the specific decision aid, behavioral state, and category characteristics. In addition, consumers gravitate toward habitual decision processes in online grocery stores, and their average price and promotion sensitivities increase first and then decrease but the level of heterogeneity rises continuously. We identify beneficial versus potentially undesirable decision aids and demonstrate how the proposed research method can help online retailers improve their store environments, design customized promotions, and quantify the payoffs of these strategies.", "e:keyword": ["Internet marketing", "Shopper marketing", "Interactive decision aids", "Customized promotions", "Store environment", "Retail management", "e-commerce", "Decision heuristics", "Click-stream data", "Bayesian statistics", "Hidden Markov model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0873", "e:abstract": "In recent years, customer lifetime value (CLV) has gained increasing importance in both academia and practice. Although many advanced techniques have been proposed, the recency/frequency/monetary value (RFM) segmentation framework, and its related probability models, remain a CLV mainstay. In this article, we demonstrate the deficiency in RFM as a basis for summarizing customer history (data compression), and extend the framework to include clumpiness (C) by a metric-based approach. Our main empirical finding is that C adds to the predictive power, above and beyond RFM and firm marketing action, of both the churn, incidence, and monetary value parts of CLV. Hence, we recommend a significant implementation change: from RFM to RFMC.This work is also motivated by noting that although statistical models based on RFM summaries can fit well in aggregate, their use can lead to significant micro-level (e.g., ranking of customers) prediction errors unless C is captured. A set of detailed empirical studies using data from a large North American retailer, in addition to six companies that vary in their business model: two traditional (e.g., CDNow.com) and four Internet (e.g., Hulu.com), demonstrate that the “clumpiness phenomena” is widely prevalent, and that companies with “bingeable content” have both high potential and high risk segments, previously unseen, but now uncovered because of the new framework: RFM to RFMC.", "e:keyword": ["Customer lifetime value", "RFM", "Clumpiness"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0874", "e:abstract": "Many video ads are designed to go <i>viral</i> so that the total number of views they receive depends on customers sharing the ads with their friends. This paper explores the relationship between the number of views and how persuasive the ad is at convincing consumers to purchase or to adopt a favorable attitude towards the product. The analysis combines data on the total views of 400 video ads, and crowd-sourced measurement of advertising persuasiveness among 24,000 survey responses. Persuasiveness is measured by randomly exposing half of these consumers to a video ad and half to a similar placebo video ad, and then surveying their attitudes towards the focal product. Relative ad persuasiveness is on average 10% lower for every one million views that the video ad achieves. The exceptions to this pattern were ads that generated views <i>and</i> large numbers of comments, and video ads that attracted comments that mentioned the product by name. Evidence suggests that such ads remained effective because they attracted views due to humor rather than because they were outrageous.", "e:keyword": ["Viral advertising", "Virality", "Video advertising", "Internet"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0875", "e:abstract": "In the last decade, design innovation has gained increasing prominence in the marketplace, with a growing number of firms innovating not only through technology but also through novel product forms (i.e., design). However, while the effect of technological innovation on product sales is a heavily studied topic, a defining theory of how design innovation influences product sales is still missing. This paper provides demand- and supply-side theories to formulate a set of coherent hypotheses about the effect of design innovativeness, i.e., the degree of novelty in a product’s design, on sales' evolution over time. The hypotheses are tested in two different samples. In the first, car models introduced in the United States from 1978 to 2006 (for a total of 2,757 model-year data) are analyzed. In the second, motorcycle models introduced in the United States from 1980 to 2008 (for a total of 2,847 model-year observations) are analyzed. I find that design innovativeness diminishes initial sales' status but increases sales' growth rates. Furthermore, design and technological innovativeness have a negative interaction effect on sales' initial status, but a positive effect on sales' growth rates. Finally, brand strength and brand advertising expenditures worsen the negative effect of design innovativeness on initial sales' status, but boost its positive effect on sales' growth rates.", "e:keyword": ["Design innovativeness", "Technological innovativeness", "Individual growth curve analysis", "Car industry", "Motorcycle industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0877", "e:abstract": "Multipart tariffs are widely favored within service industries as an efficient means of mapping prices to differential levels of consumer demand. Whether they benefit consumers, however, is far less clear as they pose individuals with a potentially difficult task of dynamically allocating usage over the course of each billing cycle. In this paper we explore this welfare issue by examining the ability of individuals to optimally allocate consumption over time in a stylized cellular-phone usage task for which there exists a known optimal dynamic utilization policy. Actual call behavior over time is modeled using a dynamic choice model that allows decision makers to both discount the future (be myopic) and be subject to random errors when making call decisions. Our analysis provides a “half empty, half full” view of intuitive optimality. Participants rapidly learn to exhibit farsightedness, yet learning is incomplete with some level of allocation errors persisting even after repeated experience. We also find evidence for an asymmetric effect in which participants who are exogenously switched from a low (high) to high (low) allowance plan make more (fewer) errors in the new plan. The effect persists even when participants make their own plan choices. Finally, interventions that provide usage information to help participants eradicate errors have limited effectiveness.", "e:keyword": ["Multipart tariffs", "Dynamic allocation", "Consumer learning", "Dynamic decision making", "Intertemporal discounting"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0878", "e:abstract": "Firms with a customer-centric structure—an organizational design that aligns each business unit with a distinct customer group—are expected to exhibit superior performance compared to firms that are internally structured. Top executives invoke these customer-centric beliefs when initiating corporate reorganizations. However, a lack of empirical evidence linking these customer-centric structures to better long-term financial performance raises doubts if corporate structure can truly foster customer centricity and better position a firm to satisfy customers and hence exhibit superior performance. The current research addresses this question by using longitudinal data (1998–2010) that links <i>Fortune</i> 500 firms’ corporate-level structure to performance. Utilizing a dueling mediator model with allowance for endogeneity in a firm’s organizational structure choice, the study reveals that a corporate-level customer-centric structure translates to greater customer satisfaction, but simultaneously adds coordinating costs. Further explaining customer-centric structure’s record of mixed success, the benefits of increased customer satisfaction diminish (1) as competitors have already adopted customer-centric structures, (2) in fragmented markets where competitors leave few unique customer needs unaddressed, and (3) in less profitable industries. Ultimately, we show that aligning corporate structure around customers pays off only in specific competitive environments.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2014.0878\">http://dx.doi.org/10.1287/mksc.2014.0878</ext-link>.", "e:keyword": ["Customer-centric structure", "Customer satisfaction", "Coordinating cost", "Competitive environment", "Financial performance"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0879", "e:abstract": "We disentangle and study the relative importance of different risk preferences in explaining extended warranty purchases and the high premia paid for them. Empirical and behavioral research on insurance is at odds with whether diminishing returns (curvature of the utility function), or loss aversion and nonlinear probability weighting lead to observed consumer behavior. This lack of consensus is primarily due to the inability of standard choice data to separate different risk preferences, and the consequent need to rely on strong parametric assumptions. We design two conjoint studies (consistent with simultaneous and sequential decision making) with choices about washing machines and extended warranties, where subjects are given failure probabilities and repair costs. Using stated choice data from the surveys, we can nonparametrically identify product and risk preferences. We find that loss aversion is significantly more important than curvature and probability weights in explaining extended warranty choices. Importantly, failure to decompose risk-averse behavior into that arising from curvature, loss aversion, and probability weighting leads to lower washer prices and profits. These findings are robust to alternate reference point assumptions. We rationalize the premia paid for warranties by exploring retailer incentives to price discriminate, and test the theory on complementary goods pricing. Finally, based on counterfactual analysis, forcing separate retailers to sell washers and extended warranties is not necessarily welfare enhancing as cited in the media and previous literature.", "e:keyword": ["Extended warranties", "Product insurance", "Prospect theory", "Loss aversion", "Risk aversion", "Insurance pricing", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0880", "e:abstract": "The focus of this paper is dual distribution channels in business-to-business markets. We take the perspective of the distributor, and examine how different forms of competition with a manufacturer-owned channel impact distributor opportunism. Next, we consider how the same forms of competition impact the distributor’s end customers. Based on a multi-industry field study of industrial distributors, we highlight the complex processes that characterize dual distribution systems. We show that while competition with a manufacturer-owned channel increases distributor opportunism, it also has the potential to benefit the distributor’s end customers. In addition, although actions taken by a manufacturer to create vertical separation between channels limit competition, such actions also reduce end customer satisfaction.", "e:keyword": ["Dual distribution", "Concurrent channels", "Forms of competition", "Distributors", "Channel outcomes", "Opportunism"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0881", "e:abstract": "This paper investigates the importance of network effects in the demand for ethanol-compatible vehicles and the supply of ethanol fuel. An indirect network effect, or positive feedback loop, arises in this context due to spatially-dependent complementarities in the availability of ethanol fuel and the installed base of ethanol-compatible vehicles. Marketers and social planners are interested in whether these effects exist, and if so, how policy might accelerate adoption of the ethanol fuel standard within a targeted population. To measure these feedback effects, I develop an econometric framework that considers the simultaneous determination of ethanol-compatible vehicle demand and ethanol fuel supply in local markets. The demand-side model considers the automobile purchase decisions of consumers and fleet operators; the supply-side model considers the ethanol market entry decisions of competing fuel retailers. The framework extends extant market entry models by endogenizing the market size shifting fuel retailer profits. I estimate the model using zip code panel data from four states over a nine-year period. The model estimates provide evidence of a network effect. Under typical market conditions, entry of an additional ethanol fuel retailer leads to a 6% increase in the probability of ethanol-compatible vehicle purchase. The entry model estimates imply that the first entrant requires a local installed base of approximately 300 ethanol-compatible vehicles to be profitable. As an application, I demonstrate that subsidizing fuel retailers to offer ethanol in selective geographic markets can be an effective policy to indirectly increase ethanol-compatible vehicle sales.", "e:keyword": ["Indirect network effects", "Market entry", "Alternative fuels", "Ethanol", "Flex-fuel vehicles"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0882", "e:abstract": "What is the role that color plays in consumers’ perception of the gist of ads during the increasingly brief and blurred exposures in practice? Two studies address this question. The first study manipulates the level of blur of the exposure and the presence or absence of color in the ad image, during exposures that lasted 100 milliseconds (msec). It reveals a buffer effect of color: color contributes little to gist perception when sufficient visual detail is available and ads are typical, but color enables consumers to continue to perceive the gist of ads accurately when the exposure is blurred. The second study finds that color inversion of the entire ad deteriorates gist perception, but that color inversion of the background scene does not affect gist perception when the exposure is blurred. This provides evidence that the color composition of the central object in the ad scene plays a key role in protecting the gist perception of advertising under adverse exposure conditions. The underlying mechanism is likely to be cognitive rather than sensory. Implications for advertising theory and design are discussed.", "e:keyword": ["Advertising", "Gist perception", "Drift diffusion model", "Bayesian ANOVA", "Color"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0883", "e:abstract": "In our paper about optimal reverse pricing mechanisms [Spann M, Zeithammer R, Häubl G (2010) Optimal reverse-pricing mechanisms. <i>Marketing Sci.</i> 29(6):1058–1070] (hereafter, ORPM), some of the mathematical derivations implicitly assume that the name-your-own-price seller interprets the outside-market posted price <i>p</i> differently than the buyers. This note shows that all of the qualitative results in ORPM continue to hold under the more natural assumption of common knowledge that <i>p</i> is the upper bound of wholesale cost. Interestingly, the proofs and algebraic expressions are often simpler than those in ORPM.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0884", "e:abstract": "Recently there has been significant interest in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing measures such as average bid, average ad position, total impressions, clicks, and cost for each keyword in the advertiser’s campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intraday variation in ad position. We show that estimating random utility models on aggregated (daily) data without accounting for this variation will lead to systematically biased estimates. Specifically, the impact of ad position on click-through rate (CTR) is attenuated and the predicted CTR is higher than the actual CTR. We analytically demonstrate the existence of the bias and show the effect of the bias on the equilibrium of the SSA auction. Using a large data set from a major search engine, we measure the magnitude of bias and quantify the losses suffered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11% due to aggregation bias. We also present a few data summarization techniques that can be used by search engines to reduce or eliminate the bias.", "e:keyword": ["Sponsored search", "Generalized second-price auctions", "Consumer choice models", "Hierarchical Bayesian estimation", "Latent instrumental variables", "Aggregation bias"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0885", "e:abstract": "Addiction creates an intertemporal link between a consumer’s past and present decisions, altering their responsiveness to price changes relative to nonaddictive products. We construct a dynamic model of rational addiction and endogenous consumption to investigate how consumers respond to policy interventions that aim to reduce purchases of cigarettes. We find that, on average, the category elasticity is about 35% higher when the model correctly accounts for addiction. However, some policies spur substitution from more expensive single packs to less expensive cartons of cigarettes, resulting in higher overall consumption for some consumers.", "e:keyword": ["Rational addiction", "Cigarettes", "Addictive goods", "Endogenous consumption", "State dependence"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0886", "e:abstract": "Different objectives such as category demand expansion or market share stealing warrant the use of different marketing instruments. To help brand managers make informed decisions, it is essential that marketing mix models appropriately measure their effects. Random Utility Models that have been applied to this problem might not be adequate because they do not allow the effects of marketing instruments of one brand to spillover to preference for competing alternatives. Additionally, they have the Invariant Proportion of Substitution (IPS) property, which in some situations imposes counter-intuitive restrictions on individual choice behavior. Recognizing that effects of marketing instruments can spill across brands in a category, we propose an alternative choice model that relaxes the IPS property: the cross attributes flexible substitution logit model. We apply the model in two very different empirical settings, i.e., consumer choices of brands of refrigerated yogurt, and prescription-writing choices of physicians in the hyperlipidemia category. In both settings the proposed model provides consistent evidence that certain marketing instruments produce sales gains primarily from growing the category pie, while others produce gains from stealing share. By contrast, the random coefficient logit and generalized nested logit models both predict that gains from all marketing instruments would have similar sources.", "e:keyword": ["Brand switching", "Choice models", "Econometrics", "Logit", "Marketing mix effects", "Category expansion", "Invariant proportion of substitution"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0888", "e:abstract": "The notion of peer influence in new product adoption or trial is well accepted. We propose that peer influence may affect repeat behavior as well, though the process and source of influence are likely to differ between trial and repeat. Our analysis of the acceptance of a risky prescription drug by physicians provides three novel findings. First, there is evidence of contagion not only in trial but also in repeat. Second, who is most influential varies across stages. Physicians with high centrality in the discussion and referral network and with high prescription volume are influential in trial but not repeat. In contrast, immediate colleagues, few of whom are nominated as a discussion or referral partner, are influential in both trial and repeat. Third, who is most influenceable also varies across stages. For trial, it is physicians who do not consider themselves to be opinion leaders, whereas for repeat, it is those located towards the middle of the status distribution as measured by network centrality. The pattern of results is consistent with informational social influence reducing risk in trial and normative social influence increasing conformity in repeat.", "e:keyword": ["New product diffusion", "Social contagion", "Social networks", "Social status", "Trial-repeat"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0889", "e:abstract": "In many consumption settings (e.g., restaurants), individuals consume products either alone or with their peers (e.g., friends). In this study, we propose a general framework for modeling peer effects by including two new peer effects: the exogenous peer effect (exogenous factors that could change the peer’s behavior) and the peer presence effect (when the peer is present but not consuming). We also include the well known endogenous peer effect. We develop an empirical model that allows us to identify all three effects simultaneously and apply the model to behavioral data from a casino setting. It is a simultaneous equation model with the structural parameters expressed as a function of the ratio of the reduced form parameters. This necessitates the use of the Minimum Expected Loss approach, allowing us to obtain consistent estimates at the individual level. Our data comprise detailed gambling activity for a panel of individuals at a single casino over a two-year period. Our results show that all three types of peer effects exist. These effects vary across individuals and exhibit considerable asymmetry within pairs of peers. We discuss how our results can help managers allocate resources more effectively and policy makers formulate regulatory guidelines with more complete information.", "e:keyword": ["Peer effects", "Social networks", "Simultaneous equation models", "Hierarchical Bayesian methods", "MELO estimator", "Casino gaming and gambling"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0890", "e:abstract": "Many firms operate customer communities online. This is motivated by the belief that customers who join the community become more engaged with the firm and/or its products, and as a result, increase their economic activity with the firm. We describe this potential economic benefit as “social dollars.” This paper contributes evidence for the existence and source of social dollars using data from a multichannel entertainment products retailer that launched a customer community online. We find a significant increase in customer expenditures attributable to customers joining the firm’s community. While self-selection is a concern with field data, we rule out multiple alternative explanations. Social dollars persist over the time period observed and arose primarily in the online channel. To assess the source of the social dollar, we hypothesize and test whether it is moderated by participation behaviors conceptually linked to common attributes of customer communities. Our results reveal that posters (versus lurkers) of community content and those with more (versus fewer) social ties in the community generated more (fewer) social dollars. We found a null effect for our measure of the informational advantage expected to accrue to products that differentially benefit from content posted by like-minded community members. This overall pattern of results suggests a stronger social than informational source of economic benefits for firm operators of customer communities. Several implications for firms considering investments in and/or managing online customer communities are discussed.", "e:keyword": ["Online customer communities", "Online customer behavior", "Social networks", "User-generated content", "Retailing", "Field data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0891", "e:abstract": "Current complex dynamic markets are characterized by numerous brands, each with multiple products and price points, and differentiated on a variety of product attributes plus a large number of new product introductions. This study seeks to analyze dynamic pricing paths in a highly complex branded market, consisting of 663 products under 79 brand names of digital cameras. The authors develop a method to classify dynamic pricing strategies and analyze the choice and correlates of observed pricing paths in the introduction and early growth phase of this market. The authors find that, despite numerous recommendations in the literature for skimming or penetration pricing, market pricing dominates in practice. In particular, the authors find five patterns: skimming (20% frequency), penetration (20% frequency), and three variants of market-pricing patterns (60% frequency), where new products are launched at market prices. Skimming pricing launches the new product 16% above the market price and subsequently increases the price relative to the market price. Penetration pricing launches the new product 18% below the market price and subsequently lowers the price relative to the market price. Firms exhibit a mix of these pricing paths across their portfolios. The specific pricing paths correlate with market, firm, and brand characteristics such as competitive intensity, market pioneering, brand reputation, and experience effects. The authors discuss managerial implications.", "e:keyword": ["Price penetration", "Price skimming", "Dynamic pricing strategy", "Product life cycle", "Consumer durables", "Brand competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0892", "e:abstract": "Blood banks rely on marketing to encourage donors to give blood. Many, if not most, blood banks in the United States are community-based not-for-profit organizations with limited marketing budgets. As a result, blood banks increasingly use novel and inexpensive online media, i.e., <i>paid</i>, <i>owned</i>, and <i>earned</i> (POE) media, in their marketing efforts. We propose a dynamic model to help blood bank marketing managers understand how blood donations can be managed via online POE media. We analytically characterize the optimal forward-looking paid media strategies, taking into account the asymmetric costs related to shortage and excess of blood, as well as the possibility of a cost-free target donation zone. We detail new advertising resource allocation rules for blood banks and show when traditional allocation recommendations do not apply. Additionally, we discover that under certain circumstances, owned/earned media activities hurt the blood bank’s performance, despite being (predominantly) free. We validate our analytical model by using daily donation data from a community-based blood bank and measure the effects of POE media activities on the level of blood donated.", "e:keyword": ["Blood bank", "Paid-own-earned media", "Social/non-profit marketing", "Optimal control", "Time series", "Bayesian estimation", "Dynamic linear model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0893", "e:abstract": "We investigate the causal effect of position in search engine advertising listings on outcomes such as click-through rates and sales orders. Because positions are determined through an auction, there are significant selection issues in measuring position effects. A simple mean comparison of outcomes at two positions is likely to be biased due to these selection issues. Additionally, experimentation is rendered difficult in this situation by competitors’ bidding behavior, which induces selection biases that cannot be eliminated by randomizing the bids for the focal advertiser. Econometric approaches to address the selection are rendered infeasible due to the difficulty of finding suitable instruments in this context. We show that a regression discontinuity (RD) approach is feasible to measure causal effects in this important context. We apply the approach to a large and unique data set of daily observations containing information on a focal advertiser as well as its major competitors. Our RD estimates demonstrate that there are significant selection biases in the more naive estimates. While a mean comparison of outcomes across positions would indicate very large position effects, we find that our RD estimates of these effects are much smaller, and exist only in some of the positions. We further investigate moderators of these effects. Position effects are stronger when the advertiser is smaller, and when the consumer has low prior experience with the keyword for the advertiser. They are weaker when the keyword phrase has specific brand or product information, when the ad copy is more specific as in exact matching options, and on weekends compared to weekdays.", "e:keyword": ["Search advertising", "Online advertising", "Position effects", "Advertising effects", "Causal effects", "Regression discontinuity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0894", "e:abstract": "We analyze two pricing mechanisms for information goods. These mechanisms are selling, where up-front payment allows unrestricted use, and pay-per-use, where payments are tailored to use. We analytically model a market where consumers differ in use frequency and where use on a pay-per-use basis invokes a psychological cost associated with the well known “ticking meter” effect. We demonstrate that pay-per-use yields higher profits in a monopoly provided the associated psychological cost is low. In a duopoly, one firm uses selling and the other uses pay-per-use. Here, in contrast to the monopoly, selling yields higher profits than pay-per-use. We demonstrate that, surprisingly, the profits of both duopolists can increase as the psychological cost associated with pay-per-use increases. Next, we show that uncertainty in consumer use frequency does not affect pay-per-use in a monopoly, but lowers profits from selling. In a duopoly, both the seller and the pay-per-use provider obtain lower profits when use frequency is uncertain. We also analyze how pricing mechanism performance is affected if the firms cannot commit to prices, if the pay-per-use provider offers a two-part tariff, and if consumers are risk-averse.", "e:keyword": ["Information goods", "Competitive strategy", "Pricing", "Digital marketing", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0895", "e:abstract": "Consumers in many emerging markets exhibit a pronounced preference for western and global brands, while domestic brands are often associated with a cheap but low quality image. Frustrated with the negative country-of-origin (COO) stereotype imposed on them, many domestic brands from emerging markets follow a variety of approaches to disguise their COO and pretend to be foreign. This paper studies the strategic aspects of this phenomenon. We consider an experience good market where consumers learn about each brand’s quality, using its COO as a prior. Each firm can invest in product quality or COO dissociation; the former generates better quality signals while the latter simply masks the firm’s COO identity. The analysis reveals a few main insights. Sharing a reputation leads to a common good problem where firms free-ride on each other’s quality investments. As a high quality firm dissociates itself from the stereotype, it ceases to contribute to the COO image but also prevents its low quality peers from free-riding on it. This incurs a negative direct effect but a positive strategic effect on the group image. Consequently, a country’s COO image may actually improve when more high quality brands shun their identities and pretend to be foreign. In equilibrium, COO image improves monotonically as firms become more efficient in providing quality. However, the prevalence of COO dissociation may first increase then decrease. We discuss the implications of these results for emerging market brands as well as policy makers.", "e:keyword": ["Game theory", "Branding", "Country-of-origin effect"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0896", "e:abstract": "Digitization of content is changing how consumers and firms use purchase and rental markets. Low transaction costs make accessing content easier for consumers. Digital technology enables firms to create nondurable “rental” versions of their content and to restrict content to the purchasing consumer, effectively shutting down resale markets. To empirically analyze the interaction of purchase and rental markets, I design a preference measurement tool to recover consumers’ intertemporal preferences through current-period choices alone. I then use these preferences to solve for a dynamic equilibrium between consumers and the firm. In the context of the online home video market, I find that when the firm is able to commit to holding prices fixed forever, providing content through the purchase market alone is sufficient. However, when the firm is unable to commit, it should serve both purchase and rental markets. Canonical theory models would predict exclusive rentals, but the purchase option enables indirect price discrimination in practice. I also find that when consumers place a premium on accessing new content, they are less likely to intertemporally substitute, thereby increasing the firm’s pricing power. Consistent with theory, commitment to future prices increases profits considerably. This finding supports the rigid pricing structure of such retailers as Apple, despite studios’ push toward more pricing flexibility.", "e:keyword": ["Purchase and rental markets", "Durable good pricing", "Online content", "Experiment design", "Conjoint analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0897", "e:abstract": "Extant research on choice designs in marketing focuses on the construction of efficient homogeneous designs where all respondents get the same design. Recently marketing scholars proposed the construction of efficient heterogeneous designs where different respondents or groups of respondents get different subdesigns, and demonstrated substantial efficiency gain when such heterogeneous designs are employed. A significant hurdle in the widespread adoption of heterogeneous designs is the high computation cost, even when the number of subdesigns contained in the heterogeneous design is restricted to be small. In this paper we propose a new approach for the construction of efficient heterogeneous choice designs. In contrast to extant approaches that are based on an exact design framework where it is computationally prohibitive to do an exhaustive search to find a globally optimal design, our proposed approach is based on the continuous design framework where well-established mathematical theories can be leveraged for quick identification of a globally optimal design. The proposed approach makes it feasible to generate a highly efficient choice design that is completely heterogeneous—a unique subdesign for each individual respondent in the choice experiment. The proposed approach is the first in the marketing literature to find a completely heterogeneous choice design with assured high global design efficiency using the continuous design framework. Results from simulation and empirical studies demonstrate superior performance of the proposed approach over extant approaches in constructing efficient heterogeneous choice designs.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2014.0897\">http://dx.doi.org/10.1287/mksc.2014.0897</ext-link>.", "e:keyword": ["Experimental design", "Conjoint analysis", "Conjoint choice designs", "Discrete choice designs", "Optimal designs", "Mixed logit model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0899", "e:abstract": "Media multitasking competes with television advertising for consumers’ attention, but may also facilitate immediate and measurable response to some advertisements. This paper explores whether and how television advertising influences online shopping. We construct a massive data set spanning $3.4 billion in spending by 20 brands, measures of brands’ website traffic and transactions, and ad content measures for 1,224 commercials. We use a quasi-experimental design to estimate whether and how TV advertising influences changes in online shopping within two-minute pre/post windows of time. We use nonadvertising competitors’ online shopping in a difference-in-differences approach to measure the same effects in two-hour windows around the time of the ad. The findings indicate that television advertising does influence online shopping and that advertising content plays a key role. Action-focus content increases direct website traffic and sales. Information-focus and emotion-focus ad content actually reduce website traffic while simultaneously increasing purchases, with a positive net effect on sales for most brands. These results imply that brands seeking to attract multitaskers’ attention and dollars must select their advertising copy carefully.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2014.0899\">http://dx.doi.org/10.1287/mksc.2014.0899</ext-link>.", "e:keyword": ["Advertising", "Content analysis", "Difference-in-differences", "Internet", "Media multitasking", "Online purchases", "Quasi-experimental design", "Television"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0900", "e:abstract": "This paper studies the optimal product and pricing decisions in a crowdfunding mechanism by which a project between a creator and many buyers will be realized only if the total funds committed by the buyers reach a specified goal. When the buyers are sufficiently heterogeneous in their product valuations, the creator should offer a line of products with different levels of product quality. Compared to the traditional situation where orders are placed and fulfilled individually, with the crowdfunding mechanism, a product line is more likely than a single product to be optimal and the quality gap between products is smaller. This paper also shows the effect of the crowdfunding mechanism on pricing dynamics over time. Together, these results underscore the substantial influence of the emerging crowdfunding mechanisms on common marketing decisions.", "e:keyword": ["Crowdfunding", "Product line design", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2014.0901", "e:abstract": "Bayesian hierarchical modeling is a popular approach to capturing unobserved heterogeneity across individual units. However, standard estimation methods such as Markov chain Monte Carlo (MCMC) can be impracticable for modeling outcomes from a large number of units. We develop a new method to sample from posterior distributions of Bayesian models, without using MCMC. Samples are independent, so they can be collected in parallel, and we do not need to be concerned with issues like chain convergence and autocorrelation. The algorithm is scalable under the weak assumption that individual units are conditionally independent, making it applicable for large data sets. It can also be used to compute marginal likelihoods.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2014.0901\">http://dx.doi.org/10.1287/mksc.2014.0901</ext-link>.", "e:keyword": ["Parallel Bayesian computation", "Rejection sampling", "Big data", "Multilevel models", "Marginal likelihood", "Customer heterogeneity", "MCMC", "Sparse optimization", "Exploiting sparsity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0902", "e:abstract": "Networks and the embedded relationships are critical determinants of how people communicate and form beliefs. The explosion of social media has significantly increased the scope and impact of social learning among consumers. This paper studies observational learning in networks of friends versus strangers. A consumer decides whether to adopt a product after receiving a private signal about product quality and observing the actions of others. The preference for the product has greater heterogeneity in the stranger-network than in the friend-network. We show that when the network is small, observing friends’ actions helps the consumer make more accurate inferences about quality. As the network grows, however, the stranger-network becomes more effective. Underlying these results are two competing effects of network heterogeneity on social learning. These are the <i>individual preference effect</i>, which allows one to make a better quality judgment when the preference element of past actions is more certain, and the <i>social conforming effect</i>, wherein private signals are underused in quality judgment as people follow others’ actions. We find cascading is more likely to occur in the friend-network than in the stranger-network. For a high-quality firm, the stranger-network generates greater sales than the friend-network when the network size is sufficiently large or the private signal is sufficiently accurate. We also examine the existence of experts and firms using advertising to influence consumers. Finally, we show how networks that are highly homogeneous or heterogeneous could impede observational learning.", "e:keyword": ["Social learning", "Social interaction", "Social networks", "Social media", "Cascade", "Observational learning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0903", "e:abstract": "Consumer search activities can be endogenously determined by the ad positions in sponsored search advertising. We model how advertisers compete for ad positions in sponsored listings and, conditional on the list of sponsored ads, how online consumers search for information and make purchase decisions. On the consumer side, assuming that users browse information from top to bottom and adopt a sequential search strategy, we develop a two-stage model of consumer search (whether to click and whether to stop the search) that extends the standard sequential search framework in economics literature. On the advertiser side, it is very difficult to fully specify the optimal strategies of advertisers because the equilibrium outcome depends on variables that researchers do not observe. As we have an incomplete model of advertiser competition, we propose using the necessary condition that, at equilibrium, no advertiser will find another available ad position more valuable than the one it has chosen. Using a data set obtained from a search engine, we find that consumers can be classified into two segments that exhibit distinct search behaviors. For advertisers, the value of search advertising comes primarily from terminal clicks, which represent the last link (including organic results) clicked by an online user. We also demonstrate that the value of ad positions depends not only on the identities and positions of the advertisers in sponsored listings but also the composition of online consumers who exhibit distinct search behaviors.", "e:keyword": ["Search advertising", "Advertiser value", "Advertising competition", "Impressions", "Clicks", "Terminal clicks", "Moment inequality"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0905", "e:abstract": "This research examines the effects of hyper-contextual targeting with physical crowdedness on consumer responses to mobile ads. It relies on rich field data from one of the world’s largest telecom providers who can gauge physical crowdedness in real-time in terms of the number of active mobile users in subway trains. The telecom provider randomly sent targeted mobile ads to individual users, measured purchase rates, and surveyed purchasers and nonpurchasers. Based on a sample of 14,972 mobile phone users, the results suggest that, counterintuitively, commuters in crowded subway trains are about twice as likely to respond to a mobile offer by making a purchase vis-à-vis those in noncrowded trains. On average, the purchase rates measured 2.1% with fewer than two people per square meter, and increased to 4.3% with five people per square meter, after controlling for peak and off-peak times, weekdays and weekends, mobile use behaviors, and randomly sending mobile ads to users. The effects are robust to exploiting sudden variations in crowdedness induced by unanticipated train delays underground and street closures aboveground. Follow-up surveys provide insights into the causal mechanism driving this result. A plausible explanation is mobile immersion: As increased crowding invades one’s physical space, people adaptively turn inwards and become more susceptible to mobile ads. Because crowding is often associated with negative emotions such as anxiety and risk-avoidance, the findings reveal an intriguing, positive aspect of crowding: Mobile ads can be a welcome relief in a crowded subway environment. The findings have economic significance because people living in cities commute 48 minutes each way on average, and global mobile ad spending is projected to exceed $100 billion. Marketers may consider the crowdedness of a consumer’s environment as a new way to boost the effectiveness of hyper-contextual mobile advertising.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0905\">http://dx.doi.org/10.1287/mksc.2015.0905</ext-link>.", "e:keyword": ["Mobile advertising", "Hyper-contextual targeting", "Crowdedness", "Field study", "New technology"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0906", "e:abstract": "This research examines how prepurchase information that reduces consumer uncertainty about a product or service can affect consumer decisions to reverse an initial product purchase or service enrollment decision. One belief commonly held by retailers is that provision of greater amounts of information before the purchase reduces decision reversals. We provide theory and evidence showing conditions under which uncertainty-reducing information provided before the purchase decision can actually increase the number of decision reversals. Predictions generated from an analytical model of consumer behavior incorporating behavioral theory of reference-dependence are complemented by empirical evidence from both a controlled behavioral experiment and econometric analysis of archival data. Combined, the theory and evidence suggest that managers should be aware that their information provision decisions taken to reduce decision reversals may actually increase them.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0906\">http://dx.doi.org/10.1287/mksc.2015.0906</ext-link>.", "e:keyword": ["Decision reversal", "Product returns", "Service cancellations", "Reference-dependence"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0908", "e:abstract": "A vexing challenge when using the utility-maximization framework to estimate consumers’ decisions on which set of goods to purchase and how much quantity to buy is obtaining a functional form of the utility that satisfies three criteria: tractability, flexibility, and global regularity. Flexibility refers to the ability of a utility function to impose minimal prior restrictions on demand elasticities. Global regularity refers to the ability of a utility function to satisfy regularity properties required by economic theory in the entire feasible space of variables. The tractable utility functions used so far are either inflexible, which could yield inaccurate estimates of underlying elasticities, or do not satisfy global regularity, which can result in invalid expressions of likelihood and invalid policy simulations. I tackle this problem by deriving necessary and sufficient conditions for global regularity of Basic Translog utility. Using simulated and scanner data, I show that the proposed demand system yields better model fit, more accurately captures underlying elasticities, and yields substantially different results in counterfactuals compared to alternatives used in prior literature. Specifically, unlike the alternatives used so far, the proposed demand system allows for complementarities between goods, and more accurately captures the extent of their inferiority, the extent of their substitutability, and asymmetries in cross price effects.", "e:keyword": ["Utility theory", "Econometrics", "Global regularity", "Flexible functional forms"]}, {"e:volume": "35", "e:issue": "1", "e:year": 2016, "e:keyword": ["Product competition", "Quality", "Entry", "Cannibalization", "Business stealing", "Retailing"], "@id": "http://dx.doi.org/10.1287/mksc.2015.0909", "e:abstract": "We empirically study the impact of the entry of a new theater on two important product decisions that incumbents in the movie exhibition industry face: (1) whether to invest in screening movies that are expected to be popular, and (2) when to adopt new releases. For theaters, both of these decisions feature a cost-demand trade-off inherent in quality decisions: Although screening popular and recent movies brings more patrons to the theater, distributors take a higher share of the revenue for such movies. The impact of competitive entry on the incumbent’s quality decisions is ambiguous, as it may simultaneously increase the competitive pressure to invest more in these dimensions of quality and also change the demand conditions that incumbents face. We find that incumbent theaters do not increase the provision of popular and recent movies in response to rival entry. To identify the role of competitive incentives, we study the differential impact of entry based on whether the entrant belongs to the same parent firm as the incumbent theaters. This comparison reveals that competitive incentives push incumbents to screen movies with high expected success more frequently and to adopt movies sooner. The product responses we document have important implications for the revenue impact of entry and the conclusions that researchers can draw from this impact. Ignoring the provision of these quality dimensions suggests cannibalization to exceed business stealing, a conclusion that is reversed when we account for endogenous product responses. We also show that our findings on popularity and recency cannot be explained by concomitant changes in theaters’ other product decisions, such as the variety of movies screened."}, {"e:volume": "35", "e:issue": "1", "e:year": 2016, "e:keyword": ["Countermarketing", "Vice goods", "Cigarette marketing"], "@id": "http://dx.doi.org/10.1287/mksc.2015.0910", "e:abstract": "Countermarketing, or efforts to reduce consumption of certain products, has become common in categories such as tobacco, junk food, fossil fuels, and furs. Countermarketing has a particularly long history in the tobacco industry. Efforts to reduce smoking have included excise taxes that increase the cost of consumption, smoke-free restrictions that make consumption less convenient, and antismoking advertising campaigns that highlight the dangers of tobacco use. This article presents an analysis of the relative effectiveness of these different strategies. We find that cigarette excise taxes are the most effective tool for reducing overall cigarette sales, followed by antismoking advertising. Smoke-free restrictions are not found to have a significant effect on cigarette sales. We also investigate how these various policy tools induce product substitution. This issue is of considerable importance because some countermarketing techniques may potentially shift consumers to more dangerous, higher nicotine and tar cigarettes. Specifically, we find that excise taxes levied on a per pack basis rather than based on nicotine levels often shift consumers to more dangerous products."}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0912", "e:abstract": "Firms are increasingly engaging with customers on social media. Despite this heightened interest, guidance for effective engagement is lacking. In this study, we investigate customers’ compliments and complaints and firms’ service interventions on social media. We develop a dynamic choice model that explicitly accounts for the evolutions of both customers’ voicing decisions and their relationships with the firm. Voices are driven by both the customers’ underlying relationships and other factors such as redress seeking. We estimate the model using a unique data set of customer voices and service interventions on Twitter. We find that redress seeking is a major driver of customer complaints, and although service intervention improves relationships, it also encourages more complaints later. Because of this dual effect, firms are likely to underestimate the returns on service intervention if measured using only voices. Furthermore, we find an “error-correction” effect in certain situations, where customers compliment or complain when others voice the opposite opinions. Finally, we characterize the distinct voicing tendencies in different relationship states, and show that uncovering the underlying relationship states enables effective targeting. We are among the first to analyze individual customer level voice dynamics and to evaluate the effects of service intervention on social media.", "e:keyword": ["Service intervention", "Social media", "Microblogging", "Word of mouth", "Customer relationship", "Hidden Markov model", "Choice model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0913", "e:abstract": "Research has shown that advertising assets and R&D (research and development) assets increase shareholder value. Although one might conclude that their impacts on bankruptcy risk are merely the inverse of their impacts on shareholder value, we argue otherwise and show that the differences hinge on the fact that shareholder value is a function of expected cash flows from all future periods, whereas bankruptcy risk is a function of expected cash flow from only the next period. We show that current market turbulence moderates the impacts of advertising assets and R&D assets on expected cash flow from the next period but not on expected cash flows from more distant future periods. Therefore, market turbulence moderates the impacts of advertising assets and R&D assets on bankruptcy risk but not shareholder value. Market <i>stability</i> increases the impact of advertising assets on reducing bankruptcy risk, whereas market <i>turbulence</i> increases the impact of R&D assets on reducing bankruptcy risk. Using a data set of more than 1,000 firms covering three decades, we find support for our hypotheses. Out-of-sample validation indicates that bankruptcy prediction performance improves when including marketing variables in addition to the usual financial predictors.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0913\">http://dx.doi.org/10.1287/mksc.2015.0913</ext-link>.", "e:keyword": ["Bankruptcy", "Advertising", "R&D", "Shareholder value", "Turbulence"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0914", "e:abstract": "We study the pricing decision for a monopolist launching a new innovation. At the time of launch, we assume that the monopolist has incomplete information about the true demand curve. Despite the lack of objective information the firm must set a retail price to maximize total profits. To model this environment, we develop a novel two-period non-Bayesian framework where the monopolist sets the price in each period based only on a <i>nonparametric set of all feasible demand curves</i>. Optimal prices are dynamic as prices in any period allow the firm to learn about demand and improve future pricing decisions. Our main results show that the direction of dynamic introductory prices (versus static prices) depends on the type of heterogeneity in the market. We find that (1) when consumers have homogeneous preferences, introductory dynamic price is higher than the static price; (2) when consumers have heterogeneous preferences and the monopolist has no ex ante information, the introductory dynamic price is the same as the static price; and (3) when consumers have heterogeneous preferences and the monopolist has ex ante information, the introductory dynamic price is lower than the static price. Furthermore, the degree of this initial reduction increases with the amount of heterogeneity in the ex ante information.", "e:keyword": ["Non-Bayesian learning", "Ambiguity", "Pricing", "New products"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0915", "e:abstract": "In keyword search advertising, many advertisers operate on a limited budget. Yet how limited budgets affect keyword search advertising has not been extensively studied. This paper offers an analysis of the generalized second-price auction with budget constraints. We find that the budget constraint may induce advertisers to raise their bids to the highest possible amount for two different reasons, i.e., to accelerate the elimination of the budget-constrained competitor, and to reduce their own advertising cost. Thus, in contrast to the current literature, our analysis shows that both budget-constrained and unconstrained advertisers could bid more than their own valuation. We further extend the model to consider dynamic bidding and budget-setting decisions.", "e:keyword": ["Keyword search advertising", "Budget constraint", "Generalized second-price auction", "Online advertising", "Competitive bidding strategy", "Analytical model"]}, {"e:volume": "35", "e:issue": "1", "e:year": 2016, "e:keyword": ["Fat tax", "Obesity", "Elasticity", "Milk pricing", "Quasi experiment"], "@id": "http://dx.doi.org/10.1287/mksc.2015.0917", "e:abstract": "Of the many proposals to reverse the obesity epidemic, the most contentious is the use of price-based interventions such as the fat tax. Previous investigations of the efficacy of such initiatives in altering consumption behavior yielded contradictory findings. In this article, we use six years of point-of-sale scanner data for milk from a sample of over 1,700 supermarkets across the United States to investigate the potential of small price incentives for inducing substitution of healthier alternatives. We exploit a pricing pattern particular to milk in the United States, whereby prices in some geographical regions are flat across whole, 2%, 1%, and skim milk; whereas in other regions they are decreasing with the fat content level. The prevailing price structure is determined at a chain and regional level, and is independent of local demand conditions. This exogenous variation in price structure provides a quasi-experimental set-up to analyze the impact of small price differences on substitution across fat content. We use detailed demographics to evaluate price sensitivity and substitution patterns for different socioeconomic groups. Results show that small price differences are highly effective in inducing substitution to lower calorie options. The impact is highest for low-income households who are also most at risk for obesity. Our results suggest that a selective taxation mechanism that lowers the relative prices of healthier options, such that those price changes are reflected in shelf prices at the point-of-purchase, can serve as an effective health policy tool in the efforts to control obesity.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0917\">http://dx.doi.org/10.1287/mksc.2015.0917</ext-link>.<ext-link ext-link-type=\"uri\"  href=\"https://www.informs.org/About-INFORMS/News-Room/Press-Releases/Fat-Tax\"><b>Press Release</b></ext-link>"}, {"e:volume": "35", "e:issue": "1", "e:year": 2016, "e:keyword": ["Customer learning", "Evaluation", "Product quality", "Search", "Vertical differentiation"], "@id": "http://dx.doi.org/10.1287/mksc.2015.0918", "e:abstract": "When match uncertainty is resolved via costly evaluation, the first product sampled by a customer is more likely to make the sale. This prompts firms to lower their products’ evaluation costs to attract customers to sample their products first. Such efforts by firms are called customer learning investment (CLI). When product quality is freely observable but horizontal match is not, we examine how CLI choices interact with quality and price competition in a duopoly. CLI has a <i>competition effect</i> in that a higher CLI of a firm increases its demand but decreases that of its rivals. In the market-covered duopoly, both qualities will decrease (increase) when the high-end (low-end) firm invests more in customer learning, and remain unchanged when they invest equally, relative to when neither firm invests. We further show that the firm with a higher relative production efficiency invests more in customer learning than the competitor. In the market-not-covered duopoly, CLI by the low-end firm also creates a <i>market-expansion effect</i> by inducing some additional low-end customers to sample and purchase its product. This may induce it to invest more than the high-end firm, even when the latter has a higher relative production efficiency."}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0919", "e:abstract": "In sponsored search advertising, advertisers bid to be displayed in response to a keyword search. The operational activities associated with participating in an auction, i.e., submitting the bid and the ad copy, customizing bids and ad copies based on various factors (such as the geographical region from which the query originated, the time of day and the season, the characteristics of the searcher), and continuously measuring outcomes, involve considerable effort. We call the costs that arise from such activities <i>keyword management costs</i>. To reduce these costs and increase advertisers’ participation in keyword auctions, search engines offer an opt-in tool called <i>broad match with automatic and flexible bidding</i>, wherein the search engine automatically places bids on behalf of the advertisers and takes over the above activities as well. The bids are based on the search engine’s estimates of the advertisers’ valuations and, therefore, may be less accurate than the bids the advertisers would have turned in themselves. Using a game-theoretic model, we examine the strategic role of keyword management costs, and of broad match, in sponsored search advertising. We show that because these costs inhibit participation by advertisers in keyword auctions, the search engine has to reduce the reserve price, which reduces the search engine’s profits. This motivates the search engine to offer broad match as a tool to reduce keyword management costs. If the accuracy of broad match bids is sufficiently high, advertisers adopt broad match and benefit from the cost reduction, whereas if the accuracy is very low, advertisers do not use it. Interestingly, at moderate levels of bid accuracy, advertisers individually find it attractive to reduce costs by using broad match, but competing advertisers also adopt broad match and the increased competition hurts all advertisers’ profits, thus creating a “prisoner’s dilemma.” When advertisers adopt broad match, search engine profits increase. It therefore seems natural to expect that the search engine will be motivated to improve broad match accuracy. Our analysis shows that the search engine will increase broad match accuracy up to the point where advertisers choose broad match, but that increasing the accuracy any further reduces the search engine’s profits.", "e:keyword": ["Paid search advertising", "Position auctions", "Bidding costs", "Automatic bidding", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0921", "e:abstract": "Firms invest in technology to increase productivity. Yet in emerging markets, where a culture of informality is widespread, information technology (IT) investments leading to greater transparency can impose a cost through higher taxes and the need for regulatory compliance. The tendency of firms to avoid productivity-enhancing technologies and remain small to avoid transparency has been dubbed the “Peter Pan Syndrome.” We examine whether firms make the trade-off between productivity and transparency by examining IT adoption in the Indian retail sector. We find that computer technology adoption is lower when firms are motivated to avoid transparency. Specifically, technology adoption is lower when there is greater corruption, but higher when there is better enforcement and auditing. So, firms have a higher productivity gain threshold to adopt computers in corrupt business environments that suffer from patchy and variable enforcement of the tax laws. Not accounting for this motivation to hide from the formal sector <i>underestimates</i> productivity gains from computer adoption. Thus, in addition to their direct effects on the economy, enforcement, auditing, and corruption can have indirect effects through their negative impact on adoption of productivity-enhancing technologies that also increase operational transparency.", "e:keyword": ["Retailing", "IT adoption", "Productivity", "Informal economy", "Emerging markets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0922", "e:abstract": "Building on the structural two-sided matching model, we develop a framework to study the sourcing market in the context of marketing firms matching with manufacturers. Both sides prefer partners that could generate significant values with better sourcing process abilities. Moreover, experienced manufacturers are preferred by the branded marketing firms who may even be willing to compensate the matching intermediary more for facilitating that preference. Empirical research, measuring the values of such matching and the intermediary’s pricing (through commission) on observed marketing firms’ characteristics and the low-cost manufacturers and the deals that result, is problematic, when some of the characteristics are only partially observed and the matching is endogenous. With the matching model, we can control for endogenous matching. We find evidence of positive assortative matching of pairs’ size on both sides of the market. We also find that manufacturers’ location and tenure, and whether the marketing firms are listed, are important factors in identifying the preferred matching partners and the related ranking. Without controlling for endogenous matching, the intermediary’s pricing equation estimates are biased, especially for the marketing firms that specialize in luxury products.", "e:keyword": ["Matching", "Sourcing", "Structural model", "Bayesian estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0923", "e:abstract": "One of the most intriguing findings in the multichannel customer management literature is the positive association between multichannel purchasing and customer profitability. The question is whether this finding can be put into action. That is, can a firm develop a marketing campaign to increase multichannel purchasing and hence average customer profitability, and if so what are the key factors that enable success. We design and implement a randomized field experiment to investigate this question. The field experiment tests four marketing campaigns that vary in the communications message and the provision of financial incentives. We find that the multichannel/profitability relationship is actionable. A properly designed marketing campaign increases the number of multichannel customers and increases average customer profitability. That campaign’s message emphasizes the benefits of multichannel shopping but does not rely on financial incentives. Moreover, we use propensity score matching to show that, after accounting for self-selection, multichannel customers are more profitable than they would be if they were not multichannel. A post-test analysis suggests that the multichannel/nonfinancial incentive campaign succeeded in inducing customers to become multichannel because it decreased customer reactance and increased perceived behavioral control.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0923\">http://dx.doi.org/10.1287/mksc.2015.0923</ext-link>.", "e:keyword": ["Multichannel shopping", "Customer profitability", "Field experiment", "Treatment effect on the treated"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0924", "e:abstract": "In managing service failures such as stockouts, most research has emphasized preventive mechanisms, whereas stockout recovery mechanisms have been largely ignored. We propose and examine a failure-recovery mechanism (i.e., contractual stockout recovery) in the presence of demand uncertainty and compare it with failure-prevention mechanisms in a dyadic distribution channel.We find that stockout recovery mechanisms can improve channel profitability under certain conditions. More importantly, we find that stockout recovery may outperform stockout prevention mechanisms such as return policy and vendor managed inventory in improving manufacturer and channel profitability. This is because stockout recovery reduces channel-wide stockout risks and allows benefits from the reduced risks to be shared between the manufacturer and the retailer, helping alleviate double marginalization. Although return policy also reduces stockout risks, it does so by increasing inventory risks in the channel without reducing channel exposure to demand uncertainty. Thus, our research suggests that stockout recovery can be an effective alternative in managing stockouts to those common methods of stockout prevention mechanisms.", "e:keyword": ["Service failure prevention and recovery", "Stockouts", "Distribution channel governance"]}, {"e:volume": "35", "e:issue": "1", "e:year": 2016, "e:keyword": ["Publicity", "Informative detailing and advertising", "Information complements and substitutes", "Demand", "Corroborative evidence", "Rational inattention", "Prescription drugs"], "@id": "http://dx.doi.org/10.1287/mksc.2015.0925", "e:abstract": "Over the past 10 years there has been increased recognition of the importance of publicity as a means of generating product awareness. Despite this, previous research has seldom investigated the impact of publicity on demand. We contribute to the literature by (i) proposing a new method for the interpretation of publicity data, one that maps the information in news articles (or broadcasts) to a multidimensional attribute space; (ii) investigating how different types of publicity affect demand; and (iii) investigating how different types of publicity interact with firms’ own marketing communication efforts. We study these issues for statins. We find that publicity plays an important role both for expanding the market for statins and for determining which statins patients or physicians choose. We also find evidence that publicity can serve as either a substitute or a complement for traditional marketing channels depending on the complexity of the information type. We argue that the interaction results are driven by the relative strengths of the corroborative and rational inattention functions in publicity. These results suggest that managers should be aware of the interactions between publicity and traditional marketing channels to better determine how to allocate their marketing expenditures."}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0926", "e:abstract": "This paper investigates the economic value of online reviews for consumers and restaurants. We use a data set from <ext-link ext-link-type=\"uri\"  href=\"http://Dianping.com\">Dianping.com</ext-link>, a leading Chinese website providing user-generated reviews, to study how consumers learn, from reading online reviews, the quality and cost of restaurant dining. We propose a learning model with three novel features: (1) different reviews offer different informational value to different types of consumers; (2) consumers learn their own preferences, and not the distribution of preferences among the entire population, for multiple product attributes; and (3) consumers update not only the expectation but also the variance of their preferences. Based on estimation results, we conduct a series of counterfactual experiments and find that the value from Dianping is about 7 CNY for each user, and about 8.6 CNY from each user for the reviewed restaurants in this study. The majority of the value comes from reviews on restaurant quality, and contextual comments are more valuable than numerical ratings in reviews.", "e:keyword": ["Online reviews", "User-generated content", "Consumer choice under uncertainty", "Learning", "Economic value to consumer and firm"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0927", "e:keyword": []}, {"e:volume": "35", "e:issue": "1", "e:year": 2016, "e:keyword": ["Structural models", "Auctions", "Freelance markets", "Seller marketpower", "Unobserved heterogeneity", "Online markets"], "@id": "http://dx.doi.org/10.1287/mksc.2015.0929", "e:abstract": "Beauty contests are auction mechanisms used to buy or sell differentiated products where the auctioneer does not specify a decision rule to pick the winning bidder. Beauty contests are widely used in procuring welfare-to-work projects, freelance services, selling online ads, real estate transactions, and hiring, dating/marriage decisions. Unlike price-only auctions, beauty contests have no closed-form bidding strategies and suffer from nonmultiplicatively separable unobserved auction heterogeneity, which makes their estimation challenging. To address these challenges, we formulate beauty contests as incomplete information games and present a two-step estimator. A key contribution of our method is its ability to account for common-knowledge auction-specific unobservables using finite unobserved types. We show that unobserved auction types and distributions of bids are nonparametrically identified and recoverable in the first step using a nonparametric Expectation-Maximization (EM)-like algorithm, and that these can then be used in the second step to recover cost distributions. We present an application of our method in the online freelancing context. We find that seller margins in this marketplace are around 15% of the bid, and that not accounting for unobserved heterogeneity can significantly bias cost estimates in this setting. Based on our estimates, we run counterfactual simulations and provide guidelines to managers of freelance firms.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0929\">http://dx.doi.org/10.1287/mksc.2015.0929</ext-link>."}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0930", "e:abstract": "Firms track consumers’ shopping behaviors in their online stores to provide individually personalized banners through a method called retargeting. We use data from two large-scale field experiments and two lab experiments to show that, although personalization can substantially enhance banner effectiveness, its impact hinges on its interplay with timing and placement factors. First, personalization increases click-through especially at an early information state of the purchase decision process. Here, banners with a high degree of content personalization (DCP) are most effective when a consumer has just visited the advertiser’s online store, but quickly lose effectiveness as time passes since that last visit. We call this phenomenon overpersonalization. Medium DCP banners, on the other hand, are initially less effective, but more persistent, so that they outperform high DCP banners over time. Second, personalization increases click-through irrespective of whether banners appear on motive congruent or incongruent display websites. In terms of view-through, however, personalization increases ad effectiveness only on motive congruent websites, but decreases it on incongruent websites. We demonstrate in the lab how perceptions of ad informativeness and intrusiveness drive these results depending on consumers’ experiential or goal-directed Web browsing modes.", "e:keyword": ["Retargeting", "Online advertising", "Personalization", "Advertising effectiveness"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0931", "e:abstract": "This paper examines position auctions with budget-constrained advertisers, a dominant bidding environment used by publishers to allocate positions in online advertising. Budget constraints play a crucial role in equilibrium bidding by inducing advertisers to strategically deplete a higher-ranked advertiser’s budget to gain in rank. This strategic consideration has consequences for the advertisers’ profits and the publisher’s revenue. An advertiser’s profit can strictly decrease with her budget when competition for an advertising space (e.g., a keyword) is intense. The publisher’s revenue can also strictly decrease when an increase in the higher-ranked advertiser’s budget induces the lower-ranked rival to reduce her bid, due to her inability to deplete the higher-ranked advertiser’s budget. Several managerial implications for advertisers and publishers are discussed.", "e:keyword": ["Position auctions", "Generalized second-price auctions", "Budget constraints", "Internet marketing", "Online advertising", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0932", "e:abstract": "Georgia Aquarium (GA), a non-profit organization, is the world’s second largest aquarium (by water volume) and is among the most popular tourist destinations in the United States. Recently GA management has observed that the organization’s growth trajectory is stagnating. While other aquariums face a similar trend, GA wants to be proactive and reverse this direction. They face a multifaceted business challenge involving four conflicting objectives: (1) How to increase revenues without increasing ticket prices; (2) how to increase attendance without compromising visitor satisfaction; (3) how to increase the impact of media investments without spending more; (4) how to attract customers with long-term value potential. To address these challenges, we developed an integrated approach consisting of multiple marketing science models including Data Envelopment Analysis, Competition Analysis, Spatial Analysis, Media Optimization Analysis, and Pass Holder Lifetime Net Revenue Analysis. Based on the findings of our analyses and the parameter estimates of our models, GA proceeded with a field implementation to validate our suggestions. As a result, they realized a 10% increase in attendance and a 12% increase in revenue in 2013, thereby enhancing their bottom line and growth.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0932\">http://dx.doi.org/10.1287/mksc.2015.0932</ext-link>.", "e:keyword": ["Non-profit", "Data envelopment analysis", "Media optimization", "Pass holder lifetime net revenue", "Effective media", "Media spend", "Type II Tobit model", "Linear regression model", "Spatial analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0933", "e:abstract": "Manufacturers in many industries frequently use vertical price policies, such as minimum advertised price (MAP), to influence prices set by downstream retailers. Although manufacturers expect retail partners to comply with MAP policies, violations of MAP are common in practice. In this research, we document and explain both the <i>extent</i> and the <i>depth</i> of MAP policy violations. We also shed light on <i>how retailers vary</i> in their propensity to violate MAP policies, and the depth by which they do so.Our inductive research approach documents managerial wisdom about MAP practices. We confront these insights from practice with a large empirical study that includes hundreds of products sold through hundreds of retailers. Consistent with managerial wisdom, we find that authorized retailers are more likely to comply with MAP than are unauthorized partners. By contrast to managerial wisdom, we find that authorized and unauthorized markets are largely separate, and that violations in the authorized channel have a small association with violations in the unauthorized channel. Last, we link our results to the literatures on agency theory, transaction cost analysis, and theories of price obfuscation.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0933.", "e:keyword": ["Pricing policy", "Legal", "Pricing", "Channel relationships"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0934", "e:abstract": "In this editorial accompanying the Special Section on Marketing Science in Emerging Markets (MSEM), we describe how research on emerging markets can contribute to richer theoretical and substantive understanding of markets and marketing. Such research can also aid in providing managerial guidance on how to operate in emerging markets. We conclude with a description of the selection and review process for the special section and an overview of the four papers being published.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0936", "e:abstract": "The window between a film’s theatrical and video releases has been steadily declining with some studios now testing day-and-date strategies (i.e., when a film is released across multiple channels at once). We present a model of consumer choice that examines trade-offs between substitutable products (theatrical and video forms), the possibility of purchasing both alternatives, a congestion externality affecting consumption at theaters with heterogeneous consumer groups, and a decay in the quality of the content over time. Our model permits a normative study of the impact of shorter release windows (zero–three months) for which there is a scarcity of relevant data. We characterize the market conditions under which a studio makes video release time and price selections indicative of direct-to-video, day-and-date, and delayed video release tactics. During seasons of peak congestion, we establish that day-and-date strategies are optimal for high-quality films with high content durability (i.e., films whose content tends to lead consumers to purchase both alternatives) whereas prices are set to perfectly segment the consumer market for films with low content durability. We find that lower congestion effects provide studios with incentives to delay release and price the video to induce multiple purchasing behavior for films with higher content durability. However, an increase in congestion effects can, in certain cases, actually lead to higher studio profitability. We also show that, at the lower range of quality, an increase in movie quality should often be accompanied by a later video release time. Surprisingly, however, we observe the opposite result at the upper range of movie quality: an increase in quality can justify an earlier release of the video.", "e:keyword": ["Channel relationships", "Game theory", "Marketing-operations interface", "Film industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0937", "e:abstract": "This paper considers the creation and consumption of content on user-generated content platforms, e.g., reviews, articles, chat, videos, etc. On these platforms, users’ expectations regarding the amount and timing of participation by others becomes germane to their own involvement levels. Accordingly, we develop a dynamic rational expectations equilibrium model of joint consumption and generation of information. We estimate the model on a novel data set from a large Internet forum site and offer recommendations regarding strategies of managing sponsored content and content quality.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0937\">http://dx.doi.org/10.1287/mksc.2015.0937</ext-link>.", "e:keyword": ["User-generated content", "Marketing strategy", "Rational expectations", "Approximate aggregation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0938", "e:abstract": "In this paper, I investigate the question of how a firm producing substitutes should coordinate price promotions of these products. I model price competition between two firms, each producing two products that are horizontally differentiated with respect to some characteristic. Consumers are divided into loyals, who always purchase their preferred product, and switchers, who have heterogeneous preferences for the four products. If consumers substitute easily between the products produced by one firm, the firms promote one product at a time to avoid cannibalization. If consumers mainly substitute between the products with the same characteristic, the firms often employ joint promotions with at least one product at a deep discount. If, at the same time, consumers easily substitute to the products in other categories, the firms use joint promotions less often and avoid simultaneous deep discounts.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0938\">http://dx.doi.org/10.1287/mksc.2015.0938</ext-link>.", "e:keyword": ["Pricing", "Price promotions", "Product line", "Consumer heterogeneity", "Mixed strategies"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0939", "e:abstract": "Beverage consumption occurs many times a day in response to short-run needs that fluctuate. We develop a model in which consumers are heterogeneous in self-regulating consumption by balancing short-run needs (e.g., hydration and mood) with long-term goals (e.g., health). The model has two novel features: (1) utility depends on the match between occasion-specific needs and product attributes, and (2) dynamics of consumption and stockpiling are at the level of product attributes. We estimate the model using unique intraday beverage consumption, activity, and psychological needs data. We find that only a third of individuals do not self-regulate. Of the two-thirds who self-regulate, over 40% self-regulate adaptively based on past choice, whereas 25% self-regulate both adaptively and anticipating future needs. Our attribute–need match model enables us to assess unmet demand for new products with attributes that match co-occurring occasion-specific needs. Specifically, we find that a product satisfying a combination of “health-hydrating” needs expands overall beverage consumption by as much as 5%. Our framework of modeling heterogeneity in self-regulation by balancing short-run needs with long-term goals is more broadly applicable in contexts where situational needs vary, and long-term effects are gradual and hard to discern (e.g., nutrition, smoking, and preventive health care).", "e:keyword": ["Dynamic discrete choice", "EM algorithm", "Self-regulation", "Stockpiling", "Health care", "Needs", "Goals", "Obesity", "Beverages", "New product introductions"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0940", "e:abstract": "This paper investigates early stage “modern” grocery retail adoption in an emerging market using primary household-level panel data on grocery purchases in India’s largest city, Mumbai. Specifically, we seek insight on which socioeconomic class is more likely to adopt, and why. We model adoption as a two-stage process of modern retail choice followed by category expenditures within a shopping trip. We find a nonmonotonic (V-shaped) relationship between socioeconomic class and preferences for modern retail; specifically, modern retail spending and relative preference are greater among the upper and lower middle classes, relative to the middle middle class. Upper middle class preference of modern retail is driven by credit card acceptance, shorter store distance (relative to other segments), and higher vehicle ownership; whereas lower prices and low travel costs drive the preferences of the lower middle class. Modern retail is preferred more for branded and less for perishable categories. Interestingly, the lower middle class share of modern grocery retail’s revenues is largest, and this share is projected to grow as prices fall and store density increases. To address concerns of endogeneity and generalizability, we replicate the key results with a “conjoint” type study with <i>exogenous variation</i> in price and distance in two cities—Mumbai and Bangalore. We discuss implications for targeting and public policy in emerging markets.", "e:keyword": ["Emerging markets", "Retailing", "Segmentation", "Socioeconomic status", "Middle class"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0941", "e:abstract": "We provide a descriptive study of the cross-category effects of satisfaction with financial services on retention behavior. Behavioral contrast and learning theories provide the bases for our understanding of these effects. Our empirical results reveal the following: (i) Across banking and investment categories, when customers have <i>different</i> providers, satisfaction with one lowers the retention probability in the other service. (ii) A customer who is dissatisfied with the investment service is more likely to stay with the current banking service. (iii) Significantly, we find that when the same firm is involved in both categories, dissatisfaction with the firm in the investment category spills over into the banking category thereby lowering its retention probability. We also find that: (a) among customers who are satisfied with banking (investment), more exposure to media increases retention probability; (b) although switching costs and order of acquisition affect retention, they do not show cross-category interactions with satisfaction. We then obtain implications for customer lifetime value (CLV) and show that it can increase satisfaction by leveraging both the within and across category effects. Bottom line: It is important for a company providing multiple services to measure satisfaction at the category level but to manage customers across categories.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0941\">http://dx.doi.org/10.1287/mksc.2015.0941</ext-link>.", "e:keyword": ["Bayesian estimation", "Endogeneity", "Surveys and services"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0942", "e:abstract": "Bucket-based price discrimination is a unique price format that involves monthly subscription fees and instantaneous quotas (the number of rental products that can be checked out). We propose an empirical model in which consumers make dynamic purchase decisions under consumption uncertainty, accounting for the constraints imposed by the instantaneous quota. Applying the model to an online DVD rental data set, we find that (1) consumers incur a large disutility (∼$8) from <i>stockout</i> (i.e., unmet consumption needs); (2) such a disutility drives consumers' <i>overpurchase</i> of the service quota as a way to avoid potential stockout situations; and (3) the dynamics of overpurchase are driven by the interplay between trends in consumption needs and the magnitude of consumers' plan-switching costs. We run counterfactual exercises to better understand how the instantaneous quota and stockout risk affect consumers' consumption rates, purchase decisions, and firm profitability. We find that the instantaneous quota induces a greater stockout compared with a monthly quota. We further demonstrate that the company should recognize the drivers of the dynamics in overpurchase to balance short- and long-term profitability—for example, by offering targeted discounts to customers with excess overpurchase.", "e:keyword": ["Bucket-based price discrimination", "Subscription service", "Usage uncertainty", "Stockout", "Overpurchase", "Dynamic purchase decisions", "Optimal pricing", "Product design"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0943", "e:abstract": "Market liberalization of the German household electricity market has led to an excessive number of competitors (1,150 electricity providers) and volatile price dynamics on price comparison sites. To date, providers that are struggling to achieve a top ranking on price comparison sites do not appear to implement a consistent or elaborate strategy for attracting customers. We developed a pricing tool, <i>E</i>lectricity <i>C</i>ontract <i>O</i>ptimization (ECO), that addresses this highly competitive market situation by integrating various available data sources, such as data from price comparison sites, demographic data, and regional sales or cost data. ECO sets regionally varying one-time bonuses to attract new customers on price comparison sites with the goal of optimizing sales and profit targets or optimally allocating sales budgets. Based on two field experiments, we demonstrate that ECO’s optimization procedure reduces ENTEGA yearly sales costs for new customer business, on average, by 35% relative to previously used pricing heuristics. ENTEGA uses ECO monthly to analyze different scenarios or to set prices and one-time bonuses on price comparison sites.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0943\">http://dx.doi.org/10.1287/mksc.2015.0943</ext-link>.", "e:keyword": ["Pricing", "Price comparison sites", "Price optimization", "Field experiments"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0944", "e:abstract": "Advertising networks have recently played an increasingly important role in the online advertising market. Critical to the success of an advertising network are two mechanisms: an allocation mechanism that efficiently matches advertisers with publishers and a pricing scheme that maximally extracts surplus from the matches. In this paper, we quantify the value and investigate the determinants of a successful advertiser-publisher match, using data from Taobao’s advertising network. A counterfactual experiment reveals that the platform’s profit under a decentralized allocation mechanism is close to the profit level when the platform centrally assigns the matching under perfect platform knowledge of matching values. In another counterfactual experiment, we explore the effect of platform technology and revenue model on the strategic choice of the pricing schemes of list price versus generalized second price (GSP) auction pricing. We find that platforms that profit from the advertiser side may have less incentive to adopt GSP auction than platforms that profit from the publisher side.", "e:keyword": ["Advertising network", "Matching game", "Maximum score estimation", "Generalized second price auction", "Platform design"]}, {"e:volume": "35", "e:issue": "1", "e:year": 2016, "e:keyword": ["Product line strategy", "Differentiation", "Multi-product competition", "Game theory"], "@id": "http://dx.doi.org/10.1287/mksc.2015.0945", "e:abstract": "In this paper we study product line scope and pricing decisions in a horizontally differentiated duopoly. Past research has shown that a firm may offer a broader product line to attract higher demand or charge a higher price (or both), and benefit at the expense of its competitor. We show that such outcomes may be reversed, especially when consumers have relatively high valuation and low heterogeneity in their preferences for the line extension. We find that an equilibrium exists such that only one firm prefers to expand scope but profits may be higher for both firms, even in the absence of market size expansion. This is because a broader scope permits that firm to effectively price discriminate by raising prices for its core customers. The competitor optimally responds by lowering prices to gain share and earn a higher profit. Thus, higher prices for the firm expanding its product line translate into higher demand for the competing firm, thus increasing profit for both. We show that our results hold when firms deploy generic, offensive or defensive strategies during product line expansion."}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0946", "e:abstract": "As technology advances, new products (e.g., digital cameras, computer tablets, etc.) have become increasingly more complex. Researchers often face considerable challenges in understanding consumers’ preferences for such products. This paper proposes an adaptive decompositional framework to elicit consumers’ preferences for complex products. The proposed method starts with collaborative-filtered initial part-worths, followed by an adaptive question selection process that uses a fuzzy support vector machine active learning algorithm to adaptively refine the individual-specific preference estimate after each question. Our empirical and synthetic studies suggest that the proposed method performs well for product categories equipped with as many as 70 to 100 attribute levels, which is typically considered prohibitive for decompositional preference elicitation methods. In addition, we demonstrate that the proposed method provides a natural remedy for a long-standing challenge in adaptive question design by gauging the possibility of response errors on the fly and incorporating the results into the survey design. This research also explores in a live setting how responses from previous respondents may be used to facilitate active learning of the focal respondent’s product preferences. Overall, the proposed approach offers new capabilities that complement existing preference elicitation methods, particularly in the context of complex products.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0946\">http://dx.doi.org/10.1287/mksc.2015.0946</ext-link>.", "e:keyword": ["New product development", "Support vector machines", "Machine learning", "Active learning", "Adaptive questions", "Conjoint analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0947", "e:abstract": "We study the pass-through of wholesale price changes onto regular retail prices using an unusually detailed data set obtained from a major retailer. We model pass-through as a two-stage decision process that reflects both whether as well as how much to change the regular retail price. We show that pass-through is strongly asymmetric with respect to wholesale price increases versus decreases. Wholesale price increases are passed through to regular retail prices 70% of the time while wholesale price decreases are passed through only 9% of the time. Pass-through is also asymmetric with respect to the magnitude of the wholesale price change, with the magnitude affecting the response to wholesale price increases but not decreases. Finally, we show that covariates such as private label versus national brand, 99-cent price endings, and the time since the last wholesale price change have a much stronger impact on the first stage of the decision process (i.e., whether to change the regular retail price) than on the second stage (i.e., how much to change the regular retail price).Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0947\">http://dx.doi.org/10.1287/mksc.2015.0947</ext-link>.", "e:keyword": ["Regular", "Retail", "Price", "Pricing", "Pass-through"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0948", "e:abstract": "Promotions are used in marketing to increase sales and drive profits by temporarily decreasing the price per unit of a good. Some price promotions apply to all quantities (20% off), some have limits on the number of units that can be purchased at a reduced price, and others only offer the discount if the volume purchased is sufficiently high. We develop a model of price promotions in the context of a direct utility model where its effects are incorporated through the budget constraint. Price promotions complicate the estimation and analysis of direct utility models because they induce kinks and points of discontinuity in the budget set. We propose a Bayesian approach to addressing these irregularities and demonstrate the ability of the direct utility model to be used in counterfactual analyses of price promotions. We investigate the stability of utility function estimates for consumers under alternative price promotions, and find that the majority of the effect of a price promotion is through the budget set, not through changes in the utility function. We also investigate the economic value of customized price promotions where the customization includes the value and format of the offer.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0948\">http://dx.doi.org/10.1287/mksc.2015.0948</ext-link>.", "e:keyword": ["Utility theory", "Bayesian estimation", "Nonlinear pricing", "Irregular budget sets"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0949", "e:abstract": "Motivated by the growing practice of using social network data in credit scoring, we analyze the impact of using network-based measures on customer score accuracy and on tie formation among customers. We develop a series of models to compare the accuracy of customer scores obtained with and without network data. We also investigate how the accuracy of social network-based scores changes when consumers can strategically construct their social networks to attain higher scores. We find that those who are motivated to improve their scores may form fewer ties and focus more on similar partners. The impact of such endogenous tie formation on the accuracy of consumer scores is ambiguous. Scores can become more accurate as a result of modifications in social networks, but this accuracy improvement may come with greater network fragmentation. The threat of social exclusion in such endogenously formed networks provides incentives to low-type members to exert effort that improves everyone’s creditworthiness. We discuss implications for managers and public policy.", "e:keyword": ["Social networks", "Credit score", "Customer scoring", "Social status", "Social discrimination", "Endogenous tie formation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0950", "e:abstract": "In large markets comprising hundreds of products, comprehensive visualization of competitive market structures can be cumbersome and complex. Yet, as we show empirically, reduction of the analysis to smaller representative product sets can obscure important information. Herein we use big search data from a product- and price-comparison site to derive consideration sets of consumers that reflect competition between products. We integrate these data into a new modeling and two-dimensional mapping approach that enables the user to visualize asymmetric competition in large markets (>1,000 products) and to identify distinct submarkets. An empirical application to the LED-TV market, comprising 1,124 products and 56 brands, leads to valid and useful insights and shows that our method outperforms traditional models such as multidimensional scaling. Likewise, we demonstrate that big search data from product- and price-comparison sites provide higher external validity than search data from Google and Amazon.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0950\">http://dx.doi.org/10.1287/mksc.2015.0950</ext-link>.", "e:keyword": ["Big data", "Competitive market mapping", "Asymmetric competition", "Online search", "Product- and price-comparison sites"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0952", "e:abstract": "In this paper, we study the effect of a firm’s local channel exits on prices charged by incumbents remaining in the marketplace. Exits could result in higher prices due to tempered competition or lower prices due to reduced colocation or agglomeration benefits. The net effect of these two countervailing forces remains unknown. In addition, little is known about how this effect could change depending on incumbents’ geographic locations. We address this research gap by examining new car price reactions by incumbent multiproduct automobile dealerships who experience the exit of a Chrysler dealership in their local markets. We find evidence that the competition effect exceeds the colocation effect: prices increase by about 1% ($318) following an exit relative to the price change in the absence of an exit. More important, we find that the price increase is lower at dealerships more proximate to the exiting dealership than dealerships farther away for the same set of cars available across these locations. This finding suggests differences in the extent of the two forces (competition and agglomeration) at different distances from the closed dealership. We assess the generalizability of our results by looking at the impact of GM’s closure of Pontiac dealerships. Taken together, our results inform consumers, firms, and policymakers about possible implications of an exit.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0952.", "e:keyword": ["Competitive reaction", "Market exit", "Pricing", "Agglomeration", "Automobiles", "Bailout"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0953", "e:abstract": "To inform the design of sales force compensation plans when carryover effects exist, we propose a dynamic model where these effects, together with present selling efforts, drive sales. Our results show that a salesperson with low risk aversion exerts effort to decrease attrition from existing business, whereas a salesperson with high risk aversion does not. Why? Because carryover increases not only expected sales but also sales uncertainty. Consequently, the manager should incentivize the high risk-aversion salesperson with a concave compensation plan to counterbalance suboptimal customer attrition, and the low risk-aversion salesperson with a convex compensation plan that limits coasting on past efforts. We generalize our results to when the firm employs multiple salespeople, and when advertising and personal selling are budgeted together.", "e:keyword": ["Sales force", "Compensation", "Sales dynamics", "Agency theory", "Differential games", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0954", "e:abstract": "This paper addresses the repositioning of Kmart Australia in 2011. It shows how by calibrating emotional as well as cognitive reactions and estimating their impact on purchase intentions, Kmart was able to focus its communications, improving market share. We measure nine key emotions, ranging from surprise to anger. Including these emotions significantly improves our model for likelihood to choose a store. Measuring emotions enabled Kmart’s advertising agency to create a television commercial that tapped into the specific emotions that most strongly predict the likelihood to choose a store; that is, the model drove the development of the advertising creative. The resulting television commercial tested well and was effective when launched. At the individual level, cognitions and emotions changed dramatically. At the aggregate level, an econometric model showed that store visits were significantly enhanced. Kmart’s EBIT (earnings before interest and tax) increased by 30%, whereas Kmart’s main rival had almost no EBIT growth, despite vigorous attempts to counter Kmart’s campaign. One of our key contributions is to incorporate emotions into marketing science models of evaluation and purchase intentions. We also provide a new methodology to measure emotions. The approach enables marketing science to participate in the design of marketing stimuli, rather than just testing preexisting ones.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0954\">http://dx.doi.org/10.1287/mksc.2015.0954</ext-link>.", "e:keyword": ["Choice", "Emotions", "Advertising effectiveness", "Services", "Retailing", "Implicit measures"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0955", "e:abstract": "An exchange promotion allows consumers to turn in an old good and receive a discount toward the purchase of a new product. The old good that is turned in can either be within the same category as the new good or it may be in a different category. For example, one can turn in an old CD player to count toward a new CD player (a within-category exchange or traditional trade-in) or toward a new television (a cross-category exchange). This paper studies both within-category and multicategory exchange promotions and analyzes their similarities and differences. In a competitive setting with two firms, we model exchange promotions and establish the equilibrium outcomes. We find that categories in which consumers have a high level of waste aversion are more likely to have multicategory exchange promotions rather than within-category or no promotions. Multicategory exchange promotions can increase both consumers’ replacement purchases and their new purchases. Interestingly, we also find that strategic considerations can lead to a prisoner’s dilemma outcome in which neither firm offers any kind of exchange promotion. However, waste aversion and multicategory exchange promotions can give firms stronger incentives to get out of the prisoner’s dilemma outcome.", "e:keyword": ["Behavioral economics", "Durable goods", "Emerging markets", "Marketing strategy", "Waste aversion", "Promotion", "Price discrimination", "Trade-in", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0956", "e:abstract": "User profile is a summary of a consumer’s interests and preferences revealed through the consumer’s online activity. It is a fundamental component of numerous applications in digital marketing. McKinsey & Company view online user profiling as one of the promising opportunities companies should take advantage of to unlock “big data’s” potential. This paper proposes a modeling approach that uncovers individual user profiles from online surfing data and allows online businesses to make profile predictions when limited information is available. The approach is easily parallelized and scales well for processing massive records of user online activity. We demonstrate application of our approach to customer-base analysis and display advertising. Our empirical analysis uncovers easy-to-interpret behavior profiles and describes the distribution of such profiles. Furthermore, it reveals that even for information-rich online firms profile inference that is based solely on their internal data may produce biased results. We find that although search engines cover smaller portions of consumer Web visits than major advertising networks, their data is of higher quality. Thus, even with the smaller information set, search engines can effectively recover consumer behavioral profiles. We also show that temporal limitations imposed on individual-level tracking abilities are likely to have a differential impact across major online businesses, and that our approach is particularly effective for temporally limited data. Using economic simulation we demonstrate potential gains the proposed model may offer a firm if used in individual-level targeting of display ads.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0956\">http://dx.doi.org/10.1287/mksc.2015.0956</ext-link>.", "e:keyword": ["Big data", "User profiling", "Behavioral targeting", "Topic models", "Internet marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0957", "e:abstract": "Using the University of Texas at Dallas database on publications in the top 24 business journals, we examine the evolution of <i>Marketing Science</i> as reflected by participation of faculty from top ranked business schools on one hand, and diversity on the other hand, as evidenced by contributions from different countries and from faculty of a wider set of schools. We show that faculty from top-ranked business schools have always published in <i>Marketing Science</i>, and continue to do so. We also show that the variety of schools with authors who publish in <i>Marketing Science</i> has increased, and that much of this expansion has occurred outside of North America. This international expansion appears to be driven by collaborations between authors in North America and those in other areas. One of the factors that may be fueling the increase in the variety of schools with authors publishing in <i>Marketing Science</i> is an increased tendency for collaboration by three or more authors. In general, <i>Marketing Science</i> has remained an outlet for authors from top schools, and has also become a place where authors from a much broader array of schools, especially those outside the United States, can publish.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0957\">http://dx.doi.org/10.1287/mksc.2015.0957</ext-link>.", "e:keyword": ["History of marketing science", "Internationalization", "Author collaboration"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0958", "e:abstract": "We introduce a method for identifying, analyzing, and visualizing submarkets in product categories. We give an overview of the market structure and competitive submarket literature and then describe a classic model for testing competitive submarkets along with associated extensions. In the era of big data and with the increasing availability of large-scale consumer purchase data, there is a need for techniques that can interpret these data and use them to help make managerial decisions. We introduce a statistical likelihood based technique for both identifying and testing market structure. We run a series of experiments on generated data and show that our method is better at identifying market structure from brand substitution data than a range of methods described in the marketing literature. We introduce tools for holdout validation, complexity control, and testing managerial hypotheses. We describe a method for visualization of submarket solutions, and we give several traditional consumer product examples and in addition give an example to show how market structure can be analyzed from online review data.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0958\">http://dx.doi.org/10.1287/mksc.2015.0958</ext-link>.", "e:keyword": ["Market structure", "Likelihood estimation", "Brand switching", "Submarket analysis", "Market segmentation", "Cluster analysis", "Big data", "Visualization"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0959", "e:abstract": "A seller often needs to determine the amount of product information to provide to consumers. We model costly consumer information search in the presence of limited information. We derive the consumer’s optimal stopping rule for the search process. We find that, in general, there is an intermediate amount of information that maximizes the likelihood of purchase. If too much information is provided, some of it is not as useful for the purchase decision, the average informativeness per search occasion is too low, and consumers end up choosing not to purchase the product. If too little information is provided, consumers may end up not having sufficient information to decide to purchase the product. The optimal amount of information increases with the consumer’s ex ante valuation of the product, because with greater ex ante valuation by the consumer, the firm wants to offer sufficient information for the consumer to be less likely to run out of information to check. One can also show that there is an intermediate amount of information that maximizes the consumer’s expected utility from the search problem (social welfare under some assumptions). Furthermore, this amount may be smaller than that which maximizes the probability of purchase; that is, the market outcome may lead to information overload with respect to the social welfare optimum. This paper can be seen as providing conditions under which too much information may hurt consumer decision making. Numerical analysis shows also that if consumers can choose to some extent which attributes to search through (but not perfectly), or if the firm can structure the information searched by consumers, the amount of information that maximizes the probability of purchase increases, but is close to the amount of information that maximizes the probability of purchase when the consumer cannot costlessly choose which attributes to search through.", "e:keyword": ["Analytical models", "Behavioral economics", "Game theory", "Search"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0960", "e:abstract": "I introduce the special section highlighting the background of the competition and the work of the finalists in the 2013–2014 Gary L. Lilien ISMS-MSI Practice Prize Competition, showcasing the best applications of rigor and relevance by marketing scientists in working with practical problems. The winning paper is by a team who developed an innovative pricing tool to enable an electric utility operating in Germany to acquire customers on online price comparison sites while optimizing various metrics of interest. The other two finalists comprise a team that developed an integrated marketing model to address multiple business objectives of the Georgia Aquarium and a team that developed a marketing science model of evaluation and purchase intentions incorporating customer emotions to test advertisement effectiveness for Kmart Australia.", "e:keyword": ["Pricing", "Media optimization", "Choice", "Emotions", "Advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0961", "e:abstract": "We study three ways firms can communicate about their brands—paid media (advertising), earned media (word of mouth and online social media), and owned media (brand websites and other owned content)—and the roles these media types play in reminding (i.e., activating memory), informing (i.e., learning their tastes for the brand), or enhancing enjoyment (e.g., gaining additional utility from socializing about the brand). We do this for a new TV show setting using a data set that contains reported viewing, exposures, expectations, and experiences. We present descriptive analyses and results from a new structural model, which indicate that earned media is more impactful than paid and owned media per exposure. However, paid media has far more exposures, so for a given percentage increase, paid media’s influence dominates earned and owned media. Earned media operate primarily through enhancing enjoyment, whereas paid media operate through reminding and owned media through reminding, but discourage live viewing. We find that media exposures help consumers learn about how well they will like the program. However, this learning can either increase or decrease the expected liking, and in our data the average audience effects are negligible. Overall, we find that earned and paid media play a central role in developing and maintaining entertainment brands.", "e:keyword": ["Social engagement", "Informative effects", "Reminding effects", "Entertainment brands", "Word of mouth", "Bayesian learning", "Earned media", "Owned media"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0962", "e:abstract": "In the last two decades, organized retailing has transformed the retailing landscape in emerging economies, where unorganized retailing has traditionally been dominant. In this paper, we build a theoretical model of unorganized and organized retailing in emerging economies by carefully modeling key characteristics of the retailing environment, the retailers, the consumers, and product categories. The primary insight that we obtain is that in a competitive market comprising of only unorganized retailers, the advent of organized retailing injects efficiency into the market leading to a reduction in the number of unorganized retailers. This, in turn, makes the market less competitive. Building on this basic insight, we obtain a number of counterintuitive results. For instance, (i) the presence of organized retailing may increase the prices charged by unorganized retailers; (ii) as the consumers’ transportation cost to the unorganized retailers increases, the market share of the unorganized retailing sector may increase; (iii) as the probability of bulk consumption increases and consumers prefer to purchase more from the organized retailer, prices and profits at the organized retailer may decrease; and (iv) the presence of organized retailing can lead to both consumer and social surplus being lower because consumers face higher prices at unorganized retailers and there is wastage in the economy due to bulk purchasing at organized retailers. Our model offers an explanation for certain surprising empirical observations related to retailing in emerging markets, such as why in the last few years in the Indian market the unorganized retailers who have survived the advent of organized retailing seem to be doing better. Implications from our research can provide guidance to policy makers grappling with issues related to the balanced growth of unorganized and organized retailing in emerging markets.", "e:keyword": ["Unorganized and organized retailing", "Kirana shops", "Developing economies", "Multinational retailers", "Salop model"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0963", "e:abstract": "Accurate predictions of a customer’s activity status and future purchase propensities are crucial for managing customer relationships. This article extends the recency–frequency paradigm of customer-base analysis by integrating regularity in interpurchase timing in a modeling framework. By definition, regularity implies less variation in timing patterns and thus better predictability. Whereas most stochastic customer behavior models assume a Poisson process of “random” purchase occurrence, allowing for regularity in the purchase timings is beneficial in noncontractual settings because it improves inferences about customers’ latent activity status. This especially applies to those valuable customers who were previously very frequently active but have recently exhibited a longer purchase hiatus. A newly developed generalization of the well-known Pareto/NBD model accounts for varying degrees of regularity across customers by replacing the NBD component with a mixture of gamma distributions (labeled Pareto/GGG). The authors demonstrate the impact of incorporating regularity on forecasting accuracy using an extensive simulation study and a range of empirical applications. Even for mildly regular timing patterns, it is possible to improve customer-level predictions; the stronger the regularity, the greater the gain. Furthermore, the cost in terms of data requirements is marginal because only one additional summary statistic, in addition to recency and frequency, is needed that captures historical transaction timing.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0963\">http://dx.doi.org/10.1287/mksc.2015.0963</ext-link>.", "e:keyword": ["Customer-base analysis", "Customer lifetime value", "Purchase regularity", "Stochastic prediction models", "Noncontractual settings", "Pareto/NBD"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0965", "e:abstract": "The marketing and operations disciplines have increasingly accounted for the presence of strategic consumer behavior. Theory suggests that such behavior exists when consumers are able to consider future distribution of prices, and that this behavior exposes firms to intertemporal competition that results with a downward pressure on prices. However, deriving future distribution of prices is not a trivial task. Online decision support tools that provide consumers with information about future distributions of prices can facilitate strategic consumer behavior. This paper studies whether the availability of such information affects transacted prices by conducting an empirical analysis in the context of the airline industry. Studying the effect at the route level, we find significant price reduction effects as such information becomes available for a route, both in fixed-effects and difference-in-differences estimation models. This effect is consistent across the different fare percentiles and amounts to a reduction of approximately 4%–6% in transactions’ prices. Our results lend ample support to the notion that price prediction decision tools make a statistically significant economic impact. Presumably, consumers are able to exploit the information available online and exhibit strategic behavior.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0965\">http://dx.doi.org/10.1287/mksc.2015.0965</ext-link>.", "e:keyword": ["Strategic consumers", "Decision support", "Revenue management", "Information availability", "Airline industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0966", "e:abstract": "This research investigates whether there are enduring effects of goal achievement and failure within customer loyalty promotion programs. We collaborated with a major hotel chain to launch a large scale field experiment involving 95,532 existing loyalty customers. We observed customers’ hotel stays for eight months before the experiment, eight months during the experiment, and eight months after the experiment. Customers in the treatment group were asked to increase their hotel nights during the 8-month promotion by a set percentage relative to their baseline to receive a reward. Overall, the promotion led to increased purchasing in the post-promotion period. However, only 20% of customers successfully reached the goal whereas 80% missed the goal. We use a propensity score analysis to examine the distinct effects of goal achievement versus goal failure. Results show that goal attainment significantly increased post-promotion purchasing whereas goal failure significantly reduced post-promotion purchasing. Additionally, we use econometric methods to empirically test a behavioral theory of relationship-based reciprocity. We find that customers in a high status tier relationship, with the most invested in the firm, are most affected by goal failure whereas customers in a low status tier relationship, with the least invested in the firm, are most affected by goal success. Because the type of loyalty program described in this paper is widely used in a variety of industries the findings suggest that marketers should set reachable goals within loyalty promotion programs. Firms should be particularly cautious about the impact of goal failures for the firm’s most loyal customers.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0966\">http://dx.doi.org/10.1287/mksc.2015.0966</ext-link>.", "e:keyword": ["Loyalty promotion program", "Goal achievement", "Goal failure", "Field experiment"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0967", "e:abstract": "This paper examines how firms should allocate their advertising budgets between consumers who have a high preference for their products (i.e., strong segment) and those who prefer competing products (i.e., weak segment). Targeted advertising transmits relevant information to otherwise uninformed consumers and it is used as a price discrimination device. With targeted advertising and price discrimination, we find that, when the attractiveness of the weak segment is low, each firm advertises more intensively in its strong segment. The same result arises when the attractiveness of the weak segment is high and advertising is sufficiently expensive. Interestingly, when the attractiveness of the weak segment is high but advertising costs are sufficiently low, it is optimal for each firm to advertise more intensively in its weak segment. The paper also investigates how advertising strategies and equilibrium profits are affected by price discrimination. Compared with uniform pricing, firms can increase or reduce the intensity of advertising targeted to each segment when price discrimination is allowed. Furthermore, when the attractiveness of the weak market is high, price discrimination boosts firms’ profits provided that advertising costs are sufficiently low. The reverse happens when advertising costs are high.", "e:keyword": ["Geotargeting", "Geoconquesting", "Customer recognition", "Dynamic pricing", "Online advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0968", "e:abstract": "Consumer perceptions are important components of brand equity and therefore marketing strategy. Segmenting these perceptions into attributes such as eco-friendliness, nutrition, and luxury enable a fine-grained understanding of the brand’s strengths and weaknesses. Traditional approaches towards monitoring such perceptions (e.g., surveys) are costly and time consuming, and their results may quickly become outdated. Extant data mining methods are unsuitable for this goal, and generally require extensive hand-annotated data or context customization, which leads to many of the same limitations as direct elicitation. Here, we investigate a novel, general, and fully automated method for inferring attribute-specific brand perception ratings by mining the brand’s social connections on Twitter. Using a set of over 200 brands and three perceptual attributes, we compare the method’s automatic ratings estimates with directly-elicited survey data, finding a consistently strong correlation. The approach provides a reliable, flexible, and scalable method for monitoring brand perceptions, and offers a foundation for future advances in understanding brand-consumer social media relationships.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0968\">http://dx.doi.org/10.1287/mksc.2015.0968</ext-link>.", "e:keyword": ["Social media", "Brand image", "Market structure", "Twitter", "Attribute ratings", "Perceptual maps", "Big data", "Data mining", "Social networks"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0970", "e:abstract": "Successful initial public offerings (IPOs) provide firms with access to valuable resources, but also put pressure on firms to impress potential investors with evidence of their current well-being and prospects for future growth. To impress investors IPO firms might curtail their marketing budgets, which appears to inflate current earnings and provide evidence of current well-being. However, curtailing marketing budgets unexpectedly during an IPO may be a myopic practice, in that the immediate benefits of these budget cuts are offset by their longer-term adverse consequences on financial well-being. The extent of these adverse consequences in turn may be moderated by external firm factors, including strategic alliances and key customer relationships, and internal firm factors, such as the strategic emphasis on value creation versus value appropriation. A sample of 1,095 IPOs during 2000–2011 reveals evidence of myopic marketing practices in more than 37% of the sample. Investors seem misled during IPOs, but they correct their beliefs in the three years following the IPO and penalize these firms. The penalty for myopic marketing budgeting practices also increases with more strategic alliances and a strategic emphasis on value creation versus value appropriation, but it decreases in the presence (versus absence) of key customer relationships.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0970\">http://dx.doi.org/10.1287/mksc.2015.0970</ext-link>.", "e:keyword": ["Initial public offering", "Myopic marketing", "Myopic management", "Strategic alliances", "Strategic emphasis", "Key customer relationships", "Real activities manipulation", "Earnings management"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0972", "e:abstract": "Accurate forecasting of sales/consumption is particularly important for marketing because this information can be used to adjust marketing budget allocations and overall marketing strategies. Recently, online social platforms have produced an unparalleled amount of data on consumer behavior. However, two challenges have limited the use of these data in obtaining meaningful business marketing insights. First, the data are typically in an unstructured format, such as texts, images, audio, and video. Second, the sheer volume of the data makes standard analysis procedures computationally unworkable. In this study, we combine methods from cloud computing, machine learning, and text mining to illustrate how online platform content, such as Twitter, can be effectively used for forecasting. We conduct our analysis on a significant volume of nearly two billion Tweets and 400 billion Wikipedia pages. Our main findings emphasize that, by contrast to basic surface-level measures such as the volume of or sentiments in Tweets, the information content of Tweets and their timeliness significantly improve forecasting accuracy. Our method endogenously summarizes the information in Tweets. The advantage of our method is that the classification of the Tweets is based on what is in the Tweets rather than preconceived topics that may not be relevant. We also find that, by contrast to Twitter, other online data (e.g., Google Trends, Wikipedia views, IMDB reviews, and Huffington Post news) are very weak predictors of TV show demand because users tweet about TV shows before, during, and after a TV show, whereas Google searches, Wikipedia views, IMDB reviews, and news posts typically lag behind the show.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0972\">http://dx.doi.org/10.1287/mksc.2015.0972</ext-link>.", "e:keyword": ["Big data", "Cloud computing", "Text mining", "User generated content", "Twitter", "Google Trends"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0973", "e:abstract": "The proliferation of free trials for high-tech services calls for a careful study of their effectiveness, and the drivers thereof. On one hand, free trials can generate new paying subscribers by allowing consumers to become acquainted with the service free of charge. On the other hand, a disappointing trial experience might alienate potential customers, when they decide not to adopt the system and are lost for good. This dilemma is particularly worrisome in early periods, when service quality has not been “tried and tested” in the field, and breakdowns occur. We accommodate these phenomena in a model of consumers’ free-trial and regular adoption decisions. Among other effects, it incorporates usage- and word-of-mouth-based learning about quality in a setting where quality itself is evolving. Consumers are forward-looking in that they account for changes in quality and anticipate uncertainty reduction due to trial usage. We estimate our model and run simulations on the basis of a rich and unique data set that incorporates customers’ trial subscription, adoption, and usage behavior for an interactive digital television service. The results underscore that free trials constitute a double-edged sword, and that timing and consumers’ usage intensity during the trial are key to the effectiveness of these promotions. Implications for managers are also discussed.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2015.0973\">http://dx.doi.org/10.1287/mksc.2015.0973</ext-link>.", "e:keyword": ["Free-trial promotions", "Adoption behavior", "High-tech consumer services", "Contractual services", "Learning", "Promotion effectiveness", "Promotion timing", "Usage"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0974", "e:abstract": "<i>Marketing Science</i> is in a very healthy state as the premier journal for quantitative research in marketing. Since its inception, it has led the way in bringing novel and innovative methodologies and expanding into new substantive areas of inquiry. The journal is now at the cusp of its next stage of creativity and innovation. I outline new research possibilities due to big data, behavioral field studies, and managerial interest in substantive areas such as health, sustainability, emerging markets, innovation, and entrepreneurship. As quantitative marketing’s leading journal, <i>Marketing Science</i> should aid the field in the efficient production of in-depth, valid, current, and relevant knowledge across the breadth of the discipline. To this end, I will actively manage incentives for exploitation and deepening of existing competencies in established areas while supporting exploration and broadening into newer, riskier topics at <i>Marketing Science</i>. To increase the field’s overall efficiency of knowledge production, I suggest a lexicographic approach to reviewing where the incremental contribution threshold is primary and demands on quality of execution be driven by what is needed for proving the validity of the incremental contribution claims.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0975", "e:abstract": "This paper examines the incentives of firms to invest in socially responsible product innovations. Our analysis connects the existence of socially responsible innovations to the presence of intrinsic and extrinsic social responsibility preferences. In addition to deriving economic value from the product, consumers have heterogeneous intrinsic needs to consume products that are socially responsible. They also have extrinsic social comparison preferences that are based on their meetings with others in social interactions. The frequency of these meetings are endogenous to the consumption choices of consumers. A consumer enjoys a social comparison benefit if her consumption decision is more socially responsible than the consumer that she meets in a social interaction and a social comparison cost if it is less socially responsible.The analysis reveals a nonmonotonic effect of social comparison effects on innovation incentives. When the economic value of a product is relatively small, the incentive to innovate decreases as social comparison effects increase. By contrast, when the economic value of a product is sufficiently large, increases in social comparison effects increase the incentive to innovate. Social comparison benefits and costs have different effects on competition between firms. In particular, social comparison benefits soften price competition, whereas social comparison costs tend to exacerbate price competition. We also identify market conditions where a monopoly invests more or less compared to a firm facing competition.", "e:keyword": ["Social responsibility", "R&D strategy", "Innovation", "Sustainability", "Altruism", "Social comparison", "Competitive strategy"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0977", "e:abstract": "We describe online consumers’ search behavior for differentiated durable goods using a data set that captures a detailed level of consumer search and attribute information for digital cameras. Consumers search extensively, engaging in 14 searches on average prior to purchase. Individual level search is confined to a small part of the attribute space. Early search is highly predictive of the characteristics of the camera eventually purchased. Search paths through the attribute space are state dependent and display “lock-in” as the search unfolds. Finally, the first-time discovery of the chosen alternative usually takes place toward the end of the search sequence. We discuss these and other findings in the context of optimal search strategies and discuss the prospects for consumer learning during search.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0977\">http://dx.doi.org/10.1287/mksc.2016.0977</ext-link>.", "e:keyword": ["Consumer search and choice", "Online purchase", "Search dynamics", "Digital cameras"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0980", "e:abstract": "Features involving the taste, smell, touch, and sight of products, as well as attributes such as safety and confidence, are not easily measured in product research without respondents actually experiencing them. Moreover, product researchers often evaluate a large number of these attributes (e.g., >50) in applied studies, making standard valuation techniques such as conjoint analysis difficult to implement. Product researchers instead rely on ratings data to assess features for which the respondent has had actual experience. In this paper we develop a method of monetizing rating data to standardize product evaluations among respondents. The adjusted data are shown to increase the accuracy of purchase predictions by about 20% relative to existing methods of scale adjustment, leading to better inference in models using ratings data. We demonstrate our method using data from a large scale product use study by a packaged goods manufacturer.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0980\">http://dx.doi.org/10.1287/mksc.2016.0980</ext-link>.", "e:keyword": ["Prototype testing", "Experiential attributes", "Scale usage heterogeneity", "Bayesian estimation", "Surveys"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0981", "e:abstract": "This paper studies information sharing in a distribution channel where the manufacturer possesses better demand-forecast information than the downstream retailer. We examine three information-sharing formats: no information sharing (i.e., the manufacturer ex ante commits to not sharing its forecast), voluntary information sharing (i.e., the manufacturer makes the sharing decision ex post after receiving the forecast), and mandatory information sharing (i.e., the manufacturer is mandated to share its forecast). We characterize the equilibrium outcomes under the three sharing formats and investigate the firms’ preferences regarding these formats. It is shown that when the retailer is risk-neutral, both firms are indifferent between voluntary and mandatory sharing. Among the three formats, ex ante, the retailer prefers the no-sharing format whereas the manufacturer prefers the mandatory-sharing format. In addition, we find that a more accurate forecast benefits both firms under voluntary- and mandatory-sharing formats, but may hurt both firms under the no-sharing format. Finally, we show that risk aversion plays a critical role in the firms’ sharing decisions and the impact of forecast accuracy. Specifically, when the retailer is risk-averse, the manufacturer may prefer the no-sharing format over the voluntary-sharing format, and improving forecast accuracy may hurt both firms even under voluntary sharing.", "e:keyword": ["Information sharing", "Signaling", "Forecast accuracy", "Risk aversion", "Distribution channel"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0982", "e:abstract": "Online Display Advertising’s importance as a marketing channel is partially due to its ability to attribute conversions to campaigns. Current industry practice to measure ad effectiveness is to run randomized experiments using placebo ads, assuming external validity for future exposures. We identify two different effects, i.e., a strategic effect of the campaign presence in marketplaces, and a selection effect due to user targeting; these are confounded in current practices. We propose two novel randomized designs to: (1) estimate the overall campaign attribution without placebo ads, (2) disaggregate the campaign presence and ad effects. Using the Potential Outcomes Causal Model, we address the selection effect by estimating the probability of selecting influenceable users. We show the ex-ante value of continuing evaluation to enhance the user selection for ad exposure mid-flight. We analyze two performance-based (CPA) and one Cost-Per-Impression (CPM) campaigns with 20 million users each. We estimate a negative CPM campaign presence effect due to cross product spillovers. Experimental evidence suggests that CPA campaigns incentivize selection of converting users regardless of the ad, up to 96% more than CPM campaigns, thus challenging the standard practice of targeting most likely converting users.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0982\">http://dx.doi.org/10.1287/mksc.2016.0982</ext-link>.", "e:keyword": ["Online advertising", "Experimental design", "User targeting", "Field experiments", "Bayesian estimation", "Econometrics"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0984", "e:abstract": "In this paper, we propose an automated and scalable garment recommender system using real-time in-store videos that can improve the experiences of garment shoppers and increase product sales. The video-based automated recommender (VAR) system is based on observations that garment shoppers tend to try on garments and evaluate themselves in front of store mirrors. Combining state-of-the-art computer vision techniques with marketing models of consumer preferences, the system automatically identifies shoppers’ preferences based on their reactions and uses that information to make meaningful personalized recommendations. First, the system uses a camera to capture a shopper’s behavior in front of the mirror to make inferences about her preferences based on her facial expressions and the part of the garment she is examining at each time point. Second, the system identifies shoppers with preferences similar to the focal customer from a database of shoppers whose preferences, purchasing, and/or consideration decisions are known. Finally, recommendations are made to the focal customer based on the preferences, purchasing, and/or consideration decisions of these like-minded shoppers. Each of the three steps can be implemented with several variations, and a retailing chain can choose the specific configuration that best serves its purpose. In this paper, we present an empirical test that compares one specific type of VAR system implementation against two alternative, <i>nonautomated</i> personal recommender systems: self-explicated conjoint (SEC) and self-evaluation after try-on (SET). The results show that VAR consistently outperforms SEC and SET. A second empirical study demonstrates the feasibility of VAR in real-time applications. Participants in the second study enjoyed the VAR experience, and almost all of them tried on the recommended garments. VAR should prove to be a valuable tool for both garment retailers and shoppers.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0984\">http://dx.doi.org/10.1287/mksc.2016.0984</ext-link>.", "e:keyword": ["Retailing", "Video analysis", "Collaborative filtering"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0985", "e:abstract": "An accurate prediction of what a customer will purchase next is of paramount importance to successful online retailing. In practice, customer purchase history data is readily available to make such predictions, sometimes complemented with customer characteristics. Given the large product assortments maintained by online retailers, scalability of the prediction method is just as important as its accuracy. We study two classes of models that use such data to predict what a customer will buy next, i.e., a novel approach that uses latent Dirichlet allocation (LDA), and mixtures of Dirichlet-Multinomials (MDM). A key benefit of a model-based approach is the potential to accommodate observed customer heterogeneity through the inclusion of predictor variables. We show that LDA can be extended in this direction while retaining its scalability. We apply the models to purchase data from an online retailer and contrast their predictive performance with that of a collaborative filter and a discrete choice model. Both LDA and MDM outperform the other methods. Moreover, LDA attains performance similar to that of MDM while being far more scalable, rendering it a promising approach to purchase prediction in large product assortments.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0985\">http://dx.doi.org/10.1287/mksc.2016.0985</ext-link>.", "e:keyword": ["Model-based predictions", "Large scale purchase prediction", "Scalability", "Purchase history data", "Latent Dirichlet allocation", "Mixture of Dirichlet-Multinomials"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0988", "e:abstract": "We present an analytical framework of multimarket competition and supporting empirical analysis to explain why and when competing firms in an existing market may prefer an alliance entry over independent entry into a new market. Our findings suggest that an alliance entry is more profitable than an independent entry (i) when the new market is larger relative to the existing market, and (ii) when the competition in the existing market is stronger relative to the new market. We compare these key predictions with archival data from the regional shopping center industry in the United States and find that instances of alliance formation in this industry are consistent with our model-based predictions.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0988\">http://dx.doi.org/10.1287/mksc.2016.0988</ext-link>.", "e:keyword": ["Competitive strategy", "Strategic alliances", "Multimarket competition"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0996", "e:keyword": ["Data science", "Computer science", "Big data", "Quantitative analysis", "Modeling", "Machine learning"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2015.0971", "e:abstract": "We study the effects of information content in 59,814 pharmaceutical sales calls on doctors’ prescription decisions for statins, in the face of entry of competing brands and generics, using a hierarchical Bayesian distributed lag model. We conclude that adding information content to the prescription response model improves the in- and out-of-sample performance of the model. In the first six months following generic entry, it is more effective for incumbent brands to detail on drug contraindications and indications, compared to other periods, to positively differentiate from generics. In the first six months following branded entry, it is less effective for incumbent brands to detail on drug indications and costs, given increased competitive clutter. We also document substantial heterogeneity among doctors in their response to information content. Our model is helpful for analysts to more accurately assess the effectiveness of detailing. Our empirical results are also informative for drug manufacturers as they set or change their messaging policies in response to entry and help firms to tailor their message content at the doctor level.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2015.0971\">https://doi.org/10.1287/mksc.2015.0971</ext-link>.", "e:keyword": ["Personal selling", "Information content", "Pharmaceuticals", "Branded and generic entry", "Competition", "Distributed lag model", "Endogeneity", "Simultaneity"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0976", "e:abstract": "Consumer-to-consumer (C2C) platforms have become a major engine of growth in Internet commerce. This is especially true in countries such as China, which are experiencing a big rush toward e-commerce. The emergence of such platforms gives researchers the unique opportunity to investigate the evolution of such platforms by focusing on the growth of both buyers and sellers. In this research, we build a utility-based model to quantify both cross and direct network effects on Alibaba Group’s Taobao.com, the world’s largest online C2C platform (based in China). Specifically, we investigate the relative contributions of different factors that affect the growth of buyers and sellers on the platform. Our results suggest that the direct network effects do not play a big role in the platform’s growth (we detect a small positive direct network effect on buyer growth and no direct network effect on seller growth). More importantly, we find a significant, large and positive cross-network effect on both sides of the platform. In other words, the installed base of either side of the platform has propelled the growth of the other side (and thus the overall growth). Interestingly, this cross-network effect is asymmetric with the installed base of sellers having a much larger effect on the growth of buyers than vice versa. The growth in the number of buyers is driven primarily by the seller’s installed base and product variety with increasing importance of product variety. The growth in the number of sellers is driven by buyer’s installed base, buyer quality, and product price with increasing importance of buyer quality. We also investigate the nature of these cross-network effects over time. We find that the cross-network effect of sellers on buyers increases and then decreases to reach a stable level. By contrast, the cross-network effect of buyers on sellers is relatively stable. We discuss the policy implications of these findings for C2C platforms in general and Taobao in particular.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.0976\">https://doi.org/10.1287/mksc.2016.0976</ext-link>.", "e:keyword": ["Platforms", "Two-sided markets", "Cross-network effect", "Direct network effect", "e-commerce", "Emerging markets", "China"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0978", "e:abstract": "Past research has primarily focused on what happens after a merger. This research attempts to determine whether anticipated benefits from the merger actually accrue. We characterize the effects of observed variables on whether pairs of firms merge, vis-à-vis roommate matching, and then link these factors to post-merger innovation (i.e., number of patents). We jointly estimate the two models using Markov Chain Monte Carlo methods with a unique panel data set of 1,979 mergers between 4,444 firms across industries and countries from 1992 to 2008. We find that similarity in national culture and technical knowledge has a positive effect on partner selection and post-merger innovation. Anticipated synergy from subindustry similarity, however, is not realized in post-merger innovation. Furthermore, some key synergy sources are unanticipated when selecting a merger partner. For example, financial synergy from higher total assets and complementarity in total assets and debt leverage as well as knowledge synergy from breadth and depth of knowledge positively influence innovation but not partner selection. Furthermore, factors that dilute synergy (e.g., higher debt levels) are unanticipated, and firms merge with firms that detract from their innovation potential. Overall, the results reveal some incongruity between anticipated and realized synergy.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.0978\">https://doi.org/10.1287/mksc.2016.0978</ext-link>.", "e:keyword": ["Merger", "Innovation", "Synergy", "Roommate-matching model", "Empirical models", "Markov Chain Monte Carlo  methods"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0987", "e:abstract": "Firms use different attribution strategies such as last-click or first-click attribution to assign conversion credits to search keywords that appear in their consumers’ paths to purchase. These attributed credits impact a firm’s future bidding and budget allocations among keywords and, in turn, determine the overall return-on-investment of search campaigns. In this paper, we model the relationship among the advertiser’s bidding decision for keywords, the search engine’s ranking decision for these keywords, and the consumer’s click-through rate and conversion rate on each keyword, and analyze the impact of the attribution strategy on the overall return-on-investment of paid search advertising.We estimate our simultaneous equations model using a six-month panel data of several hundred keywords from an online jewelry retailer. The data comprises a quasi-experiment as the firm changed attribution strategy from last-click to first-click attribution halfway through the data window. Our results show that returns for keyword investments vary significantly under the different attribution strategies. For the focal firm, first-click attribution leads to lower revenue returns and a more pronounced decrease for more specific keywords. Our policy simulation exercise shows how the firm can increase its overall returns by better attributing the real contribution of keywords. We discuss how an appropriate attribution strategy can help firms to better target customers and lower acquisition costs in the context of paid search advertising.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.0987\">https://doi.org/10.1287/mksc.2016.0987</ext-link>.", "e:keyword": ["Attribution strategies", "Paid search advertising", "ROI", "Keyword specificity", "Budget allocation", "First-click attribution", "Last-click attribution", "Multi-touch attribution"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0989", "e:abstract": "We randomize advertising content motivated by the psychology literature on sympathy generation and framing effects in mailings to about 185,000 prospective <i>new</i> donors in India. We find a significant impact on the number of donors and amounts donated consistent with sympathy biases such as the “identifiable victim,” “in-group,” and “reference dependence.” A monthly reframing of the ask amount increases donors and the amount donated relative to daily reframing. A second field experiment targeted to <i>past</i> donors, finds that the effect of sympathy bias on giving is smaller in percentage terms but statistically and economically highly significant in terms of the magnitude of additional dollars raised. Methodologically, the paper complements the work of behavioral scholars by adopting an empirical researchers’ lens of measuring <i>relative</i> effect sizes and economic relevance of multiple behavioral theoretical constructs in the sympathy bias and charity domain within one field setting. Beyond the benefit of conceptual replications, the effect sizes provide guidance to managers on which behavioral theories are most managerially and economically relevant when developing advertising content.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.0989\">https://doi.org/10.1287/mksc.2016.0989</ext-link>.", "e:keyword": ["Charitable giving", "Sympathy biases", "Identified victim effect", "In-group effect", "Reference dependence", "Nonprofit marketing", "Persuasive advertising", "Informative advertising", "Direct mail", "Behavioral economics", "Conceptual replications"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0990", "e:abstract": "This research aims to understand and predict online customers’ store visit and purchase behaviors. To this end, we develop a model that accounts for different patterns of online store visits at the individual level. Given the latency of visit patterns, we employ a changepoint modeling framework and statistically infer them using a Bayesian approach. The inferences obtained are then used to examine the effects of visit patterns on purchase dynamics across store visits. Using Internet clickstream data at an online retailer, we find that online store visit patterns tend to be clustered with significant variation across customers in terms of the number and size of visit clusters as well as the visit frequencies, both within and between clusters. Furthermore, the conversion rates vary significantly, depending on store visit patterns, such that they tend to be higher at later visits within a visit cluster, compared with earlier visits. The proposed model thereby offers superior fit and predictive performance than benchmark models that ignore clustered visit patterns and their impact on purchase behavior. We demonstrate the model’s ability to better identify prospective customers by utilizing their visit patterns, which can assist marketers in scoring customers and making targeting decisions across individuals for marketing activity.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.0990\">https://doi.org/10.1287/mksc.2016.0990</ext-link>.", "e:keyword": ["Online shopping behavior", "Timing models", "Changepoint models", "Pattern analysis", "Bayesian estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0991", "e:abstract": "Experimental methods are critical tools in marketing, psychology, and economics to isolate the effects of key variables from vagaries intrinsic to field data. As such, they are often considered exempt from the sort of sample selectivity artifacts widely documented in empirical research, in part because participants are randomly assigned to experimental conditions. To conserve time and resources, experiments often focus on items participants have chosen or are familiar with, for example, postchoice satisfaction ratings, certain free recall tasks, or specifying consideration sets preceding brand choice. When consumer input even partially influences the items about which researchers request subsequent data, the potential for <i>item selectivity</i> arises. In such situations, analyses are contingent on both the choice context(s) of the experiment and the alternatives participants elect to evaluate, potentially leading to substantial item selectivity overall and to differing degrees across conditions. We examine situations in which a nonignorable “choose one of many” (polytomous) selection process limits which items offer up subsequent information, and develop methods to allow substantive results to pertain to the full set of items, not only those selected. The framework is illustrated via two experiments in which participants choose and then evaluate a frequently purchased consumer good as well as data first examined by Ratner et al. [Ratner RK, Kahn BE, Kahneman D (1999) Choosing less-preferred experiences for the sake of variety. <i>J. Consumer Res.</i> 26(1):1–15]. Results indicate substantial item selectivity that, when corrected for, can lead to markedly different interpretations of focal variable effects, such as large effect size changes and even sign reversal. Moreover, failing to flexibly account for item selectivity <i>across experimental conditions</i>, even in well-designed experimental settings, can lead to inaccurate substantive inferences about consumers’ evaluative criteria. We further demonstrate robustness to theoretically driven (but not overtly misspecified) selection rules and provide researchers with a simple, “two-step” exploratory procedure akin to a “control function” approach—involving just one additional variable added to standard models—to determine whether and to what degree item selectivity may be affecting their substantive results.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.0991\">https://doi.org/10.1287/mksc.2016.0991</ext-link>.", "e:keyword": ["Choice models", "Consumer behavior", "Decision making", "Econometric models", "Sample selection", "Heckman model", "Markov chain Monte Carlo", "Hierarchical Bayes", "Variety seeking", "Assortment size"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0992", "e:abstract": "Consumers’ preferences can often be represented using a multimodal continuous heterogeneity distribution. One explanation for such a preference distribution is that consumers belong to a few distinct segments, with preferences of consumers in each segment being heterogeneous and unimodal. We propose an innovative approach for modeling such multimodal distributions that builds on recent advances in sparse learning and optimization. We apply the model to conjoint analysis where consumer heterogeneity plays a critical role in determining optimal marketing decisions. Our approach uses a two-stage divide-and-conquer framework, where we first divide the consumer population into segments by recovering a set of candidate segmentations using sparsity modeling, and then use each candidate segmentation to develop a set of individual-level heterogeneity representations. We select the optimal individual-level heterogeneity representation using cross-validation. Using extensive simulation experiments and three field data sets, we show the superior performance of our sparse learning model compared to benchmark models including the finite mixture model and the Bayesian normal component mixture model.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0992\">http://dx.doi.org/10.1287/mksc.2016.0992</ext-link>.", "e:keyword": ["Sparse machine learning", "Multimodal continuous heterogeneity", "Conjoint analysis"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0993", "e:abstract": "Firms collect an increasing amount of consumer feedback in the form of unstructured consumer reviews. These reviews contain text about consumer experiences with products and services that are different from surveys that query consumers for specific information. A challenge in analyzing unstructured consumer reviews is in making sense of the topics that are expressed in the words used to describe these experiences. We propose a new model for text analysis that makes use of the sentence structure contained in the reviews and show that it leads to improved inference and prediction of consumer ratings relative to existing models using data from <ext-link ext-link-type=\"uri\"  href=\"http://www.expedia.com\">www.expedia.com</ext-link> and <ext-link ext-link-type=\"uri\"  href=\"http://www.we8there.com\">www.we8there.com</ext-link>. Sentence-based topics are found to be more distinguished and coherent than those identified from a word-based analysis.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.0993\">https://doi.org/10.1287/mksc.2016.0993</ext-link>.", "e:keyword": ["Extended LDA model", "User-generated content", "Text data", "Unstructured data", "Bayesian analysis", "Big data"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0994", "e:abstract": "We explore the use of big data tools to shed new light on the idea generation process, automatically “read” ideas to identify promising ones, and help people be more creative. The literature suggests that creativity results from the optimal balance between novelty and familiarity, which can be measured based on the combinations of words in an idea. We build semantic networks where nodes represent word stems in a particular idea generation topic, and edge weights capture the degree of novelty versus familiarity of word stem combinations (i.e., the weight of an edge that connects two word stems measures their scaled co-occurrence in the relevant language). Each idea contains a set of word stems, which form a semantic subnetwork. The edge weight distribution in that subnetwork reflects how the idea balances novelty with familiarity. Based on the “beauty in averageness” effect, we hypothesize that ideas with semantic subnetworks that have a more prototypical edge weight distribution are judged as more creative. We show this effect in eight studies involving over 4,000 ideas across multiple domains. Practically, we demonstrate how our research can be used to automatically identify promising ideas and recommend words to users on the fly to help them improve their ideas.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0994\">http://dx.doi.org/10.1287/mksc.2016.0994</ext-link>.", "e:keyword": ["Creativity", "Innovation", "Idea generation", "Data mining", "Text mining"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0995", "e:abstract": "We study the identification of the search method consumers use when resolving uncertainty in the prices of alternatives. We show that the search method—simultaneous or sequential—is identified with data on consumers’ consideration sets (but not the sequence of searches), prices for the considered alternatives and marketwide price distributions. We show that identification comes from differences in the patterns of actual prices in consumers’ consideration sets across search methods. We also provide a new estimation approach for the sequential search model that uses such data. Using data on consumer shopping behavior in the U.S. auto insurance industry that contain information on consideration sets and choices, we find that the pattern of actual prices in consumers’ consideration sets is consistent with consumers searching simultaneously. Via counterfactuals we show that the consideration set and purchase market shares of the largest insurance companies are overpredicted under the incorrect assumption of sequential search. As the search method affects consumers’ consideration sets, which in turn influence brand choices, understanding the nature of consumer search and its implications for consideration and choice is important from a managerial perspective.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0995\">http://dx.doi.org/10.1287/mksc.2016.0995</ext-link>.", "e:keyword": ["Consumer search", "Simultaneous search", "Sequential search", "Auto insurance industry"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.0998", "e:abstract": "Yahoo! Research partnered with a nationwide retailer to study the effects of online display advertising on both online and in-store purchases. We use a randomized field experiment on 3 million Yahoo! users who are also past customers of the retailer. We find statistically significant evidence that the retailer ads increase sales 3.6% relative to the control group. We show that control ads boost measurement precision by identifying and removing the half of in-campaign sales data that are unaffected by the ads. Less data give us 31% more precision in our estimates—equivalent to increasing our sample to 5.3 million users. By contrast, we only improve precision by 5% when we include additional covariate data to reduce the residual variance in our experimental regression. The covariate-adjustment strategy disappoints despite exceptional consumer-level data including demographics, ad exposure levels, and two years’ worth of past purchase history.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.0998\">http://dx.doi.org/10.1287/mksc.2016.0998</ext-link>.", "e:keyword": ["Advertising effectiveness", "Field experiments", "Digital advertising"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.1000", "e:abstract": "This paper studies the ability of competing retailers to form a cartel by sharing information with their mutual manufacturer. In a market characterized by demand uncertainty, colluding retailers wish to share information about the potential market demand to coordinate on the optimal collusive retail price. However, in light of potential exposure to antitrust investigations and possible sanctions, the retailers search for mechanisms to exchange information while avoiding the risks of scrutiny by the antitrust authorities. This paper examines such a mechanism: each retailer shares his private information with the mutual manufacturer; the wholesale price set by the latter is thereafter used by the retailers to infer the market condition and coordinate on the cartel’s price. Although a cartel at the retail level limits the manufacturer’s sold quantity, under certain conditions the manufacturer is better off accepting the retailers’ private information, thereby assisting the cartel formation. Moreover, vertical information sharing between the retailers and their mutual manufacturer can result in lower consumer surplus than that would have occurred had the retailers been permitted to collude directly.", "e:keyword": ["Information sharing", "Channel coordination", "Collusion", "Regulation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.1001", "e:abstract": "Seeded marketing campaigns (SMCs) involve firms sending products to selected customers and encouraging them to spread word of mouth (WOM). Prior research has examined certain aspects of this increasingly popular form of marketing communication, such as seeding strategies and their efficacy. Building on prior research, this study investigates the effects of SMCs that extend beyond the generation of WOM for a campaign’s focal product by considering how seeding can affect WOM spillover effects at the brand and category levels. The authors introduce a framework of SMC-related spillover effects, and empirically estimate these with a unique data set covering 390 SMCs for products from 192 different cosmetics brands. Multiple spillover effects are found, suggesting that while SMCs can be used primarily to stimulate WOM for a focal product, marketers must also account for brand- and category-level WOM spillover effects. Specifically, seeding increases conversations about that product among nonseed consumers, and, interestingly, decreases WOM about other products from the same brand and about competitors’ products in the same category as the focal product. These findings indicate that marketers can use SMCs to focus online WOM on a particular product by drawing consumers away from talking about other related, but off-topic, products.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.1001\">http://dx.doi.org/10.1287/mksc.2016.1001</ext-link>.", "e:keyword": ["Advertising", "Word-of-mouth", "Econometrics", "Social media", "Influencers", "Seeding", "Viral marketing"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.1002", "e:abstract": "In this research, we investigate the relationship between television advertising and online word-of-mouth (WOM) by examining the joint consumption of television programming and production of social media by television viewers, termed social TV. We explore how television advertising impacts the volume of online WOM about advertised brands and about the programs in which the advertisements air. We also examine what encourages or discourages viewers to engage in this particular social TV activity. Using data containing television advertising instances and the volume of minute-by-minute social media mentions, our analyses reveal that television advertising impacts the volume of online WOM for both the brand advertised <i>and</i> the program in which the advertisement airs. We additionally find that the programs that receive the most online WOM are not necessarily the best programs for advertisers interested in online engagement for their brands. Finally, our results highlight the brand, advertisement, and program characteristics that can encourage or discourage social TV activity. We discuss the implications of our findings for media planning strategies and advertisement design strategies.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.1002\">http://dx.doi.org/10.1287/mksc.2016.1002</ext-link>.", "e:keyword": ["Online word-of-mouth", "Advertising", "Television", "Social media", "Social TV"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.1003", "e:abstract": "Firms develop products by manipulating the attributes of offerings, and consumers derive utility from the benefits that the attributes afford. While the field of marketing has long been aware of the distinction between attributes and benefits, it has not developed methods for understanding how attributes and benefits are related. This paper develops a benefit-based model for conjoint analysis that assumes consumers satiate on attributes that are perceived to provide the same benefit. A latent-variable model is proposed that estimates the map between attributes and benefits, and is applied to data from two conjoint studies involving a durable product and a household consumable. The model is shown to fit the data better, provide improved predictions, and lead to different product design implications than the standard conjoint model.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/mksc.2016.1003\">http://dx.doi.org/10.1287/mksc.2016.1003</ext-link>.", "e:keyword": ["Economic separability", "Satiation", "Bayesian estimation"]}, {"@id": "http://dx.doi.org/10.1287/mksc.2016.1004", "e:abstract": "Product lines are ubiquitous. For example, Marriott International manages high-end ultra-luxury hotels (e.g., Ritz-Carlton) and low-end economy hotels (e.g., Fairfield Inn). Firms often bundle core products with ancillary services (or add-ons). Interestingly, empirical observations reveal that industries with ostensibly similar characteristics (e.g., customer types, costs, competition, distribution channels, etc.) employ different bundling strategies. For example, airlines bundle high-end first class with ancillary services (e.g., breakfast, entertainment) while hotel chains bundle ancillary services (e.g., breakfast, entertainment) at the low-end. We observe, unlike hotel lines that are highly differentiated at different geographic locations, airlines suffer low core differentiation because all passengers (first-class and economy) are at the same location (i.e., same plane, weather, delays, cancellations, etc.). In general, we find product lines with low core differentiation (e.g., airlines, amusement parks) routinely bundle high-end while product lines with highly differentiated cores (e.g., hotels, restaurants) routinely bundle low-end. High-end bundling makes the high-end more attractive, increasing line differentiation (less intraline competition) while low-end bundling decreases line differentiation. Therefore, bundling allows optimal differentiation given a differentiation constraint (complex costs). Last, firms may use strategic bundling for targeting in their core products; e.g., low-end hotels bundle targeted add-ons unattractive to high-end consumers such as lower-quality breakfasts and slower Internet.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/mksc.2016.1004\">https://doi.org/10.1287/mksc.2016.1004</ext-link>.", "e:keyword": ["Product line", "Strategic bundling", "Analytic model", "Pricing", "Targeted add-ons", "Ancillary services", "Fees"]}]