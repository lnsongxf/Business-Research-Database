[{"@id": "http://dx.doi.org/10.1287/msom.1120.0396", "e:abstract": "Modular upgradability has been suggested as a strategy for improving environmental performance: as technology improves, it allows for independent replacement of improving subsystems, instead of replacing the entire product. This may extend the useful life of stable subsystems, reducing production and disposal impact. However, this argument ignores the effect of modular upgradability on a firm's development and introduction decisions and the environmental impact during the use phase. In this paper, we investigate when modular upgradability leads to lower environmental impact and higher profits. We do so by endogenizing a firm's development and introduction decisions and considering the product's environmental impact during its entire life cycle. Our results show that although modular upgradability may accelerate the replacement of some subsystems, it delays the replacement of others. We find that modular upgradability can increase the environmental impact for some product categories due to accelerated obsolescence arising from more frequent introduction and replacement. However, we also find that accelerated obsolescence, under some conditions, can actually make modular upgradability greener.", "e:keyword": ["Modularity", "Green product design", "Sustainability", "New product development", "Sustainable operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0407", "e:abstract": "Over the past six decades, numerous analytical models have been developed to determine optimal inventory levels. These models predict that inventories carried by a retailer should be a function of the product variety carried by the retailer, distribution system characteristics, economies of scale, etc. A few recent empirical studies have explored the impact of some of these factors on aggregate inventories at U.S. retailers. Building on these works, this study empirically explores the role of key factors such as product variety, number of stores, and number of warehouses in explaining inventory levels at U.S. retailers using data obtained from both primary and secondary sources. We find that variety as measured by the number of stock-keeping units carried and number of stores is associated with higher inventories, whereas scale economies are associated with lower inventories. We do not find the number of warehouses to be significant in explaining inventory levels. Increased demand fluctuations are also associated with higher inventories, although the effects are less robust. The significant variables together with retailer segment identifiers explain a substantial fraction of the variance in inventory levels and can be potentially useful to managers in benchmarking their inventory levels.", "e:keyword": ["Retailing", "Empirical research", "Econometric analysis", "Supply chain management", "Inventory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0411", "e:abstract": "The decision about which servers to dispatch to which customers is an important aspect of service systems. This decision is complicated when servers must be equitably&mdash;as well as efficiently&mdash;dispatched to customers. In this paper, we formulate a model for determining how to optimally dispatch distinguishable servers to prioritized customers given a set of equity constraints. These issues are examined through the lens of emergency medical service (EMS) dispatch, for which a Markov decision process model is developed that captures how to dispatch ambulances (servers) to prioritized patients (customers). It is assumed that customers arrive sequentially, with the priority and location of each customer becoming known upon arrival. Four types of equity constraints are considered&mdash;two of which reflect customer equity and two of which reflect server equity&mdash;all of which draw upon the decision analytic and social science literature to compare the effects of different notions of equity on the resulting dispatching policies. The Markov decision processes are formulated as equity-constrained linear programming models. A computational example is applied to an EMS system to compare the different equity models.", "e:keyword": ["Emergency medical services", "Server-to-customer systems", "Public health", "Equity", "Markov decision processes", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0412", "e:abstract": "The U.S. government has mandated that, in a catastrophic event, metropolitan areas need to be capable of caring for 50 burn-injured patients per million population. In New York City, this corresponds to 400 patients. There are currently 140 burn beds in the region, which can be surged up to 210. To care for additional patients, hospitals without burn centers will be used to stabilize patients until burn beds become available. In this work, we develop a new system for prioritizing patients for transfer to burn beds as they become available and demonstrate its superiority over several other triage methods. Based on data from previous burn catastrophes, we study the feasibility of being able to admit 400 patients to burn beds within the critical three- to five-day time frame. We find that this is unlikely and that the ability to do so is highly dependent on the type of event and the demographics of the patient population. This work has implications for how disaster plans in other metropolitan areas should be developed.", "e:keyword": ["Healthcare", "Disaster planning", "Triage"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0413", "e:abstract": "Supply chain disruptions come with catastrophic consequences in spite of their low probability of occurrence. In this paper, we consider a facility location problem in the presence of random facility disruptions where facilities can be protected with additional investments. Whereas most existing models in the literature implicitly assume that the disruption probability estimate is perfectly accurate, we investigate the impact of misestimating the disruption probability. Using a stylized continuous location model, we show that underestimation in disruption probability results in greater increase in the expected total cost than overestimation. In addition, we show that, when planned properly, the cost of mitigating the misestimation risk is not too high. Under a more generalized setting incorporating correlated disruptions and finite capacity, we numerically show that underestimation in both disruption probability and correlation degree result in greater increase in the expected total cost compared to overestimation. We, however, find that the impact of misestimating the correlation degree is much less significant relative to that of misestimating the disruption probability. Thus, managers should focus more on accurately estimating the disruption probability than the correlation.", "e:keyword": ["Logistics and transportation", "Supply chain disruptions", "Facility network design", "Estimation error", "Correlated disruptions", "Continuous approximation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0414", "e:abstract": "The necessity of surface water for irrigation and its increasing scarcity in developing economies motivate the need for its efficient distribution. The inequity in the distribution of surface water arises because of the relative physical locations of the farms. Head-reach (primary) farms are close to the source, whereas tail-end (secondary) farms are relatively farther. The lack of physical infrastructure implies that water allocated to secondary farms must pass through primary farms. Left to their individual incentives, primary farmers use more than their fair share of water by denying its release to secondary farmers. Such an inequitable sharing results in significantly suboptimal productivity of the farming community as a whole. We propose decentralized, individually rational mechanisms to achieve socially optimal distribution of surface water for a farming community under uncertainty in rainfall, choice of multiple crops, and differing risk-bearing abilities of primary and secondary farmers. We show that the mechanisms can be efficiently computed and highlight the impact of the improved sharing of surface water. We also study the movement of the price of water with its scarcity. Ideas that can help administer the mechanisms in practice are briefly discussed.", "e:keyword": ["Logistics and transportation", "Nonprofit management", "Incentives and contracts"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0415", "e:abstract": "Motivated by service capacity-management problems in healthcare contexts, we consider a multiresource allocation problem with two classes of jobs (elective and emergency) in a dynamic and nonstationary environment. Emergency jobs need to be served immediately, whereas elective jobs can wait. Distributional information about demand and resource availability is continually updated, and we allow jobs to renege. We prove that our formulation is convex, and the optimal amount of capacity reserved for emergency jobs in each period decreases with the number of elective jobs waiting for service. However, the optimal policy is difficult to compute exactly. We develop the idea of a <i>limit policy</i> starting at a particular time, and use this policy to obtain upper and lower bounds on the decisions of an optimal policy in each period, and also to develop several computationally efficient policies. We show in computational experiments that our best policy performs within 1.8&percnt; of an optimal policy on average.", "e:keyword": ["Multiresource allocation", "Markov decision process", "Healthcare operations management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0416", "e:abstract": "Gray markets are unauthorized channels of distribution for a supplier's authentic products. We study a distribution channel that consists of a supplier who offers <i>all-unit</i> quantity discounts for batch orders to enjoy cost savings, and a reseller who may divert some goods to the gray markets. We show that the impact of gray markets depends on the reseller's inventory holding cost. When the reseller's inventory holding cost is high, diversion to the gray markets improves the channel performance by enabling the reseller to make batch orders. Because the reseller's order costs decrease through quantity discounts, diversion to the gray markets reduces the resale price and expands sales to the authorized channel. On the other hand, when the reseller's inventory holding cost is low, the reseller would make the batch orders even without the gray markets. In this case the diversion to the gray markets may improve the reseller's performance by shortening the order cycles and reducing the inventory holding costs. Interestingly, because diversion to the gray markets decreases the reseller's cycle inventory volume, the reseller has the reduced incentive to push its inventory, and, consequently, the resale price rises and sales volume decreases in the authorized channel. Moreover, there exists a range of reseller's inventory holding cost and supplier's cost of scale economy such that it is optimal for the supplier to induce reseller's gray market diversion through an all-unit discount. We show that these results are robust when the gray market overlaps with the authorized channel or when the gray market price is sensitive to reseller's diversion volume.", "e:keyword": ["Gray markets", "Channel management", "Inventory", "Quantity discounts", "Supplier pricing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0417", "e:abstract": "The traditional operations management and queueing literature typically assumes that customers are fully rational. In contrast, in this paper we study canonical service models with boundedly rational customers. We capture bounded rationality using a model in which customers are incapable of accurately estimating their expected waiting time. We investigate the impact of bounded rationality from both a profit-maximizing firm's perspective and a social planner's perspective. For visible queues with the optimal price, bounded rationality results in revenue and welfare loss; with a fixed price, bounded rationality can lead to strict social welfare improvement. For invisible queues, bounded rationality benefits the firm when its level is sufficiently high. Ignoring bounded rationality, when present yet small, can result in significant revenue and welfare loss.", "e:keyword": ["Behavioral operations", "Service operations", "Bounded rationality", "Queueing", "Consumer behavior"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0418", "e:abstract": "The revenue management literature for queues typically assumes that providers know the distribution of customer demand attributes. We study an observable <i>M/M/1</i> queue that serves an <i>unknown</i> proportion of patient and impatient customers. The provider has a Bernoulli prior on this proportion, corresponding to an optimistic or pessimistic scenario. For every queue length, she chooses a low or a high price, or turns customers away. Only the high price is informative. The optimal Bayesian price for a queue state is belief-dependent if the optimal policies for the underlying scenarios disagree at that queue state; in this case the policy has a belief-threshold structure. The optimal Bayesian pricing policy as a function of queue length has a zone (or, nested-threshold) structure. Moreover, the price convergence under the optimal Bayesian policy is sensitive to the system size, i.e., the maximum queue length. We identify two cases: prices converge (1) almost surely to the optimal prices in either scenario or (2) with positive probability to suboptimal prices. Only Case 2 is consistent with the typical <i>incomplete learning</i> outcome observed in the literature.", "e:keyword": ["Bayesian learning", "Delay", "Dynamic pricing", "Revenue management", "Queueing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0420", "e:abstract": "Suppliers are increasingly being asked to share information about their vulnerability to climate change and their strategies to reduce greenhouse gas emissions. Their responses vary widely. We theorize and empirically identify several factors associated with suppliers being especially willing to share this information with buyers, focusing on attributes of the buyers seeking this information and of the suppliers being asked to provide it. We test our hypotheses using data from the Carbon Disclosure Project's Supply Chain Program, a collaboration of multinational corporations requesting such information from thousands of suppliers in 49 countries. We find evidence that suppliers are more likely to share this information when requests from buyers are more prevalent, when buyers appear committed to using the information, when suppliers belong to more profitable industries, and when suppliers are located in countries with greenhouse gas regulations. We find evidence that these factors also influence the comprehensiveness of the information suppliers share and their willingness to share the information publicly.", "e:keyword": ["Econometric analysis", "Empirical research", "Environmental operations", "Sustainable operations", "Quality management", "Supply chain management", "Risk management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0423", "e:abstract": "We examine three selling strategies of a manufacturer who produces and sells a seasonal product to a retailer under uncertain supply and demand: (1) advance selling&mdash;presells the product before observing uncertain supply and demand; (2) regular selling&mdash;sells the product after supply and demand are realized; and (3) dynamic selling&mdash;combines both advance and regular selling strategies. We model the first two strategies as single-period Stackelberg games, and we model the last strategy as a two-period dynamic Stackelberg game. By comparing the equilibria of these games, we formalize our understanding of several intuitive results. For example, from the manufacturer's perspective, dynamic selling dominates advance selling and regular selling: having more selling opportunities is beneficial to the manufacturer. However, from the retailer's perspective, we find two counterintuitive results: (a) postponing the ordering decision can be detrimental&mdash;the retailer can be worse off under regular selling than under advance selling; and (b) more ordering opportunities can be detrimental&mdash;the retailer can be worse off under dynamic selling than under advance selling. In addition, we analyze the impact of supply and demand uncertainties under these strategies and find that both types of uncertainties can be beneficial to the retailer.", "e:keyword": ["Game theory", "Healthcare management", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0424", "e:abstract": "We study salesforce contracting in an environment where excess demand results in lost sales and the demand information is censored by the inventory level. In our model, a firm contracts with a risk-neutral sales agent with limited liability whose effort increases the demand stochastically. The firm designs the incentive contract and invests in inventory; the agent decides the sales effort. We find that the sales-quota-based bonus contract is optimal in such an environment, and the quota should be set equal to the inventory level when the first-best solution is not attainable. We further reveal that demand censorship can introduce peculiar effects on the optimal sales effort and service level that the firm implements. From our analysis of the additive and multiplicative effort cases, we find that in the additive effort case, it can be optimal, under demand censorship, for the firm to induce an effort and maintain a service level both greater than those under the first-best solution. Scenarios also exist where the firm should induce zero effort. For the multiplicative effort case, the optimal sales effort under demand censorship is lower than the first-best effort, whereas the optimal service level is higher than the first-best service level. The agent earns zero rent in the additive effort case but may earn a positive rent in the multiplicative effort case. Finally, our numerical analysis shows that demand censorship can have a significant negative impact on the value of contracting with the sales agent, especially when the sales margin is low and the market uncertainty is high.", "e:keyword": ["Salesforce contracting", "Demand censorship", "Newsvendor"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0425", "e:abstract": "We consider the problem of determining the optimal assortment of products to offer in a given product category when each customer is characterized by a type, which is a list of products he is willing to buy in decreasing order of preference. We assume consumer-driven, dynamic, stockout-based substitution and random proportions of each type. No efficient method to obtain the optimal solution for this problem is known to our knowledge. However, if the number of customers of each type is a fixed proportion of demand, there exists an efficient algorithm for solving for the optimal assortment. We show that the fixed proportions model gives an upper bound to the optimal expected profit for the random proportions model. This bound allows us to obtain a measure of the absolute performance of heuristic solutions. We also provide a bound for the component-wise absolute difference in expected sales between the two models, which is asymptotically tight as the inventory vector is made large, while keeping the number of products fixed. This result provides us with a lower bound to the optimal expected profit and a performance guarantee for the fixed proportions solution in the random proportions model.", "e:keyword": ["Assortment planning", "Inventory management", "Bounds", "Stockout-based substitution"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0426", "e:abstract": "The most widely used standard for mass-casualty triage, START, relies on a fixed-priority ordering among different classes of patients, and does not explicitly consider resource limitations or the changes in survival probabilities with respect to time. We construct a fluid model of patient triage in a mass-casualty incident that incorporates these factors and characterize its optimal policy. We use this characterization to obtain useful insights about the type of simple policies that have a good chance to perform well in practice, and we demonstrate how one could develop such a policy. Using a realistic simulation model and data from emergency medicine literature, we show that the policy we developed based on our fluid formulation outperforms START in all scenarios considered, sometimes substantially.", "e:keyword": ["Health operations", "Emergency response", "Triage"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0428", "e:abstract": "This essay, based on my 2012 MSOM Fellow Lecture, discusses an idea that has been useful for developing effective methods to set staffing levels in service systems: offered load analysis. The main idea is to tackle a hard problem by first seeking an insightful simplification. For capacity planning to meet uncertain exogenous demand, offered load analysis looks at the amount of capacity that would be used if there were no constraints on its availability. This simplification is helpful because the stochastic model becomes much more tractable. Offered load analysis can be especially helpful when the demand is not only uncertain but also time varying, as in many service systems. Given the distribution of the stochastic offered load, we often can set staffing levels to stabilize performance at target levels, even in face of a strongly time-varying arrival rate, long service times, and network structure.", "e:keyword": ["Offered load analysis", "Capacity planning", "Server staffing", "Time-varying arrival rates", "Infinite-server queues"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0429", "e:abstract": "We study a family of stylized assortment planning problems, where arriving customers make purchase decisions among offered products based on maximizing their utility. Given limited display capacity and no a priori information on consumers' utility, the retailer must select which subset of products to offer. By offering different assortments and observing the resulting purchase behavior, the retailer learns about consumer preferences, but this experimentation should be balanced with the goal of maximizing revenues. We develop a family of dynamic policies that judiciously balance the aforementioned trade-off between exploration and exploitation, and prove that their performance cannot be improved upon in a precise mathematical sense. One salient feature of these policies is that they &ldquo;quickly&rdquo; recognize, and hence limit experimentation on, strictly suboptimal products.", "e:keyword": ["Assortment planning", "Online algorithm", "Demand learning"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0430", "e:abstract": "In this paper, we consider the demand for multiple, successive generations of products and develop a population-growth model that allows demand transitions across multiple product generations and takes into consideration the effect of competition. We propose an iterative-descent method for obtaining the parameter estimates and the covariance matrix, and we show that the method is theoretically sound and overcomes the difficulty that the units-in-use population of each product is not observable. We test the model on both simulated sales data and Intel's high-end desktop processor sales data. We use two alternative specifications for product strength in this market: performance and performance/price ratio. The former demonstrates better fit and forecast accuracy, likely due to the low price sensitivity of this high-end market. In addition, the parameter estimate suggests that, for the <i>innovators</i> in the diffusion of product adoption, brand switchings are more strongly influenced by product strength than within-brand product upgrades in this market. Our results indicate that compared with the Bass model, Norton&ndash;Bass model, and Jun&ndash;Park choice-based diffusion model, our approach is a better fit for strategic forecasting that occurs many months or years before the actual product launch.", "e:keyword": ["Product transitions", "Forecasting", "Multiple-generation demand model", "Diffusion"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0431", "e:abstract": "A supply stream is a continuous version of a supply chain. It is like a series inventory system, but stock can be held at any point along a continuum, not just at discrete stages. We assume stationary parameters and aim to minimize the long-run average total cost. We show that a stationary continuous-stage echelon base-stock policy is optimal. That is, at each geographic point along the supply stream, there is a target echelon inventory level, and the optimal policy at all times is to order and dispatch material so as to move the echelon inventory position as close as possible to this target. We establish this result by showing that the solutions to certain discrete-stage systems converge monotonically to a limit, as the distances between the stages become small, and this limit solves the continuous-stage system. With demand approximated by a Brownian motion, we show that, in the continuous-stage limit, the supply stream model is equivalent to one describing first-passage times. This linkage leads to some interesting and useful results. Specifically, we obtain a partial differential equation that characterizes the optimal cost function, and we find a closed-form expression for the optimal echelon base-stock levels in a certain special case, the first in the inventory literature. These expressions demonstrate that the well-known square-root law for safety stock does not apply in this context.", "e:keyword": ["Inventory/production", "Multiechelon", "Uncertainty", "Probability", "Diffusion", "Partial differential equations"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0432", "e:abstract": "This paper examines the impact of architectural decisions on the level of defects in a product. We view products as collections of components linked together to work as an integrated whole. Previous work has established <i>modularity</i> (how decoupled a component is from other product components) as a critical determinant of defects, and we confirm its importance. Yet our study also provides empirical evidence for a relationship between product quality and <i>cyclicality</i> (the extent to which a component depends on itself via other product components). We find cyclicality to be a determinant of quality that is distinct from, and no less important than, modularity. Extending this main result, we show how the cyclicality&ndash;quality relationship is affected by the centrality of a component in a cycle and the distribution of a cycle across product modules. These findings, which are based on an analysis of open source software development projects, have implications for the study and design of complex systems.", "e:keyword": ["Product architecture", "Cycles", "Modularity", "Iterative problem solving", "Defects"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0433", "e:abstract": "We study how to manage commodity risks (price and consumption volume) via physical inventory and financial hedge in a multiperiod problem (with an interperiod utility function) for a risk-averse firm procuring a storable commodity from a spot market at a random price and a long-term supplier at a fixed price. The firm also has access to financial contracts written on the commodity price, such as futures contracts and call and put options. We examine different cases of financial hedging, for example, single-contract and multicontract hedges. For each case, we dynamically maximize the mean-variance utility of the firm's cash flow and characterize an optimal integrated policy of inventory and hedging, which is easy to compute and implement. We find that as long as futures are used in each period, alone or not, the optimal inventory policy is myopic. The optimal hedging policy, however, is never myopic, but depends on all the future optimal decisions. This is contrary to findings of the literature using intraperiod utility functions, which finds myopic hedging to be optimal. Moreover, we find that hedging may lead to inventory reduction in multiperiod problems. Thus the insights from the single-period studies in the literature&mdash;hedging leads to inventory increase&mdash;do not apply. Finally, insights are offered on the role and impact of inventory and financial hedge on profitability, variance control, and service level, using both analytical and numerical results.", "e:keyword": ["Stochastic inventory", "Commodity markets", "Futures", "Options", "Risk management", "Hedging", "Risk aversion"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0434", "e:abstract": "Services such as FedEx charge up-front fees but reimburse customers for delays. However, lead-time pricing studies ignore such delay refunds. This paper contributes to filling this gap. It studies revenue-maximizing tariffs that depend on realized lead times for a provider serving multiple time-sensitive customer types. We relax two key assumptions of the standard model in the lead-time pricing literature. First, customers may be <i>risk averse</i> (RA) with respect to payoff uncertainty, where payoff equals valuation, minus delay cost, minus payment. Second, tariffs may be <i>arbitrary</i> functions of realized lead times. The standard model assumes risk-neutral (RN) customers and restricts attention to flat rates. We report three main findings: (1) With RN customers, flat-rate pricing maximizes revenues but leaves customers exposed to payoff variability. (2) With RA customers, flat-rate pricing is suboptimal. If types are distinguishable, the optimal lead-time-dependent tariffs fully insure delay cost risk and yield the same revenue as under optimal flat rates for RN customers. With indistinguishable RA types, the differentiated first-best tariffs may be incentive-compatible even for uniform service, yielding higher revenues than with RN customers. (3) Under price and capacity optimization, lead-time-dependent pricing yields higher profits with less capacity compared to flat-rate pricing.", "e:keyword": ["Delay", "Lead times", "Pricing", "Queueing systems", "Revenue management", "Risk aversion"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0435", "e:abstract": "Motivated by the dual-sourcing and contracting practices in the semiconductor industry, we study two prevailing types of contracts that deal with horizontal-capacity-coordination issues between two possible sources: an integrated device manufacturer (IDM) and a foundry. IDMs both design and manufacture semiconductor devices, whereas foundries concentrate only on manufacturing. Because production capacity is capital intensive, IDMs often make decisions on whether to manufacture each device internally or subcontract to foundries. Two types of contracts are most frequently used in such settings. Under &alpha;-contracts, a fixed fraction &alpha; of ex post realized demand is committed to subcontract to the foundry and serves as an incentive for the foundry to build capacity. Under reservation contracts, an ex ante capacity reservation fee is paid to the foundry as an incentive to build capacity. Because of the different nature of incentives under these contracts, it is unclear which type of contract maximizes the IDM's expected profit. Furthermore, IDMs and their customers often prefer dual sourcing to sole sourcing for risk-management purposes. This paper studies the relationship between the two types of contracts, both with and without dual-sourcing constraints and shows the effect of a dual-sourcing preference on contract selection. Our analysis offers supporting rationale for the coexistence of &alpha;-contracts and reservation contracts in practice and provides insights on horizontal capacity coordination beyond the semiconductor industry.", "e:keyword": ["Capacity planning and investment", "Incentives and contracting", "Risk management", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0437", "e:abstract": "We use a modified optimal market area model to examine how links between material recycling and other aspects of operations strategy can shape plant networks for the processing of recyclable materials. We characterize the complementarity of the recyclate ratio, defined as the maximum recycled content, with material versatility and miniscaling of recycling plants. We also observe that it is beneficial to coordinate investments in recycling- and production-related competencies because colocated recycling and production plants (minimills) eliminate recyclate transport. We therefore consider versatile miniplants, defined as a competency that factors in both material versatility and coordinated miniscaling of recycling and production plants, and capture how it complements both the recyclate ratio and localization of production plants, a competency that takes advantage of local adaptation and customer proximity. In numerical examples for rolled aluminum and nylon resin plant networks in Europe, we find that the complementarity effects are large, as they are for nylon resins, if recycling is nascent and challenging economically and if the plant network is too centralized at first to benefit much from an increased recyclate ratio or increased localization. We find that, for the nylon resin network, considering an investment in the recyclate ratio as part of a coordinated investment plan drives the emergence of a decentralized and localized minimill network, even though an increased recyclate ratio does not link directly with either decentralization or localization. We conclude that material recycling, versatile miniplants, and localization can fit well together in a forward-looking, sustainable operations strategy.", "e:keyword": ["Recycling", "Material versatility", "Localization", "Minimills", "Operations strategy", "Plant networks", "Optimal market area"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0438", "e:abstract": "With the recent focus on sustainability, firms making adjustments to their production or distribution capacity levels often have the option of investing in newer technologies with lower carbon footprints and/or energy consumption. These more sustainable technologies typically require a higher up-front investment but have lower operating (fuel or energy) costs. What complicates this decision is the fact that the projected dollar savings from the more sustainable technologies fluctuate considerably due to uncertainty in fuel prices, and the total capacity may not be utilized at 100&percnt; because of fluctuations in the demand for the product. We consider the firm's capacity adjustments over time given a portfolio of technology options when the demand and the fuel costs are stochastic and possibly dependent. Our model also allows for usage-based capacity deterioration. We provide the analytical structure of the optimal policy, which assigns different control limits for investing, staying put, and disinvesting in the capacities of the competing technology choices for each realization of demand and fuel costs at each period. We also present an application of our model to the problem of designing a delivery truck fleet for a beverage distributor.", "e:keyword": ["Sustainable operations", "Dynamic capacity investment", "Technology choice"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0439", "e:abstract": "In many manufacturing operations, profitable energy efficiency opportunities remain unexploited. Although previous studies have tried to explain the underinvestment, we focus on how the way in which a portfolio of opportunities is presented in a list affects adoption decisions. We use information on over 100,000 energy-saving recommendations made to more than 13,000 small and medium-sized manufacturing firms under the Industrial Assessment Centers program of the U.S. Department of Energy. We find that adoption rates are higher for initiatives appearing early in a list of recommendations. This sequence effect is consistent and large: simply moving a recommendation one position lower has the same effect on average as increasing up-front implementation cost by at least 17&percnt; from the average value. Given this impact of sequence on adoption of individual recommendations, we utilize variations within our data to examine how various sequencing approaches affect adoption at the portfolio level. Sequences in which recommendations are listed from best to worst payback achieve higher potential energy savings given the investments in energy efficiency made by the firms. We also observe a choice overload effect at the portfolio level, but the magnitude of this effect is small.", "e:keyword": ["Process improvement", "Energy efficiency", "Behavioral operations", "Order effects", "Econometric analysis", "Empirical research", "Energy-related operations", "Environmental operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0440", "e:abstract": "We study a nongovernmental organization's (NGO's) decisions when it attempts to remove a potentially hazardous substance from commercial use in a market with competing firms. Specifically, we determine under what market and regulatory conditions an NGO should target the industry versus the regulatory body to influence firms to replace the substance. We examine how the NGO's strategy changes as the NGO's pragmatism (i.e., the extent to which the NGO incorporates firms' profits into its decision making) increases. Our results demonstrate that when the NGO is less pragmatic, it should examine the existing market structure to determine whether to target the industry or the regulatory body. However, as the pragmatism of the NGO increases, the NGO should increasingly leverage the competition between firms to ensure that a replacement is available to consumers. We examine multiple extensions including varying the competition dynamics, the NGO targeting both the industry <i>and</i> the regulatory body, the time discounting of replacement costs, and a firm potentially lobbying to counteract an NGO's activism. We show that the potential for a firm to lobby can benefit consumers by motivating the NGO to exert more effort and increase the market sensitivity to a substance, thereby forcing the firm to replace.", "e:keyword": ["Nongovernmental organizations", "Environmental regulations", "Substances of concern", "Public politics", "Private politics", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0443", "e:abstract": "Carbon footprinting is a tool for firms to determine the total greenhouse gas (GHG) emissions associated with their supply chain or with a unit of final product or service. Carbon footprinting typically aims to identify where best to invest in emission reduction efforts, and/or to determine the proportion of total emissions that an individual firm is accountable for, whether financially and/or operationally. A major and underrecognized challenge in determining the appropriate allocation stems from the high degree to which GHG emissions are the result of joint efforts by multiple firms. We introduce a simple but general model of joint production of GHG emissions in general supply chains, decomposing the total footprint into processes, each of which can be influenced by any combination of firms. We analyze two main scenarios. In the first scenario, the social planner allocates emissions to individual firms and imposes a cost on them (such as a carbon tax) in proportion to the emissions allocated. In the second scenario, a carbon leader voluntarily agrees to offset all emissions in the entire supply chain and then contracts with individual firms to recoup (part of) the costs of those offsets. In both cases, we find that, to induce the optimal effort levels, the emissions need to be overallocated, even if the carbon tax is the true social cost of carbon. This is in contrast to the usual focus in the life-cycle assessment (LCA) and carbon footprinting literatures on avoiding double counting. Our work aims to lay the foundation for a framework to integrate the economics- and LCA-based perspectives on supply chain carbon footprinting.", "e:keyword": ["Carbon footprint", "Supply chain", "Sustainable operations", "Emissions allocation"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0444", "e:abstract": "A common characteristic of basic material manufacturers (which account for 85&percnt; of all industrial energy use) and of cleantech manufacturers is that they are price takers in their input and output markets. Variability in those prices has implications for how much a manufacturer should invest in three fundamental types of process improvement. Input price variability reduces the value of improving <i>input efficiency</i> (output produced per unit input) but increases that of <i>capacity efficiency</i> (the rate at which a production facility can convert input into output). Output price variability increases the value of capacity efficiency, but it increases the value of input efficiency if and only if the expected margin is small. Moreover, as the expected input cost rises, the value of input efficiency decreases. A third type of process improvement is to develop <i>flexibility in input efficiency versus capacity efficiency</i> (the ability to respond to a rise in input cost or fall in output price by increasing input-efficiency at the expense of capacity efficiency). The value of this flexibility decreases with variability in input and output prices if and only if the expected margin is thin. Together, these results suggest that a carbon tax or cap-and-trade system may reduce investment by basic material manufacturers in improving energy efficiency.", "e:keyword": ["Energy efficiency", "Process improvement", "Flexibility", "Environment"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0445", "e:abstract": "We consider the capacity planning problem during a product transition in which demand for a new-generation product gradually replaces that for the old product. Capacity for the new product can be acquired both by purchasing new production lines and by converting existing production lines for the old product. Furthermore, in either case, the new product capacity is &ldquo;retrofitted&rdquo; to be flexible, i.e., to be able to also produce the old product. This capacity planning problem arises regularly at Intel, which served as the motivating context for this research. We formulate a two-product capacity planning model to determine the equipment purchase and conversion schedule, considering (i) time-varying and uncertain demand, (ii) dedicated and flexible capacity, (iii) inventory and equipment costs, and (iv) a chance-constrained service-level requirement. We develop a solution approach that accounts for the risk-pooling benefit of flexible capacity (a closed-loop planning approach) and compare it with a solution that is similar to Intel's current practice (an open-loop planning approach). We evaluate both approaches with a realistic but disguised example and show that the closed-loop planning solution leads to savings in both equipment and inventory costs and matches more closely the service-level targets for the two products. Our numerical experiments illuminate the cost trade-offs between purchasing new capacity and converting old capacity and between a level capacity plan versus a chase capacity plan.", "e:keyword": ["Capacity planning", "Product transition", "Equipment conversion", "Flexible capacity", "Risk pooling"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0446", "e:abstract": "Energy generation from intermittent renewable sources introduces additional variability into electrical systems, resulting in a higher cost of balancing against the increased variabilities. Ways to balance demand and supply for electricity include using flexible generation resources, storage operations, and curtailing intermittent generation. This paper focuses on the operational and environmental impact of curtailing intermittent generation. We construct a stochastic dynamic optimization model that captures the critical components of the system operating cost and analyze how various generation resources should operate with and without curtailing intermittent generation. We find that the system cost reduction per unit of curtailed energy is consistently significant and the presence of storage may increase the cost saving per unit of curtailed energy. We also find that curtailing intermittent generation often leads to system emission reductions.", "e:keyword": ["Intermittent generation", "Wind power", "Economic curtailment", "Flexible generation", "Energy storage operations", "Operations management practice"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0447", "e:abstract": "Management of new product introductions is critical for nearly all firms, and one of its most important dimensions is the management of demand during the introduction. Research analyzing this area predominantly uses versions of the diffusion model to capture the demand trajectory of a new product with a fixed potential market. The classic Bass model assumes that demand for innovative products is influenced both by &ldquo;external&rdquo; media influence and &ldquo;internal&rdquo; word-of-mouth effect, but it excludes price and assumes that capacity is unlimited. In reality, both factors critically influence firms' strategies. Price fluctuations for a new product are common, and price is often a critical lever that helps to shape the demand. Also, firms often have significant capacity constraints, which influence the feasibility of their strategies. In this paper, we consider how a capacity-constrained firm prices products during new product introductions. Thus, the demand rate is influenced by price and, when capacity is insufficient, we allow some customers to be either lost or backlogged, which slows down the word-of-mouth effect. To understand the effect of both pricing and capacity, we consider the integrated optimal pricing, production, and inventory decisions, using control-theory framework (a generalization of the classic Bass model). Most of our results are fairly robust and apply under the assumption of lost sales and partial backlogging, as well as make-to-order and make-to-stock environments. We show that in most cases, the optimal trajectory of demand is unimodal, as in the Bass model, but the optimal price trajectory and the corresponding pricing policy are more complicated when capacity is limited. Using a numerical study, we explore when pricing flexibility is most valuable and whether simple pricing policies may be effective. We find that benefits of pricing flexibility are highest when capacity is neither unlimited (very large) nor very small and when word-of-mouth effect dominates direct impact from media. The ability to adjust prices is significantly more important than the option of producing in advance and holding inventory. We also find that simple pricing policies, appropriately chosen for given capacity, perform very well. In a numerical study, we show that demand uncertainty and increases in capacity over time do not affect our main insights.", "e:keyword": ["OM-marketing interface", "Retailing", "Pricing", "Bass diffusion model"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0448", "e:abstract": "We investigate whether stock price movements can inform operations managers as to where they should focus improvement efforts. We examine how unexpected performance along several dimensions of service quality&mdash;on-time performance, long delays and cancellations, lost bags, and denied boardings&mdash;impacts contemporaneous stock returns. Prior research suggests that airlines buffer their flight schedules and engage in expensive employee incentive programs to increase the likelihood of on-time arrival. We find that only long delays are penalized by the market, and we identify a number of carrier-specific factors that alter the financial impact of long delays. We find that the penalty a carrier faces for long delays is significantly higher if it operates a high percentage of short-haul or connecting flights, or if its competitors incur fewer long delays in the same time period. Our findings suggest that developing ways to curtail long delays is a useful future research area.", "e:keyword": ["Econometric analysis", "Empirical research", "Service operations", "Quality management", "OM&ndash", "Finance interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0449", "e:abstract": "How should a firm design a price/lead-time menu and scheduling policy to maximize revenues from heterogeneous time-sensitive customers with private information about their preferences? We consider this question for a queueing system with two customer types and provide the following results. First, we develop a novel problem formulation and solution method that combines the achievable region approach with mechanism design. This approach extends to menu design problems for other systems. Second, the work conserving <i>c</i>&mu; priority rule, known to be delay cost minimizing, incentive-compatible, and socially optimal, need not be revenue maximizing. A <i>strategic delay</i> policy may be optimal: It prioritizes impatient customers, but artificially inflates the lead times of patient customers. This suggests a broader guideline: Revenue-maximizing firms that lack customer-level demand information should also consider customer incentives, not only operational constraints, in their scheduling policies. Third, we identify general necessary and sufficient conditions for optimal strategic delay: a <i>price</i>, a <i>lead-time</i>, and a <i>segment-size</i> condition. We translate these into demand and capacity parameter conditions for cases with homogeneous and heterogeneous valuations for each type. In some cases strategic delay is optimal if capacity is relatively abundant, in others if it is relatively scarce.", "e:keyword": ["Congestion", "Delay", "Incentives", "Lead times", "Mechanism design", "Pricing", "Priorities", "Quality of service", "Queueing systems", "Revenue management", "Scheduling", "Service differentiation"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0450", "e:abstract": "We consider an infinite-horizon, periodic-review, single-item production/inventory system with random demand and backordering, where multiple setups are allowed in any period and a separate fixed cost is associated for each setup. Contrary to the majority of the literature on this topic, we do not restrict the order quantities to be integer multiples of the exogenously given batch size and instead allow the possibility of partial batches, in which case the fixed cost for ordering the batch is still fully charged. We build a model that particularly takes the batch-ordering cost structure into account. We introduce an alternative cost-accounting scheme to analyze the problem, which we use to develop a computationally efficient optimal solution method and several properties of the optimal solution. In addition, we propose two heuristic policies, both of which perform extremely well computationally.", "e:keyword": ["Inventory control", "Multiple setups", "Markov decision process"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0451", "e:abstract": "A systematic approach to innovating business models can help identify new business models that encourage sustainable use of products and services, or facilitate wider adoption of new environmentally friendly technologies. This paper provides a brief summary of a conceptual framework that we have developed to systematize the study and identification of new business models. Our approach advocates that the key to identifying new business models is understanding the context of decision making in existing models and the associated inefficiencies. We propose a three-step approach: First, existing business models must be audited for identifying information and incentive misalignment inefficiencies that destroy value. Next, new business models can be identified by changing the context of the decision associated with the most consequential of these inefficiencies. We conjecture that four elements of the decision context are most significant: WHAT decisions are made, WHEN they are made, WHO makes them, and WHY they are made. We provide a set of idea triggers to stimulate brainstorming of new business models by changing one of these four Ws. Finally, we advise that generated business models should be analyzed and experimented with to identify the most promising ones. We close the paper by describing the design of a pedagogical program based on this framework.", "e:keyword": ["Business models", "Sustainability", "Innovation", "Entrepreneurship", "Business model innovation", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0452", "e:abstract": "This paper studies the impact of a manufacturer-hired sales agent on a supply chain comprising a manufacturer and a retailer. The sales agent is working mainly at the retailer's location to boost demand. We focus on a wholesale price contract, under which the retailer decides how much to order from the manufacturer. The information structure within the supply chain and the efficiency of the sales agent affect the supply chain members' expected profits. We show that, because of the agency issue between the sales agent and the manufacturer, when the retailer's demand forecast accuracy is similar to the manufacturer's and the wholesale price is fixed, the retailer's profit decreases as his demand forecast accuracy improves. We also illustrate that when the retailer's forecast accuracy is much better than the manufacturer's and the wholesale price is endogenous, his expected profit decreases as his forecast accuracy improves. Moreover, we demonstrate that having a more efficient sales agent is beneficial for the retailer when the wholesale price is fixed, whereas this is not always the case when the wholesale price depends on the efficiency of the sales agent.", "e:keyword": ["Game theory", "Incentives and contracting", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0453", "e:abstract": "As supply chains become increasingly complex and global in their scale, supplier selection and management in the face of disruption risk has become one of the most challenging tasks for modern managers. Several novel model-based approaches to managing such risks have been developed in the academic literature, but how behavioral tendencies may affect procurement decisions under such conditions has received relatively less attention. In this paper, we present results from a study where paid subjects were asked to place orders from two suppliers who differ in their costs and risks to satisfy a fixed amount of end-customer demand. We show that under such a scenario, it is theoretically optimal to <i>sole source</i> either from the more reliable (and more costly) supplier or from the more risky but cheaper supplier, depending on cost and risk parameters. Subjects in our experiment, however, show a systematic tendency to <i>diversify</i> their orders between the two sources. We document this diversification tendency in procurement decisions and its possible impact on profits under various cost and risk settings as well as comment on various ordering behavior observed during the experiments. We also establish that bounded rationality of subjects can provide a possible rationale for the above phenomenon.", "e:keyword": ["Disruption management", "Supply risk management", "Experiments", "Ordering behavior", "Diversification bias", "Bounded rationality"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0454", "e:abstract": "We analyze and compare five contract schemes used in a supply chain: fixed price (FP), cost reimbursement (CR), procurement control (PC), index-linked payment (IL), and relational (RL) contracts. From interviews, we learned that (1) FP and RL are the two most popular schemes; (2) PC is less popular, but it is used more often than CR and IL; and (3) a couple of firms are considering CR and IL and will probably use them in the future. By presenting a two-stage contracting model that incorporates a risk-neutral buyer and a risk-averse supplier under raw material price uncertainty and information asymmetry, we show that overhedging the supplier's risk with an IL contract can be optimal for the buyer in many cases. Using the model to study the effects of price trends, risk attitudes, information asymmetry, and constraints on purchase time, we find that each contract can be optimal in certain situations from certain standpoints. In particular, when a long-term relationship is considered, RL is equivalent to IL as long as the contract is self-enforcing, and RL can replace IL when no index is available. Connecting the analytical results to the interviews, we find that firms' choices and considerations can be well understood.", "e:keyword": ["Raw material price volatility", "Information asymmetry", "Supply chain management", "Contracting", "Risk aversion", "Risk sharing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0455", "e:abstract": "A &ldquo;cleantech&rdquo; firm is one with an innovative technology and/or business model for serving an existing market with dramatically reduced environmental impact. This paper describes operations management (OM) challenges faced by five cleantech companies, and a few questions that these raise for OM and multidisciplinary research. It aims to fuel readers' motivation to identify and pursue others. The OM community has fundamental roles to play in the creation of an environmentally sustainable economy.", "e:keyword": ["Environment", "Energy", "Process innovation", "Supply chain management", "Capacity investment", "Bankruptcy", "Public policy"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0456", "e:abstract": "Paul Kleindorfer was among the first to weigh in on and nurture the stream of sustainable operations management. The thoughts laid out here are based on conversations we had with Paul relating to the drivers underlying sustainability as a management issue: population and per capita consumption growth, the limited nature of resources and sinks, and the responsibility and exposure of firms to ensuing ecological risks and costs. We then discuss how an operations management lens contributes to the issue and criteria to help the sustainable operations management perspective endure. This paper relates to a presentation delivered by Morris Cohen for Paul's Manufacturing and Service Operations Management Distinguished Fellows Award, given at Columbia University, June 18, 2012. We wrote this paper at Paul's request.", "e:keyword": ["Sustainable operations management", "Sustainability", "Environment", "Paul Kleindorfer"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0457", "e:abstract": "Human population growth and increasing consumption per capita are causing environmental impacts that threaten the survival and well-being of humans and other species on Earth. To sustain natural ecosystems and human populations with a high standard of living, humanity must now reinvent our systems for the production and delivery of goods and services to dramatically reduce the environmental impacts associated with consumption. The operations management community can contribute critical know-how to do so, as demonstrated by the papers in this special issue.", "e:keyword": ["Environmentally sustainable operations management", "Product design", "Energy efficiency", "Climate change", "Population", "Adaptation", "Chemical toxicity"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0458", "e:abstract": "An important trade-off organizations face in many environments is one between quality and speed. Working faster may result in greater output and less delay, but may result in lower quality and dissatisfied customers. In this work, we consider dynamic models in a monopoly setting to explore the optimal balance among the multiple dimensions of speed, price, and wait time. The impact of quality is captured via the market demand potential, which is a function of the speed (quality) in the previous period. We obtain several results and insights. First, in scenarios where speed may be difficult to change over time (e.g., some automated production lines) but price can be changed, we show that the optimal price charged is such that the demand rate remains constant over time, even though the price and market potential are changing. Furthermore, we identify conditions when the firm will work at a speed that is higher or lower than a benchmark speed and characterize the behavior of prices over time. Second, in scenarios where a firm may not be able to change prices but can adjust the speed each period, the firm starts at a speed that may be faster or slower than a benchmark speed but converges to it over time. In this constant price case, as the benchmark speed increases, the initial speed adopted by the firm is actually lower but increases more quickly thereafter. We also characterize the behavior of price and speed in settings where both can be changed over time. Interestingly, a firm typically starts at a slow speed and increases the speed, price, and demand over time. Although our main model assumes that the firm internalizes the congestion cost, several of our results extend to a scenario where the demand rate is impacted by the congestion level.", "e:keyword": ["Service operations", "Quality management", "Dynamic programming", "Queuing theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0459", "e:abstract": "Inventory-based dynamic pricing has become a common operations strategy in practice and has received considerable attention from the research community. From an implementation perspective, it is desirable to design a simple policy like a base-stock list-price (BSLP) policy. The existing research on this problem often imposes restrictive conditions to ensure the optimality of a BSLP policy, which limits its applicability in practice. In this paper, we analyze the dynamic inventory and pricing control problem in which the demand follows a generalized additive model (GAM). The GAM overcomes the limitations of several demand models commonly used in the literature, but introduces analytical challenges in analyzing the dynamic program. Via a variable transformation approach, we identify a new set of technical conditions under which a BSLP policy is optimal. These conditions are easy to verify because they depend only on the location and scale parameters of demand as functions of price and are independent of the cost parameters or the distribution of the random demand component. Moreover, although a BSLP policy is optimal under these conditions, the optimal price may not be monotone decreasing in the inventory level. We further demonstrate our results by applying a constrained maximum likelihood estimation procedure to simultaneously estimate the demand function and verify the optimality of a BSLP policy on a retail data set.", "e:keyword": ["Inventory&ndash", "Pricing", "Generalized additive models", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0460", "e:abstract": "Based on the recent incidents of H5N1, H1N1, and influenza pandemics in history (1918, 1957, and 1968) experts believe that a future influenza pandemic is inevitable and likely imminent. Although the severity of influenza pandemics vary, evidence suggests that an efficient and rapid response is crucial for mitigating morbidity, mortality, and costs to society. Hence, preparing for a potential influenza pandemic is a high priority of governments at all levels (local, state, federal), nongovernmental organizations (NGOs), and companies. In a severe pandemic, when a large number of people are ill, infected persons and their families may have difficulty purchasing and preparing meals. Various government agencies and NGOs plan to provide meals to these households. In this paper, in collaboration with the American Red Cross, we study food distribution planning during an influenza pandemic. We develop a disease spread model to estimate the spread pattern of the disease geographically and over time, combine it with a facility location and resource allocation network model for food distribution, and develop heuristics to find near-optimal solutions for large instances. We run our combined disease spread and facility location model for the state of Georgia and present the estimated number of infections and the number of meals needed in each census tract for a one-year period along with a design of the supply chain network. Moreover, we investigate the impact of voluntary quarantine on the food demand and the food distribution network and show that its effects on food distribution can be significant. Our results could help decision makers prepare for a pandemic, including how to allocate limited resources and respond dynamically.", "e:keyword": ["Influenza pandemic", "Food distribution planning", "Disease spread models", "Multiperiod facility location", "Dynamic update"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0461", "e:abstract": "In this paper, we explore how firms can better manage their sourcing by developing relationships not only with their suppliers but also with their suppliers' suppliers. We detail an empirical case study explaining how the firm developed relationships with its suppliers and raw material suppliers via a collaborative center, the sourcing hub. We then analytically model the scenarios encountered in our empirical work and examine two facets of upstream sourcing under uncertain demand scenarios: (a) firms can supply raw material directly to their suppliers, and this may be beneficial for the firm and its suppliers; and (b) firms can bring their suppliers together at the sourcing hub, and the resulting cooperation between suppliers is beneficial for the suppliers and the raw material suppliers. Overall, our work explores the market and economic conditions under which active management of upstream sourcing can add value to supply chains.", "e:keyword": ["Sourcing hub", "Raw material sourcing", "Supplier network"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0462", "e:abstract": "We consider a duopoly where firms compete on waiting times in the presence of an industry benchmark. The demand captured by a firm depends on the gap between the firm's offer and the benchmark. We refer to the benchmark effect as the impact of this gap on demand. The formation of the benchmark is endogenous and depends on both firms' choices. When the benchmark is equal to the shorter of the two offered delays, we characterize the unique Pareto optimal Nash equilibrium. Our analysis reveals a <i>stickiness effect</i> in which firms equate their delays at the equilibrium when the benchmark effect is sufficiently strong. When the benchmark corresponds to a weighted average of the two offered delays, we show the existence of a pure Nash equilibrium. In this case, we reveal a <i>reversal effect</i>, in which the market leader, i.e., the firm that offers a shorter delay, becomes the follower when the benchmark effect is sufficiently strong. In both cases, we show that customers' equilibrium waiting times are shorter with the benchmark effect than without it. Our models also capture customers' loss aversion, which, in our setting, states that demand is more sensitive to the gap between the delay and the benchmark when the delay is longer than the benchmark (loss) than when it is shorter (gain). We characterize the impact of this loss aversion on the equilibrium in both settings. Finally, we show numerically that the stickiness and reversal effects still exist when firms also compete on price.", "e:keyword": ["Waiting time competition", "Benchmark effect", "Loss aversion", "Queues", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0463", "e:abstract": "In a call center, agents may handle calls at different speeds, and also may be more or less successful at resolving customers' inquiries, even when only considering customers calling with similar requests. One common measure of successful call resolution is whether or not the call results in the customer calling back. This presents a natural trade-off between speed and quality, where speed is defined as the average time before an incoming call is answered (the average waiting time) and quality is defined as the percentage of all arriving calls that do not result in callbacks (the call resolution). The relevant control is the routing, that is, the decision concerning which agent should handle an arriving call when more than one agent is available. In an inverted-V model setting, we formulate an optimization problem with the dual performance objective of minimizing average customer waiting time and maximizing the call resolution. We solve this optimization problem asymptotically in the Halfin&ndash;Whitt many-server limit regime, interpret its solution as a routing control for the discrete-event system, and show via simulation that the interpreted routing control is on the efficient frontier. In particular, any routing control that has a lower average waiting time (higher call resolution) must also have a lower call resolution (higher average waiting time).", "e:keyword": ["Call center management", "Queueing theory", "Stochastic methods", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0464", "e:abstract": "This paper examines the effect of multitasking on overall worker performance, as measured by processing time, throughput rate, and output quality using microlevel operational data from the field. Specifically, we study the multitasking behavior of physicians in a busy hospital emergency department (ED). By drawing on recent findings in the experimental psychology literature and the nascent work in cognitive neuroscience, we develop several hypotheses for the effect of multitasking on worker performance. We first examine how multitasking affects a physician's processing time. We find that the total time taken to discharge a given number of patients has a U-shaped response to the level of physician multitasking; that is, multitasking initially helps to reduce the time taken, but only up to a certain threshold level, after which it increases in the level of multitasking. In addition, multitasking significantly impacts quality of care. Although lower levels of multitasking are associated with improved quality of care, at higher levels, additional multitasking leads to a smaller number of detected diagnoses and an increased likelihood of a 24-hour revisit rate to the ED. These findings have important implications for the design and organization of work in general and for the delivery of critical care in particular.", "e:keyword": ["Multitasking", "Productivity", "Quality", "Emergency department", "Capacity planning"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0465", "e:abstract": "It is a pleasure to announce that the 2013 <i>Manufacturing &amp; Service Operations Management</i> Best Paper Award goes to Omar Besbes, Robert Phillips, and Assaf Zeevi for Testing the Validity of a Demand Model: An Operations Perspective for its contribution to theory and practice of operations management. The three authors will share a $2,000 prize, contributed by the MSOM Society of INFORMS.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0466", "e:abstract": "In this paper we describe a database of online airline prices collected from a major online travel agent and a low-cost carrier. The database provides detailed pricing data for all nonstop flights offered in a market. Data are provided for 42 domestic U.S. markets across a 28-day booking horizon for 21 departure dates. Each of the 42 markets is served by one or more low-cost carriers. These data can be used to investigate the evolution of prices and price dispersion for monopoly, duopoly, and oligopoly markets. The data can be used to create simulated data sets for benchmarking the performance of revenue management algorithms that consider competitors' prices. Analysis of the data may enable researchers to observe general patterns that would be useful to motivate research and/or teaching in revenue management.Data, as supplemental material, are available at <ext-link ext-link-type=\"uri\"  href=\"http://dx.doi.org/10.1287/msom.2013.0466\">http://dx.doi.org/10.1287/msom.2013.0466</ext-link>.", "e:keyword": ["Airline pricing", "Low-cost carriers", "Online pricing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0469", "e:abstract": "The benefits of inventory risk pooling are well known and documented. It has been proven in the literature that the expected costs of a centralized system are increasing in the degree of (positive) dependence of demand in an idealized newsvendor setting. Using the supermodular stochastic order to characterize dependence, we study a general two-tiered supply chain structure, in which both demand and supply yields are random, and prove that the expected costs are increasing in the degrees of positive dependence between demand and supply yield loss factors. Furthermore, using a distributionally robust optimization framework, we prove an analogous result for the case where demand and yield distributions are not precisely known.", "e:keyword": ["Supply chain design", "Inventory sharing", "Stochastic orders", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0470", "e:abstract": "Managing appointments for service systems with random job durations is a challenging task. We consider a class of appointment planning problems that involve two sets of decisions: <i>job sequencing</i>, i.e., determining the order in which a list of jobs should be performed by the server, and <i>appointment scheduling</i>, i.e., planning the starting times for jobs. These decisions are interconnected because their joint goal is to minimize the expected server idle time and job late-start penalty costs incurred because of randomness in job durations. In this paper, we design new heuristics for sequencing appointments. The idea behind the development of these heuristics is the structural connection between such appointment scheduling problems and stochastic inventory control in serial supply chains. In particular, the decision of determining time allowances as <i>buffers</i> against random job durations is analogous to that of selecting inventory levels as <i>buffers</i> to accommodate random demand in a supply chain; having excess buffers in appointment scheduling and supply chain settings incurs idle time and excess inventory holding costs, respectively, and having inadequate buffers leads to delays of subsequent jobs and backorders, respectively. Recognizing this connection, we propose tractable approximations for the job sequencing problem, obtain several insights, and further develop a very simple sequencing rule of ordering jobs by duration variance to late-start penalty cost ratio. Computational results show that our proposed heuristics produce close-to-optimal job sequences with significantly reduced computation times compared with those produced using an exact mixed-integer stochastic programming formulation based on the sample-average approximation approach.", "e:keyword": ["Appointment scheduling", "Service operations", "Stochastic inventory control", "Serial supply chains", "Stochastic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0471", "e:abstract": "This essay, based on my 2013 MSOM Distinguished Fellow lecture, presents my view on new opportunities and challenges for research in operations management.", "e:keyword": ["Operations management", "Research", "Impact on practice", "Problem-driven research", "Data-driven research", "Data-driven models", "DDM"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0472", "e:abstract": "Many real-world supply networks source required materials from multiple suppliers. Existing multiechelon inventory optimization approaches either restrict their scope to multiple supply sources in two-echelon systems or single suppliers in multiechelon systems. We develop an exact mathematical model for static dual supply in a general acyclic <i>N</i>-echelon network structure, which builds on the guaranteed-service framework for safety stock optimization. It is assumed that the suppliers are allocated static fractions of demand. We prove that for normally distributed demand an extreme point property holds. We present a real example from the industrial electronics industry consisting of five echelons and three dual-sourced materials. This example forms the basis for a numerical analysis. Compared with the only previously published approximate solution, our exact approach results in considerable cost savings because the exact model captures inventory pooling in a way that the approximation is unable to do. For a set of test problems, total safety stock cost savings are 9.1&percnt;, on average.", "e:keyword": ["Dual sourcing", "Guaranteed service", "Multiechelon inventory system", "Safety stock optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0473", "e:abstract": "Spot and forward purchases for delivery on the usage date play an important role in matching the supply and the uncertain demand of energy because storage capacity for energy, such as electricity, natural gas, and oil, is limited. Transaction costs tend to be larger in spot than forward energy markets near maturity. Partially procuring supply in the forward market, rather than entirely in the spot market, is thus a potentially valuable real option, which we call the <i>forward procurement option</i>. We investigate the optimal value and management of this real option as well as their sensitivities to parameters of interest. Our research quantifies the value of the forward procurement option on realistic natural gas instances, also suggesting that procuring the demand forecast in the forward market is nearly optimal. This policy greatly simplifies the management of this real option without an appreciable loss of value. We provide some theoretical support for this numerical finding. Beyond energy, our research has potential relevance for the procurement of other commodities, such as metals and agricultural products.", "e:keyword": ["Correlated price and demand uncertainty", "Energy and commodities", "Newsvendor model", "OM&ndash", "Finance interface", "Procurement", "Real options", "Spot and forward markets", "Transaction costs", "Valuation"]}, {"@id": "http://dx.doi.org/10.1287/msom.2013.0474", "e:abstract": "We analyze a queueing model that we call Erlang-R, where the &ldquo;R&rdquo; stands for reentrant customers. Erlang-R accommodates customers who return to service several times during their sojourn within the system, and its modeling power is most pronounced in time-varying environments. Indeed, it was motivated by healthcare systems, in which offered-loads vary over time and patients often go through a repetitive service process. Erlang-R helps answer questions such as how many servers (physicians/nurses) are required to achieve predetermined service levels. Formally, it is merely a two-station open queueing network, which, in a steady state, evolves like an Erlang-C (<i>M</i>/<i>M</i>/<i>s</i>) model. In time-varying environments, on the other hand, the situation differs: here one must account for the reentrant nature of service to avoid excessive staffing costs or undesirable service levels. We validate Erlang-R against an emergency ward (EW) operating under normal conditions as well as during a mass casualty event (MCE). In both scenarios, we apply time-varying fluid and diffusion approximations: the EW is critically loaded and the MCE is overloaded. In particular, for the EW we propose a time-varying square-root staffing policy, based on the modified offered-load, which is proved to perform well over small-to-large systems.", "e:keyword": ["Healthcare", "Queueing networks", "Modified offered-load", "Time-varying queues", "Halfin&ndash", "Whitt regime", "QED regime", "ED regime", "Emergency department staffing", "Mass casualty events", "Patient flow"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0475", "e:abstract": "We develop a parameter estimation routine for multinomial logit discrete choice models in which one alternative is completely censored, i.e., when one alternative is never observed to have been chosen in the estimation data set. Our method is based on decomposing the log-likelihood function into marginal and conditional components. Our method is computationally efficient, provides consistent parameter estimates, and can easily incorporate price and other product attributes. Simulations based on industry hotel data demonstrate the superior computational performance of our method over alternative estimation methods that are capable of estimating price effects. Because most existing revenue management choice-based optimization algorithms do not include price as a decision variable, our estimation procedure provides the inputs needed for more advanced product portfolio availability and price optimization models.", "e:keyword": ["Choice-based revenue management", "Discrete choice modeling", "Censored alternatives", "Sampling of alternatives"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0476", "e:abstract": "We model a two-tier queuing system with free and toll service options as two parallel <i>M</i>/<i>M</i>/1 servers. We solve for the welfare-maximizing toll service capacity and toll subject to the constraint that the toll service cover its costs. If the free and toll services are both used in equilibrium, a larger free-service capacity implies longer expected waiting time for the free service and lower welfare: an analogue to the Downs&ndash;Thomson paradox in transportation economics. The paradox is caused by the presence of scale economies in the toll service combined with the requirement that it be self-financing.", "e:keyword": ["Queuing system", "Two-tier service system", "Equilibrium arrival rates", "Pricing and capacity decisions", "Downs&ndash", "Thomson paradox"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0477", "e:abstract": "We study a buyer who periodically auctions off short-term supply contracts among her supply base. To mitigate significant cost shocks to procurement, the buyer can diversify her supply base by selecting suppliers from different regions. We find that the buyer's decision to diversify depends on her bid-taker power&mdash;that is, her ability to choose the auction mechanism. At one extreme, when the buyer has full bid-taker power and thus can dictatorially implement the optimal mechanism, she always prefers to diversify. At the other extreme, when the buyer uses a reverse English auction with no reserve price due to her lack of bid-taker power, she generally prefers to protect herself against potential price escalation from cost-advantaged suppliers by diversifying less. The managerial insight is that the more bid-taker power the buyer has to control price escalation from cost-advantaged suppliers the more she prefers a diversified supply base. This insight is shown to be robust to correlation between regional costs, ex ante asymmetry between regions, and intermediate levels of bid-taker power.", "e:keyword": ["Supply base design", "Procurement auctions", "Bid-taker power"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0478", "e:abstract": "A firm hires a consultant to acquire demand information. The outcome of information acquisition may turn out to be successful such that the firm learns much about the market demand, thus becoming &ldquo;informed,&rdquo; or unsuccessful such that it learns very little about the market demand, thus remaining &ldquo;uninformed.&rdquo; After the outcome becomes clear, the firm knows its information status, informed or uninformed, and the information content if informed. The client firm usually requires strict confidentiality that forbids the consultant to make any disclosure about the information acquisition, believing that greater informational advantage will surely be to its own benefits. As a result, neither the information content nor the information status is known to any third party. But should the firm always care so much about strict confidentiality? Will it be beneficial if the firm's information status, but not the information content, is known to its partners or any other firms? We investigate this issue in the context of a two-tier supply chain. A manufacturer offers a menu of contracts for supplying a product to a retailer who sells it in a market with random demand that has a known continuous distribution. The retailer hires a consultant to acquire demand information, with uncertain outcome. With probability <i>t</i>, the retailer becomes informed about the market demand, and with probability 1 &minus; <i>t</i>, he remains uninformed, where the probability <i>t</i> can be regarded as representing the retailer's information acquisition capability. We find that disclosing its information status benefits the retailer if its information acquisition capability is less than stellar and the market variability is intermediate. Our investigation shows that there are benefits that are foregone by following strict confidentiality but can potentially be recovered by switching to a policy of partial confidentiality.", "e:keyword": ["Transparency", "Market research", "Confidentiality", "Supply chain"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0479", "e:abstract": "We consider revenue optimization in an <i>M</i>/<i>M</i>/1 queue with price and delay sensitive customers, and we study the performance of demand-independent pricing that does not require any arrival rate information. We formally characterize the optimal demand-independent price and its performance relative to pricing with precise arrival rate knowledge. We find that demand-independent pricing can perform remarkably well and its performance improves as customers become more delay sensitive. In particular, for uniformly distributed customer valuations, under a large set of parameters, we find that demand-independent prices can capture more than 99&percnt; of the optimal revenue. We also study social optimization and find that demand-independent pricing can perform quite well; however, the performance is better under revenue optimization.", "e:keyword": ["Static pricing", "Dynamic pricing", "Revenue optimization", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0480", "e:abstract": "Whereas theoretical studies on dynamic pricing typically assume that consumers are either fully strategic or fully myopic, systematic empirical investigations into how consumers behave under dynamic pricing contexts are relatively rare. Focusing on scarce products, we constructed and experimentally tested a two-stage model in which a firm sells a seasonal good under exogenous inventory constraints to a market of strategic buyers. In our experiment, subjects assigned the role of buyers made purchase decisions in response to prices set by an automated seller. We find that equilibrium predictions assuming fully strategic buyers largely accounted for aggregate behavior in the experiment, and the ex post optimal decisions for subjects were overwhelmingly consistent with equilibrium prescriptions. Moreover, subjects tended to become individually more strategic as the session progressed. However, there were also nuanced systematic patterns of deviations from equilibrium that had profit and pricing implications for the seller. First, a nonnegligible minority of subjects exhibited completely myopic buying behavior even with practice. Second, when the product was relatively more scarce, myopic buying had a stronger impact on demand at higher prices; the upshot is that the seller's season-profit-maximizing price could be considerably higher than what would be optimal with fully strategic buyers.", "e:keyword": ["Dynamic pricing", "Revenue management", "Consumer behavior", "Experiments", "Behavioral operations management", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0481", "e:abstract": "We consider service systems with a finite number of customer arrivals, where customer interarrival times and service times are both stochastic and heterogeneous. Applications of such systems are numerous and include systems where arrivals are driven by events or service completions in serial processes as well as systems where servers are subject to learning or fatigue. Using an embedded Markov chain approach, we characterize the waiting time distribution for each customer, from which we obtain various performance measures of interest, including the expected waiting time of a specific customer, the expected waiting time of an arbitrary customer, and the expected completion time of all customers. We carry out extensive numerical experiments to examine the effect of heterogeneity in interarrival and service times. In particular, we examine cases where interarrival and service times increase with each subsequent arrival or service completion, decrease, increase and then decrease, or decrease and then increase. We derive several managerial insights and discuss implications for settings where such features can be induced. We validate the numerical results using a fluid approximation that yields closed-form expressions.", "e:keyword": ["Queueing systems", "Finite arrivals", "Heterogeneous interarrival and service times", "Transient analysis", "Fluid approximation"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0484", "e:abstract": "Millions of Americans undergo colonoscopy screening for colorectal cancer (CRC) prevention and surveillance every year. The efficiency of colonoscopy operations depends on how often patients are screened, which is a complex and controversial decision, as reflected by the discrepancy between clinical practice and guidelines. We develop a partially observable Markov decision process to optimize colonoscopy screening policies for the objective of maximizing total quality-adjusted life years. Our model incorporates age, gender, and risk of having CRC into the screening decisions and therefore provides a novel framework for personalized CRC screening. In addition to deriving the maximum attainable benefit from colonoscopy screening, which reflects the opportunity cost of following current guidelines, our results have several policy implications. Using clinical data, we show that the optimal colonoscopy screening policies may be more aggressive than the guidelines under some conditions. Optimal screening policies recommend that females with CRC history undergo colonoscopy more frequently than males. In contrast, females without CRC history should be screened less frequently than males. This result, which was not recognized before, signifies the role of gender in optimal CRC screening decisions.", "e:keyword": ["Stochastic modeling", "Partially observable Markov decision processes", "Operations research applications in healthcare", "Cancer screening", "Colorectal cancer prevention and surveillance", "Colonoscopy"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0485", "e:abstract": "The use of multiattribute auctions for procurement of products and services when both price and quality matter is becoming more frequent. Such auctions often employ scoring rules and are open ended in winner determination. Yet there is a significant gap in the literature on the efficiency of these procurement mechanisms. In this paper, providing a theoretical model and utilizing data from legal service procurement auctions, we study how open-ended scoring auctions can be used effectively in procurement and demonstrate the roles supplier quality and incumbency play in this process. We demonstrate that open-ended auctions can generate substantial savings to a buyer without compromising quality. We study the underlying mechanism and show how the auction format can work to achieve such performance. We find that the buyer's revealed preferences significantly differ from her stated preferences. Finally, we contribute to the understanding of the role of incumbency in procurement auctions by providing evidence that what may be perceived as incumbency bias can in fact be a revelation of preference for quality.", "e:keyword": ["Supply chain management", "Service operations", "Operations strategy", "Auctions and mechanism design", "Econometric analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0486", "e:abstract": "Inpatient staffing costs are significantly affected by nurse absenteeism, which is typically high in U.S. hospitals. We use data from multiple inpatient units of two hospitals to study which factors, including unit culture, short-term workload, and shift type, explain nurse absenteeism. The analysis highlights the importance of paying attention to heterogeneous absentee rates among individual nurses. We then develop models to investigate the impact of demand and absentee rate variability on the performance of staffing plans and obtain some structural results. Utilizing these results, we propose and test three easy-to-use heuristics to identify near-optimal staffing strategies. Such strategies could be useful to hospitals that periodically reassign nurses with similar qualifications to inpatient units in order to balance workload and accommodate changes in patient flow. Although motivated by staffing of hospital inpatient units, the approach developed in this paper is also applicable to other team-based and labor-intensive service environments.", "e:keyword": ["Absenteeism", "Staffing", "Hospital operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0487", "e:abstract": "Hospital emergency departments (EDs) typically use triage systems that classify and prioritize patients almost exclusively in terms of their need for timely care. Using a combination of analytic and simulation models, we demonstrate that adding an up-front estimate of patient complexity to conventional urgency-based classification can substantially improve both patient safety (by reducing the risk of adverse events) and operational efficiency (by shortening the average length of stay). Moreover, we find that EDs with high resource (physician and/or examination room) utilization, high heterogeneity in the treatment time between simple and complex patients, and a relatively equal number of simple and complex patients benefit most from complexity-augmented triage. Finally, we find that (1) although misclassification of a complex patient as simple is slightly more harmful than vice versa, complexity-augmented triage is relatively robust to misclassification error rates as high as 25&percnt;; (2) streaming patients based on complexity information and prioritizing them based on urgency is better than doing the reverse; and (3) separating simple and complex patients via streaming facilitates the application of lean methods that can further amplify the benefit of complexity-augmented triage.", "e:keyword": ["Healthcare operations", "Emergency department", "Triage", "Priority queues", "Patient prioritization", "Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0488", "e:abstract": "We apply the concept of multimodularity in three stochastic dynamic inventory problems in which state and decision variables are economic substitutes. The first is clearance sales of perishable goods. The second is sourcing from multiple suppliers with different lead times. The third is transshipment under capacity constraints. In all three problems, we establish monotone optimal polices with bounded sensitivity. Multimodularity proves to be an effective tool for these problems because it implies substitutability, it is preserved under minimization, and it leads directly to monotone optimal policies with bounded sensitivity.", "e:keyword": ["Dynamic programming", "Multimodularity", "Substitutability and complementarity", "Stochastic inventory models"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0489", "e:abstract": "Many factors introduce the prospect of changes in the demand environment that a firm faces, with the specifics of such changes not necessarily known in advance. If and when realized, such changes affect the delicate balance between demand and supply and thus current prices should account for these future possibilities. We study the dynamic pricing problem of a retailer facing the prospect of a change in the <i>demand function</i> during a finite selling season with no inventory replenishment opportunity. In particular, the time of the change and the postchange demand function are unknown upfront, and we focus on the fundamental trade-off between collecting revenues from current demand and doing so for postchange demand, with the capacity constraint introducing the main tension. We develop a formulation that allows for isolating the role of dynamic pricing in balancing inventory consumption throughout the horizon. We establish that, in many settings, optimal pricing policies follow a monotone path up to the change in demand. We show how one may compare upfront the attractiveness of pre- and postchange demand conditions and how such a comparison depends on the problem primitives. We further analyze the impact of the model inputs on the optimal policy and its structure, ranging from the impact of model parameter changes to the impact of different representations of uncertainty about future demand.", "e:keyword": ["Revenue management", "Dynamic pricing", "Nonstationary demand", "Model uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0490", "e:abstract": "Service systems such as call centers and hospitals typically have strongly time-varying arrivals. A natural model for such an arrival process is a nonhomogeneous Poisson process (NHPP), but that should be tested by applying appropriate statistical tests to arrival data. Assuming that the NHPP has a rate that can be regarded as approximately piecewise-constant, a Kolmogorov&ndash;Smirnov (KS) statistical test of a Poisson process (PP) can be applied to test for a NHPP by combining data from separate subintervals, exploiting the classical conditional-uniform property. In this paper, we apply KS tests to banking call center and hospital emergency department arrival data and show that they are consistent with the NHPP property, but only if that data is analyzed carefully. Initial testing rejected the NHPP null hypothesis because it failed to account for three common features of arrival data: (i) data rounding, e.g., to seconds; (ii) choosing subintervals over which the rate varies too much; and (iii) overdispersion caused by combining data from fixed hours on a fixed day of the week over multiple weeks that do not have the same arrival rate. In this paper, we investigate how to address each of these three problems.", "e:keyword": ["Arrival processes", "Nonhomogeneous Poisson process", "Kolmogorov&ndash", "Smirnov statistical test", "Data rounding", "Overdispersion"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0491", "e:abstract": "Display advertising is a &dollar;25 billion business with a promising upward revenue trend. In this paper, we consider an online display advertising setting in which a web publisher posts display ads on its website and charges based on the cost-per-click pricing scheme while promising to deliver a certain number of clicks to the ads posted. The publisher is faced with uncertain demand for advertising slots and uncertain traffic to its website as well as uncertain click behavior of visitors. We formulate the problem as a novel queueing system, where the slots correspond to service channels with the service rate of each server inversely related to the number of active servers. We obtain the closed-form solution for the steady-state probabilities of the number of ads in the publisher's system. We determine the publisher's optimal price to charge per click and show that it can increase in the number of advertising slots and the number of promised clicks. We show that the common heuristic used by many web publishers to convert between the cost-per-click and cost-per-impression pricing schemes using the so-called click-through-rate can be misleading because it may incur substantial revenue loss to web publishers. We provide an alternative explanation for the phenomenon observed by several publishers that the click-through-rate tends to drop when they switch from the cost-per-click to cost-per-impression pricing scheme.", "e:keyword": ["Queueing systems", "Online advertising", "Pricing", "Markov chains", "Cost-per-click"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0492", "e:abstract": "This paper studies coordinated pricing and production decisions in an assemble-to-order system. We first show that unlike in make-to-stock systems, a state-dependent base-stock list-price policy is optimal. The optimal state-dependent base-stock levels and list prices may increase or decrease as demand backlogs increase, whereas demand backlogs always improve the optimal expected profit. Because the problem easily becomes intractable under general system settings, we next develop a simple heuristic policy. The heuristic policy decouples inventory replenishment, pricing, and component allocation decisions in a coordinated way. We provide a sufficient condition that ensures the optimality of the heuristic policy, and present a numerical study to demonstrate its performance when the condition is not met. The numerical study also shows how the performance of the heuristic policy is affected by various market and operational conditions, and by the structure of the assemble-to-order system. By focusing on the simple W-model, we show how the heuristic pricing decisions are made in response to changes in inventory levels and various cost parameters.", "e:keyword": ["Assemble-to-order system", "Dynamic pricing", "Inventory control", "Heuristic policy", "Marketing&ndash", "Operations interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0493", "e:abstract": "We study call routing policies for call centers with multiple call types and multiple agent groups. We introduce new weight-based routing policies where each pair (call type, agent group) is given a matching priority defined as an affine combination of the longest waiting time for that call type and the longest idle time or the number of idle agents in that agent group. The coefficients in this combination are parameters to be optimized. This type of policy is more flexible than traditional ones found in practice, and it performs better in many situations. We consider objective functions that account for the service levels, the abandonment ratios, and the fairness of occupancy across agent groups. We select the parameters of all considered policies via simulation-based optimization heuristics. This requires only the availability of a simulation model of the call center, which can be much more detailed and realistic than the models used elsewhere in the literature to study the optimality of certain types of routing rules. We offer a first numerical study of realistic routing rules that takes into account the complexity of real-life call centers.", "e:keyword": ["Multiskill call centers", "Contact centers", "Call routing policies", "Simulation", "Stochastic optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0494", "e:abstract": "We consider a two-echelon inventory system with a capacitated centralized production facility and several distribution centers (DCs). Both production and transportation times are stochastic with general distributions. Demand arrives at each DC according to an independent Poisson process and is backlogged if the DC is out of stock. We allow different holding and backlog costs at the different DCs. We assume that inventory at DCs is managed using the one-for-one replenishment policy. The main objective of this paper is to investigate the control of the multiechelon <i>M</i>/<i>G</i>/1 setting with general transportation times. To achieve this objective, we analyze several decentralized allocation policies including the first-come, first-served (FCFS), strict priority (SP), and multilevel rationing (MR) policies. For our analytic results, we assume no order crossing. We derive the cost function for a capacitated two-echelon inventory system with general transportation times under these policies. Our numerical examples show that the FCFS policy may outperform the MR policy, even though the latter has been shown to be better in the centralized setting. This suggests that in decentralized settings there is a need to focus on policies that prioritize customers when there is backlog. This focus is in contrast to the centralized settings, where inventory rationing policies that focus on prioritization when there is available inventory are effective. We therefore introduce and analyze the generalized multilevel rationing (GMR) priority policy. We compare the GMR policy with other policies and show that the GMR policy outperforms the three policies used in the centralized setting. We also compare the GMR policy with the myopic (T), longest queue first (LQF), and the optimal (when order crossing is allowed during the transportation time) policies. Our results show that when the uncertainty of the transportation times is low, the GMR policy outperforms the myopic (T) and LQF policies and that the gap between the optimal policy and the GMR policy is not high.", "e:keyword": ["Multiechelon inventory management", "M/G/1 queue", "Strict priority", "Multilevel rationing", "Stochastic production times", "Stochastic transportation times"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0495", "e:abstract": "In services, customers provide significant inputs into the production process. In particular, these inputs may be the customers themselves participating in the service delivery. Although many service firms have explored different ways of involving customers in their production process, there is no clear guideline for the design of such coproductive systems. In this paper, we develop an analytical model of joint production between a service provider and a customer and characterize how a service firm should design its coproductive system. We show that, as a task becomes more standard, it is desirable to decrease the degree of interaction between the provider and the customer by making their efforts more substitutable and to allocate most of the work to whoever is the most efficient. Conversely, as a task becomes less standard, it is optimal to increase interaction by making efforts more complementary and to balance the work allocation. Our analysis gives rise to a service-process framework with three archetypes of coproductive services: collaborative services, service factories, and self-services. We discuss the implications of our results for service process reengineering.", "e:keyword": ["Operations strategy", "Service operations", "Joint production", "Monotone comparative statics"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0496", "e:abstract": "Barter exchange markets are markets in which agents seek to directly trade their goods with each other. Exchanges occur in cycles or in chains in which each agent gives a good to the next agent. Kidney exchange is an important type of barter exchange market that allows incompatible patient&ndash;donor pairs to exchange kidneys so the involved patients can receive a transplant. The clearing problem is to find an allocation of donors to patients that is optimal with respect to multiple criteria. To achieve the best possible score on all criteria, long cycles and chains are often needed, particularly when there are many hard-to-match patients. In this paper we show why this may pose difficulties for existing approaches to the optimization of kidney exchanges. We then present a generic iterative branch-and-price algorithm that can deal effectively with multiple criteria, and we show how the pricing problem may be solved in polynomial time for a general class of criteria. Our algorithm is effective even for large, realistic patient&ndash;donor pools. Our approach and its effects are demonstrated by using simulations with kidney exchange data from the Netherlands and the United States.", "e:keyword": ["Kidney exchange", "Multicriteria optimization", "Math programming", "Branch-and-price", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0497", "e:abstract": "Funding for humanitarian operations in the global health sector is highly variable and unpredictable. We study the problem of managing inventory in the presence of funding constraints over a finite planning period. Our goal is to determine the optimal procurement policy given the complexities associated with funding and also to analyze the impact of funding amount, funding schedule, and uncertainty around the funding timing on operations. We use a multiperiod stochastic inventory model with financial constraints and demonstrate that despite the funding complexities, the optimal replenishment policy is a state-independent policy that can be easily implemented. We also provide analytical results and several insights based on our computational study regarding the effect of funding timing uncertainty and variability on the operating costs and fill rates. Among other results, we find that receiving funding early is beneficial in underfinanced systems while avoiding funding delays is critical in fully financed systems. Our analysis also indicates that receiving less overall funding in a timely manner might actually be better than delayed full funding.", "e:keyword": ["Inventory management", "Humanitarian operations", "Funding"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0498", "e:abstract": "Motivated by the trend toward more collaboration in work flows, we study networks where some activities require the simultaneous processing by multiple types of multitasking human resources. Collaboration imposes constraints on the capacity of the process because multitasking resources have to be simultaneously at the right place. We introduce the notions of collaboration architecture and unavoidable bottleneck idleness to study the maximal throughput or capacity of such networks. Collaboration and multitasking introduce synchronization requirements that may inflict unavoidable idleness of the bottleneck resources: even when the network is continuously busy (processing at capacity), bottleneck resources can never be fully utilized. The conventional approach that equates network capacity with bottleneck capacity is then incorrect because the network capacity is below that of the bottlenecks. In fact, the gap between the two can grow linearly with the number of collaborative activities. Our main result is that networks with nested collaboration architectures have no unavoidable bottleneck idleness. Then, regardless of the processing times of the various activities, the standard bottleneck procedure correctly identifies the network capacity. We also prove necessity in the sense that, for any nonnested architecture, there are values of processing times for which unavoidable idleness persists. The fundamental trade-off between collaboration and capacity does not disappear in multiserver networks and has important ramifications to service-system staffing. Yet, even in multiserver networks, a nested collaboration architecture still guarantees that the bottleneck capacity is achievable. Finally, simultaneous collaboration, as a process constraint, may limit the benefits of flexibility. We study the interplay of flexibility and unavoidable idleness and offer remedies derived from collaboration architectures.", "e:keyword": ["Simultaneous collaboration", "Multitasking", "Architecture", "Work-flow design", "Organizational design", "Capacity", "Stability", "Flexibility", "Control", "Priorities", "Bottlenecks"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0499", "e:abstract": "This paper studies the role of demand uncertainty in temporal discrimination when the retailer applies markdown pricing facing strategic customers. We consider a model in which a retail firm announces a pair of declining prices for two selling periods, and customers with heterogeneous valuations each decide whether to buy a unit early, later or never. In this model, if the demand function is linear and its parameters are common knowledge, there <i>never</i> exist any markdown prices that achieve temporal discrimination for any feasible model parameters. Either all buying customers wait, or all buy early. By contrast, if the demand level is unknown, there always exists a temporally discriminating markdown pricing scheme for all feasible model parameters. We derive qualitative insights to the way demand uncertainty and Bayesian updating contribute to temporal discrimination, which broadly apply to nonlinear demand functions as well. We also show that in case of demand uncertainty, there always exists a temporally discriminating pricing scheme that yields a strictly higher profit to the retailer than the optimal static pricing scheme. Ironically, however, the retailer cannot implement the optimal scheme due to the same demand uncertainty.", "e:keyword": ["Pricing and revenue management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0500", "e:abstract": "Goal achieving is a commonly observed phenomenon in practice, and it plays an important role in decision making. In this paper, we investigate the impact of a target on newsvendor decisions. We take into account the risk and model the effect of a target by maximizing the satisficing measure of a newsvendors profit with respect to that target. We study two satisficing measures: (i) conditional value at risk (CVaR) satisficing measure that evaluates the highest confidence level of CVaR achieving the target; (ii) entropic satisficing measure that assesses the smallest risk tolerance level under which the certainty equivalent for exponential utility function achieves the target. For both satisficing measures, we find that the optimal ordering quantity increases with the target level. We determine an optimal order quantity for a target-based newsvendor and characterize its properties with respect to, for example, products profit margin.", "e:keyword": ["Newsvendor", "Risk measure", "Target profit"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0501", "e:abstract": "Traditional one-price-for-all extended warranties do not differentiate customers according to their risk attitudes, usage rates, or operating environment. These warranties are priced to cover the cost of high-usage customers who have more failures and are willing to pay a risk premium for their risk aversion. That makes traditional warranties economically unattractive to low-usage customers and those who are less risk averse. These issues can be addressed by residual value warranties, which refund part of the up-front price to customers who have zero or few claims according to a predetermined refund schedule. Residual value warranties may induce strategic claim behavior, since customers may prefer to pay for small failures out of pocket rather than claim failures now and give up potential refunds later.We design and price residual value warranties to maximize expected profits, taking into account strategic claim behavior and risk attitudes. For the constant absolute risk aversion model, we characterize customers optimal claim strategy as well as the net value and support cost for residual value warranties. Surprisingly, the total support cost to the service provider, including repair costs and refunds, is lower for more risk-averse customers under the residual value warranties, whereas their willingness to pay is higher. As contingent contracts, residual value warranties can better price discriminate customers than traditional warranties. We identify conditions under which residual value warranties are strictly more profitable than traditional warranties in a homogeneous market, as well as in heterogeneous markets that differ in various dimensions, such as risk attitude, failure rate, and repair cost.", "e:keyword": ["Warranty", "Service pricing", "Risk aversion", "Strategic consumer behavior", "Market segmentation"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0502", "e:abstract": "In this paper we examine the impact of different types of advance demand information on firm profit and on the benefits of resource flexibility. Specifically, we consider a firm that must choose capacities of resources that will be used to satisfy stochastic demand for multiple products, where demands follow a multivariate normal distribution. Prior to the capacity decision, the firm receives information revealing either the total volume of demand across products or the mix of demand between products. We examine two different scenarios: a <i>dedicated resource</i> setting with product-specific resources and a <i>common resource</i> scenario with one flexible resource. For both scenarios we derive the distribution of the (possibly imperfect) volume or mix demand signal, as well as the conditional distributions of demand given the particular signal. We explore the impact of either type of information on optimal capacities and profit. We find that commonality and volume information are strategic complementsso that it is more valuable to obtain volume information in settings with a common resource. On the other hand, commonality and mix information are strategic substitutes. Moreover, we find that mix and volume information themselves are complements in systems with dedicated resources. Having either type of information is valuable in reducing uncertainty for each individual product demand, but having both of them together provides information on two different dimensions, allowing for a much greater reduction in demand uncertainty. In systems with a common resource, however, the two types of information are substitutes. Because volume information is well aligned with commonality (both focus on total demand), such information already provides much of the value that can be obtainedhaving mix information adds limited additional value.", "e:keyword": ["Advance demand information", "Capacity planning and investment", "Multiproduct system", "Flexible resources"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0503", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0504", "e:abstract": "Key to the mass adoption of electric vehicles (EVs) is the establishment of successful business models based on sound understanding of consumer behavior in adopting this new technology. In this paper, we study the impact of two major barriers to mass adoption of EVs: (i) range anxiety, the concern that the driving range of EVs may be insufficient to meet the driving needs, and (ii) resale anxiety, the concern that used values of EVs may deteriorate quickly. Using a stylized model calibrated to a data set based on the San Francisco Bay Area, we show that although both types of consumer anxieties typically harm the firms profit, they often improve consumer surplus. In addition, we show that a business model that requires consumers to lease the EV batteries (rather than purchase them) may lead to a greater level of adoption and emission savings when the level of resale anxiety is high. Further, a business model that offers EV range improvement through enhanced charging infrastructure typically yields greater adoption and consumer surplus, but lowers the firms profit, compared with one that offers enlarged batteries. Overall, we find that the combinations of battery owning/leasing with enhanced charging service, referred to as the (O, E) and (L, E) models in our paper, typically yield the best balance among the objectives of EV adoption, emission savings, profitability, and consumer surplus, when the degree of resale anxiety is low and high, respectively.", "e:keyword": ["Electric vehicles", "Consumer anxieties", "Durable goods", "Secondary market", "Emission savings"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0505", "e:abstract": "Relative to brick-and-mortar retailers, online retailers have the potential to offer more options to their customers, with respect to both inventory as well as delivery times. To do this entails the management of a distribution network with more decision options than a traditional retailer. The online retailer, not the customer, decides from where items will ship, by what shipping method, and how or whether multiple-item orders will be broken up into multiple shipments. What is the best way to fulfill each customers order to minimize average outbound shipping cost? We partner with an online retailer to examine this question. We develop a heuristic that makes fulfillment decisions by minimizing the immediate outbound shipping cost plus an estimate of future expected outbound shipping costs. These estimates are derived from the dual values of a transportation linear program (LP). In our experiments on industry data, we capture 36% of the opportunity gap assuming clairvoyance, leading to reductions in outbound shipping costs on the order of 1%. These cost savings are achieved without any deterioration in customer service levels or any increase in holding costs. The transportation LP also serves as the basis for a metric that provides information on the quality of the inventory position. Based on initial successful piloting, our industrial partner has implemented the metric as well as a version of the heuristic that it is applying to every fulfillment decision for each of its stock keeping units in North America.", "e:keyword": ["Online retailing", "Inventory management", "Dynamic allocation", "Fulfillment policies"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0506", "e:abstract": "We consider an inventory policy that expedites delivery times of open orders if the inventory level drops below a certain threshold. By expediting open orders, back orders can be reduced. Order expediting is costly, and we include various types of expediting costs in the model. We prove structural properties of the model and show how the optimal parameters of the expediting policy can be computed efficiently. The expediting policy is easy to implement and, for situations with variable expediting cost only, the structure of the policy is optimal. For situations with nonvariable expediting costs, the expediting policy that we consider is generally not optimal. The optimal policy can be computed by dynamic programming, but this approach is computationally feasible only for small-problem instances. We conduct numerical experiments that are based on data from the service division of a global equipment manufacturer to evaluate the performances of the expediting policy and the optimal policy. The results show that substantial cost savings can be achieved by order expediting and that the expediting policy realizes a great share of the cost-saving potential offered by order expediting.", "e:keyword": ["Order expediting", "Lead time flexibility", "Inventory management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0507", "e:abstract": "We consider a manager who invests in knowledge development of a product and a process design team as well as knowledge transfer between teams throughout a new product development (NPD) project. Knowledge development at a particular time (e.g., prototyping and experimentation) increases a teams level of knowledge at that time. In contrast, the recipients benefits from knowledge transfer may be lagged because of the difficulties in articulating and documenting knowledge as well as the challenges regarding its interpretation and application. Over time, as each team embeds knowledge in the NPD project, the levels of product and process performance increase, thereby increasing the net revenue earned at the product launch time. In a key contribution to the literature, analytic conditions are given that characterize the dynamic rates at which knowledge development and knowledge transfer occur throughout the project. We show that the investment in knowledge development for each team and knowledge transfer between teams may be constant, front-loaded, back-loaded, U-shaped, or the peak rate may be delayed over time. As such, we show how concurrent engineering is optimally pursued throughout the NPD project.", "e:keyword": ["Product and process development and design", "Operations managementorganizational behavior interface", "Technology management and process design"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0508", "e:abstract": "Motivated by collaboration with a global spare parts service provider, we consider a two-echelon inventory system with multiple local warehouses, a so-called support warehouse, and a central warehouse with ample capacity. In case of stock-outs, the local warehouses can receive emergency shipments from the support warehouse or the central warehouse at an extra cost. Our focus is on using information on orders in the replenishment pipeline, i.e., pipeline information, to achieve cost-efficient policies for requesting emergency shipments. We introduce a policy where the request for an emergency shipment is based on the time until an outstanding order will reach the stock point considered. The goal is to determine how long one should wait for stock in the replenishment pipeline before requesting an emergency shipment, and the cost effects of using pipeline information in this manner. The analysis utilizes results from queuing theory and provides a decomposition technique for optimizing the policy parameters that reduces the complex multiechelon problem to more manageable single-echelon problems. The performance of our policy indicates that there can be a significant benefit in using pipeline information.", "e:keyword": ["Emergency shipments", "Inventory", "Multiechelon", "Pipeline information", "Spare parts"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0509", "e:abstract": "This paper, based on my remarks at the 2013 MSOM Distinguished Fellow Award ceremony, describes my views on the interface between operations and finance and the lessons that each field can gain from considering their interactions. The key points are that, whereas operational models often avoid financial considerations such as the role of investors and other market participants, both firm and market financial activity can have a significant effect on the impact of operational decisions. Some of these considerations, such as the implications of market arbitrage, can void the relevance of operational models that ignore financial issues. The paper discusses such operational areas where financial activity has significant potential for impact on operations, as well as points where operational considerations provide new perspectives on financial decisions. In particular, I review basic concepts, provide examples, and give some empirical observations in my work about the implications of the absence of arbitrage, the differences between systematic and idiosyncratic risk, the valuation of limited production resources, and the inclusion of imperfect market assumptions.", "e:keyword": ["Arbitrage", "Systematic risk", "Production", "Inventory policy"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0510", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0511", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0512", "e:abstract": "Early infant diagnosis (EID) programs in many resource-limited settings are aimed at diagnosing infants born to HIV-infected mothers. Because of the complexity of the diagnostic technology, EID programs are often highly centralized with few laboratories testing blood samples from a large network of health facilities. This leads to long diagnostic delays and consequent failure of patients to collect results in a timely manner. Several point-of-care (POC) devices that provide rapid diagnosis within the health facilities are being developed to mitigate these drawbacks of centralized EID networks. We study the decision of which facilities should receive the POC device (the placement plan) using the EID program in Mozambique as a case study. We argue that the choice of an appropriate plan is critical to maximizing the public health impact of POC devices in the presence of tight budget constraints. To formalize this argument, we develop a detailed simulation model to evaluate the impact of a placement plan. It comprises two parts: an operational model that quantifies the impact of a POC placement plan on the diagnostic delay and a behavioral part that quantifies the impact of diagnostic delay on the likelihood of result collection by infants caregivers. We also develop an approximate version of these operational and patient behavior dynamics and embed them in an optimization model to generate candidate POC placement plans. We find that the optimization-based plan can result in up to 30% more patients collecting their results compared to rules of thumb that have practical appeal. Finally, we show that the effectiveness of POC devices is much higher than other operational improvements to the EID network such as increased laboratory capacity, reduced transportation delay, and more regularized transport.", "e:keyword": ["Point-of-care testing", "Resource-limited settings", "Pediatric HIV", "Diagnosis delay"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0513", "e:abstract": "The bullwhip effect and production smoothing appear antithetical because their empirical tests oppose one another: production variability exceeding sales variability for bullwhip, and vice versa for smoothing. But this is a false dichotomy. We distinguish between the phenomena with a new production smoothing measure, which estimates how much more variable production would be absent production volatility costs. We apply our metric to an automotive manufacturing sample comprising 162 car models and find 75% smooth production by at least 5%, despite the fact that 99% exhibit the bullwhip effect. Indeed, we estimate both a strong bullwhip (on average, production is 220% as variable as sales) and robust smoothing (on average, production would be 22% more variable without deliberate stabilization). We find firms smooth both production variability and production uncertainty. We measure production smoothing with a structural econometric production scheduling model, based on the generalized order-up-to policy.", "e:keyword": ["Production smoothing", "Bullwhip effect", "Demand signal processing", "Generalized order-up-to policy", "Martingale model of forecast evolution"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0514", "e:abstract": "This paper explores the merits of hedging stochastic input costs (i.e., reducing the risk of adverse changes in costs) in a decentralized, risk-neutral supply chain. Specifically, we consider a generalized version of the well-known selling-to-the-newsvendor model in which both the upstream and the downstream firms face stochastic input costs. The firms operations are intertwinedi.e., the downstream buyer depends on the upstream supplier for delivery and the supplier depends on the buyer for purchase. We show that if left unmanaged, the stochastic costs that reverberate through the supply chain can lead to significant financial losses. The situation could deteriorate to the point of a supply disruption if at least one of the supply chain members cannot profitably make its product. To the extent that hedging can ensure continuation in supply, hedging can have value to at least some of the members of the supply chain. We identify conditions under which the risk of the supply chain breakdown will cause the supply chain members to hedge their input costs: (i) the downstream buyers market power exceeds a critical threshold; or (ii) the upstream firm operates on a large margin, there is a high baseline demand for downstream firms final product, and the downstream firms market power is below a critical threshold. In absence of these conditions there are equilibria in which neither firm hedges. To sustain hedging in equilibrium, both firms must hedge and supply chain breakdown must be costly. The equilibrium hedging policy will (in general) be a partial hedging policy. There are also situations when firms hedge in equilibrium although hedging reduces their expected payoff.", "e:keyword": ["Risk management", "Commodity procurement", "Supply chain contracting", "Hedging"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0515", "e:abstract": "We analyze a model of rental and return process where limited inventory of a product is rented to two customer classes that differ in their return behavior and penalty costs. The rental demand is a decreasing function of time. We consider two cases: where a demand that is not met is lost and where an unmet demand returns. We show that to minimize penalty cost, the optimal allocation policy may give priority to different classes at different points in time and may decline lower-class demand for some time. Computational results show the benefit of the optimal allocation policy over a priority scheme reportedly used in practice.", "e:keyword": ["Inventory theory and control"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0516", "e:abstract": "We consider the partitioning of care types into wings from the perspective of a hospital administrator who wishes to optimize the use of a fixed number of beds that provide services for heterogeneous care types. The hospital administrator decides on the number of wings to form, the number of beds to allocate to each wing, and the set of care types to assign to each wing to maximize the total utility to the hospital. The administrator faces an inherent trade-off between forming large wings to pool demand and bed capacity, and forming specialized wings to focus on narrow ranges of care types. Specialized wings not only provide advantages from focused care but also allow the protection of beds for high-utility care types. We provide an optimization model for the wing formation decision and address the advantages of focus endogenously in our model. Using data from a large urban teaching hospital in the United States along with a national database, we report on a number of managerial insights. In particular, as the overall demand increases across all care types, wings are formed to reserve more beds for higher-utility types, which leads to higher overall hospital utility but also some disparity across types, such as increased hospital access for some and decreased access for others. Furthermore, overall bed occupancy decreases as the hospital is split into wings. However, if sufficient focus is attained, shorter lengths-of-stay associated with focused care may increase overall patient throughput. We also observe that when patients are willing to wait longer for admission, the hospital tends to form more wings. This implies that hospitals that garner longer waits can form more specialized wings and thereby benefit from focused care, whereas hospitals that cannot will tend to form fewer, if any, wings, choosing to pool demand and bed capacity.", "e:keyword": ["Hospital bed capacity management", "Care partitioning", "Focus"]}, {"@id": "http://dx.doi.org/10.1287/msom.2014.0517", "e:abstract": "Consider a buyer who would like to procure certain products for the current period and the underlying technologies so that he can become a supplier and compete with current suppliers in the future market. One potential procurement mechanism for such a buyer is to bundle the procurement project with technology acquisition. We propose a dynamic stochastic game-theoretic model that analyzes the optimal technology offer strategies of the asymmetric suppliers and highlights how the size of the current project, relative to the size of the future market, and supplier competition determine the effectiveness of the bundled procurement mechanism for the buyer. For the two-supplier case, we find that each supplier has a dominant technology offer strategy that is independent of the opponents strategy. When the relative size of the project is small, suppliers only offer obsolete technologies even if their technologies are perfect substitutes. While suppliers offer better technologies as the project size increases, their responses in technology offers are not continuous with respect to the project sizeonce the project size reaches some threshold, suppliers optimal responses jump to their best technologies. We also observe that the premium needed for technology acquisition under the bundled procurement mechanism can be negligible compared to the expected profit from the future market.", "e:keyword": ["Procurement", "Game theory", "Technology transfer", "China policy"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0518", "e:abstract": "Merchant operations involves valuing and hedging the cash flows of commodity- and energy-conversion assets as real options based on stochastic models that inevitably embed model error. In this paper we quantify how empirically calibrated model errors concerning the futures term structure affect the valuation and hedging of natural gas storage. We find that even small model errorson the order of 1%2% of the empirical futures price variancecan have a disproportionate impact on storage valuation and hedging. In particular, theoretically equivalent hedging strategies have very different sensitivities to model error, with one natural strategy exhibiting potentially catastrophic performance in the presence of small model errors. We propose effective approaches to mitigate the negative effect of futures term-structure model error on hedging, also taking into account futures contract illiquidity, and provide theoretical justification for some of these approaches. Beyond commodity storage, our analysis has relevance for other real and financial options that depend on futures term-structure dynamics, as well as for inventory, production, and capacity investment policies that rely on demand-forecast term structures.", "e:keyword": ["Model error", "Commodity and energy real options", "Natural gas storage", "Futures term structures", "Delta hedging and mean-variance hedging"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0519", "e:abstract": "We consider a periodic review joint pricing and inventory control model in which a firm faces both stochastic demand and fluctuating procurement costs. To address procurement cost fluctuation, the firm adopts a dual-sourcing strategy, under which it procures from a spot market with immediate delivery and through a forward-buying contract with postponed delivery. Our analysis offers the unique insight that a risk-neutral firm may earn higher expected profit under a more volatile procurement cost process. This is because the firm makes its pricing and sourcing decisions in response to the realized cost in each period. Moreover, we characterize how the firm should dynamically adjust its pricing and sourcing decisions in accordance to cost evolution. For example, if sourcing through the forward-buying contract is less expensive than sourcing directly from the spot market, the optimal safety stock is decreasing in the current spot market purchasing cost. However, the optimal order quantity through the forward-buying contract is, in general, not monotone in the current spot-purchasing cost. Finally, we conduct extensive numerical experiments to show that dynamic pricing and dual sourcing may be either strategic complements or substitutes in the presence of fluctuating procurement costs and uncertain demand. This is because dynamic pricing mitigates demand uncertainty risk and exploits procurement cost fluctuation, whereas dual sourcing may either intensify or dampen demand risk.", "e:keyword": ["Joint pricing and inventory management", "Fluctuating procurement cost", "Dual sourcing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0520", "e:abstract": "A retailer cannot sell more than it has in stock; therefore, its sales observations are a censored representation of the underlying demand process. When a retailer forecasts demand based on past sales observations, it requires an estimation approach that accounts for this censoring. Several authors have analyzed inventory management with demand learning in environments with censored observations, but the authors assume that inventory levels are known and hence that stockouts are observed. However, firms often do not know how many units of inventory are available to meet demand, a phenomenon known as inventory record inaccuracy. We investigate the impact of this unknown on demand estimation in an environment with censored observations. When the firm does not account for inventory uncertainty when estimating demand, we discover and characterize a systematic downward bias in demand estimation under typical assumptions on the distribution of inventory record inaccuracies. We propose and test a heuristic prescription that relies on a single error statistic and that sharply reduces this bias.", "e:keyword": ["Inventory theory and control", "Demand estimation", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0522", "e:abstract": "The development of organizational knowledge and the depreciation of knowledge within organizations are processes that invariably occur concurrently. In the quality domain, many researchers have examined how the development of organizational knowledge (organizational learning) enhances quality performance. We build on this literature and investigate how the depreciation of organizational knowledge (organizational forgetting) affects quality performance. We analyze information on 2,732 quality improvement initiatives implemented by 295 vendors of a car manufacturer and find that organizational forgetting affects quality gains obtained from both learning-by-doing (autonomous learning) and quality improvement initiatives (induced learning); more than 16% of quality gains from autonomous learning and 13% of quality gains from induced learning depreciate every year. Furthermore, the impact of organizational forgetting (i) differs across the types of quality improvement efforts (quality gains from process improvement initiatives depreciate, whereas those from quality assurance initiatives do not), and (ii) depends on where quality knowledge was embedded (depreciation is lower for knowledge embedded in technology than for knowledge embedded in organizational routines or organizational members). Our results highlight the ubiquity of organizational forgetting and suggest the need for continued attention to sustain and enhance quality performance in supply chains.", "e:keyword": ["Quality management", "Process improvement", "Quality assurance", "Design quality", "Organizational learning", "Organizational forgetting", "Vendor management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0523", "e:abstract": "We develop a model to understand and predict customers observed multichannel behavior in a customer support setting. Using individual-level data from a U.S.-based health insurance firm, we model a customers query frequency and choice of using the telephone or web channel for resolving queries as a stochastic function of her latent information stock. The information stock is a function of the customers information needs (which arise when customers file health insurance claims) and information gains (which customers obtain when they resolve their queries through the telephone and web support channels), and other factors such as seasonal effects (for instance, queries that arise at the time of annual contract renewal). We find that average information gain from a telephone call is twice as much as that from visiting the web portal; customers prefer the telephone channel for health event-related information but prefer the web portal for structured seasonal information; and customers are polarized in their propensities of using the web channel and can be broadly classified into web avoiders and web seekers. Our model provides superior in-sample and out-of-sample fit than multiple benchmark models for aggregate and individual-level customer activity and has several managerial uses, such as capacity planning.", "e:keyword": ["Multichannel customer behavior", "Customer service", "Call center", "Empirical operation management", "Probability modeling"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0524", "e:abstract": "Counterfeit goods are becoming more sophisticated, from shoes to infant milk powder to aircraft parts, creating problems for consumers, firms, and governments. By comparing two types of counterfeitersdeceptive, so infiltrating a licit (but complicit) distributor, or nondeceptive in an illicit channelwe provide insights into the impact of anticounterfeiting strategies on a brand-name company, a counterfeiter, and consumers. Our analysis highlights that the effectiveness of these strategies depends critically on whether a brand-name company faces a nondeceptive or deceptive counterfeiter. For example, by improving quality, the brand-name company can improve her expected profit against a nondeceptive counterfeiter when the counterfeiter steals an insignificant amount of brand value. However, the same strategy does not work well against the deceptive counterfeiter unless high quality facilitates the seizure of deceptive counterfeits significantly. Similarly, reducing price works well in combating the nondeceptive counterfeiter, but it could be ineffective against the deceptive counterfeiter. Moreover, the strategies that improve the profit of the brand-name company may benefit the counterfeiter inadvertently and even hurt consumer welfare. Therefore, firms and governments should carefully consider a trade-off among different objectives in implementing an anticounterfeiting strategy.", "e:keyword": ["Game theory", "Global operations management", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0525", "e:abstract": "The U.S. economy is already dominated by service and information-intensive industries in terms of both gross national product and jobs, and these trends are visible in all major world economies. These economic shifts are driven by productivity changes, which today often depend on new information and communication technologies. These changes can be thought of as service industrialization, which underlies productivity improvements. Industrialization is, in turn, closely related to the design and operation of service processes at the level of firms and sectors. Some of the implications for process economics, operations strategy, and process management are outlined, and the opportunities for research in operations and technology management related to these trends are discussed.", "e:keyword": ["Service economy", "Information economy", "Research", "Service industrialization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0526", "e:abstract": "The U.S. tax law taxes the global income of multinational firms (MNFs) at their home country tax rate. To avoid double taxation, it permits <i>tax cross-crediting</i>. Through this strategy, global firms can use excess foreign tax credits (FTCs), the portion of foreign tax payments that exceed their home country tax liabilities, generated from a subsidiary located in a high-tax country to offset the tax liabilities of their low-tax divisions. This paper studies manufacturing capacity decisions in the subsidiary of an MNF with tax cross-crediting. Casting the problem on a newsvendor model, and assuming the objective of maximizing the global firms worldwide after-tax profits, we show that the optimal capacity decision under the effects of tax cross-crediting can behave very differently from that of the traditional newsvendor model. In particular, we show that an improvement in the firms after-tax profitability (through tax cross-crediting, an increased profit margin, or a reduced tax rate) might reduce the optimal capacity and that the optimal capacity decision under certain circumstances can be made without the knowledge of the demand distribution. We also discuss the issue of motivating the division manager to use an after-tax performance measure with a managerial tax rate.", "e:keyword": ["Global supply chain management", "International tax planning", "Foreign tax credit", "Tax cross-crediting"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0527", "e:abstract": "In many markets consumers incur search costs, and firms choose a long-run pricing strategy that determines how they respond to market conditions. A pricing strategy may involve commitments to take actions that do not optimize short-term revenue given the information the firm learns about demand. For example, as already suggested in the literature, the firm could commit to a single price no matter whether demand is strong or weak. We introduce a new strategycharge a high price only if demand is indeed high, otherwise offer a discount. This strategy discounts more frequently than would maximize revenue conditional on demand. Nevertheless, the frequent discounts attract consumers. We show that (i) the discount-frequently strategy is optimal (whether capacity is adjustable or not), (ii) discount-frequently is often much better than other pricing strategies, especially if no price commitment is made, and (iii) overbuying capacity (e.g., inventory) to attract consumers (by signaling availability and the likelihood of discounts) is a poor strategy. Contrary to some recommendations in the literature to limit markdowns and to purchase ample capacity, our results provide support for a strategy that embraces frequent discounts and moderate capacity.", "e:keyword": ["Consumer behavior", "Game theory", "Pricing and revenue management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0528", "e:abstract": "In this paper we propose an incentive payment contract for stochastic projects defined by a series of stages or tasks that are outsourced to independent subcontractors. Projects defined by sequentially completed independent stages are common in new product development and other high-risk projects. Our goal is to maximize the clients expected discounted profit. Our proposed contract reflects the convex time-cost trade-off that is well known in the project scheduling literature. We show that this type of contract dominates a fixed price contract with respect to expected clients profit and schedule performance, regardless of payment timing considerations. Using a piecewise linear approximation, we show that our contract is a generalization of an incentive/disincentive contract that is frequently used in practice. We show how our contract can be used to find the optimal due date and penalties/bonuses in an incentive/disincentive contract. We compare this contract with several variations and discuss implications for both the client and subcontractors.", "e:keyword": ["Incentives and contracting", "Production planning and scheduling", "Product development and design"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0529", "e:abstract": "This study examines the use of wine futures (i.e., advance selling of wine before it is bottled) as a form of operational flexibility to mitigate quality rating risk. At the end of a harvest season, the winemaker obtains a certain number of barrels of wine that can be produced for a particular vintage. While the wine is aging in the barrel, expert reviewers taste the wine and create a <i>barrel score</i>, indicating the potential quality of the wine and offering clues as to whether, when bottled, it will be superior wine. Based on the barrel score, the wine producer determines (1) the percentage of its wine to be sold as futures and (2) the price of the wine futures. After one more year of aging, the wine is bottled, and the reviewers provide a second review of the wine and assign a <i>bottle score</i> that influences the market price of the wine. Our study makes three contributions. First, we develop an analytical model that incorporates uncertain consumer valuations of wine futures and bottled wine and the uncertain bottle rating that is assigned to the wine at the end of the production process. Our analysis provides insights into how the barrel score, consumer preference (through a conditional-value-at-risk perspective) and the winemakers preference influence the winemakers allocation and pricing decisions. Our second contribution relates to the impact of consumer heterogeneity on the optimal allocation and pricing decisions. Contrary to common belief that the winemaker may be better off when consumers are more homogeneous, our results demonstrate that the winemaker can achieve a higher level of profitability when the market is filled with consumers that are heterogeneous. Third, we test our findings using data collected from Bordeaux wineries engaging in wine futures. Our empirical analysis demonstrates that (1) barrel scores play a significant role in the two decisions regarding the quantity and price of wine futures, and (2) the wine futures market provides a sizable financial benefit to the winemakers. Our analysis yields recommendations for artisanal and boutique wineries that have limited or no experience selling wine futures.", "e:keyword": ["Wine futures", "Advance selling", "Quality uncertainty", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0532", "e:abstract": "We study a newsvendor who sells a perishable asset over repeated periods to consumers with a given consumption valuation for the product. The market size in each period is random, following a stationary distribution. Consumers are loss averse with <i>stochastic</i> reference points that represent their beliefs about possible <i>price</i> and <i>product availability</i>. Given the distribution of reference points, they choose purchase plans to maximize their expected total utility, including gain-loss utility, before visiting the store, and follow the plans in the store. In anticipation of consumers' purchase plans, in each period, before demand uncertainty resolves, the firm chooses an initial order quantity. After the uncertainty resolves, the firm chooses a contingent price depending on the demand realization, with the option of clearing inventory by charging a sale price, and otherwise, posting a full price. Over repeated periods, the interaction of the firms operational decisions about ordering and contingent pricing and the consumers' purchase actions results in a distribution of reference points, and, in equilibrium, this distribution is consistent with consumers' beliefs. Under this framework of <i>endogenized</i> reference points, we fully characterize the firms optimal inventory and contingent pricing policies. We identify conditions under which the firms expected price and profit are increasing in the consumer loss aversion level. We also show that the firm can prefer demand variability over no-demand uncertainty. We obtain a set of insights into how consumers' loss aversion affects the firms optimal operational policies that are in stark contrast to those obtained in classic newsvendor models. As examples, the optimal full price increases in the initial order quantity; and the optimal full price decreases, while the optimal sales frequency increases, in the procurement cost.", "e:keyword": ["Newsvendor", "Behavioral operations", "Loss aversion", "Contingent pricing", "Marketing promotion", "Stochastic reference points"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0533", "e:abstract": "Suppliers of complementary goods often package their items together when selling to downstream retailers. One motivation behind this behavior is to reduce double marginalization through coordinated pricing so that system efficiency is improved and individual members can also benefit. The objective of this paper is to understand how competition in supply chains would impact such joint selling partnerships among complementary suppliers. We first model competition at the supply level, which is generated from the existence of multiple partially substitutable brands (or suppliers) for a particular component. We then extend the analysis to a model that also involves retail competition caused by decentralization among retailers who assemble suppliers' components into final products and sell to customers. The analysis of a model with two complementary components, one of which has multiple brands, indicates that the supply-level competition discourages joint selling of complementary goods. That is, when competing brands become more alike (or substitutable), complementary suppliers act more independently in pricing and selling their items. However, retail competition leads to an opposite effect: Competition among retailers would actually encourage complementary suppliers to package their goods together and act jointly.", "e:keyword": ["Joint selling", "Complementary components", "Brand competition", "Retail competition"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0534", "e:abstract": "We use a detailed data set from the U.S. auto industry spanning from 2002 to 2009 and a variety of econometric methods to characterize the relationship between the availability of production mix flexibility and firms use of responsive pricing. We find that production mix flexibility is associated with reductions in observed manufacturer discounts, resulting from the increased ability to match supply and demand. Under the observed market conditions, mix flexibility accounts for substantial average savings by reducing price discounting by approximately 10% of the average industry discount. We test three supplementary hypotheses and find that the reduction in discounts for vehicles manufactured at flexible plants is (1) higher for higher demand uncertainty, (2) higher for vehicles coproduced with vehicles that belong to a different segment, and (3) lower in situations with higher local competition.", "e:keyword": ["Empirical operations management", "Flexibility", "Pricing", "Automotive industry"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0536", "e:abstract": "We study an organizations one-time capacity investment in a renewable energy-producing technology with supply intermittency and net metering compensation. The renewable technology can be coupled with conventional technologies to form a capacity portfolio that is used to meet stochastic demand for energy. The technologies have different initial investments and operating costs, and the operating costs follow different stochastic processes. We show how to reduce this problem to a single-period decision problem and how to estimate the joint distribution of the stochastic factors using historical data. Importantly, we show that data granularity for renewable yield and electricity demand at a fine level, such as hourly, matters: Without energy storage, coarse data that does not reflect the intermittency of renewable generation may lead to an overinvestment in renewable capacity. We obtain solutions that are simple to compute, intuitive, and provide managers with a framework for evaluating the trade-offs of investing in renewable and conventional technologies. We illustrate our model using two case studies: one for investing in a solar rooftop system for a bank branch and another for investing in a solar thermal system for water heating in a hotel, along with a conventional natural gas heating system.", "e:keyword": ["Supply intermittency", "Renewable energy", "Capacity investment", "Sustainability"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0537", "e:abstract": "This paper focuses on salespeople behavior in business-to-business transactions. The paper investigates how salespeople use the information provided to them to set prices; of particular interest is how salespeople use price recommendations from a decision support tool. The investigation builds reduced-form models and tests them on a data set obtained by a grocery products distributor. The analysis shows that salespeoples decisions are explained well by a two-stage decision model whereby salespeople make an initial decision on whether or not to change the price (a binary decision) and then decide on the magnitude of change (a continuous response). We find that salespeople in our data set do not blindly adopt the recommended price change generated by the pricing tool. Rather, our two-stage model allows for us to uncover a nuanced association between the recommended price and the actual price change by identifying customer-specific and salesperson-specific market factors that moderate the influence of price recommendations.", "e:keyword": ["B2B", "Pricing", "Decision making", "Sales force", "Regression", "Logit model", "Two-stage model"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0541", "e:abstract": "Most previous studies on reliable facility location design assume that disruptions at different locations are independent. In this paper, we present a model that allows disruptions to be correlated with an uncertain joint distribution, and we apply distributionally robust optimization to minimize the expected cost under the worst-case distribution with given marginal disruption probabilities. The worst-case distribution has a practical interpretation with disruption propagation, and its sparse structure allows solving the problem efficiently. Our numerical results show that ignoring disruption correlation could lead to significant loss that increases dramatically in key factors such as source disaster probability, disruption propagation effect, and service interruption penalty. On the other hand, the robust model results in very low regret, even when disruptions are independent, and starts to outperform the model assuming independence when disruptions are mildly correlated. Most of the benefit of the robust model can be captured with a very low additional cost, which makes it easy to implement. Given these advantages, we believe that the robust model can serve as a promising alternative approach for solving reliable facility location problems.", "e:keyword": ["Facility location", "Supply chain disruption", "Distributional uncertainty"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0542", "e:abstract": "We model the competition among multiple pharmacy benefit managers (PBMs) for the patronage of a client organization. Each PBM selects a list of prices to be charged to the client organization for each of the branded and generic drugs within a therapeutic class (price decision) and a formulary list that assigns branded drugs to preferred or nonpreferred tiers (formulary decision). Drug manufacturers offer rebates to PBMs for drugs on preferred tier of formularies. The individuals participating in the clients pharmacy benefit plan are the ones consuming the drugs and making purchasing decisions, whereas the client organization is paying the majority of drug cost. The choices of the individuals and the client organization are governed by different utility measures. For this complex drug distribution setting and for competing PBMs, we show the existence and uniqueness of a pure Nash equilibrium on aggregate formulary and price decisions, which represent the welfare-adjusted cost and welfare-adjusted price of each PBMs plan, respectively. We characterize each PBMs optimal formulary and equilibrium price decisions and discuss the impact of various model primitives. We apply our model to gain insights on the impact of mergers in the PBM industry.", "e:keyword": ["Pharmacy benefit manager", "Drug distribution", "Tiered-formulary", "Pricing", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0543", "e:abstract": "We consider a retailer with limited storage capacity selling <i>n</i> independent products. Each product is produced by a distinct manufacturer, who is offered a consignment contract with revenue sharing by the retailer. The retailer first sets a common revenue share for all products, and each manufacturer then determines the retail price and production quantity for his product. Under certain conditions on price elasticities and cost fractions, we find a unique optimal revenue share for all products. Surprisingly, it is optimal for the retailer not to charge any storage fee in many situations even if she is allowed to do so. Both the retailers and manufacturers profits first increase and then remain constant as the capacity increases, which implies that an optimal capacity exists. We also find that the decentralized system requires no larger storage space than the centralized system at the expense of channel profit. If products are complementary, as the degree of complementarity increases, the retailer will decrease her revenue share to encourage the manufacturers to lower their prices.", "e:keyword": ["Incentives and contracting", "Supply chain management", "Capacity planning and investment", "Game theory", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0544", "e:abstract": "We consider a retailer with limited inventory of identically priced, substitutable products. The retailer faces a market with multiple segments of customers that are heterogeneous with respect to their product preferences. Customers arrive sequentially, and the firm decides which subset of products to offer to each arriving customer depending on the customers preferences, the inventory levels, and the remaining time in the season. We show that it is optimal to limit the choice set of some customers (even when the products are in stock), reserving products with low inventory levels for future customers who may have a stronger preference for those products. In certain settings, we prove that it is optimal to follow a threshold policy under which a product is offered to a customer segment if its inventory level is higher than a threshold value. The thresholds are decreasing in time and increasing in the inventory levels of other products. We introduce two heuristics derived by approximating the future marginal expected revenue by the marginal value of a newsvendor function that captures the substitution dynamics between products. We test the impact of assortment customization using data from a fashion retailer. We find that the potential revenue impact of assortment customization can be significant, especially when customer heterogeneity is high and when the products inventory-to-demand ratios are asymmetric. Our findings suggest that assortment customization can be used as another lever for revenue maximization in addition to pricing.", "e:keyword": ["Retailing", "Dynamic programming", "Pricing and revenue management", "Personalization", "Online retailing", "Customer-centric retailing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0545", "e:abstract": "The global fight against HIV/AIDS is hindered by a lack of drugs in the developing world. When patients in these countries initiate treatment, they typically remain on it until death; thus, policy makers and physicians follow <i>nonabandonment</i> policies. However, treated patients develop resistance to treatment, so in many cases untreated patients might benefit more from the drugs. In this paper we quantify the opportunity cost associated with restricting attention to nonabandonment policies. For this, we use an approximate dynamic programming framework to bound the benefit from allowing premature treatment termination. Our results indicate that in sub-Saharan Africa, the price associated with restricting attention to nonabandonment policies lies between 4.4% and 8.1% of the total treatment benefit. We also derive superior treatment allocation policies, which shed light on the role behavior and health progression play in prioritizing treatment initiation and termination.", "e:keyword": ["HIV", "Resistance", "Nonabandonment allocation policy", "Approximate dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0546", "e:abstract": "We develop and test an integrated forecasting and stochastic programming approach to workforce management in call centers. We first demonstrate that parametric forecasts, discretized using Gaussian quadrature, can be used to drive stochastic programs whose results are stable with relatively small numbers of scenarios. We then extend our approach to include forecast updates and two-stage stochastic programs with recourse and provide a general modeling framework for which recent, related models are special cases. In our formulations, the inclusion of multiple arrival-rate scenarios allows call centers to meet long-run average quality-of-service targets, and the use of recourse actions helps them to lower long-run average costs. Experiments with two large sets of call-center data highlight the complementary nature of these elements.", "e:keyword": ["Call-center management", "Production planning and scheduling", "Service operations", "Distributional forecast updating", "Stochastic programming with recourse"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0549", "e:abstract": "We develop a qualitative analysis theory for the convex-cost dynamic multicommodity production planning problem, which can be used, without performing any computational work, to provide invaluable insight to managers when faced with the task of deciding how to respond to changes in problem environment. We first formulate the problem as a multicommodity flow problem with parameters associated with each arccommodity pair. We then reduce the problem to an equivalent single-commodity flow problem and develop a complete characterization of conformality among production, sales, and inventory activities in various instances of the problem. By combining the conformality characterizations with the monotonicity theory of Granot and Veinott [Granot F, Veinott AF Jr (1985) Substitutes, complements and ripples in network flow. <i>Math. Oper. Res.</i> 10:471497] for single-commodity problems, we study the effects of changes in problem environment on optimal production, sales, and inventory schedules in the multicommodity problem. Numerous applications are presented and analyzed.", "e:keyword": ["Convex multicommodity flows", "Qualitative analysis", "Monotonicity", "Substitutes and complements", "Dynamic production planning problem"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0550", "e:abstract": "Prominent buyers brands have been damaged because their suppliers caused major harm to workers or the environment, e.g., through a deadly factory fire or release of toxic chemicals. How can buyers motivate suppliers to exert greater care to prevent such harm? This paper characterizes a backfiring condition under which actions taken by prominent buyers (increasing auditing, publicizing negative audit reports, providing loans to suppliers) motivate a supplier to exert greater effort to pass the buyers audit by hiding information and <i>less</i> care to prevent harm. Intuitively appealing actions for a buyer (penalizing a supplier for harming workers or the environment, or for trying to deceive an auditor) may be similarly counterproductive. Contrary to conventional wisdom, squeezing a suppliers margin (by reducing the price paid to the supplier or increasing wages for workers) motivates the supplier to exert greater care to prevent harmunder the backfiring condition. Whereas the necessary and sufficient condition depends on the relative convexity of the suppliers hiding cost function, a simple sufficient condition is that the supplier is likely to successfully hide information from the auditor, in equilibrium. Anecdotal evidence suggests that the backfiring condition is prevalent or becoming increasingly so. Similar insights apply to mitigation of unauthorized subcontracting.", "e:keyword": ["Supply chain management", "Environmental operations", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0552", "e:abstract": "Which vascular access to use is considered one of the most important questions in the care of patients on hemodialysis (HD). An arteriovenous fistula (AVF) is often considered the gold standard for delivering HD due to better patient survival, higher quality of life, and fewer complications. However, AVFs have some limitations: they require surgery, it takes approximately three months to know whether the surgery was successful, and a majority of these surgeries end in failure. Conversely, another common vascular access, the central venous catheter, can be inserted via a simple procedure and used immediately after placement. In this research, we address the question of whether and when to perform AVF surgery on incident and established HD patients, with the aim of finding individualized policies that maximize a patients probability of survival and remaining quality-adjusted life expectancy. Using a continuous-time dynamic programming model and under certain data-driven assumptions, we establish structural properties of the optimal policy for each objective. We provide further insights for policy makers through our numerical experiments.", "e:keyword": ["Dynamic programming", "Medical decision making", "Optimal treatment policies", "Hemodialysis", "Vascular access"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0553", "e:abstract": "Should OM research become more relevant? I provide my own perspective on this question and some practical ideas for our community to explore. This article is based on my Manufacturing and Service Operations Management (MSOM) Distinguished Fellow inaugural lecture given at the University of Toronto on June 29, 2015.", "e:keyword": ["Operations management research", "Innovation", "Relevance", "Rigor", "Research process"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0554", "e:abstract": "An extensive body of literature argues for the benefits of planned obsolescence, the strategy of designing products with low durability to induce repeat purchases from the consumers and allow the firm to sell a larger volume. Yet, several firms avoid planned obsolescence and instead offer products with high durability. In this paper, we offer a demand-side rationale for a high-durability product design strategy: the exclusivity-seeking consumer behavior associated with conspicuous consumption. In the presence of consumers who value exclusivity, we find that firms benefit from designing products with higher durability in conjunction with a high-price, low-volume introduction strategy. A higher durability in such a context leads to greater resale value, allowing the firm to charge a higher price and lower the sales volume to achieve the product exclusivity valued by the consumers. This contrasts with the planned obsolescence strategy that capitalizes on the high sales volume achieved by setting a low new product price. We also show that offering higher durability and charging a higher price are complementary levers to respond to consumers who value exclusivity. Our analysis unearths insights regarding the effect of exclusivity-seeking behavior on a firms demand and pricing. We show that firms durability choice may explain the joint increase in price and demand for conspicuous goods.", "e:keyword": ["Durable products", "Product design", "Product obsolescence", "Exclusivity-seeking consumers", "Demand externalities"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0555", "e:abstract": "We consider the problem of selecting the number of advanced life support (ALS) and basic life support (BLS) ambulancesthe <i>vehicle mix</i>to deploy in an emergency medical service (EMS) system, given a budget constraint. ALS ambulances can treat a wider range of emergencies, whereas BLS ambulances are less expensive to operate. To this end, we develop a framework under which the performance of a system operating under a given vehicle mix can be evaluated. Because the choice of vehicle mix affects how ambulances are dispatched to incoming calls, as well as how they are deployed to base locations, we adopt an optimization-based approach. We construct two modelsone a Markov decision process, the other an integer programto study the problems of dispatching and deployment in a tiered system, respectively. In each case, the objective function value attained by an optimal decision serves as our performance measure. Numerical experiments performed with both models suggest that, under reasonable choices of inputs, a wide range of tiered systems perform comparably to all-ALS fleets.", "e:keyword": ["Ambulance dispatching", "Ambulance deployment", "Markov decision processes", "Queues", "Integer programming", "Advanced life support", "Basic life support"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0556", "e:abstract": "We present a modeling framework for facilities that provide both screening (preventive) and diagnostic (repair) services. The facility operates in a random environment that represents the condition of the population that needs screening and diagnostic services, such as the disease prevalence level. We model the environment as a partially endogenous process: the populations health can be improved by providing screening services, which reduces future demand for diagnostic services. We use event-based dynamic programming to build a framework for modeling different kinds of these facilities. This framework contains a number of service priority policies that are concerned with prioritizing screening versus diagnostic services. The main trade-off is between serving urgent diagnostic needs and providing screening services that may decrease future diagnostic needs. Under certain conditions, this trade-off reverses the famous <i>c</i> rule; i.e., the patients with lower waiting cost are given priority over the others. We define appropriate event operators and specify the properties preserved by these operators. These characterize the structure of optimal policies for all models that can be built within this framework. A numerical study on colonoscopy services illustrates how the framework can be used to gain insights on developing good screening policies.", "e:keyword": ["Healthcare management", "Public policy", "Dynamic programming", "Stochastic methods"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0557", "e:abstract": "Elective operation scheduling significantly affects the financial health of a hospital and additional metrics important to stakeholders in an operating theater (OT) environment. In this research, we develop a novel scheduling formulation that explicitly considers the uncertainty in elective operation durations and also plans for potential randomly arriving urgent demands. Using a scenario-based modeling approach, the objective of this formulation is to maximize the expected profit associated with the OT schedule. Since the complexity of the problem is NP-hard, we develop a two-step, heuristic solution approach that allows us to solve practical-sized instances in reasonable time. Experimentation shows that incorporating uncertainty via scenarios increases profit and OT utilization when compared to deterministic scheduling methods. Moreover, explicitly considering urgent arrivals results in a significant reduction in the time that patients of this type wait to receive service, with little impact on other key metrics.", "e:keyword": ["Healthcare management", "Math programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0558", "e:abstract": "This paper investigates a practical batching decision problem that arises in the batch annealing operations in the cold rolling stage of steel production faced by most large iron and steel companies in the world. The problem is to select steel coils from a set of waiting coils to form batches to be annealed in available batch annealing furnaces and choose a median coil for each furnace. The objective is to maximize the total reward of the selected coils less the total coilcoil and coilfurnace mismatching cost. For a special case of the problem that arises frequently in practical settings where the coils are all similar and there is only one type of furnace available, we develop a polynomial-time dynamic programming algorithm to obtain an optimal solution. For the general case of the problem, which is strongly NP-hard, an exact branch-and-price-and-cut solution algorithm is developed using a column and row generation framework. A variable reduction strategy is also proposed to accelerate the algorithm. The algorithm is capable of solving medium-size instances to optimality within a reasonable computation time. In addition, a tabu search heuristic is proposed for solving larger instances. Three simple search neighborhoods, as well as a sophisticated variable-depth neighborhood, are developed. This heuristic can generate near-optimal solutions for large instances within a short computation time. Using both randomly generated and real-world production data sets, we show that our algorithms are superior to the typical rule-based planning approach used by many steel plants. A decision support system that embeds our algorithms was developed and implemented at Baosteel to replace their rule-based planning method. The use of the system brings significant benefits to Baosteel, including an annual net profit increase of at least 1.76 million U.S. dollars and a large reduction of standard coal consumption and carbon dioxide emissions.", "e:keyword": ["Steel production", "Batch annealing", "Batching decisions", "Integer programming", "Dynamic programming", "Branch-and-price-and-cut", "Tabu search"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0559", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0562", "e:abstract": "We study a supply chain with manufacturer encroachment in which product quality is endogenous and customers have heterogeneous preferences for quality. It is known that, when quality is exogenous, encroachment could make the retailer better off. Yet, when quality is endogenous and the manufacturer has enough flexibility in adjusting quality, we find that encroachment always makes the retailer worse off in a large variety of scenarios. We also establish that, while a higher manufacturers cost of quality hurts the retailer in absence of encroachment, it could benefit the retailer with encroachment. In addition, we show that a manufacturer offering differentiated products through two channels prefers to sell its high-quality product through the direct channel. Contrary to conventional wisdom, quality differentiation does not always benefit either manufacturer or retailer. Our results may explain why, despite extant theoretical predictions, retailers almost always resent encroachment. These findings also suggest that firms must be cautious when adopting quality differentiation as a strategy to ease channel conflict caused by encroachment.", "e:keyword": ["Encroachment", "Quality differentiation", "Dual-channel supply chain"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0567", "e:abstract": "This paper discusses the origins of the supply chain contracting literature as well as its current state. It argues that its origins go back further than many scholars realize. It also makes the case that the field expanded rapidly, creating a bubble that has largely burst. However, there have been lasting implications that affect both how we teach supply chain management and the kind of research that is done under the heading of operations management.", "e:keyword": ["Supply chain management", "Incentives and contracting"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0568", "e:abstract": "For many entrepreneurs, time is a key constraint. They need to invest time to achieve growth, but also lose time because of recurring crises. We develop a simple stochastic dynamic program to model how an entrepreneur should prioritize between improving processes to reduce crises versus harvesting revenue or ensuring future growth. We show that it is initially optimal to prioritize process improvement: an entrepreneur should strive for high process quality early in the ventures growth process. We numerically analyze a simple heuristic derived from this optimal policy and identify the conditions under which it is (or is not) effective. It performs near optimally except when process quality or revenue rate may deteriorate too fast or when the cost of process improvement or revenue enhancement is too high. Our work provides a theoretical foundation for the advice found in the popular entrepreneurship and time management literature to invest time now to save time later.", "e:keyword": ["Entrepreneurship", "Process improvement", "Time allocation", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0570", "e:abstract": "Online shoppers value lenient return policies as they are unable to assess whether products match their expectations. We study how consumers discount-seeking purchase deferrals affect online retailers return policy choices. Lenient returns may induce higher full-price sales by limiting consumer regret, while signaling clearance unavailability risk. Contrasting earlier research that concluded a monopolist must set the refund at the clearance price when strategic consumers were overlooked, we find that an optimal refund bounded by the clearance price can mitigate purchase deferrals only when a monopolist salvages at mild discounts. To explore conditions under which a monopolist permits full-refund returns, we consider three scenarios: we permit clearing inventory without a loss; we assume lenient returns stimulate aggregate demand; we consider consumers transaction costs. We also derive a unique rational expectations equilibrium for competing retailers, wherein each retailers equilibrium refund is nondecreasing in its clearance price. Furthermore, retailers with clearance prices below a threshold should not allow returns, and those who do, higher clearance revenues imply higher full prices, higher quantities, and higher profits. We conclude that a credible clearance partner salvaging at higher prices than those at competing clearance mediums helps retailers gain competitive advantage when selling to strategic consumers.", "e:keyword": ["OM-marketing interface", "Retailing", "Pricing and revenue management", "Returns", "Strategic purchase deferrals", "Rational expectations"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0571", "e:abstract": "This paper examines the differences in the behaviors of high (HIT) and low inventory turnover (LIT) retailers in responding to demand shocks. We identify quantity and price responsiveness as two mediating mechanisms that distinguish how high and low inventory turnover retailers manage demand shocks. Using quarterly firm-level data of 183 U.S. retailers between 1985 and 2012, we find that HIT retailers are able to respond quickly by changing their purchase quantities in response to demand shocks, whereas LIT retailers primarily rely on price changes to manage demand shocks. In addition, we examine the differential implications of these mechanisms on the financial performance of HIT and LIT retailers. We find price responsiveness to be a less effective strategy, compared to quantity responsiveness, in reducing excesses and shortages of inventory. Finally, the negative financial impact of a given amount of excess and shortage of inventory is eight times more severe for LIT retailers compared to HIT retailers.", "e:keyword": ["Inventory turnover", "Retailing", "Demand shock", "Supply chains", "Financial performance", "Econometric analyses"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0572", "e:abstract": "Co-production (simultaneous production of multiple outputs) occurs in some emission-intensive basic material and agricultural industries. This paper is motivated by ones in which a supplier sells its primary product to a buyer that incurs an emissions cost (voluntarily, or due to government-imposed climate policy) and sells co-products into markets without emissions costs. Emission-accounting standards provide three candidate rules for allocating the suppliers emissions among its products. This paper shows that under the value-based allocation, imposing an emissions tax on the primary product can increase emissions, by motivating the supplier to lower the price and sell a larger quantity. In contrast, with the socially optimal choice of allocation rule characterized in this paper, imposing the emissions tax on the primary product can greatly reduce emissions and increase welfare. In the absence of climate policy, under value-based allocation, a buyer might achieve greater profit by paying to offset its supply chain emissions. That can motivate supplier innovation to reduce its production cost. In numerical examples, considering the rare earth cerium oxide (co-produced with iron) and palm oil (co-produced with palm meal), the choice of allocation rule has a large impact on emissions, a buyers profit, and social welfare.", "e:keyword": ["Greenhouse gas emissions", "Allocation", "Supply chain", "Voluntary offsetting", "Border adjustment"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0573", "e:abstract": "The development of predictive models in healthcare settings has been growing; one such area is the prediction of patient arrivals to the emergency department (ED). The general premise behind these works is that such models may be used to help manage an ED that consistently faces high congestion. In this work, we propose a class of <i>proactive</i> policies that utilize future information of potential patient arrivals to effectively manage admissions into an ED while reducing waiting times for patients who are eventually treated. Instead of the standard strategy of waiting for queues to build before diverting patients, the proposed policy utilizes the predictions to identify when congestion is going to increase and proactively diverts patients before things get too bad. We demonstrate that the proposed policy provides delay improvements over standard policies used in practice. We also consider the impact of errors in the information provided by the predictive models and find that even with noisy predictions, our proposed policies can still outperform (achieving shorter delays while serving the same number of patients) standard diversion policies. If the quality of the predictive model is insufficient, then it is better to ignore the future information and simply rely on real-time, current information for the basis of decision making. Using simulation, we find that our proposed policy can reduce delays by up to 15%.", "e:keyword": ["Healthcare", "Queueing", "Emergency departments", "Predictive models"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0574", "e:abstract": "Although influenza vaccine shortage is often attributed to low supply, it has been observed that even with abundant supply, a major shortage can still occur because of late delivery. In this paper, motivated by the influenza vaccine industry, we study a supply chain contracting problem in the presence of uncertainties surrounding design, delivery, and demand of the influenza vaccine. In this supply chain, a manufacturer has insufficient incentive to initiate at-risk early production prior to the design freeze because it is a retailer who reaps the most benefits from selling more vaccines delivered on time. Anticipating that late delivery will lead to potential loss in demand, the retailer tends to reduce the order size, which further discourages the manufacturer from making an effort to improve its delivery performance. To break this negative feedback loop, a supply contract needs to achieve two objectives: incentivize at-risk early production and eliminate double marginalization. We find that two commonly observed supply contracts in practice, the delivery-time-dependent quantity flexibility (D-QF) contract and the late-rebate (LR) contract, may fail to coordinate the supply chain because of the tension between these two objectives. To resolve such a tension, we construct a buyback-and-late-rebate (BLR) contract and show that a properly designed BLR contract can not only coordinate the supply chain but also can provide full flexibility of profit division between members of the supply chain. Numerical experiments further demonstrate that the BLR contract significantly improves supply chain efficiency compared to the contracts used in the industry.", "e:keyword": ["Influenza vaccine", "Supply contract", "On-time delivery", "Coordination"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0575", "e:abstract": "We investigate information flow in two-tier supply chains, where retailers order from suppliers and sell in a market with uncertain demand. The retailers each have access to a demand signal and can exchange signals (horizontal information sharing). The suppliers can offer the retailers differential payments to gain access to their signals (vertical information acquisition). We demonstrate that retailer competition is a necessary condition to sustain information flow, whereas supplier competition precludes vertical information acquisition. Facing horizontal competition, the retailers can have an incentive to exchange signals if competition is less intense; and this incentive is stronger when they order from independent suppliers than when they order from a monopolist supplier. In the setting where two retailers order from a monopolist supplier, once the retailers exchange signals, the supplier will acquire signals from them both; otherwise, it will have an incentive to acquire signals if the signals are sufficiently correlated. It can be incentive compatible for horizontal information sharing and vertical information acquisition to coexist so that the retailers signals are available to all parties. Under this circumstance, the supplier will profit from information flow and the retailers can earn profit gains as well, whereas the consumers will be worse off.", "e:keyword": ["Information acquisition", "Information sharing", "Nash equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0576", "e:abstract": "Motivated by innovative rental business models, we study a rental system with random loss of inventory due to use. We utilize a discrete-time model in which the inventory level is chosen before the start of a finite rental season, and customers not immediately served are lost. Demand, rental durations, and rental unit lifetimes are stochastic; sample path coupling allows us to derive structural results that hold under limited distributional assumptions. Considering different recirculation rules (i.e., which unit to select to meet a demand), we prove the concavity of the expected profit function and identify the optimal recirculation rule under two different models of a rental units state: the number of times rented out or its condition. We develop two upper bounds on the number of lost rental units and two heuristics for the inventory decision. Numerical study shows the following: (1) accounting for rental unit loss can increase the expected profit by 7% for a single season; (2) the optimal inventory level in response to increasing loss probability is nonmonotonic; (3) both heuristics perform well; and (4) choosing the optimal recirculation rule over a commonly used policy can increase the profit-maximizing service level by up to six percentage points.", "e:keyword": ["Service operations", "Capacity planning and investment", "Inventory theory and control", "Supply chain management", "Stochastic methods"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0577", "e:abstract": "We generalize the guaranteed-service (GS) model for safety-stock placement in supply chains to include capacity constraints. We first examine the guaranteed-service model for a capacitated single-stage system with bounded demand. We characterize the optimal inventory policy, which depends on the entire demand history. Due to this complexity, we develop a heuristic, namely a constant base-stock policy with censored ordering. This is an order-up-to policy but with its replenishment orders censored by the capacity constraint. We refer to this heuristic as the modified constant base-stock policy (MCBS). We use a numerical experiment to compare the performance of the heuristic policy to the optimal policy. We find that the performance of the heuristic relative to the optimal policy improves with a tighter capacity constraint. We also observe that the performance of the heuristic itself can sometimes be improved by tightening the capacity constraint. We then use the results for a single-stage system to model a multistage serial system that operates with a constant base-stock policy with censored ordering, i.e., an MCBS policy. We show how to adapt the existing dynamic programming algorithm for the unconstrained case to solve for the safety-stock locations and base-stock levels in a capacitated serial system. We describe how to extend this model to other supply chain topologies. We report on numerical tests for serial systems and find that the best MCBS policy in a capacitated system can outperform the best constant base-stock policy in an identical but uncapacitated system.", "e:keyword": ["Guaranteed service", "Multiechelon inventory system", "Safety-stock optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0531", "e:abstract": "Store liquidation is the time-constrained divestment of retail outlets through an in-store sale of inventory. The retail industry depends extensively on store liquidation, both to allow managers of going concerns to divest stores in efforts to enhance performance and as a means for investors to recover capital from failed ventures. Retailers sell billions of dollars of inventory annually during store liquidations. This paper introduces the store liquidation problem to the literature and presents a technique for optimizing key store liquidation decisions, including markdowns, inventory transfers, and the timing of store closings. We propose a heuristic for solving the store liquidation problem and evaluate the performance of this method. Through applications, we show that our approach could improve net recovery on cost (i.e., the profit obtained during a liquidation stated as a percentage of the cost value of liquidated inventory) by two to five percentage points in the cases we examined. Further, we discuss ways in which current practice in store liquidation differs from the decisions identified by our method, and we trace the consequences of these differences.", "e:keyword": ["Retailing", "Pricing and revenue management", "Inventory theory and control", "OM practice", "OMfinance interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0547", "e:abstract": "Do service provider efforts to educate customers influence customer outcomes? We analyze the outcome of a field experiment executed by a major public cloud infrastructure services provider in 2011. Out of 2,673 customers who adopted the service during the experiment, 366 received a service intervention: an engagement through which the provider offered initial guidance on how to use basic features of the service. Before execution, it was unclear if this proactive customer education would have positive or negative effects on customer retention and demand for technology support. We show the treatment reduces by half the number of customers who churn from the service during the first week. Further, treated customers ask 19.55% fewer questions during the first week of their tenure than the controls. Although the treatments effects decay within one week, we show that such proactive customer education can have significant economic benefits for the provider. In particular, we find that treated customers increase their accumulated usage of the service by 46.57% in the eight months after sign-up. Finally, we provide evidence that the effects of the treatment are strongest among customers who have less experience with the provider.", "e:keyword": ["Field experiment", "Proactive service", "Service coproduction", "Customer retention", "Technology support", "Cloud computing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0548", "e:abstract": "A new class of online services allows Internet media sites to direct users from articles they are currently reading to other content they may be interested in. This process creates a browsing path along which there is potential for repeated interaction between the user and the provider, giving rise to a dynamic optimization problem. A key metric that often underlies this recommendation process is the click-through rate (CTR) of candidate articles. Whereas CTR is a measure of instantaneous click likelihood, we analyze the performance improvement that one may achieve by some lookahead that accounts for the potential future path of users. To that end, by using some data of user path history at major media sites, we introduce and derive a representation of content along two key dimensions: clickability, the likelihood to click to an article when it is recommended; and engageability, the likelihood to click from an article when it hosts a recommendation. We then propose a class of heuristics that leverage both clickability and engageability, and provide theoretical support for favoring such path-focused heuristics over myopic heuristics that focus only on clickability (no lookahead). We conduct a live pilot experiment that measures the performance of a practical proxy of our proposed class, when integrated into the operating system of a worldwide leading provider of content recommendations, allowing us to estimate the aggregate improvement in clicks per visit relative to the CTR-driven current practice. The documented improvement highlights the importance and the practicality of efficiently incorporating the future path of users in real time.", "e:keyword": ["Online services", "Dynamic assortment selection", "Data-driven optimization", "Recommendation systems", "Content marketing", "Digital marketing", "Path data"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0551", "e:abstract": "We demonstrate the value of utility-based choice models to estimate demand and plan inventory for new and used textbooks in the presence of consumer choice and stockout-based substitution at a university textbook retailer. Demand information is censored, the exact time of stockout is not observed, and the short selling season often does not allow for replenishment. Using data for 26,749 book titles from 2007 to 2011 and a simulation experiment calibrated on real data, we show that an attribute-based choice model generates accurate demand estimates (mean absolute percentage error less than 1%) even when nearly 90% of the textbooks in the fit sample experience stockout. This performance is driven by the heterogeneity of product attributes and is robust to the occurrence of product returns. We implement this model at the bookstore in a controlled field experiment and obtain over 10% increase in profit. The results show that accounting for asymmetric and stockout-based substitution in demand estimation and inventory planning enables us to make systematic corrections in inventory mix and inventory level compared to the existing process.", "e:keyword": ["OM practice", "Retailing", "Inventory theory and control", "Demand estimation", "Stockout-based substitution"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0560", "e:abstract": "This paper proposes the Q-Lasso method for wait time prediction, which combines statistical learning with fluid model estimators. In historical data from four remarkably different hospitals, Q-Lasso predicts the emergency department (ED) wait time for low-acuity patients with greater accuracy than rolling average methods (currently used by hospitals), fluid model estimators (from the service operations management literature), and quantile regression methods (from the emergency medicine literature). Q-Lasso achieves greater accuracy largely by correcting errors of underestimation in which a patient waits for longer than predicted. Implemented on the external website and in the triage room of the San Mateo Medical Center (SMMC), Q-Lasso achieves over 30% lower mean squared prediction error than would occur with the best rolling average method. The paper describes challenges and insights from the implementation at SMMC.", "e:keyword": ["OM practice", "Service operations", "Empirical research", "Healthcare management"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0561", "e:abstract": "We present our work with an online retailer, Rue La La, as an example of how a retailer can use its wealth of data to optimize pricing decisions on a daily basis. Rue La La is in the online fashion sample sales industry, where they offer extremely limited-time discounts on designer apparel and accessories. One of the retailers main challenges is pricing and predicting demand for products that it has never sold before, which account for the majority of sales and revenue. To tackle this challenge, we use machine learning techniques to estimate historical lost sales and predict future demand of new products. The nonparametric structure of our demand prediction model, along with the dependence of a products demand on the price of competing products, pose new challenges on translating the demand forecasts into a pricing policy. We develop an algorithm to efficiently solve the subsequent multiproduct price optimization that incorporates reference price effects, and we create and implement this algorithm into a pricing decision support tool for Rue La Las daily use. We conduct a field experiment and find that sales does not decrease because of implementing tool recommended price increases for medium and high price point products. Finally, we estimate an increase in revenue of the test group by approximately 9.7% with an associated 90% confidence interval of [2.3%, 17.8%].", "e:keyword": ["Online retailing", "Flash sales", "Initial pricing", "Revenue management", "Price optimization", "Machine learning", "Regression trees", "Demand forecasting", "Demand interdependency", "Model implementation"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0563", "e:abstract": "This paper studies the pricing strategies for personalized product bundles. In such problems, a seller provides a variety of products for which customers can construct a personalized bundle and send a request for quote (RFQ) to the seller. The seller, after reviewing the RFQ, has to determine a price based on which the customer either purchases the whole bundle or nothing. Such problems are faced by many companies in practice, and they are very difficult because of the potential unlimited possible configurations of the bundle and the correlations among the individual products. In this paper, we propose a novel top-down and bottom-up approach to solve this problem. In the top-down step, we decompose the bundle into each component and calibrate a value score for each component. In the bottom-up step, we aggregate the components back to the bundle, define important features of the bundle, and segment different RFQs by those bundle features as well as customer attributes. Then we estimate a utility function for each segment based on historical sales data and derive an optimal price for each incoming RFQ. We show that such a model overcomes the aforementioned difficulties and can be implemented efficiently. We test our approach using empirical data from a major information technology service provider and the test result shows that the proposed approach can improve the effectiveness of pricing significantly.", "e:keyword": ["Pricing", "Personalized bundle", "Utility model", "Data analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0564", "e:abstract": "Spine surgeries tend to be lengthy (mean time of 4 hours) and highly variable (with some surgeries lasting 18 hours or more). This variability along with patient preferences driving scheduling decisions resulted in both low operating room (OR) utilization and significant overtime for surgical teams at Mayo Clinic. In this paper we discuss the development of an improved scheduling approach for spine surgeries over a rolling planning horizon. First, data mining and statistical analysis was performed using a large data set to identify categories of surgeries that could be grouped together based on surgical time distributions and could be categorized at the time of case scheduling. These surgical categories are then used in a hierarchical optimization approach with the objective of maximizing a weighted combination of OR utilization and net profit. The optimization model is explored to consider trade-offs and relationships among utilization levels, financial performance, overtime allowance, and case mix. The new scheduling approach was implemented via a custom Web-based application that allowed the surgeons and schedulers to interactively identify best surgical days with patients. A pilot implementation resulted in a utilization increase of 19% and a reduction in overtime by 10%.", "e:keyword": ["Operating room scheduling", "Surgery scheduling", "Mixed-integer program"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0565", "e:abstract": "We study an assembly system with a number of parallel multistage processes feeding a multistage final assembly process. Each stage has a stochastic throughput time. We assume that the system is controlled by planned leadtimes at each stage. From these planned leadtimes the start and due times of all stages can be derived. If a job finishes at a particular stage and has to wait before the start of the next job(s), a holding cost proportional to the waiting time is incurred. A penalty cost proportional to the lateness is incurred when the last stage of the final assembly process finishes after its due time. The objective is to determine planned leadtimes for each individual stage, such that the expected cost of a customer order is minimized.We derive the recursive equations for the tardiness and earliness at all stages and an exact expression for the expected cost. We discuss the similarity between these expressions and those for serial inventory systems. Based on this observation and a conjecture related to the generalized Newsvendor equations, we develop an iterative heuristic procedure. Comparison with a numerical optimization method confirms the accuracy of the heuristic. Finally, we discuss an application of the model to a real-life case, showing the added value of a system-wide optimization of planned leadtimes compared to current practice.", "e:keyword": ["Planned leadtimes", "Customer-order-driven", "Assembly systems"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0566", "e:abstract": "Practice-based researchresearch performed with the intent of improving the operation of a collaborating practitioneris an important endeavor for our field: such work may reveal new problems, interesting phenomena, and may also generate data, educational material, and solutions to important practical problems. We argue that the practical relevance of any operations management (OM) research is driven by the two dimensions of generalizability and validity, which together offer a framework for contrasting the potential strengths and weaknesses of theory-based and practice-based research. We review challenges and strategies for successfully engaging in practice-based research, including: choosing a good problem; establishing and managing a relationship with a practitioner; validation; and impact estimation. Finally, we discuss possible ways to encourage more practice-based research in OM. In particular, we argue that our field should, in general, put more emphasis on research validity.", "e:keyword": ["OM practice", "Empirical research", "Experiments"]}, {"@id": "http://dx.doi.org/10.1287/msom.2015.0569", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0581", "e:abstract": "We study a new dynamic order acceptance and assignment problem of a make-to-order manufacturing or service system that produces heterogeneous products or services using heterogeneous servers. Unlike traditional dynamic order acceptance and assignment problems, in our settings the system can strategically postpone acceptance and assignment decisions for orders in hand. The system does this while waiting for more profitable orders to come and/or more productive servers to become available, with the risk of losing orders in hand, low server utilization, and high waiting penalties. We formulate this problem as a stochastic dynamic program, characterize the structure of the optimal policy, and discuss managerial insights gained from the optimal policy. The paper also provides a methodological contribution by finding general structural properties and their preservation conditions, which can be reused in related future operations management studies.", "e:keyword": ["Postponable acceptance and assignment", "Structure of the optimal policy", "Stochastic dynamic program"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0582", "e:abstract": "To set inventory service levels, suppliers must understand how changes in inventory service level affect demand. We build on prior research, which uses analytical models and laboratory experiments to study the impact of a suppliers service level on demand from retailers, by testing this relationship in the field. We analyze a field experiment at the supplier Hugo Boss to determine how the suppliers inventory service level affects demand from its retailer customers. We find increases in historical fill rate to be associated with statistically significant and managerially substantial increases in current retailer orders (i.e., demand, not just sales). Specifically, a one percentage point increase in fill rate, measured over the prior year, is associated with a statistically significant 11% increase in current retailer demand, controlling for other factors that might affect retailer demand. We explore the drivers of this demand increase, including changes in retailer assortment and order frequency. We discuss features of a retail buyers decision context identified through our field work that may explain the magnitude of the relationship we observe.", "e:keyword": ["Supply chain management", "Retailing", "Empirical research", "Inventory theory and control"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0583", "e:abstract": "Globalization, innovation, social media, and exposure to natural and man-made disasters have increased organizations need to cope with demand surges: random, significant increases in demand in an otherwise relatively stable demand environment. To build supply chain capabilities, organizations face a choice between two fundamentally different sourcing strategiesreactive capacity and safety stock. We develop a framework to guide the joint sourcing strategy that minimizes the long-run average cost under a target service level. A salient feature of our modeling framework is a novel demand model that captures important continuous-time, non-Markovian characteristics of demand surge trajectories. In addition to the total magnitude as typically modeled in the literature, we define several other metrics of surges, including duration, intensity, compactness, peak position, volatility, and frequency. The resulting optimization problem is challenging because it requires evaluating, for any sample path, whether surge demand can be satisfied at every point in timea refined feature that traditional models do not have. To identify the optimal strategy, we first characterize the optimal production and deployment policy for any given strategy and then transform the original problem into two equivalent but more tractable problems. Finally, through stochastic comparison techniques, we show how the magnitude and predictability of surge demand characteristics (mentioned above) and the cost profiles of each strategy impact the optimal joint strategy.", "e:keyword": ["Supply chain risk management", "Inventory planning", "Demand surges", "Markov modulated demand", "Reactive capacity", "Safety stock", "Stochastic comparison"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0584", "e:abstract": "The high cost of lenient return policies force consumer electronics original equipment manufacturers (OEMs) to look for ways to recover value from lightly used consumer returns, which constitute a substantial fraction of sales and cannot be resold as new products. Refurbishing to remarket or to fulfill warranty claims are the two common disposition options considered to unlock the value in consumer returns, which present the OEM with a challenging problem: How should an OEM dynamically allocate consumer returns between fulfilling warranty claims and remarketing refurbished products over the products life cycle? We analyze this dynamic allocation problem and find that when warranty claims and consumer returns are jointly taken into account, the remarketing option is generally dominated by the option of refurbishing and earmarking consumer returns to fulfill warranty claims. Over the products life cycle, the OEM should strategically emphasize earmarking of consumer returns at the early stages of the life cycle to build up earmarked inventory for the future warranty demand, whereas it should consider remarketing at the later stages of the life cycle after enough earmarked inventory is accumulated or most of the warranty demand uncertainty is resolved. These findings show that, for product categories with significant warranty coverage and refund costs, remarketing may not be the most profitable disposition option even if the product has strong remarketing potential and the OEM has the pricing leverage to tap into this market. We also show that the optimal dynamic disposition policy is a price-dependent base-stock policy where the earmarked quantity is capacitated by the new and refurbished product sales quantities. We compare with the myopic policy and show that it is a good heuristic for the optimal dynamic disposition policy.", "e:keyword": ["Consumer returns", "Consumer electronics", "Warranty", "Refurbishing", "Closed-loop supply chains"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0585", "e:abstract": "We use a unique empirical setting to investigate the spillover of quality knowledge across supply chains and to the examine contingencies that affect such spillover. We analyze the quality performance of 191 suppliers, who utilize the same facilities to manufacture similar products for two distinct businesses: one that makes cars and the other that makes commercial vehicles. From 2006 to 2009, the car business undertook 2,121 quality improvement initiatives at these suppliers, while the commercial vehicles business did not undertake any such initiatives. We find that the quality knowledge developed through the quality improvement initiatives undertaken by the car business does not easily spill over to benefit the commercial vehicles business. Quality knowledge spills over under three conditions: (1) when quality improvement efforts are focused on organizational members, as opposed to when they focus on routines or technology; (2) when quality improvement efforts focus on the output activities of suppliers, not when they focus on the input or in-process activities; and (3) when quality knowledge is developed at suppliers with low complexity in their operations. Our results provide insights on managing quality at shared suppliers.", "e:keyword": ["Quality knowledge", "Shared suppliers", "Organizational learning", "Knowledge spillover"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0586", "e:abstract": "We model how a judge schedules cases as a multiarmed bandit problem. The model indicates that a first-in-first-out (FIFO) scheduling policy is optimal when the case completion hazard rate function is monotonic. But there are two ways to implement FIFO in this context: at the hearing level or at the case level. Our model indicates that the former policy, prioritizing the oldest hearing, is optimal when the case completion hazard rate function decreases, and the latter policy, prioritizing the oldest case, is optimal when the case completion hazard rate function increases. This result convinced six judges of the Roman Labor Court of Appealsa court that exhibits increasing hazard ratesto switch from hearing-level FIFO to case-level FIFO. Tracking these judges for eight years, we estimate that our intervention decreased the average case duration by 12% and the probability of a decision being appealed to the Italian supreme court by 3.8%, relative to a 44-judge control sample.", "e:keyword": ["Multitasking", "Multiarmed bandits", "Field experiment", "Production scheduling", "Italian judiciary"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0587", "e:abstract": "For many early-stage entrepreneurs, hiring the first employee is a critical step in the firms growth. Doing so often requires significant time and monetary investments. To understand the trade-offs involved in deciding when to hire the first employee and how hiring differs in entrepreneurial settings from more established firm settings, we present a simple growth model that depends on two critical inputs for revenue generation: the entrepreneurs time and money. We show that without hiring, the entrepreneurs time eventually becomes more valuable than money in contributing to the firms growth. In that context, the value of the employee is driven by how much relief he provides to the entrepreneur. We characterize the optimal timing of hiring in terms of the firms cash position and how the firm is affected if it requires an upfront fixed investment in time and/or money. We find that the upfront investment in time needed for hiring cannot be converted to an equivalent upfront investment in money and that mistiming hiring can be very costly, especially when these upfront investments are high.", "e:keyword": ["Entrepreneurial operations", "Hiring", "Optimal stopping problem"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0588", "e:abstract": "We study a periodically reviewed multiechelon serial inventory system with a capacity constraint on the order quantity at each stage. The cost criterion we use to evaluate inventory policies for this system is the sum of the expected long-run average holding and shortage costs. It is well known that for this problem, characterizing the structure of the optimal policy and computing it are very difficult. We consider the use of echelon base-stock policies for our system (even though they are known to be suboptimal) and propose algorithms for finding base-stock levels that are easy to understand and implement. We derive bounds on the ratios between the costs achieved by our algorithms and the optimal costs (over all policies). For light-tailed demand distributions, our algorithms are shown to be asymptotically optimal in the sense that our bounds are close to one in high service-level environments. Our computational investigations reveal that our algorithms perform well even under modest service levels.", "e:keyword": ["Multiechelon inventory", "Policies", "Capacity"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0589", "e:abstract": "When does increased service quality competition lead to customer defection? And, which customers are most likely to defect? Our empirical analysis of 82,235 customers exploits the varying competitive dynamics in 644 geographically isolated markets in which a nationwide retail bank conducted business over a five-year period. We find that customers defect at a higher rate from the incumbent following increased service quality (price) competition only when the incumbent offers high (low) quality service relative to existing competitors in a local market. We provide evidence that these results are due to a sorting effect, whereby firms trade off service quality and price, and, in turn, the incumbent attracts service (price) sensitive customers in markets where it has supplied relatively high (low) levels of service quality in the past. Furthermore, we show that it is the high quality incumbents most profitable customers who are the most attracted by superior quality alternatives. Our results appear to have long-run implications whereby sustaining a high level of service quality is associated with the incumbent attracting and retaining more profitable customers over time.", "e:keyword": ["Empirical service operations", "Service quality competition", "Customer defection", "Firm performance", "Customer compatibility"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0590", "e:abstract": "The bullwhip effect is a phenomenon commonly observed in supply chains. It describes how demand variance amplifies from a downstream site to an upstream site due to demand information distortion. Two different bullwhip effect measures have been used in the literature. Theorists analyze the bullwhip effect based on the <i>information</i> flow (i.e., order and demand information), whereas most empiricists measure it according to the <i>material</i> flow (i.e., shipment and sales data). It is unclear how much the discrepancy between these two measures is, and, if significant, how to reconcile the discrepancy. In this paper, we illustrate and quantify the discrepancy under three inventory systems. For the system with stationary demand and ample supply, we show that the bullwhip effect measure based on the material-flow data is always greater than that based on the information flow. For the system with correlated demand and for the system with supply shortages, we derive conditions under which the material flow measure is either greater or less than the information flow measure. We find that the discrepancy is driven by four factors: stocking level, lead time, demand correlation, and supply service level. We further propose a method to reduce the discrepancy by using the sample variances of aggregated sales data. Our method works for common demand processes with short-range dependence, and it does not require the knowledge of the underlying base-stock levels.", "e:keyword": ["The bullwhip effect", "Information and material flows", "Measurement"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0591", "e:abstract": "This paper, motivated by a collaboration with a healthcare service provider, focuses on stochastic open-shop service networks with two objectives: more traditional macrolevel measures (such as minimizing total system time or minimizing total number of tardy customers) and the atypical microlevel measure of reducing the incidents of excessively long waits at any workstation within the process. While work-conserving policies are optimal for macrolevel measures, scheduling policies with strategic idleness (SI) might be helpful for microlevel measures. Using the empirical data obtained from the service provider, we provide statistical evidence that SI is used by its schedulers to manage the macro- and microlevel measures. However, the company has no specific rules on implementing SI and the schedulers make decisions based on their own experience. Our primary goal is to develop a systematic framework for the joint usage of SI with dynamic scheduling policies (DSPs). We suggest to use <i>threshold-based policies</i> to intelligently combine SI and DSPs and show that the resulting policies provide an efficient way to simultaneously address both macro- and microlevel measures. We build two simulation models: one based on empirical data and one based on a randomly generated open-shop network. We use both models to demonstrate that an open-shop service network can be systematically and effectively managed to deliver improved service level by using SI.", "e:keyword": ["Strategic idleness", "Dynamic scheduling policy", "Open shop", "Healthcare", "Service network"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0592", "e:abstract": "We study leader-based collective bargaining (LCB), under which a leading buyer (leader) and a following buyer (follower) form an alliance to jointly purchase a common component from a supplier. Although the leader and the follower cooperate in their component purchase, they compete in selling their end products. We first analyze the most common and simple form of LCB, equal price LCB, under which the follower pays to the leader the same wholesale price that the leader obtains from his negotiation with the supplier. We compare each buyers profit under the equal price LCB with the benchmark where each buyer purchases separately from the supplier. We find that although the alliance might obtain a lower wholesale price and although the leader is always better off under equal price LCB, the follower can be worse off if the competition intensity of the leaders and followers products is within an intermediate region. We identify a competition effect resulting from equal price LCB that can place the follower at a disadvantage in the competition. This finding implies that the equal price LCB might not be sustainable in practice. In view of this limitation, we investigate an alternative form of LCB, fixed price LCB, under which the follower pays a fixed price to the leader regardless of the wholesale price the leader obtains from the supplier. We show that fixed price LCB benefits not only the leader but also the follower, compared with separate purchases, which implies that fixed price LCB always achieves a winwin outcome for the buyers. Our analysis further shows that even the supplier might benefit from this form of LCB.", "e:keyword": ["Leader-based collective bargaining", "Separate purchase", "Commitment", "Equal price mechanism", "Fixed price mechanism"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0593", "e:abstract": "This paper studies how retailers can effectively deliver online and offline information to omnichannel consumers who strategically choose whether to gather information online or offline and whether to buy products online or offline. Information resolves two types of uncertainty: product value uncertainty (i.e., consumers realize valuations when they inspect the product in store, but may end up returning the product when they purchase online) and availability uncertainty (i.e., store visits are futile when consumers encounter stockouts). We consider three information mechanisms: <i>physical showrooms</i> allow consumers to learn valuations anytime they visit the store, even during stockouts; <i>virtual showrooms</i> give consumers online access to an imperfect signal of their valuations; <i>availability information</i> provides real-time information about whether the store has a product in stock. Our main results follow. First, physical showrooms may prompt retailers to reduce store inventory, which increases availability risk and discourages store patronage. Second, virtual showrooms may increase online returns and hurt profits, if they induce excessive customer migration from store to online channels. Third, availability information may be redundant when availability risk is low and may render physical showrooms ineffective when implemented jointly. Finally, when customers are homogeneous, these mechanisms may not exhibit significant complementarities and the optimal information structure may involve choosing only one of the three.", "e:keyword": ["Retail operations", "Strategic consumer behavior", "Inventory availability", "Product value uncertainty", "Omnichannel"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0594", "e:abstract": "Motivated by a collaborative study with one of the most comprehensive ocular imaging programs in the United States, we investigate the underlying three-way trade-off among operational, clinical, and financial considerations in physicians decisions about ordering imaging tests. Laboratory tests may be processed in parallel and thus have a limited effect on patients waiting times; imaging tests, by contrast, require patient presence and thus directly influence patients waiting times. We use a strategic queueing framework to model a physicians decision of ordering imaging tests and show that insurance coverage is the key driver of overtesting. Our further analysis reveals the following: (i) Whereas existing studies hold that lower out-of-pocket expenses lead to higher consumption levels, we refine this statement by showing the copayment and the coinsurance rate drive the consumption in different directions. Thus, simply expanding patient cost sharing is not the solution to overtesting. (ii) Setting a low reimbursement ceiling alone cannot eliminate overtesting. (iii) The joint effect of misdiagnosis concerns and insurance coverage can lead to both overtesting and undertesting even when no reimbursement ceiling exists. These and other results continue to hold under more general conditions and are therefore robust. We enrich our model along two extensions: one with patient heterogeneity in diagnostic precision, and the other with disparities in health insurance coverage. Our findings have implications for other healthcare settings with similar trade-offs.", "e:keyword": ["Queueing games", "Service operations", "Healthcare management", "Consumer behavior", "Incentives in healthcare operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0595", "e:abstract": "We study supply function competition among conventional power generators with different levels of flexibility and the impact of intermittent renewable power generation on the competition. Inflexible generators commit production before uncertainties are realized, whereas flexible generators can adjust their production after uncertainties are realized. Both types of generators compete in an electricity market by submitting supply functions to a system operator, who solves a two-stage stochastic program to determine the production level for each generator and the corresponding market prices. We aim to gain an understanding of how conventional generators (in)flexibility and renewable energys intermittency affect the supply function competition and the market price. We find that the classic supply function equilibrium model overestimates the intensity of the market competition, and even more so when more intermittent generation is introduced into the system. The policy of economically curtailing intermittent generation intensifies the market competition, reduces price volatility, and improves the systems overall efficiency. Furthermore, these benefits of economic curtailment are most significant when the production-based subsidies for renewable energy are absent.", "e:keyword": ["Electricity market", "Supply function equilibrium", "Flexible/inflexible/variable generators", "Renewable energy", "Intermittent generation", "Economic curtailment", "Production-based subsidies"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0596", "e:abstract": "An important challenge faced by hotels is how to set their premium room price differential over their standard rooms and how to manage the upsell process. Standby upgrades, where the customer is only charged if the upgrade is available at the time of arrival, is one technique that has become increasingly popular in practice for monetizing the premium room inventory that may otherwise go unused. We develop a model of premium room and standby upgrade pricing under an uncertain market size and examine how and when standby upgrades can provide additional revenue for a hotel. When guests are myopic, we show that standby upgrades can be used as a powerful price discrimination tool, especially for hotel properties with high premium-to-standard room ratios. When guests are strategic, the benefit of standby upgrades is significantly diminished; we show that standby upgrades only increase revenue when the hotel property has a low premium-to-standard room ratio. Our findings thus provide guidance on the hotel types and environments that are most suitable for standby upgrades.The online appendix is available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/msom.2016.0596\">https://doi.org/10.1287/msom.2016.0596</ext-link>.", "e:keyword": ["Revenue management", "Pricing", "Hotel industry", "Upgrades", "Strategic consumers"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0597", "e:abstract": "Fluid teams are commonly used by a variety of organizations to perform similar and repetitive yet highly critical and knowledge-intensive tasks. Such teams operate for a limited time, after which they dissolve and some of their members may work together again as part of another team. Using a granular data set of 6,206 cardiac surgeries from a private hospital in Europe over seven years, our study offers a new and detailed account of how team familiarity (i.e., shared work experience) influences team productivity. We highlight the role of nuanced team composition dynamics beyond average team familiarity. We observe that teams with high dispersion of pairwise familiarity exhibit lower team productivity, and the existence of a bottleneck-pair may significantly hinder overall knowledge transfer capability, thus, productivity of fluid teams. In addition, we find that the higher the percentage of familiarity gained from complex tasks, the higher the productivity of the team. Finally, our results suggest that the positive effect of average team familiarity on productivity is enhanced when performing more complicated tasks. Our study provides new operational insights to improve productivity of fluid teams with better team composition strategies.", "e:keyword": ["Healthcare management", "Fluid teams", "Productivity", "Team familiarity", "Dispersion", "Bottleneck"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0598", "e:abstract": "Competing technologies in emerging industries create uncertainties that discourage supplier investments. Open technology can induce supplier investments, but may also lead to intensified future competition. In this paper, we study competing manufacturers open-technology strategies. We show that despite the risk of intensifying future competition, open technologies by competing manufacturers may constitute an equilibrium and can indeed induce supplier investments. In addition, we identify a technology-risk-pooling benefit; namely, by opening technologies, competing manufacturers can induce supplier investments in both technologies and later adopt the one preferred by the market. However, manufacturers may also exhibit the prisoners dilemma and close their technologies despite the risk-pooling benefit. In this case, there is potential for collaborative technology sharing through cross licensing. Finally, we show that manufacturers may sometimes <i>close</i> their technologies to force supplier investments.This paper has an e-companion at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/msom.2016.0598doi.org/10.1287/msom.2016.0598\">https://doi.org/10.1287/msom.2016.0598</ext-link>.", "e:keyword": ["Open technology", "Technology choice", "Competitive strategy", "Supplier investment", "Procurement"]}, {"@id": "http://dx.doi.org/10.1287/msom.2016.0601", "e:abstract": "By modifying the charging patterns of a pool of electric vehicles (EVs), aggregators are able to provide services to the electric grid. The parameters that shape the agreement between aggregators and EV owners, such as plug-in duration and guaranteed driving range, affect the appeal of signing up with an aggregator. Our study examines how these contract parameters influence the profitability of vehicle pools. We use a bottom-up model that encompasses the entire planning problem faced by an aggregator. The model is applied to the German secondary reserve market using actual driving patterns and market data. Our results indicate that contract parameters influence EVs value to aggregators in surprising ways, partially contradicting findings in previous literature. In essence, high levels of plug-in durations or low levels of guaranteed driving for planned trips do not necessarily provide the highest profit. Furthermore, we find that realistic nonanticipative strategies lead to annual profits in the range of 167 to 125 per vehicle, which is in the lower range of previous studies. Our results are a starting point for a discussion about optimal contract parameters and the design of corresponding marketing strategies.The online appendix is available at <ext-link ext-link-type=\"uri\"  href=\"https://doi.org/10.1287/msom.2016.0601\">https://doi.org/10.1287/msom.2016.0601</ext-link>.", "e:keyword": ["Electric vehicles", "Grid integrated vehicles", "Ancillary services", "Reserve market", "Stochastic optimization", "OR in energy"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.1.1", "e:abstract": "We argue that industries and industry segments are characterized by a clockspeed that gauges the velocity of change in the external business environment and sets the pace of their firms' internal operations. Using data from the electronics industry, we develop and validate an integrated metric for clockspeed that takes into account both demand- and supply-side factors. We show that after controlling for product complexity and other factors, higher industry clockspeed is associated with faster execution in product development and manufacturing (e.g., shorter development time, quicker stabilization of production) and more frequent changes in organizational structure. Our findings on the effects of clockspeed can help researchers studying other industries, and our results provide benchmarks against which practitioners can compare and classify their own organizations.", "e:keyword": ["Clockspeed", "Dynamics", "Time-based Competition", "Electronics Industry", "Computer Industry", "Development Speed", "Velocity of Change", "Product Life Cycle", "Innovation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.1.21", "e:abstract": "We study the problem of allocating stocked fibers to made-to-order cables with the goals of satisfying due dates and reducing the costs of scrap, setup, and fiber circulation. These goals are achieved by generating remnant fibers either long enough to satisfy future orders or short enough to scrap with little waste. They are also achieved by manufacturing concatenations, in which multiple cable orders are satisfied by the production of a single cable that is afterwards cut into the constituent cables ordered. We use a function that values fibers according to length, and which can be viewed as an approximation to the optimal value function of an underlying dynamic programming problem. The daily policy that arises under this approximation is an integer program with a simple linear objective function that uses changes in fiber value to take into account the multi-period consequences of decisions. We describe our successful implementation of this integer program in the factory, summarizing our computational experience as well as realized operational improvements.", "e:keyword": ["Cable Manufacturing", "Resource Pricing and Allocation", "Remnant Inventory System", "Discrete Optimization", "Dynamic Programming Approximations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.1.36", "e:abstract": "The relentless pursuit of increased product quality via continuous improvement is an important long-term strategy for achieving competitive advantage. However, manufacturers must still achieve high product quality in the short run. Hence, short-run quality improvement strategies are necessary, and, if possible, should complement (rather than substitute for) a longer run continuous improvement strategy with suppliers. We propose a novel short-run quality improvement strategy with suppliers which is based on a combinatorial optimization model for determining the optimal matching of raw material batches (from suppliers) with process variable parameters (of the manufacturer) by exploiting their interaction effects. Although the model can be difficult to solve, we develop sufficient conditions for a special case where the attainment of an optimal solution is straightforward. We demonstrate the use of the proposed approach in an actual pharmaceutical process. Using data from the pharmaceutical case, we develop several simulation scenarios where matching is shown (predicted) to greatly improve process yield. We also discuss the use of matching to illuminate various strategies for long-term continuous improvement of supplier and manufacturer processes in general.", "e:keyword": ["Matching", "Quality Improvement", "Combinatorial Optimization", "Heuristics", "Logit Model", "Likelihood Ratio"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.1.50", "e:abstract": "In this paper, we consider an adaptive base-stock policy for a single-item inventory system, where the demand process is nonstationary. In particular, the demand process is an integrated moving average process of order (0, 1, 1), for which an exponential-weighted moving average provides the optimal forecast. For the assumed control policy we characterize the inventory random variable and use this to find the safety stock requirements for the system. From this characterization, we see that the required inventory, both in absolute terms and as it depends on the replenishment lead-time, behaves much differently for this case of nonstationary demand compared with stationary demand. We then show how the single-item model extends to a multi-stage, or supply-chain context; in particular we see that the demand process for the upstream stage is not only nonstationary but also more variable than that for the downstream stage. We also show that for this model there is no value from letting the upstream stages see the exogenous demand. The paper concludes with some observations about the practical implications of this work.", "e:keyword": ["Single-Item Inventory Model", "Nonstationary Demand", "Base-Stock Policy", "Amplification of Demand Variability Across a Supply Chain"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.1.62", "e:abstract": "The bottleneck in a production-inventory network is commonly taken to be the facility that most limits flow through the network and thus the most highly utilized facility. A further connotation of \"bottleneck,\" however, is the facility that most constrains system-wide performance or the facility at which additional resources would have the greatest impact. Adopting this broader sense of the term, we look for fill-rate bottlenecks: facilities in a production-inventory network that most constrain the system-wide fill rate (the proportion of demands filled within a fixed delivery leadtime) or facilities at which either additional production capacity or additional inventory would have the greatest impact on the fill rate. We consider systems in which various components are produced through a series of stages holding intermediate inventories and are then assembled into finished goods to meet external demands. With each station in the network we associate precise measures of the station's propensity to constrain the fill rate. We call a station with a minimal measure a fill-rate bottleneck and justify this label both theoretically and numerically. Examples show that even the least utilized facility can be a fill-rate bottleneck. Unlike utilization, our bottleneck criteria capture information about process variability.", "e:keyword": ["Multistage Assemble-to-Order System", "Response Time", "Order Fill Rate", "Bottleneck", "Leadtime", "Inventory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.1.77", "e:abstract": "This paper considers serial production-transportation systems. In recent years, researchers have developed a fairly simple functional equation that characterizes optimal system behavior, under the assumption of constant leadtimes. We show that the equation covers a variety of stochastic-leadtime systems as well. Still, many basic managerial issues remain obscure: When should stock be held at upstream stages? Which system attributes drive overall performance, and how? To address these questions, we develop and analyze several heuristic methods, inspired by observation of common practice and numerical experiments. One of these heuristics yields a bound on the optimal average cost. We also study a set of numerical examples, to gain insight into the nature of the optimal solution and to evaluate the heuristics.", "e:keyword": ["Inventory/Production", "Multistage", "Solutions and Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.1.v", "e:abstract": "The publication of Volume 1, Number 1 of M&SOM marks the end of a process that began more than three years ago: a process whose goal was to create the premier journal in operations management. I will leave it to you, the reader of 1:1, to decide whether or not we have been successful \"right out of the blocks\" or if more issues must be published before we can rightfully claim to have succeeded. But we will suceed. I am committed to it. So are the distinguished group of senior editors and the esteemed members of M&SOM's Editorial Review Board, whose names are, and will remain, prominently featured in every issue. And most important, so are the authors who have submitted their manuscripts to us.", "e:keyword": ["New journal introduction", "Editorial", "Operations management", "Manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.2.89", "e:abstract": "The Quantity Flexibility (QF) contract is a method for coordinating materials and information flows in supply chains operating under rolling-horizon planning. It stipulates a maximum percentage revision each element of the period-by-period replenishment schedule is allowed per planning iteration. The supplier is obligated to cover any requests that remain within the upside limits. The bounds on reductions are a form of minimum purchase commitment which discourages the customer from overstating its needs. While QF contracts are being implemented in industrial practice, the academic literature has thus far had little guidance to offer a firm interested in structuring its supply relationships in this way. This paper seeks to address this need, by developing rigorous conclusions about the behavioral consequences of QF contracts, and hence about the implications for the performance and design of supply chains with linkages possessing this structure. Issues explored include the impact of system flexibility on inventory characteristics and the patterns by which forecast and order variability propagate along the supply chain. The ultimate goal is to provide insights as to where to position flexibility for the greatest benefit, and how much to pay for it.", "e:keyword": ["Supply Chain Management", "Supply Contracts", "Quantity Flexibility", "Forecast Revision", "Materials Planning", "Bullwhip Effect"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.2.112", "e:abstract": "Paced or Synchronous assembly lines are a popular class of assembly systems consisting of a series of assembly stations arranged in tandem. Every job (or order) visits all assembly stations in the same sequence and spends the same amount of time (known as the production cycle) at each station. Industries such as aircraft, fire-engine, and automobile assembly have production cycles of a few hours and are labor intensive. In spite of increased automation in such industries, human capital remains the most expensive and important contributor to a flexible production system. In this article we formulate the cross-training problem on a paced assembly line with m stations (mCT). We assume that each worker possesses a number of skills referred to as a skill vector. Our objective is to schedule a set of work orders through the assembly system so as to minimize the size of the required workforce and/or the workforce cross-training costs. We analyze the complexity of mCT and identify polynomially solvable cases. A variety of lower bounds is developed based on optimization techniques. These lower bounds are used to develop a branch and bound algorithm as well as to evaluate our heuristics. A computational experiment reports the performance of all algorithms. Using these algorithms, we examine how the formation of skill vectors affects the workforce size and draw guidelines for cross-training programs in organizations with labor intensive assembly operations.", "e:keyword": ["Workforce Planning", "Paced Assembly line", "Cross-Training", "Integer Programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.2.132", "e:abstract": "In this paper, we address the product-family design problem of a firm in a market in which customers choose products based on some measure of product performance. By developing products as a family, the firm can reduce the cost of developing individual product variants due to the reuse of a common product platform. Such a platform, designed in an aggregate-planning phase that precedes the development of individual product variants, is itself expensive to develop. Hence, its costs must be weighted against the benefits of its reuse in a family. We offer a model for capturing costs of product development when the family consists of variants based on a common platform. It is shown that the model can be converted into a network-optimization problem, and the optimal product-family can be identified under fairly general conditions by determining the shortest path of its network formulation. We also analytically examine the effect of alternative product designs on product-family composition, and discuss the implications of investing in new-product technology. Finally, we illustrate our model and managerial insights with an application from the electronics industry.", "e:keyword": ["Product Family", "Platform", "Reusability"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.2.157", "e:abstract": "We consider a stochastic, capacitated production-inventory model in which the customer provides information about the expected timing of future orders to the supplier. We allow for randomness in customer order arrivals as well as the quantity demanded, but work under the assumption that the customer is making every effort to follow the schedule provided. We term this as a target reverting policy. This gives rise to an interesting nonstationary inventory control model at the supplier. After characterizing the optimal policy, we develop solution procedures to compute the optimal parameters. An extensive computational study provides insights into the behavior of this model at optimality. Further, comparing the cost of the optimal policy to the cost of simple policies that either ignore the customer's information or the capacity constraint, we are able to provide insights as to when these simplifications could be costly.", "e:keyword": ["Supply Chain", "Capacitated Production-Inventory Model", "Optimal Policy", "Simulation-Based Optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.1.2.174", "e:abstract": "In preparing a review I recently discovered an important reference for a key result in Graves (1999). Wecker (1979) had previously derived the variance for demand over a deterministic lead-time for an IMA (0, 1, 1) demand process. I develop effectively the same result, given as Equation (8) in Graves (1999). I state this as the variance of the inventory random variable for an inventory system that is subject to an IMA (0, 1, 1) demand process, a deterministic replenishment lead-time and an adaptive base-stock control policy given by (7). But given these assumptions, the variance of the inventory is the same as the variance of the demand over the lead-time. Although the Wecker manuscript has not been published, Eppen and Martin (1988) reference it and use the key result as part of their research.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.2.1.1.23266", "e:abstract": "Some firms, particularly in high-tech, appear to view technology leadership and cost leadership as separate and distinct ways of achieving high profits within a given product market. In contrast, we develop a model of technology competition suggesting that a firm's success in sustaining its technology leadership may hinge on its ability to produce the new product (resulting from the new technology) at lower cost, an ability we call the firm's \"cost competence.\" In our model, cost competence, above a critical hurdle level, is essential to the incumbent firm in its bid to retain technology leadership. The model also clarifies the role of \"innovative competence,\" which characterizes a firm's ability to turn an investment in new technology into a marketable product. We present an example of a standard product market model (for determining prices and production quantities for two competing products in the aftermath of the technology competition) in which the assumptions of our technology competition model hold. An additional key finding is that there can be discontinuities in the returns that accrue from enhanced cost competence. If a firm is on the right side of a \"jump point,\" its expected profits can be dramatically more than if it were only slightly less competent. These jump points arise where the firm's cost competence becomes sufficient to cause a competitor to decide against competing in the new technology, or to significantly drop its investment amount. When a potential competitor backs down, the prospect of high returns for the incumbent opens up.", "e:keyword": ["Manufacturing strategy", "Cost competence", "Product innovation", "Process innovation", "Technological change", "Technology leadership"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.1.19.23269", "e:abstract": "Manufacturing and service operations decisions depend critically on capacity and resource limits. These limits directly affect the risk inherent in those decisions. While risk consideration is well developed in finance through efficient market theory and the capital asset pricing model, operations management models do not generally adopt these principles. One reason for this apparent inconsistency may be that analysis of an operational model does not reveal the level of risk until the model is solved. Using results from option pricing theory, we show that this inconsistency can be avoided in a wide range of planning models. By assuming the availability of market hedges, we show that risk can be incorporated into planning models by adjusting capacity and resource levels. The result resolves some possible inconsistencies between finance and operations and provides a financial basis for many planning problems. We illustrate the proposed approach using a capacity-planning example.", "e:keyword": ["Real options", "Stochastic programming", "Financial optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.1.32.23268", "e:abstract": "Does cross-training workers allow a firm to achieve economies of scale when there is variability in the content of work, or does it create a workforce that performs many tasks with consistent mediocrity? To address this question we integrate a model of a stochastic service system with models for tenure- and experience-based service quality. When examined in isolation, the service system model confirms a well-known \"rule of thumb\" from the queueing literature: Flexible or cross-trained servers provide more throughput with fewer workers than specialized servers. However, in the integrated model these economies of scale are tempered by a loss in quality. Given multiple tasks, flexible workers may not gain sufficient experience to provide high-quality service to any one customer, and what is gained in efficiency is lost in quality. Through a series of numerical experiments we find that low utilization in an all-specialist system can also reduce quality, and therefore the optimal staff mix combines flexible and specialized workers. We also investigate when the performance of the system is sensitive to the staffing configuration choice. For small systems with high learning rates, the optimal staff mix provides significant benefits over either extreme case (a completely specialized or completely flexible workforce). If the system is small and the rate of learning is slow, flexible servers are preferred. For large systems with high learning rates, the model leans toward specialized servers. In a final set of experiments, the model analyzes the design options for an actual call center.", "e:keyword": ["Queues: approximations", "Service quality", "Learning curves", "Crosstraining", "Worker turnover", "Personnel"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.1.49.23263", "e:abstract": "Worksharing occurs in serial manufacturing when machines are not uniquely assigned to workers. For example, two workers can operate three machines by alternating usage of the middle one. In some other worksharing systems operators move down the line carrying an item (or batch) with them, working on it at each machine until they are met by another worker who is coming back upstream. The work is then handed off even if the operation is underway. \"Bucket Brigade\" and \"TSS\" are worksharing systems based on this idea. This paper examines worksharing in a variety of situations, including unequal work content across machines, uncertain processing times, unequal workers, handoffs with and without preemption, and a range of machine-to-worker ratios. Some systems restrict workers to \"zones\" of machines. Work zones must overlap if sharing is to occur. The appropriate size of the overlapis shown to depend on the circumstances. Inventory-based rules to control access to shared machines are demonstrated to increase productivity when processing times vary. Worker sequence is found to be quite important; Slowest-to-Fastest is recommended for some situations, but is shown to perform poorly in others. Finally, the issue of whether or not to allow preemption is shown to have a large impact on productivity if inventory is not allowed, and to strongly affect the choice of designs when inventory is allowed. With and without preemption, proper combinations of worker sequence, zone size, and production control rules are shown to nearly eliminate idle time.", "e:keyword": ["Worksharing", "Serial production lines", "Cross training", "Bucket brigade"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.1.68.23267", "e:abstract": "Manufacturing managers face increasing pressure to reduce inventories across the supply chain. However, in complex supply chains, it is not always obvious where to hold safety stock to minimize inventory costs and provide a high level of service to the final customer. In this paper we develop a framework for modeling strategic safety stock in a supply chain that is subject to demand or forecast uncertainty. Key assumptions are that we can model the supply chain as a network, that each stage in the supply chain operates with a periodic-review base-stock policy, that demand is bounded, and that there is a guaranteed service time between every stage and its customers. We develop an optimization algorithm for the placement of strategic safety stock for supply chains that can be modeled as spanning trees. Our assumptions allow us to capture the stochastic nature of the problem and formulate it as a deterministic optimization. As a partial validation of the model, we describe its successful application by product flow teams at Eastman Kodak. We discuss how these flow teams have used the model to reduce finished goods inventory, target cycle time reduction efforts, and determine component inventories. We conclude with a list of needs to enhance the utility of the model.", "e:keyword": ["Base-stock policy", "Dynamic programming application", "Multi-echelon inventory system", "Multi-stage supply-chain application", "Safety stock optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.1.84.23264", "e:abstract": "We characterize optimal policies for the problem of allocating a single server to a set of jobs from N families. Each job is an instance of demand for an item and is associated with a family, a holding cost rate, and a mean processing time. Set-up times are required to switch from one family to another, but are not required to switch within a family. We consider the case in which the order of jobs within the family is unconstrained, and a variation in which the order is fixed. The optimization is with respect to the weighted flowtime, and we treat problems both with and without a makespan-constraint. Practical examples based on this model are described. We partially characterize an optimal policy by means of a Gittins rewardrate index and a similar switching index derived from multi-armed bandit theory. For deterministic problems with a makespan constraint, we present an optimization algorithm for the special case of two families and at most three set-ups . Without a makespan constraint and without preemption, we prove that our analysis of a deterministic model extends to stochastic set-up and processing times without loss of optimality. Managerial insights based on our technical results are provided.", "e:keyword": ["Job scheduling", "Job families", "Multi-armed bandit", "Group technology", "Weighted flowtime", "Makespan", "Set-up time"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.1.100.23270", "e:abstract": "A returns policy, which specifies a schedule of rebates from manufacturer to retailer for product left over at the end of the selling season, encourages larger order quantities and can increase manufacturer profit. One downside from a manufacturer's perspective is the possibility of very low profit due to high rebate expense when demand is lower than expected. We take the viewpoint of a manufacturer selling a short life-cycle product to a single risk-neutral retailer and describe returns policies that, when compared to no returns, satisfy two conditions: (1) the retailer's expected profit is increased and (2) the manufacturer's profit is at least as large as when no returns are allowed. We call such a returns policy risk-free.", "e:keyword": ["Supply chain", "Contacts", "Returns policies"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.2.119.12354", "e:abstract": "Demand forecasts do not become consistently more accurate as they are updated. We present examples demonstrating this counterintuitive phenomenon and some theoretical results to explain its occurrence. Specifically, we analyze the effect of demand randomness on forecast-update performance. A surprising result is that under various theoretical models involving demand randomness alone, updated forecasts will be less accurate between 30% and 50% of the time.", "e:keyword": ["Forecast performance", "Forecast updates", "Exponential smoothing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.2.128.12350", "e:abstract": "We focus on a logistics system where inventory is held at three levels: the customers, the retail store, and the warehouse. Retail customer segments are heterogeneous and differ in their reservation prices for product as well as their holding costs. They purchase product from a retail store managed by a retailer. The retailer chooses a retail pricing scheme to maximize his expected profit given a model of customer temporal response to retail pricing. This retailer is supplied product from a warehouse managed by a manufacturer. The manufacturer is responsible for maintaining inventory level at the warehouse and providing 100% service level for retailer orders. The manufacturer uses all available information to generate an inventory policy that maximizes expected profit subject to the service-level requirement. We evaluate the manufacturer's optimal expected profit under two possible schemes: (1) no information regarding the timing of retail promotion plans, and (2) full information regarding the timing of retail promotion plans. We show: (1) as the predictability of the sales impact of a promotion decreases, it may be optimal for the retailer to eliminate retail promotions; (2) increased stockpiling tendency of customers increases retailer profits and decreases manufacturer profits; and (3) retail-promotion information sharing can make retail promotions change from being less profitable than no promotions to being more profitable than no promotions for the manufacturer. We show the impact of fitting the model to a grocery store data set that provided data regarding retail sales (and associated prices) of canned tomato soup over two years. We also explore managerial insights suggested by the model.", "e:keyword": ["Information sharing", "Retail promotions", "Customer heterogeneity", "Inventory models"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.2.144.12353", "e:abstract": "We investigate a simple dynamic model of firm behavior in which firms compete by investing in capacity that is used to provide a good or service to their customers. There is a fixed total market of customers whose demands for the good or service are random and who divide their patronage between the firms in each period. Periodically, the market shares of the two firms can change based on the realized level of customer service provided in the prior period. We assume that the expected level of customer service can be expressed as a function of the (per customer) capacity of the firms' service delivery systems, and that service declines as the capacity decreases. The firms differ in their customers' willingness to defect when confronted by service failure. The primary issue we address is the firms' capacity decisions in response to customer service concerns and competitive pressure. We provide conditions under which the firms' optimal (i.e., equilibrium) capacity levels in a period are proportional to the size of their respective customer bases in that period. Further, we develop expressions for the value of a firm's customers and the implicit cost of service failure. Results for both single-period and finite-horizon problems are investigated and applied to two examples: (1) competition between Internet service providers who operate systems that we approximate by simple loss-type queueing models, and (2) competition between make-to-stock producers who operate systems that we approximate by newsvendor inventory models. For both examples, solutions are derived and interpreted.", "e:keyword": ["Capacity management", "Game theory", "Customer loyalty", "Customer service", "Newsvendor model"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.2.166.12349", "e:abstract": "Designing product lines with substitutable components and subassemblies permits companies to offer a broader variety of products while continuing to exploit economies of scale in production and inventory costs. Past research on models incorporating component substitutions focuses on the benefits from reduced safety-stock requirements. This paper addresses a dynamic requirements-planning problem for two-stage multi product manufacturing systems with bill-of-materials flexibility, i.e., with options to use substitute components or subassemblies produced by an upstream stage to meet demand in each period at the downstream stage. We model the problem as an integer program, and describe a dynamic-programming solution method to find the production and substitution quantities that satisfy given multi period downstream demands at minimum total setup, production, conversion, and holding cost. This methodology can serve as a module in requirements-planning systems to plan opportunistic component substitutions based on relative future demands and production costs. Computational results using real data from an aluminum-tube manufacturer show that substitution can save, on average, 8.7% of manufacturing cost. We also apply the model to random problems with a simple product structure to develop insights regarding substitution behavior and impacts.", "e:keyword": ["Inventory-production: applications", "Inventory-production: scale-economies-lot sizing", "Inventory-production: deterministic", "Network-graphs: applications"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.2.186.12351", "e:abstract": "This article studies the problem of sales-force compensation by considering the impact of sales-force behavior on a firm's production and inventory system. The sales force's compensation package affects how the salespeople are going to exert their effort, which in turn determines the sales pattern for the firm's product and ultimately drives the performance of the firm's production and inventory system. In general, a smooth demand process facilitates production/inventory planning. Therefore, it is beneficial for a firm to induce its salespeople to exert effort in a way that actually smoothes the demand process. The article proposes a compensation package to induce such behavior. It evaluates and compensates the sales force on a moving-time-window basis, where the length of the time window is determined by the production lead time. Numerical examples show that the proposed package is beneficial to the firm relative to a widely used compensation plan based on annual quotas.", "e:keyword": ["Sales-force compensation", "Agency theory", "Sales-force management", "Demand smoothing", "Production/inventory planning"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.2.203.12352", "e:abstract": "The decentralized supply chain management scheme of Lee and Whang (1999) can be viewed as operationalizing the decentralized management scheme implicit in Clark and Scarf (1960). This paper proposes the use of what are called responsibility tokens (RTs) to further facilitate that operationalization. The proposal assumes that a management information system, presumably electronic, is established to monitor inventories and shipment quantities, and to carry out transfer payments between players. As in Lee and Whang(1999), the incentives of the system are aligned, so if each player is brilliantly self-serving, the system optimal solution will result. While the system administrator need not know how the system should be managed, the most upstream player must know how to manage the system optimally for the system optimal solution to be achieved. RTs endow the system with an attractive self-correcting property: An example illustrates that upstream players are given a mechanism and the incentive to correct for downstream overordering. The downstream players who over-order are penalized, but system performance is not degraded much. Extensions and further research are also discussed.", "e:keyword": ["Supply chain management", "Responsibility tokens", "Multiechelon", "Inventory management", "Management information system", "Incentives", "Stochastic demand", "Cost accounting", "Decentralization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.3.221.12348", "e:abstract": "In a firm that makes schedule, orders are always processed within a fixed time frame. In a congested facility, such a situation would be impossible using conventional queuing logic. We propose a conceptual model of a firm in which workers make schedule by rushing jobs, if necessary, with potential quality consequences. Hence, time and quality are substitutes, a feature that we recognize explicitly in our definition of the firm's capacity. Production yields are not exogenous parameters but endogenously determined by workers responding to schedule pressures. The plant manager can authorize overtime to relieve this pressure or live with the quality consequences of rushing. The model reveals the close relationships among the firm's workforce policies, the integrity of the inspection system, and the cost performance of the firmas its volume and/or product line expands. We consider pure congestion (driven by volume) and pure complexity (driven by product line breadth) effects in the context of our plant performance model. We consider two root causes of complexity costs: time and quality. The time effects of complexity will only have cost consequences in congested facilities, but quality effects are always present. Also in contrast to time effects, quality effects of complexity can be present in nonbottleneck workstations. Hence, the quality consequences of complexity can be as, or more important than the time consequences.", "e:keyword": ["Product line complexity", "Congestion", "Capacity", "Quality"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.3.240.12345", "e:abstract": "The principal-agent paradigm, in which a principal has a primary stake in the performance of some system but delegates operational control of that system to an agent, has many natural applications in operations management (OM). However, existing principal-agent models are of limited use to OM researchers because they cannot represent the rich dynamic structure required of OM models. This paper formulates a novel dynamic model that overcomes these limitations by combining the principal-agent framework with the physical structure of a Markov decision process. In this model one has a system moving from state to state as time passes, with transition probabilities depending on actions chosen by an agent, and a principal who pays the agent based on state transitions observed. The principal seeks an optimal payment scheme, striving to induce the actions that will maximize her expected discounted profits over a finite planning horizon. Although dynamic principal-agent models similar to the one proposed here are considered intractable, a set of assumptions are introduced that enable a systematic analysis. These assumptions involve the \"economic structure\" of the model but not its \"physical structure.\" Under these assumptions, the paper establishes that one can use a dynamic-programming recursion to derive an optimal payment scheme. This scheme is memoryless and satisfies a generalization of Bellman's principle of optimality. Important managerial insights are highlighted in the context of a two-state example called \"the maintenance problem\".", "e:keyword": ["Dynamic principal agent problem", "Incentives in operations management", "Maintenance"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.3.264.12344", "e:abstract": "Delivering high quality service during the service encounter is central to competitive advantage in service organizations. However, achieving such high quality while controlling for costs is a major challenge for service managers. The purpose of this paper is to present an approach for addressing this challenge. The approach entails developing a model linking service process operational variables to service quality metrics to provide guidelines for service resource allocation. The approach enables the service operations manager to take specific actions toward service quality improvement, in light of the costs involved. A novel feature of the approach is the development of robust optimization models, which provide optimal operational guidelines while accounting for uncertainty in the model's parameters. We demonstrate the applicability of the approach in a large health care facility.", "e:keyword": ["Service quality", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.3.287.12346", "e:abstract": "We consider a problem faced by a contract assembler that both assembles finished goods and procures the associated component parts for one of its major customers. Because of rapid changes in technology and ongoing engineering changes, all parts subject to obsolescence are purchased only for the current customer order. The procurement lead times of the components are random. Moreover, although the order for the finished product has a defined due date, the contract allows the customer to change the order quantity. Consequently, the assembler also faces a random demand. The assembler must determine how much to order and when to order each component part. The objective is to minimize the total expected cost, including the cost of holding components prior to their assembly, penalties for tardiness visa-vis the assembly due date, and overage and underage costs in satisfying the demand quantity. We present some structural results and discuss insights regarding optimal policies. We also present several simple heuristic policies and compare them to optimal policies. Computational results indicate that ignoring lead time variability can be costly, but relatively simple heuristics that consider lead time variability perform quite well.", "e:keyword": ["Inventory/production", "Multiitem", "Optimal policies", "Stochastic models"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.3.297.12347", "e:abstract": "The third-party logistics industry has grown rapidly in recent years, accounting for $46 billion of the total $921 billion in logistics spending in the United States during 1999. This figure is expected to grow by 15 to 20% annually as manufacturing firms increasingly partner with third-party logistics providers to cost effectively distribute their products, while meeting increasingly stringent service expectations of customers. These logistics partnerships have introduced a new set of decision requirements to negotiate compensation for distribution services. Based on a collaborative project with a leading building products manufacturer, this paper describes the development and implementation of a novel linear programming model to decide the delivery fees paid to distributors. The model applies to manufacturer-distributor partnerships where distributors are compensated using fee values that depend on delivery weights and distances. It ensures that the expected compensation, considering stochastic demands, is adequate to cover the aggregate distribution costs for each distributor, and permits imposing various consistency conditions to ensure that fee values are credible. The model proved effective in helping the manufacturer develop a new fee table that generated considerable economic savings and provided more equitable compensation to distributors.", "e:keyword": ["Third-party logistics", "Distribution", "Supply chain", "Compensation", "Applied optimization", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.4.317.12337", "e:abstract": "Suppose you are a Marketing Manager envisioning a new product, or an Operations Manager contemplating a process improvement, or a CEO who commissioned an integrated new product development team. If our assumptions hold, our model offers you a single numerical measure, called the degree of product/process innovation, to determine your initiative's impact on potential sales, prices, market segments, and profits. Our simple, single-period model is a variation of the existing vertically differentiated products model: There are two competing substitute products, and customers will buy at most one of them. Our contribution is to allow new relationships between the valuations of the two products by potential customers, and to allow differing unit production costs. We identify equilibrium results when two competing firms each offer one product, and find the profit maximizing result when one (monopolistic) firm offers both products. The new product infringes on the market in one of two ways: High-end encroachment results when the new product attracts the best customers (those with the highest reservation prices), while low-end encroachment identifies a situation where the new product attracts fringe (lower-end) customers. Low-end encroachment may help explain why an incumbent sometimes fails to recognize the threat of an entrant's product, as we illustrate with an example from the disk drive industry. In short, we offer insight into the value of both a marketing objective (enhancing the product design attributes) and a manufacturing goal (lowering the production cost) in a product and/or process improvement project.", "e:keyword": ["Degree of product/process innovation", "Low-end encroachment", "High-end encroachment", "Reservation price", "Disruptive technology", "Operations strategy", "Vertically differentiated products"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.4.337.12341", "e:abstract": "For most firms, new product development is the engine for growth and profitability. A firm's new product success depends on its ability to manage the product development process in a way that employs scarce resources to achieve the goal of the firm as well as the specific project's objectives. Simple and measurable performance metrics have been proposed and applied to monitor and compensate the development teams. In this paper, we develop a modeling frame work to analyze the implications of setting managerial priorities for three commonly used new product performance metrics: (1) time-to-market, (2) product performance, and (3) total development cost. We model new product development as a \"product performance production\" process that requires scarce development resources. Setting a target for development teams for each of these performance metrics can constrain this performance production process and, thereby, affect the other performance metrics. We model the constrained process as a restricted case of a general process that does not have such constraints. We benchmark each constrained process against the optimal, unrestricted process with respect to the level of the resource intensity employed during the development process, the time-to-market, and the performance level of the new product at launch. We show that an overly ambitious time-to-market target leads to an upward bias in resource intensity usage and a downward bias in product performance (i.e., evolutionary product innovation). In addition, our results suggest that the target time-to-market approach may ignore the effect of cannibalization and, thus, can perform suboptimally if a significant degree of cannibalization in the existing product market is expected. Given a target product performance, we show that the coordination between marketing and R&D is easier because the resulting development resource intensity and time-to-market decisions becomes separable. However, an overly ambitious product performance target leads to an upward bias in the development resource intensity and a delayed product launch that misses the window of opportunity. Finally, we show that the target development cost approach can lead a downward bias in product performance and a premature product launch. The above analyses are performed for a monopolistic firm, and they are extended to passive and active competitive environment.", "e:keyword": ["New product development", "Product performance", "Time-to-market", "Development costs", "Performance metrics"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.4.350.12340", "e:abstract": "Advances in information technology have opened new venues for companies to create flexible supply chains by offering high-speed communication and tight connectivity. A growing number of companies are taking advantage of new opportunities to outsource portions of their production and other operations. Given the importance of the supplier selection process in the ultimate success of a product, a purchasing manager must understand the different sourcing strategies that she or he can use and the suitability of each sourcing arrangements for her or him. This paper provides an overview of the research that has been done in the fields of operations research and economics on the topic of sourcing strategies. In aggregate, this paper provides a blueprint of what market characteristics can heavily influence a buyer-supplier relationship and, hence, are important to identify and incorporate into the supplier selection process.", "e:keyword": ["Auctions", "Sole", "Dual", "Parallel", "And multiple sourcing", "Mechanism design", "Procurement", "Outsourcing"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.4.372.12342", "e:abstract": "This paper studies a distribution system in which a manufacturer supplies a common product to two independent retailers, who in turn use service as well as retail price to directly compete for end customers. We examine the drivers of each firm's strategy, and the consequences for total sales, market share, and profitability. We show that the relative intensity of competition with respect to each competitive dimension plays a key role, as does the degree of cooperation between the retailers. We discover a number of insights concerning the preferences of each party regarding competition. For instance, there will be circumstances under which both retailers would prefer an increase in competitive intensity. Our analysis generalizes existing knowledge about manufacturer wholesale pricing strategies, and rationalizes behaviors that would not be evident without both price and service competition. Finally, we characterize the structure of wholesale pricing mechanisms that can coordinate the system, and show that the most commonly used formats (those that are linear in the order quantity) can achieve coordination only under very limiting conditions.", "e:keyword": ["Channels of distribution", "Supply chain management", "Coordination", "Competition", "Pricing", "Service levels", "Manufacturing/marketing interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.4.392.12336", "e:abstract": "Many service firms use delivery time guarantees to compete for customers in the marketplace. In this research we develop a stylized model to analyze the impact of using time guarantees on competition. Demands are assumed to be sensitive to both the price and delivery time guarantees, and the objective of each firm is to select the best price and time guarantee to maximize its operating profit. We first analyze the optimization problem for the individual firms and then study the equilibrium solution in a multiple-firm competition. Using a numerical study, we further illustrate how the different firm and market characteristics would affect the price and delivery time competition in the market. Our results suggest that the equilibrium price and time guarantee decisions in an oligopolistic market with identical firms behave in a similar fashion as the optimal solution in a monopolistic situation from a previous study. However, when there are heterogeneous firms in the market, these firms will exploit their distinctive firm characteristics to differentiate their services. Assuming all other factors being equal, the high capacity firms provide better time guarantees, while firms with lower operating costs offer lower prices, and the differentiation becomes more acute as demands become more time-sensitive. Furthermore, as time-attractiveness of the market increases, firms compete less on price, and the equilibrium prices of the firms increase as a result. Our findings provide important implications about firm behaviour under price and time competition.", "e:keyword": ["Time-based competition", "Price competition", "Service guarantees", "Competitive games"]}, {"@id": "http://dx.doi.org/10.1287/msom.2.4.410.12339", "e:abstract": "We consider a single-period inventory model in which a risk-averse retailer faces uncertain customer demand and makes a purchasing-order-quantity and a selling-price decision with the objective of maximizing expected utility. This problem is similar to the classic newsvendor problem, except: (a) the distribution of demand is a function of the selling price, which is determined by the retailer; and (b) the objective of the retailer is to maximize his/her expected utility. We consider two different ways in which price affects the distribution of demand. In the first model, we assume that a change in price affects the scale of the distribution. In the second model, a change in price only affects the location of the distribution. We present methodology by which this problem with two decision variables can be simplified by reducing it to a problem in a single variable. We show that in comparison to a risk-neutral retailer, a risk-averse retailer in the first model will charge a higher price and order less; where as, in the second model a risk-averse retailer will charge a lower price. The implications of these findings for supply-chain strategy and channel design are discussed. Our research provides a better understanding of retailers' pricing behavior that could lead to improved price contracts and channel-management policies.", "e:keyword": ["Pricing", "Demand uncertainty", "Risk aversion", "Inventory"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.1", "e:abstract": "Each year the Manufacturing and Service Operations Management (MSOM) Society of INFORMS conducts a student paper competition. In Volume 2, Issue 2 of M&SOM, we published the extended abstracts of the 1999 winners in the hopes that this could become an annual event. Our hopes have become a reality. Sridhar Seshadri and Ravi Anupindi, New York University, cochaired the 2000 competition. The judges for the final round were Garrett van Ryzin, Columbia University; Yehuda Bassok, University of Southern California; and Michael Pinedo, New York University. The first-prize winner received $400, while second prize received $200. Prizes were awarded at the INFORMS meeting in San Antonio, Texas in November 2000. All finalists received a $50.00 discount coupon redeemable at an \"e-tailer.\" The winners and their faculty mentors were: First Prize Jrmie Gallien, Massachusetts Institute of Technology Faculty Mentor: Lawrence M. Wein, MIT \"Design and Analysis of a Smart Market for Industrial Procurement\" Second Prize Serguei Netessine, University of Rochester Faculty Mentors: Gregory Dobson and Robert A. Shumsky, University of Rochester \"Flexible Service Capacity: Optimal Investment and the Impact of Demand Correlation\" M&SOM would like to congratulate the winners as well as all five finalists. The extended abstracts of these papers follow.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.14.9995", "e:abstract": "Numerous normative models have been developed to determine optimal inventory levels, and several articles and case studies have been written about the concerted efforts and practices adopted by manufacturing firms in the United States to reduce inventories. But little is known about whether inventories have indeed decreased in U.S. manufacturing and whether such a decrease has been restricted to a few well-publicized firms or is true at an industry level. Using data published by the U.S. Census Bureau, the authors study trends in materials, work-in-process, and finished-goods inventory ratios during the period 1961 to 1994 in 20 manufacturing industry sectors and the total U.S. manufacturing sector to determine whether a significant decrease was seen in these ratios. Further, since a great deal of momentum for inventory reduction began in the early 1980s, the authors investigate whether greater improvement was seen in the post-1980 period as compared with the pre-1980 period. We find that material and work-in-process inventories did decrease in a majority of the two-digit industry sectors from 1961 to 1994 and showed greater improvement in about half the sectors in the post-1980 period relative to the pre-1980 period. Finished-goods inventories did decrease in some industry sectors and increase in a few others but did not show a significant trend in more than half the sectors. Total manufacturing inventory ratios decreased from 1961 to 1994 at all three stages---material, work-in-process, and finished goods. However, total manufacturing inventory ratios did not improve at a higher rate during the post-1980 period as compared with the pre-1980 period in any of the three stages. Overall, the analysis provides an encouraging but somewhat mixed picture about the results of U.S. manufacturing inventory-reduction efforts.", "e:keyword": ["Manufacturing", "Inventory", "Empirical Study", "Time Series"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.25.9996", "e:abstract": "The study examines the sustainability of manufacturing quality improvements following the implementation of work teams on production lines. We posit that the impact on manufacturing quality, measured as the defect rate trajectory, is monotonically nonincreasing over time and may, more specifically, assume the shape of an inverted S-curve. Employing a longitudinal research design, we investigate four work teams over a 28-month period in a field setting. Each team corresponds to one of the four interconnected production lines in an electromechanical assembly plant operated by a Fortune 500 firm. Results of our empirical analysis support the sustainability of quality improvements associated with work team implementation and partially support the S-shaped trajectory as the particular form of sustainability. However, variations in the manufacturing quality trajectories reflect the characteristics of the work team and the production line on which each the team is instituted. From the standpoint of practice, this study highlights the importance of work-team design and implementation decisions, especially the need to be proactive in identifying and resolving initial implementation difficulties.", "e:keyword": ["Work-Teams", "Manufacturing Quality", "Field Study"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.43.10000", "e:abstract": "We analyze the problem of minimizing average inventory costs subject to fill-rate type of service-level constraints in serial and assembly production/distribution systems. We propose optimal and heuristic procedures to solve this problem. Our model and solution procedures can be used to manage the fill rate or fill rate within a \"time window\" service measures. We also relate our service-constrained model to the traditional model with back-order costs and show that it is possible to prespecify backorder cost rates to achieve desired service levels. We explore the inventory cost impact of such a practice, and we find that the cost penalty can be very high.", "e:keyword": ["Inventory/Production", "Multistage", "Serial", "Fill Rate", "Base-Stock Policy", "Solution and Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.51.9994", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.53.9993", "e:abstract": "A monopolist sells a single product to a market where the customers may be enticed to accept a delay as to when their orders are shipped. The enticement is a discounted price for the product. The market consists of several segments with different degrees of aversion to delays. The firm offers a price schedule under which the customers each self-select the price they pay and when their orders are to be shipped. When a customer agrees to wait, the firm gains advanced demand information that can be used to reduce its supply chain costs. This article shows how an optimal pricing-replenishment strategy that balances the costs due to discounted prices and the benefits due to advanced demand information can be determined.", "e:keyword": ["Supply Chain Management", "Market Segmentation", "Pricing", "Shipping Delays", "Value of Information", "Product Variety"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.68.9999", "e:abstract": "We present a model for forecast evolution that captures two notions related to forecasts: (1) forecasts are not exact; (2) forecasts over longer horizons are less certain than those over shorter horizons. We model the forecast of discrete demand as a band defined by the lower and upper bounds on demand, such that future forecasts lie within the current band. We develop a capacitated production planning model for a single product with terminal demand. We develop four heuristics for the problem and characterize their performance. In particular, two of the heuristics are optimal for the no holding-cost case. In our computational study, we analyze the performance of our heuristics and compare them to the optimal solution and to a simple heuristic that simulates common industrial practice using point forecasts. We find that two of our heuristics are very close to the optimal solution (less than 0.5% away from optimal on average under the conditions studied). Further, we consider forecast update patterns with primarily early, intermediate, and late information updates and provide insights on the effect of information update patterns on optimal costs.", "e:keyword": ["Forecast Evolution", "Production Planning", "Discrete Demand", "Capacity", "Heuristics", "Information Updates"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.1.82.9998", "e:abstract": "Consider a manufacturer or wholesaler who supplies some item to retailers facing demand rates that depend on the shelf or display space that is devoted to that product by themselves and their competitors. The manufacturer, via the use of financial levers at her disposal, wishes to coordinate this decentralized chain while making a profit. We model the physical scenario as one of constant displayed inventory level (on which demand rate depends positively) and continuous replenishment. With a single retailer, we show that to coordinate the channel and make a profit the manufacturer needs to augment the wholesale price lever by another-an inventory holding costs subsidy offered to the retailer. When multiple retailers compete in that product's market, there are two ways to envision and model the demand and market split. One assumes that market demand depends on aggregate inventory displayed, and then splits according to individual display levels. The other \"assigns\" customers to retailers according to their display levels, and then assumes that purchases are a function of the display level at the retailer selected. We characterize retailers' Nash equilibria in these models, and we explore whether the manufacturer can coordinate such channels.", "e:keyword": ["Coordination", "Shelf-Space-Dependent Demand", "Retail Competition", "Nash Equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.2.89.9991", "e:abstract": "We extend previous work evaluating the quality versus time-to-market trade-off for a single product generation to the case of multiple generations. While a single generation framework is appropriate when either the technology is not extendable or when additional launch costs outweigh benefits, we find that it is important to recognize whether a technology is extendable and explicitly consider the potential for multiple generations. We evaluate the factors determining optimal development-cycle length and intensity using a forward-looking model that allows for multiple product generations. Comparisons are made with restricted versions of the model that reflect pure single generation and sequential single generation approaches. Against an active competitor, the multiple generation approach is much more profitable, with the greatest differences in fast-moving industries. More total time is spent in development when a multiple generation model is used. Further, this time is dedicated to the more frequent introduction of improved product generations---a \"rapid inch-up\" strategy---resulting in more, higher quality products over time. Factors affecting optimal time-to-market differ substantially for the single versus multiple generation approaches. A key difference is that faster rates of quality improvement lead to longer development cycles for the single and sequential single generation models, but shorter cycles with the forward-looking multiple generation model. With a single generation, variable costs have the biggest impact on cycle length (higher costs shorten cycles), but with multiple generations, fixed costs have the biggest impact on cycle length (higher costs lead to longer cycles).", "e:keyword": ["New Product Development", "Development-Cycle Time", "Development Intensity"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.2.105.9988", "e:abstract": "We consider a manufacturing facility that produces a single item that is demanded by several different classes of customers. The inventory-related cost performance of such a system can be improved by effective allocation of production and inventories. We obtain the optimal parameters for three easily implementable allocation policies. Our results cover the case of linear backorder costs as well as fill-rate constraints. We compare the optimal performance of these control policies to gain insights into the benefits of different production and stock-allocation rules.", "e:keyword": ["Inventory/Production", "Optimal Policies", "Stock Allocation", "Queues", "Make-to-Stock System"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.2.122.9992", "e:abstract": "This paper considers a production-scheduling problem arising when there are random yields and demands as well as two sequential production periods before demand occurs. A typical instance is the production of seed corn. The paper makes three contributions. First, we verify that the objective function for the problem is smooth and concave so that optimal solutions are easily computed. Second, by examining data that represents actual costs, prices, and yields encountered in the seed corn industry, we gain some insight into the value that the second production period provides. Third, for a representative sample of hybrids from a major seed corn producer, we show that margins could be enhanced considerably by using the model. The results of this paper will assist seed corn producers in making production-scheduling decisions.", "e:keyword": ["Supply Chain", "Optimization", "Newsvendor Model", "Production-Inventory Model", "Agricultural Modeling"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.2.138.9990", "e:abstract": "Traditionally, assembly lines are laid out in a straight-line configuration where a worker covers only adjacent stations. In a U-line layout, on the other hand, two or more non-adjacent stations can be physically close to each other, making it possible for a worker to cover nonadjacent stations. This added flexibility increases the decision space for U-line layouts and can result in better balanced lines. This paper examines the impact of stochastic task times on the relative performance of U-line and straight-line layouts. Several analytical and simulation results are presented, and insights are provided to explain the difference in the performance of U-line and straight-line layouts. To summarize our main results, although balanced U-line layouts are at least as productive as balanced straight-line layouts given deterministic task times, they can be less productive given stochastic task times if they are balanced deterministically using mean times.", "e:keyword": ["Assembly Line", "U-Line", "Stochastic Times", "Throughput", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.2.151.9989", "e:abstract": "This paper models a type of vendor-managed inventory (VMI) agreement that occurs in practice called a (z, Z) contract. We investigate the savings due to better coordination of production and delivery facilitated by such an agreement. The optimal behavior of both the supplier and the retailer are characterized. The optimal replenishment and production policies for a supplier are found to be up-to policies, which are shown to be easily computed by decoupling the periods when the supplier outsources from those when the supplier does not outsource. A simple application of the newsvendor relation is used to define the retailer's optimal policy. Numerical analysis is conducted to compare the performance of a single supplier and a single retailer operating under a (z, Z) VMI contract with the performance of those operating under traditional retailer-managed inventory (RMI) with information sharing. Our results verify some observations made in industry about VMI and show that the (z, Z) type of VMI agreement performs significantly better than RMI in many settings, but can perform worse in others.", "e:keyword": ["Vendor-Managed Inventory", "Supply Chain", "Contract", "Markov Decision Process", "Multi-Echelon"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.3.175.9887", "e:abstract": "Order crossover occurs whenever replenishment orders do not arrive in the sequence in which they were placed. This paper argues that order crossover is becoming more prevalent and analyzes the dangers of ignoring it. We present an exact iterative algorithm for computing the distribution of the number of orders outstanding, and formulae for the inventory shortfall distribution (the quantity of inventory in replenishment at the start of a period) and the more common lead-time demand distribution, which are different when order crossover is possible. The lead-time demand distribution can have much higher variability than the shortfall distribution. We show that basing inventory policies on the lead-time demand distribution---rather than the shortfall distribution---can lead to significantly higher inventory cost, even if the probability of order crossover is small. We give an alternative proof to that of Zalkind (1976), which shows that the variance of shortfall is less than the variance of the standard lead-time demand.", "e:keyword": ["Inventory Policies", "Stochastic Lead Time", "Order Crossover"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.3.189.9892", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.3.3.191.9891", "e:abstract": "The complexity of managing a category assortment has grown tremendously in recent years due to the increased product turnover and proliferation rates in most categories. It is an increasingly difficult task for managers to find an effective assortment due to uncertain consumer preferences and the exponential number of possible assortments. This paper presents an empirically based modeling framework for managers to assess the revenue and lost sales implication of alternative category assortments. Coupled with a local improvement heuristic, the modeling framework generates an alternative category assortment with higher revenue. This framework, which consists of a category-purchase-incidence model and a brand-share model, is calibrated and validated using 60,000 shopping trips and purchase records. Specifically, the purchase-incidence model predicts the probability of an individual customer who purchases (and who does not purchase) from a given product category during a shopping trip. The no-purchase probability enables us to estimate lost sales due to assortment changes in the category. The brand-share model predicts which brand the customer chooses if a purchase incidence occurs in the category. Our brand-share model extends the classical Guadagni and Little model (1983) by utilizing three new brand-width measures that quantify the similarities among products of different brands within the same category. We illustrate how our modeling framework is used to reconfigure the category assortment in eight food categories for five stores in our data set. This reconfiguration exercise shows that a reconfigured category assortment can have a profit improvement of up to 25.1% with 32 products replaced. We also demonstrate how our modeling framework can be used to gauge lost sales due to assortment changes. We find the level of lost sales could range from 0.9% to 10.2% for a period of 26 weeks.", "e:keyword": ["Retailing", "Product Assortment", "Brand Reconfiguration", "Purchase Incidence", "Brand Share", "Logit Model"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.3.211.9893", "e:abstract": "Retailers must constantly strive for excellence in operations; extremely narrow profit margins leave little room for waste and inefficiency. This article reports a retailer's challenge to balance transportation, shelf space, and inventory costs. A retailer sells multiple products with stochastic demand. Trucks are dispatched from a warehouse and arrive at a store with a constant lead time. Each truck has a finite capacity and incurs a fixed shipping cost, no matter the number of units shipped. There is a per unit shelf-space cost as well as holding and backorder penalty costs. Three policies are considered for dispatching trucks: a minimum quantity continuous review policy, a full service periodic review policy, and a minimum quantity periodic review policy. The first policy ships a truck when demand since the previous shipment equals a fixed fraction of a truck's capacity, i.e., a minimum truck utilization. The exact analysis of that policy is the same as the analysis of reorder point policies for the multiechelon problem with one-warehouse, multiple retailers, and stochastic demand. That analysis is not computationally prohibitive, but the minimum quantity level can be chosen with a simple economic order quantity (EOQ) heuristic. An extensive numerical study finds the following: Either of the two periodic review policies may have substantially higher costs than the continuous review policy, in particular when the warehouse to store lead time is short; the EOQ heuristic performs quite well; the minimum quantity policy's total cost is relatively insensitive to the chosen transportation utilization, and its total cost is close to a lower bound developed for this problem.", "e:keyword": ["Inventory Management", "Stochastic Demand", "Joint Setup Cost"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.3.230.9889", "e:abstract": "We consider the problem of determining (for a short lifecycle) retail product initial and replenishment order quantities that minimize the cost of lost sales, back orders, and obsolete inventory. We model this problem as a two-stage stochastic dynamic program, propose a heuristic, establish conditions under which the heuristic finds an optimal solution, and report results of the application of our procedure at a catalog retailer. Our procedure improves on the existing method by enough to double profits. In addition, our method can be used to choose the optimal reorder time, to quantify the benefit of leadtime reduction, and to choose the best replenishment contract.", "e:keyword": ["Retailing", "Inventory Replenishment", "Stochastic Dynamic Programming", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.3.242.9888", "e:abstract": "We analyze the problem of determining inventory and pricing decisions in a two-period retail setting when an opportunity to refine information about uncertain demand is available. The model extends the newsvendor problem with pricing by allowing for multiple suppliers, the pooling of procurement resources, and more general informational dynamics. One contribution is the solution procedure: We show that all decisions (up to 7 in all, including recourse decisions) can be determined uniquely as a function of a surrogate first-period decision called the stocking factor. Hence, the two-period decision problem with recourse reduces to a search for one decision variable. A second contribution is the policy implications: We find that the cost of learning is (1) a consequence of censored information because, on the margin, learning is free if full information is guaranteed; (2) measured in the form of an increased stocking factor; and (3) shared with the consumer in the form of a higher selling price when demand uncertainty is additive. A third contribution is the application of the results to three motivating examples: a market research problem in which a product is introduced in a test market prior to a widespread launch; a global newsvendor problem in which a seasonal product is sold in two different countries with nonoverlapping selling seasons; and a minimum-quantity commitment problem in which procurement resources for multiple purchases may be pooled.", "e:keyword": ["Inventory Management", "Pricing", "Informational Dynamics", "Operations/Marketing Interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.3.264.9890", "e:abstract": "The relationship between customer loyalty and the order procurement and order fulfillment processes of electronic retailers is empirically examined in this paper. The study sample contains data from 52 electronic food retailers. After controlling for the retailers' product categories based on design flexibility, our regression analysis results indicate that three order procurement variables---website navigation, product information, and price; and three order fulfillment variables---product availability, timeliness of delivery, and ease of return have significant association with customer loyalty. In terms of their relative contributions toward improving customer loyalty, these variables can be ordered in a descending order as follows: ease of return, timeliness of delivery, website navigation, product availability, price, and product information.", "e:keyword": ["Order Procurement", "Order Fulfillment", "Electronic Retailing", "Empirical Analysis", "Customer Loyalty"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.273.9969", "e:abstract": "This research demonstrates that operations agility---defined as the ability to excel simultaneously on operations capabilities of quality, delivery, flexibility, and cost in a coordinated fashion---is a viable option for retail banks encountering increasing environmental change. The question of whether there is empirical evidence that services, specifically retail banks, display the characteristics of agility like their manufacturing counterparts is open to debate. Conventional wisdom in operations management posits that most successful services trade off one capability for another. Drawing from the resource-based view of the firm, combinative capabilities view, and the cybernetics work of Ashby (1958), theoretical arguments suggest the contrary. The agility paradigm is viable in environments calling for a mix of strategic responses. Applying cluster analytic techniques to a sample of retail banks, using capabilities as taxons, we identify four strategic service groups: agile, traditionalists, niche, and straddlers. Our empirical results provide thematic explanations consistent with theory that account for how the agile strategic group offers a unique configuration of service concept, resource competencies, strategic choices, and business orientation. Profiles of the operations strategies of each strategic service group suggest that each group has found a fit between what certain segments of the market may want and what they have to offer. In particular, we found that the agile group exhibited greater resource competencies than its counterparts, requiring greater investments in infrastructure and technology. Consistent with theory, agile banks performed better over time on an absolute measure of return on assets.", "e:keyword": ["Agility", "Retail Banking", "Service Operations Strategy", "Empirical Research"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.293.9971", "e:abstract": "We consider a simple supply-chain contract in which a manufacturer sells to a retailer facing a newsvendor problem and the lone contract parameter is a wholesale price. We develop a mild restriction satisfied by many common distributions that assures that the manufacturer's problem is readily amenable to analysis. The manufacturer's profit and sales quantity increase with market size, but the resulting wholesale price depends on how the market grows. For the cases we consider, we identify relative variability (i.e., the coefficient of variation) as key: As relative variability decreases, the retailer's price sensitivity decreases, the wholesale price increases, the decentralized system becomes more efficient (i.e., captures a greater share of potential profit), and the manufacturer's share of realized profit increases. Decreasing relative variability, however, may leave the retailer severely disadvantaged as the higher wholesale price reduces his profitability. We explore factors that may lead the manufacturer to set a wholesale price below that which would maximize her profit, concentrating on retailer participation in forecasting and retailer power. As these and other considerations can result in a wholesale price below what we initially suggest, our base model represents a worst-case analysis of supply-chain performance.", "e:keyword": ["Supply-Chain Contracting", "Channel Coordination", "Supply-Chain Performance"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.306.9970", "e:abstract": "Many firms face environments with long manufacturing leadtimes, great product variety, and uncertain, nonstationary demand. A challenge is how to plan production and inventories to provide the best customer service at the least cost. In this paper, we first describe an application at Teradyne in which we implemented an inventory hedge to protect against cyclic demand variability. Based on this experience, we develop a model to better understand the efficacy of this hedging policy. We consider an inventory system for a single aggregate product with a Markov-modulated Poisson demand process. We provide approximate performance measures for this system and develop an optimization problem for determining the size and location of an intermediate-decoupling inventory. We use this optimization to show the value of an intermediate-decoupling inventory as a hedge for cyclic demand environments.", "e:keyword": ["Multiechelon Inventory Policy", "Cyclic Demand", "Nonstationary Demand", "Master Schedule"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.321.9968", "e:abstract": "We examine leadtime-quoting polices for minimizing average lead time subject to customer service constraints on fill rate, tardiness, or relative tardiness in simple systems with exponential and normal processing times. By studying the resulting safety leadtimes implied by each policy we gain insight into why some policies perform more robustly with respect to different measures of customer service than do others. This analysis suggests that a simple constant safety leadtime policy should work reasonably well under most conditions. A series of simulation experiments of more complex production environments indicates that the constant safety leadtime policy does indeed exhibit robust performance. This, plus the fact that it is extremely simple to adapt to a wide range of production environments, makes it an attractive basis for real-world leadtime-quoting systems.", "e:keyword": ["Leadtime Quoting", "Customer Service", "Available to Promise", "Capable to Promise"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.337.9974", "e:abstract": "We study the coordination of a two-echelon distribution system where a vendor distributes a single product to a set of independent buyers. The problem is analyzed as a Stackelberg game in which the vendor acts as the leader and buyers act as followers. A simple strategy is developed for the vendor to employ a uniform quantity-discount policy to coordinate buyers' replenishment times by the power-of-two policy. Solution procedures are developed for the equilibrium strategy. It is shown that time coordination generally has a substantial benefit for the vendor, although the benefit to the buyers may be limited. Furthermore, uniform quantity discounts to all buyers are normally feasible but not sufficient to achieve perfect channel coordination when buyers act independently. The proposed strategy obtains a high proportion of the maximum benefit under perfect channel coordination.", "e:keyword": ["Quantity Discounts", "Channel Coordination", "Supply Chain Management"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.349.9973", "e:abstract": "We develop a general framework for the analysis of decentralized distribution systems. We carry the analysis in terms of a simplified model which entails N retailers who face stochastic demands and hold stocks locally and/or at one or more central locations. An exogenously specified fraction of any unsatisfied demand (demand greater than locally available stock) at a retailer could be satisfied using excess stocks at other retailers and/or stocks held at a central location. We consider inventory ordering and allocation decisions. The operational decisions of inventory and allocation of stocks and the financial decision of allocation of revenues/costs must be made in a way consistent with the individual incentives of the various independent retailers. We develop a \"coopetitive\" framework for the sequential decisions of inventory and allocation. We introduce the notion of claims that allows us to separate the ownership (with decision rights) and the location of inventories in the system. For the cooperative shipping and allocation decision, we use the concept of core and develop sufficient conditions for the existence of the core. For the inventory decision, we develop conditions for the existence of a pure strategy Nash Equilibrium. For this decentralized system, we show that there exists an allocation mechanism that achieves the first-best solution for inventory deployment and allocation. We develop conditions under which the first-best equilibrium will be unique. Our model can be easily generalized to include complicated ownership structures such as \"super dealers,\" partnerships, \"inventory speculators,\" and situations in retail e-commerce settings such as \"click-through arrangements,\" separation of \"demand generators,\" and \"fulfillment houses,\" etc. It can also be applied to situations involving capacity allocations and product substitutions.", "e:keyword": ["Decentralization", "Distribution", "Game Theory", "Inventory", "e-Business"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.369.9967", "e:abstract": "Every week, motion picture exhibitors must decide whether to keep or replace the movies playing in their theaters in light of the past week's sales data. This decision is complex because of the dynamic decision environment, the uncertainty of demand, the complex revenue-sharing terms between the retailer and the distributor, the need to commit to new movies for several weeks, and the competitive release patterns of movies. We formulate this problem as a Markov Decision Process (MDP) model, using it to obtain replacement policies for the exhibitor. We examine the effect of differences in quality and quantity of available movies, and their respective release dates on the returns from model-based normative solutions. We also show that two practical heuristics are significantly outperformed by the optimal policy for the MDP model. We conclude by applying the model to industry data.", "e:keyword": ["Movies", "Retailing", "Replacement", "Markov Decision Processes"]}, {"@id": "http://dx.doi.org/10.1287/msom.3.4.387.9972", "e:abstract": "Mathematical programming techniques were used in the steel industry as early as 1958, and many applications of optimization in steel production have been reported since then. In this survey, we summarize published applications in the largest steel plants by type, including national steel planning, product-mix optimization, blending, scheduling, set covering, and cutting stock.", "e:keyword": ["Steel", "Applications", "Mathematical Programming", "Optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.2.290", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.4.288", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.7.286", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.12.284", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.17.293", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.21.289", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.25.287", "e:abstract": "We present a classified bibliography of the literature in the area of forecast, solution, and rolling horizons primarily in operations management problems. Each one of over 200 selected papers is categorized on five dimensions that identify the horizon type, the model type (deterministic or stochastic), the sources of the horizon, the methods used to obtain horizon results, and the subject area of the paper. The majority of the papers treat dynamic problems in inventory management, production planning, capacity expansion, machine replacement, and warehousing. We discuss the relationship of the horizon results with the theory and practice of rolling-horizon procedures and future research directions.", "e:keyword": ["Multiperiod Problems", "Forecast Horizons", "Rolling Horizons"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.44.294", "e:abstract": "Deferring capacity expansion may be a cost effective decision when there is anticipation of cheaper capacity in the near future and/or the current demand is too low to justify an immediate expansion. This paper studies a finite-horizon capacity expansion problem (CEP) with deferred capacity expansion. The operating cost and the cost of holding unused capacity in each period depend on the time when the capacity is acquired, and the shortage cost depends on the time when the shortage occurred. Our model is a generalization of the Wagner-Whitin formulation of the CEP and an extension (with deferred expansion) of two other polynomially solvable CEPs in the literature. We explore structural properties of the problem and develop an efficient dynamic programming algorithm to solve the problem in polynomial time.", "e:keyword": ["Capacity Planning", "Capacity Expansion with Shortage", "Dynamic Progamming"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.55.285", "e:abstract": "In this paper we consider a cooperative, two-level supply chain consisting of a retailer and a supplier. As in many practical settings, the supply chain members progressively observe market signals that enable them to explain future demand. The demand itself evolves according to an auto-regressive time series. We examine three types of supply chain configurations. In the first setting, the retailer and the supplier coordinate their policy parameters in an attempt to minimize systemwide costs, but they do not share their observations of market signals. In the second setting, resembling many original vendor-managed inventory (VMI) programs, the supplier takes the full responsibility of managing the supply chain's inventory, but the retailer's observations of market signals are not transferred to him. In our third setting, reminiscent of collaborative forecasting and replenishment partnerships, inventory is managed centrally, and all demand related information is shared. We propose a set of stylized models to study the three settings and use them to provide managerial insights into the value of information sharing, VMI, and collaborative forecasting.", "e:keyword": ["Collaborative Forecasting", "CPFR", "Supply Chain Management", "VMI"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.1.75.291", "e:abstract": "This paper considers a periodic-review, two-echelon inventory system with one central warehouse and several retailers facing stochastic demand. The retailers replenish their stock from the warehouse, which in turn places orders at an outside supplier with infinite capacity. Transportation times and costs are constant. No ordering costs are considered, but warehouse replenishments must be multiples of a given batch quantity. The objective is to find policies that minimize holding and backorder costs. The standard approach to approximately solve this problem is to use a \"balance\" assumption, meaning that negative stock allocations to the retailers are possible. This approach may lead to considerable errors for problems with large differences between the retailers in terms of service requirements and demand characteristics. To handle such situations we suggest and evaluate two computationally tractable heuristics: the Virtual Assignment ordering rule for warehouse replenishments and the Two-step Allocation rule for allocating stock from the warehouse to the retailers. Numerical evidence shows that, especially when combining these heuristics, we obtain considerable improvements for many problems over the standard approach. Savings of up to 50% have been recorded.", "e:keyword": ["Supply Chain Inventories", "Stochastic Demand", "Ordering And Allocation"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.2.99.283", "e:abstract": "We study the value of information, production flexibility, and supplier flexibility for a good for which an initial and a subsequent order may be placed. We consider a Bayesian model of demand in which the unknown mean demand rate is assumed to have a prior, which is a mixture of two normal distributions corresponding to the demand forecast for an innovative (fashion) good. We develop three models of production flexibility: a static model requiring initial placement of both orders, a partially dynamic model requiring a fixing of the time that the second order will be made, and a fully dynamic model with no restrictions on ordering. Supplier flexibility is modeled through supply lead times. We observe that the magnitude of the savings from the static to the fully flexible model, corresponding to the sum of the values of information and production flexibility, reflects all sources of variability: differences between demand means of the prior mixture, variability within each prior, and variability about the observed mean. We observe that as the difference between high and low demand cases increases, the value of information increases, though for long lead times, production flexibility is required to take advantage of the updated information. Further, we observe that the greater the uncertainty within each prior distribution, the greater the value of information relative to the value of production flexibility, particularly for long lead times. However, the greater the uncertainty around the mean demand, which is the uncertainty that cannot be resolved through observation, the lower the value of information. Finally, we observe that the value of supply flexibility grows initially in a concave then convex manner as a function of the supply lead times.", "e:keyword": ["Supply Chain Management", "Flexibility", "Bayesian Forecasting", "Lead time", "Flexibility"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.2.114.282", "e:abstract": "This study is motivated by a process-reengineering problem in personal computer (PC) manufacturing, i.e., to move from a build-to-stock operation that is centered around end-product inventory towards a configure-to-order (CTO) operation that eliminates endproduct inventory. In fact, CTO has made irrelevant the notion of preconfigured machine types and focuses instead on maintaining the right amount of inventory at the components. CTO appears to be the ideal operational model that provides both mass customization and a quick response time to order fulfillment. To quantify the inventory-service trade-off in the CTO environment, we develop a nonlinear optimization model with multiple constraints, reflecting the service levels offered to different market segments. To solve the optimization problem, we develop an exact algorithm for the important case of demand in each market segment having (at least) one unique component, and a greedy heuristic for the general (nonunique component) case. Furthermore, we show how to use sensitivity analysis, along with simulation, to fine-tune the solutions. The performance of the model and the solution approach is examined by extensive numerical studies on realistic problem data. We present the major findings in applying our model to study the inventory-service impacts in the reengineering of a PC manufacturing process.", "e:keyword": ["Supply Chain Optimization", "Inventory/Production", "Assembly systems", "Configure-to-Order", "Postponement", "Stochastic Models"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.2.133.281", "e:abstract": "Most manufacturing systems are large and complex and operate in an uncertain environment. One approach to managing such systems is that of hierarchical decomposition. This paper reviews the research devoted to proving that a hierarchy based on the frequencies of occurrence of different types of events in the systems results in decisions that are asymptotically optimal as the rates of some events become large compared to those of others. The paper also reviews the research on stochastic optimal control problems associated with manufacturing systems, their dynamic programming equations, existence of solutions of these equations, and verification theorems of optimality for the systems. Manufacturing systems that are addressed include single-machine systems, dynamic flowshops, and dynamic jobshops producing multiple products. These systems may also incorporate random production capacity and demands, and decisions such as production rates, capacity expansion, and promotional campaigns. Related computational results and areas of applications are also presented. A more detailed survey is available at www.utdallas.edu/~sethi/ITORMS/index.html.", "e:keyword": ["Manufacturing systems", "Hierarchical control", "Stochastic control", "Dynamic programming", "Viscosity solution", "Singular perturbations"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.3.171.7754", "e:abstract": "We investigate the role of options (contingent claims) in a buyer-supplier system. Specifically using a two-period model with correlated demand, we illustrate how options provide flexibility to a buyer to respond to market changes in the second period. We also study the implications of such arrangements between a buyer and a supplier for coordination of the channel. We show that, in general, channel coordination can be achieved only if we allow the exercise price to be piecewise linear. We develop sufficient conditions on the cost parameters such that linear prices coordinate the channel. We derive the appropriate prices for channel coordination which, however, violate the individual rationality constraint for the supplier. Contrary to popular belief (based on simpler models) we show that credit for returns offered by the supplier does not always coordinate the channel and alleviate the individual rationality constraint. Credit for returns are useful only on a subset of the feasibility region under which channel coordination is achievable with linear prices. Finally, we demonstrate (numerically) the benefits of options in improving channel performance and evaluate the magnitude of loss due to lack of coordination.", "e:keyword": ["Supply Contracts", "Real Options", "Coordination", "Flexibility"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.3.208.7753", "e:abstract": "The most common model to support workforce management of telephone call centers is the M/M/N/B model, in particular its special cases M/M/N (Erlang C, which models out busy signals) and M/M/N/N (Erlang B, disallowing waiting). All of these models lack a central prevalent feature, namely, that impatient customers might decide to leave (abandon) before their service begins. In this paper, we analyze the simplest abandonment model, in which customers' patience is exponentially distributed and the system's waiting capacity is unlimited (M/M/N + M). Such a model is both rich and analyzable enough to provide information that is practically important for call-center managers. We first outline a method for exact analysis of the M/M/N + M model, that while numerically tractable is not very insightful. We then proceed with an asymptotic analysis of the M/M/N + M model, in a regime that is appropriate for large call centers (many agents, high efficiency, high service level). Guided by the asymptotic behavior, we derive approximations for performance measures and propose \"rules of thumb\" for the design of large call centers. We thus add support to the growing acknowledgment that insights from diffusion approximations are directly applicable to management practice.", "e:keyword": ["Tele-Queues", "Erlang C", "Erlang A", "Telephone Call and Contact Centers", "Multiserver Exponential Queues", "Workforce Management or Staffing", "Queues with Abandonment", "Diffusion Approxiamtion"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.3.228.7755", "e:abstract": "Increasing product variety through the use of alternate package sizes is a commonly observed mechanism in the grocery industry. Under such a scheme, however, the response to pricing decisions for each of the different package sizes is affected by how customers make demand choices. We build a demand model in which customers react smart to retail promotions through stockpiling and package size switching. The demand model combines a customer choice model with a model in which customers differ in their stockpiling and reservation price levels. We utilize data from the German grocery industry for an empirical fitting of the model. We then develop a store-level inventory model for each SKU and optimize price promotions to maximize expected profit. We show the benefit of capturing the smart customer response to price promotions by demonstrating its impact on the reduced inventory costs. We use the model to generate a number of managerial implications of the model for the German grocery environment.", "e:keyword": ["Retail Promotions", "Price Transparency", "Supply Chain Management", "Grocery Industry"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.4.241.5729", "e:abstract": "Replenishment and pricing strategies are traditionally determined by entirely separate units of a firm, the former by production and the latter by marketing. In a large organization production and marketing are traditionally measured in terms of performance criteria appropriate and relevant within the world in which they operate, rather than in terms of overall company performance. We consider a situation where the headquarters uses a simple linear transfer price between the two functions to govern the transactions. The misaligned incentives among these functional managers are caused by a transfer price between them that distorts the marginal production cost and revenue, as well as by a misallocation of cost: Marketing's pricing strategy influences expected leftover inventory, but only production incurs the cost. The misalignment can be mitigated through the following two ways. First, if production commits to a service level instead of an inventory level, both production and marketing, and hence the firm as a whole, are better off. Second, the same improvement can be achieved through organizational changes so that marketing becomes the dominant function. We also propose a mechanism that aligns the functional managers' incentives to be compatible to the firm.", "e:keyword": ["Inventory", "Pricing", "Coordination", "Manufacturing/marketing interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.4.258.5732", "e:abstract": "Split-case sorting systems are used in many retail supply chains where items must be distributed in less-than-case quantities, such as orders shipped directly to customers by catalogers and e-tailers or shipments made in less-than-case quantities from a distribution center to a retail store. These systems are particularly effective in order-packing systems where the same item is needed for multiple orders. Items are inducted into a circular sorting conveyor system one unit at a time and then delivered to an order-packing bin designated for a particular customer or retail store. We develop analytical performance models that incorporate the stochastic operating conditions faced by these systems. Our model allows system designers to predict the sorting capacity for different system configurations. More importantly, we use the model to develop insight into the system design and operation.", "e:keyword": ["Distribution", "Retail", "Sortation", "Material handling", "Throughput models", "Approximate queueing models"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.4.275.5730", "e:abstract": "We consider the problem of managing inventories and dynamically adjusting retailer prices in distribution systems with geographically dispersed retailers. More specifically, we analyze the following single item, periodic review model. The distribution of demand in each period, at a given retailer, depends on the item's price according to a stochastic demand function. These stochastic demand functions may vary by retailer and by period. The replenishment process consists of two phases: In some or all periods, a distribution center may place an order with an outside supplier. This order arrives at the distribution center after an order leadtime and is then, in the second phase, allocated to the retailers. Allocations arrive after a second allocation leadtime. We develop an approximate model that is tractable and in which an optimal policy of simple structure exists. The approximate model thus provides analytically computable approximations for systemwide profits and other performance measures. Moreover, the approximate model allows us to prove how various components of the optimal strategy (i.e., prices and order-up-to levels) respond to shifts in the model parameters, e.g., to shifts in the retailers' demand functions. In addition, we develop combined pricing, ordering, and allocation strategies and show that the system's performance under these strategies is well gauged by the above approximations. We use this model to assess the impact of different types of geographic dispersion on systems with dynamically varying prices and how different system parameters (e.g., leadtimes, coefficients of variation of individual retailers' demand, price elasticities) contribute to this impact. Similarly, we use the model to gauge the benefits of coordinated replenishments under dynamic pricing, and how these benefits increase as the allocation decisions of the systemwide orders to individual retailers are postponed to a later point in the overall replenishment leadtime. We report on a comprehensive numerical study based on data obtained from a nationwide department store chain.", "e:keyword": ["Inventory", "Pricing strategies", "Two-echelon system"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.4.296.5731", "e:abstract": "We consider the problem of scheduling operations in bufferless robotic cells that produce identical parts. The objective is to find a cyclic sequence of robot moves that minimizes the long-run average time to produce a part or, equivalently, maximizes the throughput rate. The robot can be moved in simple cycles that produce one unit or, in more complicated cycles, that produce multiple units. Because one-unit cycles are the easiest to understand, implement, and control, they are widely used in industry. We analyze one-unit cycles for a class of robotic cells called constant travel-time robotic cells. We complete a structural analysis of the class of one-unit cycles and obtain a polynomial time algorithm for finding an optimal one-unit cycle. Constant travel-time robotic cells are used in real manufacturing operations that the authors have encountered during their interactions with companies. The results and the analysis in this paper offer practitioners (i) a tool to experiment with and study the design of a proposed robotic cell during a prototyping exercise, (ii) a lower bound on the throughput of a robotic cell to help them make an informed assessment of the ultimate productivity level, and (iii) a benchmark throughput level (for comparison purposes) for robotic cells whose operations differ slightly from those discussed in this paper.", "e:keyword": ["Maufacturing", "Robotic cell", "Identical parts", "Polynomial time algorithm"]}, {"@id": "http://dx.doi.org/10.1287/msom.4.4.313.5728", "e:abstract": "We introduce a class of models, called newsvendor networks, that allow for multiple products and multiple processing and storage points and investigate how their single-period properties extend to dynamic settings. Such models provide a parsimonious framework to study various problems of stochastic capacity investment and inventory management, including assembly, commonality, distribution, flexibility, substitution and transshipment. Newsvendor networks are stochastic models with recourse that are characterized by linear revenue and cost structures and a linear input-output transformation. While capacity and inventory decisions are locked in before uncertainty is resolved, some managerial discretion remains via ex-post input-output activity decisions. Ex-post decisions involve both the choice of activities and their levels and can result in subtle benefits. This discretion in choice is captured through alternate or nonbasic activities that can redeploy inputs and resources to best respond to resolved uncertain events. Nonbasic activities are never used in a deterministic environment; their value stems from discretionary flexibility to meet stochastic demand deviations from the operating point. The optimal capacity and inventory decisions balance overages with underages. Continuing the classic newsvendor analogy, the optimal balancing conditions can be interpreted as specifying multiple critical fractiles of the multivariate demand distribution; they also suggest appropriate measures for and trade-offs between product service levels. This paper shows that the properties of optimal newsvendor network solutions extend to a dynamic setting under plausible conditions. Indeed, we establish dynamic optimality of inventory and capacity policies for the lost sales case. Depending on the nonbasic activities, this also extends to the backordering case. Analytic- and simulation-based solution techniques and graphical interpretations are presented and illustrated by a comprehensive example that features discretionary input commonality and a flexible processing resource.", "e:keyword": ["Inventory", "Capacity", "Assembly", "Commonality", "Distribution", "Flexibility", "Substitution", "Transshipment", "Multiple products"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.1.12764", "e:abstract": "We model an isolated portion of a competitive supply chain as a M/M/1 make-to-stock queue. The retailer carries finished goods inventory to service a Poisson demand process, and specifies a policy for replenishing his inventory from an upstream supplier. The supplier chooses the service rate, i.e., the capacity of his manufacturing facility, which behaves as a single-server queue with exponential service times. Demand is backlogged and both agents share the backorder cost. In addition, a linear inventory holding cost is charged to the retailer, and a linear cost for building production capacity is incurred by the supplier. The inventory level, demand rate, and cost parameters are common knowledge to both agents. Under the continuous-state approximation where the M/M/1 queue has an exponential rather than geometric steady-state distribution, we characterize the optimal centralized and Nash solutions, and show that a contract with linear transfer payments replicates a cost-sharing agreement and coordinates the system. We also compare the total system costs, the agents' decision variables, and the customer service levels of the centralized versus Nash versus Stackelberg solutions.", "e:keyword": ["Make-to-stock queue", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.18.12757", "e:abstract": "In this paper, we study a profit-maximizing firm selling two substitutable products in a price and time sensitive market. The products differ only in their prices and delivery times. We assume that there are dedicated capacities for each product and that there is a standard industry delivery time for the regular (slower) product. The objective of the firm is to determine the delivery time of the express (faster) product and appropriately price the two products, taking into consideration the impact of delivery time reduction on capacity requirements and costs. We develop a model that integrates pricing and delivery time decisions with capacity requirements and costs, and study scenarios where the firm is constrained in capacity for none, one, or both product(s). We show how product differentiation decisions are influenced by capacity costs, and how the firm should adapt its differentiation strategy in response to a change in its operating dynamics. We first identify a market characteristic that governs the optimal pricing structure. We then show that the degree of product differentiation depends on both the absolute, as well as the relative values of the capacity costs. Provided that the capacity cost differential remains the same, higher capacity costs induce less time differentiation and less price differentiation. An increase in capacity cost differential increases price differentiation, but decreases time differentiation. The optimal prices depend, in addition to the above, on the market characteristic. We find that prices can actually decrease when the firm incurs capacity-related costs. We also explore the impact of substitutability on product differentiation, and illustrate our results in a numerical study.", "e:keyword": ["Product differentiation", "Pricing", "Delivery-time guarantees", "Time-based competition", "Capacity management", "Substitution"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.37.12761", "e:abstract": "This paper compares the commonly used periodic review, replenishment interval, order-up-to (R, T ) policy to the continuous review, reorder point, order quantity, (Q, r) model. We show that long-run average cost function for the single-product (R, T ) policy has a structure similar to that of the (Q, r) model. Consequently, many of the useful properties of the latter model are applicable. In particular, the optimal cost is insensitive to the choice of the replenishment interval, T, provided the optimal order-up-to level, R, corresponding to T is used. For instance, a suboptimal T obtained from a deterministic analysis increases costs by no more than 6.125%. For continuous demand, we analytically prove that use of a (R, T)policy instead of the optimal policy increases costs by at most 41.42% in the worst case. Computational experiments on Poisson demand demonstrate that the average-case relative error of using a (R,T)policy is under 7.5%. This relative error is lower when the demand rate and leadtime are high and the fixed order costs are either very low or very high. When coordination of order placement epochs is desirable, the (R,T) policy may sometimes be preferred to the (Q, r) policy. In this context, we illustrate application of our single-product results to more complex systems. In particular, we show that a simple power-of-two, (R,T) based heuristic for the stochastic multiproduct joint replenishment problem has a worst-case performance guarantee of 1.5. A similar result is explored for a special case of a two-echelon serial inventory system", "e:keyword": ["Inventory/production", "Joint replenishment", "Worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.55.12765", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.59.12763", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.63.12762", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.67.12760", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.70.12759", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.5.1.74.12758", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.5.2.79.16071", "e:abstract": "Telephone call centers are an integral part of many businesses, and their economic role is significant and growing. They are also fascinating sociotechnical systems in which the behavior of customers and employees is closely intertwined with physical performance measures. In these environments traditional operational models are of great value---and at the same time fundamentally limited---in their ability to characterize system performance. We review the state of research on telephone call centers. We begin with a tutorial on how call centers function and proceed to survey academic research devoted to the management of their operations. We then outline important problems that have not been addressed and identify promising directions for future research.", "e:keyword": ["Telephone Call Center", "Contact Center", "Teleservices", "Telequeues", "Capacity Management", "Staffing", "Hiring", "Workforce Management Systems", "ACD Reports", "Queueing", "Abandonment", "Erlang C", "Erlang B", "Erlang A", "QED Regime", "Time-Varying Queues", "Call Routing", "Skills-Based Routing", "Forecasting", "Data Mining"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.2.142.16073", "e:abstract": "Component sharingusing the same version of a component across multiple productsis an approach adopted by many assembled-product manufacturers to achieve high final product variety with lower component variety and cost. This paper presents a methodology for determining which versions of a set of related components should be offered to optimally support a defined finished product portfolio. We develop optimization models that determine which versions of each component should be introduced and which of these versions each product should use to minimize design and production costs. This approach is appropriate for components with a relatively low impact on consumers perceptions about product differentiation, which can be shared across a set of products if they meet the most stringent performance requirements in the set. We illustrate our procedure on automotive braking systems, but also discuss its applicability to other components and industries. We identify three conceptually different organizational approaches to component sharing: a coordinated projects approach that requires higher-level organizational echelons above the individual project, a project-by-project approach that does not, and a hybrid partially coordinated approach. We use our model to examine how the gain from the coordinated projects approach relative to the project-by-project approach varies with the number of component versions in consideration, warranty costs, complexity costs, and demand variability. Further, we use our model to highlight the risk of using simplistic heuristics to determine design sequence within a component system in a partially coordinated approach.", "e:keyword": ["Component Systems Sharing", "Managing Variety", "Assembled Products Design"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.2.157.16072", "e:abstract": "Customers and downstream supply chain partners often place, or can be induced to place, orders in advance of future requirements. We show how to optimally incorporate advance demand information into periodic-review, multiechelon, inventory systems in series. While the state space for series systems appears to be formidably large, we decompose the problem across locations, as in Clark and Scarf (1960), and reduce the state space at each location by using modified echelon inventory positions (that nets known requirements). We prove the optimality of state-dependent, echelon base-stock policies for finite and infinite horizon problems. We also show that myopic policies are optimal and very easy to compute when costs and demands are stationary. We provide managerial insights into the value of advance demand information through a numerical study.", "e:keyword": ["Multiechelon", "Stochastic Inventory System", "Facilities-in-Series", "Advance Demand Information"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.2.176.16074", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.5.3.179.16032", "e:abstract": "Operations management (OM) and human resources management (HRM) historically have been very separate fields. In practice, operations managers and human resource managers interact primarily on administrative issues regarding payroll and other matters. In academia, the two subjects are studied by separate communities of scholars publishing in disjoint sets of journals, drawing on mostly separate disciplinary foundations. Yet, operations and human resources are intimately related at a fundamental level. Operations are the context that often explains or moderates the effects of human resource activities such as pay, training, communications, and staffing. Human responses to OM systems often explain variations or anomalies that would otherwise be treated as randomness or error variance in traditional operations research models. In this paper, we probe the interface between operations and human resources by examining how human considerations affect classical OM results and how operational considerations affect classical HRM results. We then propose a unifying framework for identifying new research opportunities at the intersection of the two fields.", "e:keyword": ["Multidisciplinary", "Cross-Training", "Work Design", "Scheduling", "Low Inventory", "Behavioral Science", "Motivation", "Turnover", "Worker Performance", "Worker Attitude"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.3.203.16031", "e:abstract": "In this paper, we examine the research and results of dynamic pricing policies and their relation to revenue management. The survey is based on a generic revenue management problem in which a perishable and nonrenewable set of resources satisfy stochastic price sensitive demand processes over a finite period of time. In this class of problems, the owner (or the seller) of these resources uses them to produce and offer a menu of final products to the end customers. Within this context, we formulate the stochastic control problem of capacity that the seller faces: How to dynamically set the menu and the quantity of products and their corresponding prices to maximize the total revenue over the selling horizon.", "e:keyword": ["Revenue Management", "Dynamic Pricing"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.3.230.16033", "e:abstract": "We consider an assemble-to-order (ATO) system: Components are made to stock by production facilities with finite capacities, and final products are assembled only in response to customers' orders. The key performance measures in this system, such as order fill rates, involve evaluation of multivariate probability distributions, which is computationally demanding if not intractable. The purpose of this paper is to develop computationally efficient performance estimates. We examine several ideas scattered in diverse literatures on approximations for multivariate probability distributions, and determine which approach is most effective in the ATO application. To do so, we first tailor different approximation ideas to the ATO setting to derive performance bounds, and then compare these bounds theoretically and numerically. The bounds also allow us to make connections between capacitated and uncapacitated ATO systems and gain various insights.", "e:keyword": ["Assemble-to-Order Manufacturing Systems", "Performance Measures", "Setwise Lower/Upper Bounds"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.3.252.16030", "e:abstract": "An assembler needs sets of components, each produced by a different supplier. To produce the components and assemble the final product, the firms involved need to construct their individual production capacities before observing the actual demand. The firms have an incentive scheme (contract) to induce a \"proper\" capacity build-up. The key parameters of the contract are the set of transfer prices the assembler pays each supplier for a unit of its component. We consider two game settings as to how the terms of the contract are determined. The first is one where the assembler sets the prices, and the second is for the suppliers to simultaneously select the prices each wants to charge for its component. We first characterize the optimal capacity decision when the system is centralized, and then derive the decentralized equilibrium capacities under each of the two game settings. We show that the decentralized channel performances depend heavily on system structure/parameters. In particular, under the first setting, the performance improves as the assembler's share of the systemwide capacity cost increases and it is not affected by the number of suppliers in the system, while under the second setting, the performance degrades both in the assembler's share of capacity cost and in the number of suppliers. We show that the first setting dominates the second in terms of system performance if and only if the assembler's share of capacity cost is larger than the reciprocal of the number of firms involved.", "e:keyword": ["Assembly Systems", "Capacity Management", "Supply Chain Coordination"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.4.269.24882", "e:abstract": "This paper reviews the literature on strategic capacity management concerned with determining the sizes, types, and timing of capacity investments and adjustments under uncertainty. Specific attention is given to recent developments to incorporate multiple decision makers, multiple capacity types, hedging, and risk aversion. Capacity is a measure of processing abilities and limitations and is represented as a vector of stocks of various processing resources, while investment is the change of capacity and includes expansion and contraction. After discussing general issues in capacity investment problems, the paper reviews models of capacity investment under uncertainty in three settings: The first reviews optimal capacity investment by single and multiple risk-neutral decision makers in a stationary environment where capacity remains constant. Allowing for multiple capacity types, the associated optimal capacity portfolio specifies the amounts and locations of safety capacity in a processing network. Its key feature is that it is unbalanced; i.e., regardless of how uncertainties are realized, one typically will never fully utilize all capacities. The second setting reviews the adjustment of capacity over time and the structure of optimal investment dynamics. The paper ends by reviewing how to incorporate risk aversion in capacity investment and contrasts hedging strategies involving financial versus operational means.", "e:keyword": ["Capacity", "Investment", "Expansion", "Planning", "Real Options", "Hedging", "Risk", "Mean-Variance"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.4.303.24883", "e:abstract": "The profitability of remanufacturing depends on the quantity and quality of product returns and on the demand for remanufactured products. The quantity and quality of product returns can be influenced by varying quality-dependent acquisition prices, i.e., by using product acquisition management. Demand can be influenced by varying the selling price. We develop a simple framework for determining the optimal prices and the corresponding profitability. We motivate and illustrate our framework using an application from the cellular telephone industry.", "e:keyword": ["Remanufacturing", "Product Acquisition", "Econometric Models"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.4.317.24881", "e:abstract": "In this paper, we investigate price-only contracts in supply chain capacity procurement games. For a two-party supply chain, comprising a manufacturer and a supplier that both invest in capacity, we prove the existence of a class of coordinating price-only contracts that arbitrarily allocate the supply chain profit. Moreover, if the supplier's reservation profit is below a certain threshold, the manufacturer's optimal contract is a quantity-premium price-only schedule, that is, the average wholesale price per unit increases in the order size. We prove that the manufacturer prefers simple piecewise-linear quantity-premium contracts to linear contracts and show numerically that such contracts are highly efficient. We extend our results for piecewise-linear price schedules to N-supplier assembly systems. We also enrich the voluntary compliance regime of Cachon and Lariviere (2001). With this enrichment, we prove that share-the-pain contracts, such as firm commitment and options contracts, increase supplier capacity in the full information case, a result that contrasts with that of Cachon and Lariviere. Finally, we investigate when a manufacturer prefers single-breakpoint quantity premiums to firm commitments.", "e:keyword": ["Supply Chain", "Capacity", "Pricing", "Coordination"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.4.334.24886", "e:abstract": "We consider product and process design problems (hereafter collectively called process design problems) that address issues associated with the assessment of optimum levels for process inputs that influence multiple-process performance measures. While this problem context encompasses many possible applications, we focus primarily on multiple-response design problems that have been widely studied in the quality improvement and quality management literature. For such problems, several optimization criteria have been proposed, including maximization of process yield, maximization of process capability, minimization of process costs, etc. In this research, we propose a method that accounts for many of these criteria via a procedure that interacts with and relies on the preferences of a decision maker (DM). The interactive procedure evolves from the convergence of three areas of research: notably, the research in multiple-response design, the research in multicriteria optimization, and recent developments in global optimization. The proposed interactive method is illustrated and comparatively assessed via two well-known problems in multiple-response design. Although the interactive procedure is developed for application in multiple-response design, it is not limited to this problem context. The concepts and methods developed in this research have applicability to problems that can be characterized by process inputs and process performance, such as supply chain management and multidisciplinary design optimization.", "e:keyword": ["Multiple-Response Design", "Multicriteria Optimization", "Global Optimization", "Interactive Decision Making", "Process Capability"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.4.348.24884", "e:abstract": "The idea of price-directed control is to use an operating policy that exploits optimal dual prices from a mathematical programming relaxation of the underlying control problem. We apply it to the problem of replenishing inventory to subsets of products/locations, such as in the distribution of industrial gases, so as to minimize long-run time average replenishment costs. Given a marginal value for each product/location, whenever there is a stockout the dispatcher compares the total value of each feasible replenishment with its cost, and chooses one that maximizes the surplus. We derive this operating policy using a linear functional approximation to the optimal value function of a semi-Markov decision process on continuous spaces. This approximation also leads to a math program whose optimal dual prices yield values and whose optimal objective value gives a lower bound on system performance. We use duality theory to show that optimal prices satisfy several structural properties and can be interpreted as estimates of lowest achievable marginal costs. On real-world instances, the price-directed policy achieves superior, near optimal performance as compared with other approaches.", "e:keyword": ["Inventory Routing", "Approximate Dynamic Programming", "Price-Directed Operations", "Semi-Markov Decision Processes"]}, {"@id": "http://dx.doi.org/10.1287/msom.5.4.372.24885", "e:abstract": "We noticed an error in the upper bound on the optimal system stock in Boyaci and Gallego (2001). We provide a procedure to compute the correct bound.", "e:keyword": ["Inventory/Production", "Multistage", "Serial", "Fill Rate", "Base-Stock Policy", "Bounds", "Solution and Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0044", "e:abstract": "A regular feature of Manufacturing & Service Operation Management, \"In this issue\" briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Editor's comments", "Overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0018", "e:abstract": "Plagued by high labor costs, low profitability margins, airspace and airport congestion, high capital and operating costs, security and safety concerns, and complex and large-scale management and operations decisions, the airline industry has armed its planners with sophisticated optimization tools to improve decision making and increase airline profits. In this paper, we describe optimization approaches for airline schedule planning, demonstrating how optimization can facilitate the management of a diverse and finite set of expensive, highly constrained resources. We focus on the art and science of modeling and solving these problems, providing illustrative examples of the associated impacts and challenges, and highlighting effective techniques that might be applicable to problems arising in other industries.", "e:keyword": ["Airline scheduling", "Network design", "Large-scale optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0030", "e:abstract": "We consider a simple game in which strategic agents select arrival times to a service facility. Agents find congestion costly and, hence, try to arrive when the system is underutilized. Working in discrete time, we characterize pure-strategy Nash equilibria for the case of ample service capacity. In this case, agents try to spread themselves out as much as possible and their self-interested actions will lead to a socially optimal outcome if all agents have the same well-behaved delay cost function. For even modest sized problems, the set of possible pure-strategy Nash equilibria is quite large, making implementation potentially cumbersome. We consequently examine mixed-strategy Nash equilibria and show that there is a unique symmetric Nash equilibrium. Not only is this equilibrium independent of the number of agents and their individual delay cost functions, the arrival pattern it generates approaches a discrete-time Poisson process as the number of agents and arrival points gets large. Our results extend to the case of time varying preferences. With an appropriate initialization, the results also extend to a system with limited capacity. Our model lends support to the traditional literature on managing service systems. This work has generally ignored customers strategically choosing arrival times. Rather it is commonly assumed that customers seek service according to some well-behaved process (e.g., that interarrival times follow a renewal process). We show that assuming Poisson arrivals is an acceptable assumption even with strategic customers if the population is large and the horizon is long.", "e:keyword": ["Poisson process", "Service management", "Queuing", "Game theory", "Mixed-strategy equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0027", "e:abstract": "A supply system's fill rate is the fraction of demand that is met from on-hand inventory. This paper presents formulas for the fill rate of periodic review supply systems that use base-stock-level policies. The first part of the paper contains fill-rate formulas for a single-stage system and general distributions of demand. When demand is normally distributed, an exact expression uses only the standard normal distribution and density functions, and a good approximation uses only the standard normal distribution function. The second part of the paper derives the probability distribution of the finished goods inventory level for serial systems with buffer inventories between stages. This distribution leads to fill-rate formulas and the conclusion that shorter supply chains have higher fill rates.", "e:keyword": ["Fill rate", "Service level", "Supply system", "Base-stock-level policy", "Multistage", "Supply chain", "Logistics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0029", "e:abstract": "As widely accepted performance measures in supply chain management practice, frequency-based service levels such as fill rate and stockout rate are often considered in supply contracts under vendor-managed-inventory (VMI) programs. Using a decentralized two-party capacitated supply chain model consisting of one manufacturer and one supplier in a VMI environment, we demonstrate that supplier's service level is in general insufficient for the manufacturer to warrant the desired service level at the customer end. The method by which the supplier achieves her service level to the manufacturer also affects customer service level. By developing bounds on the customer service level, we show that the expected backorders at the supplier should also be taken into account. We suggest a supply contract that offers a menu of different combinations of supplier's service level and expected backorders according to a linear function. Under this contract, the manufacturer can control the end customer service regardless of how the supplier manages her inventory. The supplier has complete flexibility on which combination of the two quantities on the menu to choose according to her own cost functions. Because it does not require any detailed information on supplier's operational characteristics nor her costs, this kind of contract is expected to be easily implementable. In addition, we derive an estimate of the customer service level in terms of the new measures. Our findings have direct implications to supply chain metrics in general: The local service levels are insufficient measures to guarantee the system wide performance. Alternative local measures and/or coordination mechanisms should be employed to achieve desired system performance. Our analysis illustrates a possible way to explore such alternative measures.", "e:keyword": ["Service-level guarantees", "Supply contracts", "Supplier performance measures", "Vendor-managed inventory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0026", "e:abstract": "The dynamic pricing problem concerns the determination of selling prices over time for a product whose demand is random and whose supply is fixed. We approach this problem in a novel way by formulating a dynamic optimization model in which the demand function is isoelastic but the random demand process is quite general. Ultimately, what we find is a strong parallel between the dynamic pricing problem and dynamic inventory models. This parallel leads to a reinterpretation of the dynamic pricing problem as a price-setting newsvendor problem with recourse, which is useful not only because it yields insights into the optimal solution, but also because it leads to additional insights into how pricing recourse affects the actions and profits of a price-setting newsvendor. We make contributions in three areas: First, we develop structural properties that define an optimal pricing strategy over a finite horizon and investigate how that policy impacts a newsvendor's optimal procurement policy and optimal expected profit. Second, we establish a practical and efficient algorithm for computing the optimal prices. Third, we examine how market parameters affect the optimal solution through a series of numerical experiments that utilize the algorithm.", "e:keyword": ["Revenue management", "Dynamic pricing", "Newsvendor model", "Pricing recourse"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0041", "e:abstract": "As is our tradition at the journal, we are pleased to publish the extended abstracts from the winners of the 2003 MSOM Society Student Paper Competition. We do this to celebrate the achievements of these young scholars and provide you with the opportunity to learn about their work in more detail. The 2003 prize committee was chaired by Professor Gerard Cachon from the University of Pennsylvania. The other committee members were: Dan Adelman (University of Chicago), Narendra Agrawal (Santa Clara University), Yossi Aviv (Washington University), Rene Caldentey (New York University), Wedad Elmaghraby (Georgia Institute of Technology), Noah Gans (University of Pennsylvania), Roman Kapuscinski (University of Michigan), Pinar Keskinocak (Georgia Institute of Technology), Constantinos Maglaras (Columbia University), Joe Mazzola (Georgetown University), Serguei Netessine (University of Pennsylvania), Michael Pinedo (New York University), Erica Plambeck (Stanford University), Nils Rudi (University of Rochester), Sergei Savin (Columbia University), Kevin Shang (Duke University), Terry Taylor (Columbia University), Christian Terwiesch (University of Pennsylvania), Brian Tomlin (University of North Carolina), Tunay Tunca (Stanford University).", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0044", "e:abstract": "A regular feature of Manufacturing & Service Operation Management, \"In this issue...\" briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Editor's comments", "Overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0033", "e:abstract": "Biological cells run complicated and sophisticated production systems. The study of the cell's production technology provides us with insights that are potentially useful in industrial manufacturing. When comparing cell metabolism with manufacturing techniques in industry, we find some striking commonalities, but also some important differences. Like today's well-run factories, the cell operates a very lean production system, assures quality at the source, and uses component commonality to simplify production. While we can certainly learn from how the cell accomplishes these parallels, it is even more interesting to look at how the cell operates differently. In biological cells, all products and machines are built from a small set of common building blocks that circulate in local recycling loops. Production equipment is added, removed, or renewed instantly when needed. The cell's manufacturing unit is highly autonomous and reacts quickly to a wide range of changes in the local environment. Although this \"organic production system\" is very different from existing manufacturing systems, some of its principles are applicable to manufacturing, and indeed, a few can even be seen emerging today. Thus, the organic production system can be viewed as a possible scenario for the future of manufacturing.", "e:keyword": ["Organic production", "Bionics", "Manufacturing strategy", "Local production", "Part commonality", "Volume flexibility", "Recycling"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0028", "e:abstract": "The terms pull and lean production have become cornerstones of modern manufacturing practice. However, although they are widely used, they are less widely understood. In this paper, we argue that while the academic literature has steadily revealed the richness of the pull/lean concepts, the practitioner literature has progressively simplified these terms to the point that serious misunderstandings now exist. In hopes of reducing confusion, we offer general, but precise definitions of pull and lean. Specifically, we argue that pull is essentially a mechanism for limiting WIP, and lean is fundamentally about minimizing the cost of buffering variability.", "e:keyword": ["Pull production", "Just-in-time", "CONWIP", "Lean manufacturing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0034", "e:abstract": "Managers often engage in forecast updating with the expectation that forecast updating reduces expected shortage and inventory costs. One undesirable effect of forecast updating is that it may lead to the bullwhip effect, a phenomenon where the variability of demand increases as one moves up the supply chain. The bullwhip effect can be undesirable for the supplier because more volatile orders from the downstream stage can be very costly to the supplier. It can make it more difficult for the supplier to forecast demand, and it can lead to large fluctuations in supplier production levels from period to period. Using \"stale\" or old forecasts may sound foolish, but their judicious use in a two-stage supply chain can improve fulfillment from the upstream stage to the downstream stage and reduce the fluctuations in production levels. We study a two-stage supply chain where the demand process is nonstationary and both stages use an adaptive base stock policy. We propose a policy that uses old forecasts to set base stock levels at the downstream stage while using current forecasts to communicate upcoming orders from the downstream stage to the upstream stage. We study a decentralized supply chain setting, and we find that our policy can reduce the expected supply chain inventory and shortage costs and significantly reduce the fluctuations in production levels compared to that of using current information. We also study a cooperative supply chain setting and, surprisingly, we find in numerical examples that our proposed policy results in very small increases in the expected systemwide inventory and shortage costs compared to a systemwide optimal policy, while reducing the fluctuations in production levels.", "e:keyword": ["Inventory control", "Forecasting", "Forecast revision", "Bullwhip effect", "Production smoothing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0031", "e:abstract": "Effective distribution using collaborative fulfillment networks requires coordination among the multiple participating firms at different stages of the supply chain. Acting independently, supply chain partners fail to weigh the cost burden they impose on upstream suppliers when their replenishment order quantities vary from period to period. This paper explores a new approach to coordinate multiple stages in the supply chain by controlling, through appropriate downstream inventory management, the demand variability that is propagated to upstream stages. We propose and analyze a coordinated inventory replenishment policy that uses \"order smoothing\" to reduce order-size variability and thus reduce overall system costs, including both inventory and transportation costs. We characterize the optimal parameter values for smoothing alternatives (such as exponential smoothing and moving weighted average policies), assess their economic benefits, and develop insights regarding supply chain contexts that might benefit most significantly from reducing the variability of orders to upstream stages. Using the distribution network for specialty brand appliances as an illustrative example, we demonstrate the potential cost savings that order-smoothing strategies can yield compared to the uncoordinated case when individual firms separately minimize their costs. The magnitude of savings depends on several factors, including the variability in consumer demand, level of product variety, and degree of inventory aggregation in the distribution system. Based on our analytical results, we develop a framework to assess cost reduction opportunities through variability control for different supply chain scenarios.", "e:keyword": ["Supply chain coordination", "Inventory control", "Variability reduction", "Distribution systems", "Collaborative fulfillment", "Order smoothing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0032", "e:abstract": "In this paper we extend forecast band evolution and capacitated production modelling to the multiperiod demand case. In this model, forecasts of discrete demand for any period are modelled as bands and defined by lower and upper bounds on demand, such that future forecasts lie within the current band. We develop heuristics that utilize knowledge of demand forecast evolution to make production decisions in capacitated production planning environments. In our computational study we explore the efficiency of our heuristics, and also explore the impact of seasonality on demand and availability of information updates.", "e:keyword": ["Forecast evolution", "Production planning", "Discrete demand", "Capacity", "Heuristics", "Information updates"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0042", "e:abstract": "This paper shows that an ARMA demand generates an ARMA order history when ordering decisions are made based on an order-up-to policy. The order history preserves the autoregressive structure of the demand and transforms its moving average structure according to a simple algorithm. We apply this ARMA-in-ARMA-out property to examine the evolution of the demand signal in supply chains. Its practical implications are discussed in the context of quantifying the bullwhip effect, coordinating forecasting, and evaluating information sharing.", "e:keyword": ["Supply chain", "Time-series forecasting", "Demand propagation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0053", "e:abstract": "A regular feature of Manufacturing & Service Operation Management, In This Issue briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Special issue editors' comments", "Overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0051", "e:abstract": "The supply chain in the food and agribusiness sector is characterized by long supply lead times combined with significant supply and demand uncertainties, and relatively thin margins. These challenges generate a need for management efficiency and the use of modern decision technology tools. We review some of the literature on applications of decision technology tools for a selected set of agribusiness problems and conclude by outlining what we see as some of the significant new problems facing the industry. It is our hope that we will stimulate interest in these problems and encourage researchers to work on solving them.", "e:keyword": ["Applied optimization", "Agriculture", "Crop planning"]}, {"@id": "http://dx.doi.org/10.1287/msom.1030.0024", "e:abstract": "This paper studies production planning with random yield and demand. It is a departure from previous studies of random yield in that it defines the sale price and the purchasing cost as exogenous and increasing with decreasing yield. While this behavior can be observed in various industries (e.g., citrus), the paper focuses on the olive oil industry as its application. Production of olive oil is a challenging business as olives grow every other year; thus, a risky investment is involved. A new practice among olive oil producers involves leasing farm space from farmers to grow olives. When the yield of olives is low (because of weather, disease, etc.), the oil producer gets a second chance to buy olives from other farmers at a unit cost varying with the yield. In this case, the sale price of olive oil increases in the market place because of the reduced supply. When the yield is high, the company uses some of its olives for olive oil production and some are salvaged. After olives are pressed and olive oil is produced, the company experiences an uncertain demand. The paper makes four contributions: First, it is shown that the objective function is concave in the amount of farm space leased, so that the first-order conditions provide the globally optimal solution. Second, it illustrates how the total production of olive oil changes with the yield. Third, it proves that the optimal amount of farm space leased decreases under the presence of a second (and reliable) source of supply. Finally, unlike traditional yield papers, the fourth result shows that increased yield variance does not necessarily increase the optimal amount of farm space to be leased when there is a second chance to obtain supplies.", "e:keyword": ["Production planning", "Yield and demand uncertainty", "Stochastic programming", "Yield-dependent price", "Yield-dependent cost", "Olive oil production"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0035", "e:abstract": "Gathering the harvest represents a complex managerial problem for agricultural cooperatives involved in harvesting and processing operations: balancing the risk of overinvestment with the risk of underproduction. The rate to harvest crops and the corresponding capital investment are critical strategic decisions in situations where poor weather conditions present a risk of crop loss. In this article, we discuss a case study of the Concord grape harvest and develop a mathematical model to control harvest risk. The model involves differentiation of a joint probability distribution that represents risks associated with the length of the harvest season and the size of the crop. This approach is becoming popular as a means of dealing with complex problems involving operational and supply chain risk. Significant cost avoidance, in the millions of dollars, results from practical implementation of the Harvest Model. Using real data, we found that the Harvest Model provides lower-cost solutions in situations involving moderate variability in both the length of season and the crop size as compared to solutions based on imposed risk policies determined by management.", "e:keyword": ["Harvest risk", "Agriculture"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0043", "e:abstract": "Professional market advisory services provide specific advice (advisory programs) to grain producers on how to market their commodities, and assist them in their efforts to manage price risk. Previous studies analyzed the effectiveness of individual services and could find only weak evidence that these services help farmers improve their returns beyond market prices. The present study applies portfolio theory to determine an efficient combination of advisory programs using a nonlinear integer programming framework. The optimization results provide some evidence that an efficient portfolio provides significantly greater risk and return benefits compared to individual programs and external benchmarks. In a holdout period analysis, efficient portfolios have superior average return performance, but they fail to dominate the relevant benchmarks in terms of mean and variance. However, these out-of-sample results should be considered cautiously, given the small number of observations available.", "e:keyword": ["Agricultural market advisory services", "Corn", "Soybeans", "Efficient portfolios", "Nonlinear integer programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0045", "e:abstract": "We analyze planning and scheduling of multiproduct batch operations in the food-processing industry. Such operations are encountered in many applications including manufacturing of sorbitol, modified starches, and specialty sugars. Unlike discrete manufacturing, batch sizes in these operations cannot be set arbitrarily, but are often determined by equipment size. Multiple batches of the same product are often run sequentially in campaigns to minimize setup and quality costs. We consider a multiproduct, single-stage, single-equipment batch-processing scheme and address the problem of determining the timing and duration of product campaigns to minimize average setup, quality, and inventory holding costs over a horizon. We formulate the deterministic, static version of this problem over an infinite horizon. We show that, in general, a feasible, finite, cyclic solution may not exist. We provide sufficient conditions for the existence of a finite cycle, use single-product problems to provide lower bounds on the costs for the multiproduct problem, and use them to test heuristics developed for this problem. Next, we modify this formulation to incorporate fixed cycles that may be necessary due to factors such as product obsolescence, perishability, or contracts with customers. We do this by allowing for disposal of excess stock so that finite cycles are always feasible, though they might not be optimal; we also develop bounds and heuristic solution procedures for this case. These methods are applied to data from a leading food-processing company. Our results suggest that our methods could potentially reduce total annual costs by about 7.7% translating to an annual savings of around $7 million.", "e:keyword": ["Process industries", "Batch reactor scheduling", "Economic lot scheduling problem", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0053", "e:abstract": "A regular feature of Manufacturing & Service Operations Management, In This Issue briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Editor's comments", "Overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0055", "e:abstract": "In this article a challenge is provided to the operations management community to broaden our perspective from the traditional, functional one to the cross-functional (interdisciplinary) view of process management. At the same time, the challenge is to de-emphasize mathematical optimization and instead seek improvements through better representations of problems or situations of real interest to management. I believe that adoption of these suggestions will enhance the opportunities and reputation of our research and educational activities.", "e:keyword": ["Process management", "Interdisciplinary", "Improvement/redesign"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0056", "e:abstract": "This paper develops and analyzes a queueing model to examine the role of patient choice on the high rate of organ refusals in the kidney transplant waiting system. The model is an M/M/1 queue with homogeneous patients and exponential reneging. Patients join the waiting system and organ transplants are reflected by the service process. In addition, unlike the standard M/M/1 model, each service instance is associated with a variable reward that reflects the quality of the transplant organ, and patients have the option to refuse an organ (service) offer if they expect future offers to be better. Under an assumption of perfect and complete information, it is demonstrated that the queueing discipline is a potent instrument that can be used to maximize social welfare. In particular, first-come-first-serve (FCFS) amplifies patients' desire to refuse offers of marginal quality, and generates excessive organ wastage. By contrast, last-come-first-serve (LCFS) contains the inefficiencies engendered by patient choice and achieves optimal organ utilization. A numerical example calibrated using data from the U.S. transplantation system demonstrates that the welfare improvements possible from a better control of patient choice are equivalent to a 25% increase in the supply of organs.", "e:keyword": ["Kidney allocation", "Queues", "Priority", "Last-come-last-served", "Stochastic games", "Efficiency-equity trade-off"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0050", "e:abstract": "In this paper, we consider the case when two profit-maximizing firms enter a new market with a competing product that has a finite (and known) life cycle. Both firms make design decisions simultaneously without information about the other firm's decisions. The order of entry is a function of the two firms' product design levels and design capabilities. The first firm entering the market sets a monopoly price for its product and enjoys a monopoly situation until the second firm enters the market. When the second firm enters the market, both firms simultaneously set (or reset) their product prices knowing the design of both products at that time (and we assume those prices are fixed for the remainder of the product's life). We develop a game-theoretic model that represents the new product introduction process and show that a subgame-perfect Nash equilibrium occurs under certain conditions defined by the expected product life span, product cost, development time, and customer preferences. Our model shows that product differentiation always arises at equilibrium due to the joint effects of resource utilization, price competition, and product life cycle. A critical parameter for our model is a product-specific index B that we define; we show how it can be easily calculated from existing data. We then use a numerical example to illustrate managerial implications for a new product development process when the product life span is finite. We show that the strategy of time-based competition is the natural result of firms' improving development capability, reducing product cost, and increasing customer preference. In other words, it is not wise for profit-maximizing firms to arbitrarily shorten product life cycle for the sake of competition, because all firms are worse off. Our results also indicate that the first entrant into the market does not necessarily earn the greatest profit, and that a firm with low-cost advantage or fast design capability might not choose to come to market first to maximize its profit in the product life cycle.", "e:keyword": ["New product development", "Game theory", "Product index"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0054", "e:abstract": "A flexible product is a menu of two or more alternative, typically substitute, products offered by a constrained supplier using a sales or booking process. The supplier reserves the right to assign customers who purchase a flexible product to one of the alternatives at a time near the end of the booking process. An example would be an airline offering a morning flight consisting of specific flights serving the same market. Flexible products are currently offered by a number of industries including air cargo, tour operators, and Internet advertising. Flexible products have the advantage of increasing overall demand and enabling better capacity utilization at the cost of potentially cannibalizing high-fare demand for specific products. This paper introduces the concept of flexible products and derives conditions and algorithms for the management of a single flexible product consisting of two specific products. We use numerical simulation to illustrate the benefits from offering flexible products and discuss extension of the approach to more general settings.", "e:keyword": ["Revenue management", "Flexibility", "Stochastic models", "Real options"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0048", "e:abstract": "This paper studies service-delivery design in settings where firms engage in value-creation activities that have the objective of generating additional revenue from customer interactions. The paper provides a general modelling framework to analyze the ties between market segmentation decisions, incentives, and process performance in such service-delivery systems. The firm is modelled as a single-server queue, in a principal-agent framework. Customers have different value-generation potentials whose realizations are observed by the server but not by the manager of the firm. The manager determines a market segmentation scheme given an overall customer value-generation profile, which divides customers into two groups (high and low), and also determines a service level for each segment. The server decides which of the two available service levels (high and low) to provide for each customer, given a compensation scheme offered by the manager. The optimal market segmentation decision, optimal service-level choice, and a set of optimal linear incentive contracts that enable their implementation are characterized. The robustness of these strategies is explored with respect to model parameters and assumptions. It is shown that a market segmentation scheme that combines revenue generation concerns with their process implications is essential for success. Characteristics of appropriate incentive schemes are identified.", "e:keyword": ["Call centers", "Cross-selling", "Incentives", "Queueing", "Marketing-operations interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0047", "e:abstract": "In this paper I review the component commonality literature through a management-accounting lens, focusing on the cost effects of an increase in the use of the same version of a component across multiple products. The bulk of this literature is of a theoretical nature, for example, analytical models, programming models, or conjectures based on casual observations of practice. Some of this literature purports, especially in introductions to the topic, that cost generally decreases with increasing commonality. However, based on a review of the theoretical literature using an activity-based costing framework and distinguishing between cost-driver and cost-rate effects, I conclude that the cost picture is more subtle. In other words, it is too early to make any general statement about the effect of increasing commonality on total costs. Moving to the limited empirical literature on the topic, consisting of case studies (sometimes combined with simulation) and empirical research on larger data sets, the conclusion that there is even more room for future research becomes evident.", "e:keyword": ["Component commonality", "Unique versus common components", "New product development", "Activity-based costing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0053", "e:abstract": "A regular feature of Manufacturing & Service Operations Management, \"In This Issue\" briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Editor's comments", "Issue overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0062", "e:abstract": "We evaluate operations management--related journals based on a novel indicator of journal quality---the Author Affiliation Index (AAI). We explain the basic rationale behind the AAI, as well as its advantages and disadvantages with respect to other such indicators of journal quality. We provide a specific recipe for its calculation and apply it to 27 journals in which researchers in the field of operations management might wish to publish. We compare the resulting journal rankings to those from published survey reports and citation analyses and test AAI for sensitivity to its inputs. We find the rankings from AAI to be consistent with other studies and to be robust with respect to changes in inputs.", "e:keyword": ["Measures", "Journal quality", "Operations management journals"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0052", "e:abstract": "We consider a call center model with m input flows and r pools of agents; the m-vector \\lambda  of instantaneous arrival rates is allowed to be time dependent and to vary stochastically. Seeking to optimize the trade-off between personnel costs and abandonment penalties, we develop and illustrate a practical method for sizing the r agent pools. Using stochastic fluid models, this method reduces the staffing problem to a multidimensional newsvendor problem, which can be solved numerically by a combination of linear programming and Monte Carlo simulation. Numerical examples are presented, and in all cases the pool sizes derived by means of the proposed method are very close to optimal.", "e:keyword": ["Capacity sizing", "Call centers", "Fluid analysis", "Multidimensional newsvendor", "Nonstationarity", "Queueing", "Random environment", "Stochastic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0063", "e:abstract": "We connect the mix-flexibility and dual-sourcing literatures by studying unreliable supply chains that produce multiple products. We consider a firm that can invest in product-dedicated resources and totally flexible resources. Product demands are uncertain at the time of resource investment, and the products can differ in their contribution margins. Resource investments can fail, and the firm may choose to invest in multiple resources for a given product to mitigate such failures. In comparing a single-source dedicated strategy with a single-source flexible strategy, we refine the common intuition that a flexible strategy is strictly preferred to a dedicated strategy when the dedicated resources are costlier than the flexible resource. We prove that this intuition is correct if the firm is risk neutral or if the resource investments are perfectly reliable. The intuition can be wrong, however, if both of these conditions fail to hold, because there is a resource-aggregation disadvantage to the flexible strategy that can dominate the demand pooling and contribution-margin benefits of the flexible strategy when resource investments are unreliable and the firm is risk averse. We investigate the influence that resource attributes, firm attributes, and product-portfolio attributes have on the attractiveness of various supply-chain structures that differ in their levels of mix flexibility and diversification, and we investigate the influence these attributes have on the optimal resource investments within a given supply-chain structure. Our results indicate that the appropriate levels of diversification and flexibility are very sensitive to the resource costs and reliabilities, the firm's downside risk tolerance, the number of products, the product demand correlations and the spread in product contribution margins.", "e:keyword": ["Reliability", "Flexibility", "Dual sourcing", "Loss aversion", "Risk"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0058", "e:abstract": "In this paper we analyze the impact of supply-side externalities existing among downstream retailers on supply chain performance. Namely, multiple retail firms face stochastic demand, purchase the product from the upstream wholesaler, and make stocking decisions that affect all other retailers in the same echelon. Two sources of inefficiencies exist in such a supply chain: One is double marginalization and the other is externalities among retailers. Whereas double marginalization always leads to inventory understocking at the retail echelon, we find that the implications of externalities are more complex, because different externalities can improve or deteriorate supply chain performance, relative to the situation without externalities. We show that the effect of externalities depends critically on whether the stocking decisions of retailers exhibit positive (complementarity) or negative (substitutability) externalities, and whether retailers are managed centrally or competitively. Under complementarity, competing retailers tend to understock the product (compared with the centralized inventory management at the retail level), thus aggravating the double-marginalization effect. This is the opposite of what happens under substitutability, where competing retailers tend to overstock the product, thus compensating for the double-marginalization effect. Hence, we conclude that supply chain coordination between retailers and the wholesaler is most important when there is downstream competition that exhibits complementarity. From the wholesaler's point of view, competition among retailers is preferable over centralization of retailers when externalities are negative, and vice versa when externalities are positive. Moreover, with competition on complements, both retailers and the wholesaler have incentives to coordinate the supply chain.", "e:keyword": ["Supply chain", "Game theory", "Competition", "Nash equilibrium", "Complementarity", "Supermodularity"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0064", "e:abstract": "The standard treatment of fill rate relies on stationary and serially independent demand over an infinite horizon. Even if demand is stationary, managers are held accountable for performance over a finite horizon. In a finite horizon, the fill rate is a random variable. Studying the distribution is relevant because a vendor may be subject to financial penalty if she fails to achieve her target fill rate over a specified finite period. It is known that for a zero lead time, base-stock model, the expected value of a finite-horizon fill rate exceeds the long-run fill rate. In this paper, I investigate the behavior of the distribution of the finite-horizon fill rate when a stationary base-stock policy is used to control inventory. For a vendor facing a finite-horizon, fill-rate-level contract and using a stationary stocking policy, I examine how the the length of the review horizon (i.e., monthly or quarterly), the demand distribution, and the cost of failing to meet the target affect the stocking decision.", "e:keyword": ["Inventory", "Base-stock policy", "Service-level constraint"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0075", "e:abstract": "As is our tradition at the journal, we are pleased to publish the extended abstracts from the winners of the 2004 MSOM Student Paper Competition. We do this to celebrate the achievements of these young scholars and provide you with the opportunity to learn about their work in more detail. The 2004 prize committee was chaired by Professor Phil Kaminsky (University of California, Berkeley). The other committee members were: Naren Agrawal (University of Santa Clara), Hyun-Soo Ahn (University of Michigan), Damian Beil (University of Michigan), Fernando Bernstein (Duke University), Izak Duenyas (University of Michigan), Wedad Elmaghraby (Georgia Institute of Technology), Jeremie Gallien (Massachusetts Institute of Technology), Teck Ho (University of Pennsylvania), Seyed Iravani (Northwestern University), Ananth Iyer (Purdue University), Eric Johnson (Dartmouth College), Roman Kapuscinski (University of Michigan), Pinar Keskinocak (Georgia Institute of Technology), Anton Kleywegt (Georgia Institute of Technology), zalp zer (Stanford University), Georgia Perakis (Massachusetts Institute of Technology), Alan Scheller-Wolf (Carnegie Mellon University), Sridhar Seshadri (New York University), Max Shen (University of California, Berkeley), David Simchi-Levi (Massachusetts Institute of Technology), Jay Swaminathan (University of North Carolina), Terry Taylor (Columbia University), Beril Toktay (INSEAD), Scott Webster (Syracuse University), and David Wu (Lehigh University).", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0053", "e:abstract": "A regular feature of Manufacturing & Service Operations Management, \"In This Issue\" briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Editor's comments", "Issue overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0061", "e:abstract": "We address the problem of hedging inventory risk for a short life cycle or seasonal item when its demand is correlated with the price of a financial asset. We show how to construct optimal hedging transactions that minimize the variance of profit and increase the expected utility for a risk-averse decision maker. We show that for a wide range of hedging strategies and utility functions, a risk-averse decision maker orders more inventory when he or she hedges the inventory risk. Our results are useful to both risk-neutral and risk-averse decision makers because (1) the price information of the financial asset is used to determine both the optimal inventory level and the hedge, (2) this enables the decision maker to update the demand forecast and the financial hedge as more information becomes available, and (3) hedging leads to lower risk and higher return on inventory investment. We illustrate these benefits using data from a retailing firm.", "e:keyword": ["Demand forecasting", "Financial hedging", "Newsboy model", "Real options", "Risk aversion"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0059", "e:abstract": "One way to organize workers that lies between traditional assembly lines, where workers are specialists, and craft assembly, where workers are generalists, are \"bucket brigades.\" We describe how one firm used bucket brigades as an intermediate strategy to migrate from craft assembly to assembly lines. The adoption of bucket brigades led to a narrowing of tasks for each worker and thus accelerated learning. The increased production more than compensated for the time lost when workers walk back to get more work, which was significant in this implementation. To understand the trade-offs in migrating from craft to assembly lines, we extend the standard model of bucket brigades to capture hand-off and walk-back times.", "e:keyword": ["Bucket brigades", "Assembly line", "Work sharing", "Dynamical system", "Self-organizing system"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0066", "e:abstract": "Motivated by the incentive programs that have been offered by energy companies under tight market conditions in the past few years, we consider a production control problem in which time alternates randomly between peak and nonpeak periods. During peak periods, the energy supplier offers the manufacturing firm---the energy user---an incentive program to reduce its energy usage by shutting down its production facility. Participation in the incentive program, however, is totally voluntary, and the user firm is rewarded for each unit of time that it participates in the program. We consider two problems that face the manufacturing firm. The first is whether it is worth shutting down production to participate in the incentive program when it is offered, and in which part (portion) of the peak period the firm should participate. The second problem is how the firm should decide on its production policy in both the peak and nonpeak periods in the presence of such an incentive program. In this paper, we provide simple models to give insight into the nature of these problems. Two cases are studied. In the first case, the peak duration is assumed to be exponentially distributed, and in the second, its length becomes known at the beginning of a peak period. In both cases, the occurrence times of the peak periods are uncertain. We characterize the optimal production and shutdown policy for both the peak and nonpeak periods. We also study the effect of seasonality on the optimal control policies.", "e:keyword": ["Continuous-review inventory systems", "Incentive program", "Optimal production strategy", "Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0060", "e:abstract": "This article studies a single-location inventory model with random leadtimes. Inventory is replenished by a single supplier who, at the time a replenishment order is received, knows exactly when the order will be delivered. In other words, the supplier knows the leadtime for every replenishment order. Suppose the single-location inventory system is managed by a retailer. The objective of this article is to quantify the value of the information about leadtimes to the retailer. This is achieved by considering and comparing the performances of two scenarios whether or not the supplier shares his leadtime information with the retailer. Numerical evidence suggests that the value of leadtime information can be significant.", "e:keyword": ["Stochastic inventory system", "Random leadtime", "Information sharing", "Value of information"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0074", "e:abstract": "Decentralized supply chains are traditionally order focused: stage managers focus on meeting replenishment orders of downstream stages; market demand information is relayed up through these orders, and therefore is subject to costly delay and distortion. This paper shows that sharing real-time sales data across all stages and a change in focus to meeting customer demand can mitigate performance impairment caused by the order focus. We show that a change of managerial focus in a decentralized chain can be made by measuring stages' performance based on their respective echelon stocks, which only depends on how well they respond to the market demand. A demand-focused measurement scheme can be made incentive compatible with the knowledge of the demand distribution; a heuristic scheme independent of the demand distribution can be used to achieve near incentive compatibility.", "e:keyword": ["Stochastic inventory systems", "Multiechelon", "Coordination", "Order delays", "Information sharing", "Beer Game"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0053", "e:abstract": "A regular feature of Manufacturing & Service Operations Management, \"In This Issue\" briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Editor's comments", "Issue overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0077", "e:abstract": "This paper addresses the strategic impact of modular design on the optimal length and price of a differentiated product line. We represent consumer demand with a Bayesian logit model. We also break operations costs into product design and production components. Our analysis shows that reducing product development costs via modular design always makes it attractive to offer greater product variety. However, reducing production costs can sometimes motivate a reduction in variety for a risk-averse producer in a multiple-segment market. We also characterize the impacts of degree of modularity and production cost on price markup and market share. Finally, we show that the optimal product line length is monotonic in risk attitude and the monotonic weak majorization, partial order on product assortment.", "e:keyword": ["Majorization", "Modular design", "Multinomial logit", "Product differentiation", "Product variety"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0083", "e:abstract": "When designing supply chains, firms are often faced with the competing demands of improved customer service and reduced cost. We extend a cost-based location-inventory model (Shen et al. 2003) to include a customer service element and develop practical methods for quick and meaningful evaluation of cost/service trade-offs. Service is measured by the fraction of all demands that are located within an exogenously specified distance of the assigned distribution center. The nonlinear model simultaneously determines distribution center locations and the assignment of demand nodes to distribution centers to optimize the cost and service objectives. We use a weighting method to find all supported points on the trade-off curve. We also propose a heuristic solution approach based on genetic algorithms that can generate optimal or close-to-optimal solutions in a much shorter time compared to the weighting method. Our results suggest that significant service improvements can be achieved relative to the minimum cost solution at a relatively small incremental cost.", "e:keyword": ["Customer service", "Integrated supply chain design", "Multiobjective optimization", "Trade-off analysis", "Genetic algorithm"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0080", "e:abstract": "Most durable products have two distinct types of customers: first-time buyers and customers who already own the product, but are willing to replace it with a new one or purchase a second one. Firms usually adopt a price-discrimination policy by offering a trade-in rebate only to the replacement customers to hasten their purchase decisions. Any return flow of products induced by trade-in rebates has the potential to generate revenues through remanufacturing operations. In this paper, we study the optimal pricing/trade-in strategies for such durable, remanufacturable products. We focus on the scenario where the replacement customers are only interested in trade-ins. In this setting, we study three pricing schemes: (i) uniform price for all customers, (ii) age-independent price differentiation between new and replacement customers (i.e., constant rebate for replacement customers), and (iii) age-dependent price differentiation between new and replacement customers (i.e., age-dependent rebates for replacement customers). We characterize the roles that the durability of the product, the extent of return revenues, the age profile of existing products in the market, and the relative size of the two customer segments play in shaping the optimal prices and the amount of trade-in rebates offered. Throughout the paper we highlight the operational decisions that might influence the above factors, and we support our findings with real-life practices. In an extensive numerical study, we compare the profit potential of different pricing schemes and quantify the reward (penalty) associated with taking into account (ignoring) customer segmentation, the price-discrimination option, return revenues, and the age profile of existing products. On the basis of these results, we are able to identify the most favorable pricing strategy for the firm when faced with a particular market condition and discuss implications on the life-cycle pricing of durable, remanufacturable products.", "e:keyword": ["Trade-in rebates", "Pricing", "Remanufacturing", "Durable products", "Product-age profile"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0082", "e:abstract": "In this article, we analyze a decentralized supply chain consisting of a supplier and two independent retailers. In each order cycle, retailers place their orders at the supplier to minimize inventory-related expected costs at the end of their respective response times. There are two types of lead times involved. At the end of the supplier lead time, retailers are given an opportunity to readjust their initial orders (without changing the total order size), so that both retailers can improve their expected costs at the end of respective retailer lead times (the time it takes for items to be shipped from the supplier to the retailers). Because of the possibility of cooperation at the end of supplier lead time, each retailer will consider the other's order-up-to level in making the ordering decision. Under mild conditions, we prove the existence of a unique Nash equilibrium for the retailer order-up-to levels, and show that they can be obtained by solving a set of newsboy-like equations. We also present computational analysis that provides valuable managerial insight for design and operation of decentralized systems under the possibility of partial cooperation.", "e:keyword": ["Multiperiod decentralized inventory", "Partial cooperation", "Nash equilibrium", "Transshipment"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0081", "e:abstract": "We propose the use of robust optimization (RO) as a powerful methodology for multiperiod stochastic operations management problems. In particular, we study a two-echelon multiperiod supply chain problem, known as the retailer-supplier flexible commitment (RSFC) problem with uncertain demand that is only known to reside in some uncertainty set. We adopt a min-max criterion, whereby the cost function is minimized against the worst case demand occurrence. To solve the min-max RSFC problem we employ a recent extension of the RO method adapted to dynamic decision problems and known as the affinely adjustable robust counterpart (AARC) methodology. The AARC solution is tested by a large simulation study and found to provide excellent results.", "e:keyword": ["Robust optimization", "Affinely adjustable robust optimization", "Flexible commitment contracts", "Supply chain management", "Min-max criterion"]}, {"@id": "http://dx.doi.org/10.1287/msom.1040.0053", "e:abstract": "A regular feature of Manufacturing & Service Operations Management, \"In This Issue\" briefly describes each issue's articles and highlights their contributions.", "e:keyword": ["Editor's comments", "Issue overview", "Summaries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0086", "e:abstract": "Call centers usually handle several types of calls, but it is usually not possible or cost effective to have every agent be able to handle every type of call. Thus, the agents tend to have different skills, in different combinations. In such an environment, it is challenging to route calls effectively and determine the staff requirements. This paper addresses both of these routing and staffing problems by exploiting limited cross-training. Consistent with the literature on flexible manufacturing, we find that minimal flexibility can provide great benefits: Simulation experiments show that when (1) the service-time distribution does not depend on the call type or the agent and (2) each agent has only two skills, in appropriate combinations, the performance is almost as good as when each agent has all skills. We apply this flexibility property to develop an algorithm for both routing and staffing, aiming to minimize the total staff subject to per-class performance constraints. With appropriate flexibility, it suffices to use a suboptimal routing algorithm. Simulation experiments show that the overall procedure can be remarkably effective: The required staff with limited cross-training can be nearly the same as if all agents had all skills. Hence, the overall algorithm is nearly optimal for that scenario.", "e:keyword": ["Telephone call centers", "Customer contact centers", "Staffing", "Routing", "Skill-based routing", "Resource pooling", "Cross-training", "Flexible servers", "Chaining", "Queues", "Multiserver queues", "Multiclass queues", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0087", "e:abstract": "We study the safety stock positioning problem in single-product multistage supply chains with tree network structures, where each stage controls its inventory using an installation continuous-time base-stock policy. External demands follow independent Poisson processes, and unsatisfied demands at each stage are fully backordered. The processing (e.g., production) cycle times and transportation lead times are assumed to be stochastic, sequential, and exogenously determined. We derive recursive equations for the backorder delays (because of stockout) at all stages in the supply chain. Based on the recursive equations, we characterize the dependencies of the backorder delays across different stages in the network, and develop insights into the impact of safety stock positioning in various supply chain topologies. We present approximations and algorithms to coordinate the base-stock levels in these supply chains, so as to minimize systemwide inventory cost subject to meeting certain service-level requirements of the external customers.", "e:keyword": ["Stock positioning", "Stochastic sequential lead time", "Recursive equation", "Backorder delay", "Tree network structure"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0085", "e:abstract": "Order crossovers occur when replenishment orders arrive in a sequence that is different than the one in which they were placed. Order crossovers require that optimal reorder levels be set with regard to the inventory shortfall distribution rather than the lead-time demand distribution. Assuming periodic review and independent lead times, this paper suggests simple approximations of the shortfall distribution by showing that the variance of the number of orders outstanding is bounded above by the standard deviation of lead time divided by \\root 3. Using this bound in a normal approximation improves significantly upon the common practice of basing policies on the lead-time demand distribution. A negative binomial approximation of the shortfall, based on its exact variance, offers even greater improvement, at the cost of some additional informational and computational requirements.", "e:keyword": ["Inventory policies/management", "Base-stock policies", "Stochastic lead time", "Order crossover"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0088", "e:abstract": "Consumers often know what kind of product they wish to purchase, but do not know which specific variant best fits their needs. As a result, a consumer may find an acceptable product in one retailer but nevertheless purchase nothing, opting to search other retailers for an even better product. We study several models of retail assortment planning, some of which explicitly account for consumer search and one that does not, which we call the \"no-search\" model. Even though the no-search model never includes an unprofitable variant in the assortment, in the presence of consumer search, it may indeed be optimal to include an unprofitable variant. Furthermore, we find that the no-search model can lead to an assortment with an expected total profit that is significantly less than optimal. In the extreme, the no-search model may recommend closing down a category (i.e., carry no variants) even if a profitable assortment exists (a 100% profit loss). We conclude that failing to incorporate consumer search into an assortment planning process may cause a retailer to underestimate the substantial value a broad assortment has in preventing consumer search. We discuss how the insights from our stylized models may apply to actual assortment planning processes.", "e:keyword": ["Product proliferation", "Multinomial logit model", "Assortment optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0090", "e:abstract": "This paper provides a model of the competitive newsvendor problem in which there is price competition following the inventory decisions. Using the biform game formalism of Brandenburger and Stuart (2004), the price competition is modeled by considering the core of the induced cooperative game. Such an analysis allows price competition to be modeled without a priori assumptions about price-setting power or pricing procedures. The paper shows that with no uncertainty, the inventory decision is equivalent to the capacity decision in Cournot competition. With uncertainty, the analysis again reduces to Cournot competition if the demand uncertainty is characterized by an appropriately constructed, expected demand curve. The results highlight the critical role of the fixed-price assumption in newsvendor models.", "e:keyword": ["Biform game", "Cooperative game", "Capacity competition", "Cournot"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0084", "e:abstract": "We develop a model and analyze reverse information sharing, a growing business practice in supply chain management in which a manufacturer shares information about supply with a retailer. We model the manufacturer as a production queue with finished goods warehouse, the retailer as an inventory location, and other customers as an external demand stream. In our model, the manufacturer allows the retailer access to inventory status at the warehouse. To take advantage of this new information, the retailer changes from a single-level base-stock policy to a two-level, state-dependent base-stock policy. We provide an exact method for computing performance and develop a procedure for evaluating optimal policy. We demonstrate the impact of the new policy on the manufacturer and other customers. Numerical computations lead to insights about the value of information to the retailer, and to guidelines for the manufacturer on sharing information.", "e:keyword": ["Information sharing", "Stochastic production-inventory system", "State-dependent base stock", "Markov chains"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0110", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0095", "e:abstract": "The view that adopting an environmental perspective on operations can lead to improved operations is in itself not novel; phrases such as \"lean is green\" are increasingly commonplace. The implication is that any operational system that has minimized inefficiencies is also more environmentally sustainable. However, in this paper we argue that the underlying mechanism is one of extending the horizons of analysis and that this applies to both theory and practice of operations management. We illustrate this through two principal areas of lean operations, where we identify how successive extensions of the prevailing research horizon in each area have led to major advances in theory and practice. First, in quality management, the initial emphasis on statistical quality control of individual operations was extended through total quality management to include a broader process encompassing customer requirements and suppliers' operations. More recently, the environmental perspective extended the definition of customers to stakeholders and defects to any form of waste. Second, in supply chain management, the horizon first expanded from the initial focus on optimizing inventory control with a single planner to including multiple organizations with conflicting objectives and private information. The environmental perspective draws attention to aspects such as reverse flows and end-of-life product disposal, again potentially improving the performance of the overall supply chain. In both cases, these developments were initially driven by practice, where many of the benefits of adopting an environmental perspective were unexpected. Given that these unexpected side benefits seem to recur so frequently, we refer to this phenomenon as the \"law of the expected unexpected side benefits.\" We conclude by extrapolating from the developmental paths of total quality management and supply chain management to speculate about the future of environmental research in operations management.", "e:keyword": ["Environmental management", "Sustainability", "Quality management", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0089", "e:abstract": "A fundamental decision for any manufacturer is when to sell to a downstream retailer. A manufacturer can sell either early, i.e., well in advance of the selling season, or late, i.e., close to the selling season. This paper examines the impact of information asymmetry, retailer sales effort, and contract type on the manufacturer's sale-timing decision. We find that if information is symmetric, demand is not influenced by sales effort, and the contract specifies that the price paid is linear in the order quantity, the manufacturer prefers to sell late. This result extends to the case where the retailer exerts sales effort during the selling season. However, if the retailer exerts sales effort prior to the selling season or has superior information about market demand, the manufacturer may prefer to sell early. We characterize the manufacturer's sale-timing preference in these settings, providing clear conditions under which the manufacturer prefers to sell either early or late. We show that the retailer, manufacturer, and total system may be hurt by the retailer's having higher-quality information.", "e:keyword": ["Supply-chain contracting", "Pricing", "Sales effort", "Asymmetric information"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0097", "e:abstract": "We examine the competition between procurement auctions and long-term relational contracts that emerges from the increased usage of electronic marketplaces. Procurement auctions create supply chain efficiencies by selecting the least costly bidder, and long-term relational contracts ensure the quality of the procured products or services when these have nonverifiable attributes. The following two-layer supply chain model is analyzed: Suppliers of an industrial part with nonverifiable quality trade with several manufacturers that utilize that part in the production of an end product. A price-based reverse auction is used for the procurement of low-quality parts, and a relational long-term contract is used for high-quality parts. A formal model of the competition between the two procurement models identifies conditions under which the two coexist, and conditions under which one will push the other out of the market. Market and product parameters that increase the relative economic appeal of the relational contract are determined. It is demonstrated that the competition from the auction market can either facilitate or undermine the relational contract compared with a benchmark case of a monopolist supplier. This implies that competition between the two procurement models plays an important role in the supply of high-quality products.", "e:keyword": ["Supply chain management", "Procurement auctions", "Relational contracting", "Quality differentiation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0101", "e:abstract": "Many firms in the business-to-consumer market sell identical products online using auctions and posted prices at the same time. In this paper, we develop and analyze a model of the key trade-offs sellers face in such a dual-channel setting built around the optimal choice of three design parameters: the posted price, the auction lot size, and the auction duration. Our results show how a monopolist seller can increase his revenues by offering auctions and a fixed price concurrently, and we identify when either a posted price only or a dual-channel strategy is optimal for the seller. We model consumer choice of channels, and thus market segmentation, and find a unique (symmetric) auction-participation equilibrium exists in which consumers who value the item for more than its posted price use a threshold policy to choose between the two channels. The threshold defines an upper bound on the remaining time of the auction. We explain how optimizing the design parameters enables the seller to segment the market so that the two channels reinforce each other and cannibalization is mitigated. Our findings also demonstrate that there are two dominant auction design strategies in this setting: one-unit auctions that tend to be short and long multiunit auctions. The optimal strategy for the seller depends on the consumer arrival rate and the disutility of delivery delay incurred by high-valuation consumers. In either case, the optimal design of the dual channel can significantly outperform a single posted-price channel. We show even greater benefits over a naive approach to managing the two channels that optimizes each independently. Our results suggest that unless firms jointly manage these online channels, they may find that adding auctions actually reduces their revenues.", "e:keyword": ["Marketing", "e-commerce", "Online auctions"]}, {"@id": "http://dx.doi.org/10.1287/msom.1050.0094", "e:abstract": "The service system design problem seeks to locate a set of service facilities, allocate enough capacity, and assign stochastic customer demand to each of them, so as to minimize the fixed costs of opening facilities and acquiring service capacity, as well as the variable access and waiting costs. This problem is commonly known in the location literature as the facility location problem with immobile servers, stochastic demand, and congestion. It is often set up as a network of M/M/1 queues and modeled as a nonlinear mixed-integer program (MIP). Because of the complexity of the resulting model, the current literature focuses on approximate and/or heuristic solution methods. This paper proposes a linearization based on a simple transformation and piecewise linear approximations and an exact solution method based on cutting planes. This leads to the exact solution of models with up to 100 customers, 20 potential service facilities, and 3 capacity levels.", "e:keyword": ["Service system design", "Facility location", "Immobile servers", "Stochastic demand", "Congestion", "Nonlinear MIP", "Piecewise linearization", "Cutting plane methods"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0102", "e:abstract": "This paper considers a firm's decisions on the introduction timing for successive product generations. We examine the case where a firm introduces multiple generations of a durable product for which demand is characterized by a demand diffusion process. Under fixed introduction costs, we consider the case where available product technology improves stochastically. As such, delaying introduction to a later date may lead to the capture of further technology improvements, potentially at the cost of slowing sales for the existing product (and a decline in market potential for the product to be introduced, given our focus on durable products). We specify a state-based model of demand diffusion and construct a decision model to solve the firm's introduction timing problem. By incorporating technology improvement in our model, we prove the optimality of a state-dependent threshold policy governing the firm's product-introduction decisions. Numerical analysis reveals the influence of key model parameters on the pace of product introduction. Our model helps to explain the product-introduction behavior of firms and provides an alternative to previous explanations of IBM's introduction timing decisions for successive generations of its mainframe computers.", "e:keyword": ["Product innovation", "Uncertain technology", "Introduction to market"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0105", "e:abstract": "Consider a firm that owns a fixed capacity of a resource that is consumed in the production or delivery of multiple products. The firm strives to maximize its total expected revenues over a finite horizon, either by choosing a dynamic pricing strategy for each product or, if prices are fixed, by selecting a dynamic rule that controls the allocation of capacity to requests for the different products. This paper shows how these well-studied revenue management problems can be reduced to a common formulation in which the firm controls the aggregate rate at which all products jointly consume resource capacity, highlighting their common structure, and in some cases leading to algorithmic simplifications through the reduction in the control dimension of the associated optimization problems. In the context of their associated deterministic (fluid) formulations, this reduction leads to a closed-form characterization of the optimal controls, and suggests several natural static and dynamic pricing heuristics. These are analyzed asymptotically and through an extensive numerical study. In the context of the former, we show that \"resolving\" the fluid heuristic achieves asymptotically optimal performance under fluid scaling.", "e:keyword": ["Revenue management", "Dynamic pricing", "Capacity controls", "Fluid approximations", "Efficient frontier"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0100", "e:abstract": "We study determining prices and production jointly in a multiple period horizon under a general, nonstationary stochastic demand function with a discrete menu of prices. We assume that the available production capacity is limited and that unmet demand is lost. We incorporate discretionary sales, when inventory may be set aside to satisfy future demand even if some present demand is lost. We analyze and compare partial planning or delayed strategies. In delayed strategies, one decision may be planned in advance, whereas a second decision is delayed until the beginning of each time period, after observing the results of previous decisions. For example, in delayed production (delayed pricing), pricing (production) is determined at the beginning of the horizon, and the production (pricing) decision is made at the beginning of each period before new customer orders are received. A special case is where a single price is chosen over the horizon. We describe policies and heuristics for the strategies based on deterministic approximations and analyze their performances. Computational analysis yields additional insights about the strategies, such as that delayed production is usually better than delayed pricing except sometimes when capacity is tight. On average, the delayed production (pricing) heuristic achieved 99.3% (99.8%) of the corresponding optimal strategy.", "e:keyword": ["Pricing", "Production", "Inventory control", "Discretionary sales", "Worst-case analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0104", "e:abstract": "We develop an approximation scheme for performance evaluation of serial supply systems when each stage operates like a single-server queue, and its planned inventories are managed according to a base-stock policy. We also present a near-exact matrix-geometric procedure for benchmarking our approximation relative to two other methods proposed in the literature. Through numerical tests, we demonstrate that our method is superior, both for performance estimation and for policy parameter optimization. Using this technique, we then perform experiments that address the following issues. What proportion of the optimal total inventory should managers allocate to upstream production stages to minimize the sum of inventory and backorder costs? If managerial action could lower holding cost rate or add capacity, which stages of the supply system should be targeted for maximum net benefit? Such concerns have been the subject of several recent studies relating to supply networks with constant and random independent lead times. We shine light on optimal actions for serial supply systems that experience congestion.", "e:keyword": ["Capacitated supply systems", "Queues with planned inventories", "Approximations", "Service constraints", "Matrix-geometric procedure"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0099", "e:abstract": "As manufacturers in various industries evolve toward predominantly make-to-order production to better serve their customers' needs, increasing product mix flexibility emerges as a necessary strategy to provide adequate market responsiveness. However, the implications of increased flexibility on overall system performance are widely unknown. We develop analytical models and an optimization-based simulation tool to study the impact of increasing flexibility on shortages, production variability, component inventories, and order variability induced at upstream suppliers in general multiplant multiproduct make-to-order manufacturing systems. Our results show that (1) Partial flexibility leads to a considerable increase in production variability, and consequently in higher component inventory levels and upstream order variability. Although a modest increase in flexibility yields most of the sales benefits, production variability is reduced as more flexibility is added to the system. Consequently, investments in additional flexibility may be justified when component inventories are expensive, or simply by the benefits associated with the smoother production. (2) The performance of flexible systems is highly dependent on the capacity allocation policies implemented. Policies that evenly distribute product demands to the available plants lead to consistently better performance because they avoid the misplacement of inventories by replicating the performance of a single-plant system. These insights and the simulation tool can be used by practitioners to guide the design of their flexible production systems, trading off the initial capital outlay versus the sales benefits and the expected operational costs.", "e:keyword": ["Manufacturing capacity and flexibility planning", "Production variability", "Demand allocation", "Make-to-order manufacturing systems"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0103", "e:abstract": "Market pressure for low prices paired with customer demand for high product variety presents a considerable dilemma for many manufacturers. Industry practice and research to date suggest that approaches based on component commonality can substantially lower the costs of proliferated product lines, but at the cost of reducing product differentiation and revenues. We analyze a stylized model of a manufacturer who designs a product line consisting of two products for sale to two market segments with different valuations of quality. The manufacturer determines the component quality levels, the amount of effort to reduce production costs, and whether to use common or different components for the two products. Explicitly considering potential interdependencies between cost-reduction effort and quality decisions, we characterize environments where the optimal product line involving component commonality features products of higher quality and yields higher revenues. Counter to earlier research we show that it can be preferable to make those components common that, relative to their production cost, are attributed a higher importance by customers. Disregarding the interactions between commonality, production cost, quality, and effort decisions can lead manufacturers to offer product lines with excessive differentiation and inefficiently low quality.", "e:keyword": ["Component commonality", "Marketing-manufacturing interface", "Product line design"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0098", "e:abstract": "Consider two markets of different sizes but similar costs and fare structure. All other things being equal, is an airline's expected revenue larger in the market with larger demand? If not, under what circumstances is it possible to compare expected revenues without carrying out a detailed analysis? In this article, we provide answers to these questions by studying the relationship between the optimal expected revenue and the demand distributions when the latter are comparable according to various stochastic orders. For the two-fare class problem with dependent demand we obtain three results. We show that airlines should prefer lesser positive dependence between fare classes when marginal demand distributions are the same. We also describe particular dependence structures under which stochastically larger marginal demand distributions improve optimal expected revenue. Finally, when the dependence between effective demands in the two fare classes arises due to \"sell ups,\" we show that stochastically larger marginal demand distributions should be preferred. (Sell ups occur when some lower-fare-class customers buy higher-fare tickets upon finding that the former tickets are sold out.) For a problem with an arbitrary number of fare classes and independent demands, we show that stochastically larger demand distributions should be preferred. Numerical examples demonstrating the effect of parameterized demand distributions (with appropriate stochastic ordering) and dependence structures are also presented.", "e:keyword": ["Revenue management", "Stochastic order relations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0106", "e:abstract": "Amathematical model is developed to help analyze the benefit in contact-center performance obtained from increasing employee (agent) retention, which is in turn obtained by increasing agent job satisfaction. The contact-center performance may be restricted to a traditional productivity measure such as the number of calls answered per hour, or it may include a broader measure of the quality of service, e.g., revenue earned per hour or the number of problems successfully resolved per hour. The analysis is based on an idealized model of a contact center in which the number of employed agents is constant over time, assuming that a new agent is immediately hired to replace each departing agent. The agent employment periods are assumed to be independent and identically distributed random variables with a general agent-retention probability distribution, which depends on management policy and actions. The steady-state staff-experience distribution is obtained from the agent-retention distribution by applying renewal theory. An increasing real-valued function specifies the average performance as a function of agent experience. Convenient closed-form expressions for the overall performance as a function of model elements are derived when either the agent-retention distribution or the performance function has exponential structure. Management actions may cause the agent-retention distribution to change. The model describes the consequences of such changes on the long-run average staff experience and the long-run average performance.", "e:keyword": ["Contact centers", "Call centers", "Retention", "Employee turnover", "Churn", "Agent job satisfaction", "Compensation", "Autonomy", "Stress", "Stochastic models", "Renewal theory", "Stochastic comparisons"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0111", "e:abstract": "This paper examines the interactions between routing and inventory-management decisions in a two-level supply chain consisting of a cross-docking warehouse and N retailers. Retailer demand is normally distributed and independent across retailers and over time. Travel times are fixed between pairs of system sites. Every m time periods, system inventory is replenished at the warehouse, whereupon an uncapacitated vehicle departs on a route that visits each retailer once and only once, allocating all of its inventory based on the status of inventory at the retailers who have not yet received allocations. The retailers experience newsvendor-type inventory-holding and backorder-penalty costs each period; the vehicle experiences in-transit inventory-holding costs each period. Our goal is to determine a combined system inventory-replenishment, routing, and inventory-allocation policy that minimizes the total expected cost/period of the system over an infinite time horizon. Our analysis begins by examining the determination of the optimal static route, i.e., the best route if the vehicle must travel the same route every replenishment-allocation cycle. Here we demonstrate that the optimal static route is not the shortest-total-distance (TSP) route, but depends on the variance of customer demands, and, if in-transit inventory-holding costs are charged, also on mean customer demands. We then examine dynamic-routing policies, i.e., policies that can change the route from one system-replenishment-allocation cycle to another, based on the status of the retailers' inventories. Here we argue that in the absence of transportation-related cost, the optimal dynamic-routing policy should be viewed as balancing management's ability to respond to system uncertainties (by changing routes) against system uncertainties that are induced by changing routes. We then examine the performance of a change-revert heuristic policy. Although its routing decisions are not fully dynamic, but determined and fixed for a given cycle at the time of each system replenishment, simulation tests with N = 2 and N = 6 retailers indicate that its use can substantially reduce system inventory-related costs even if most of the time the chosen route is the optimal static route.", "e:keyword": ["Supply chain", "Routing", "Inventory allocation", "Inventory replenishment"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0108", "e:abstract": "This paper studies a two-echelon assembly system and explores several important issues in managing decentralized supply chains. First, we investigate the behavior of the assembly system under decentralized control. It is shown that the Nash equilibrium of the competitive assembly system exists but is never system optimal. Next, we examine the role of information in the assembly system. We consider horizontal information sharing on the inventory status between the suppliers and demonstrate when information is valuable from the perspectives of the system and the individual player. We find that information sharing mitigates the system's competition penalty in some cases, but there are also instances in which information sharing can hurt a decentralized system. Although the manufacturer always prefers information sharing, it is sometimes in the interest of the suppliers to refuse to share information. Finally, we propose a demand-independent coordination scheme for managing decentralized supply chains with either a serial or an assembly structure. The coordination scheme has practical value because very often the firm's head (or the owner of the supply chain) does not have the accurate demand information that the local managers have. Alternatively, the demand varies over time, and the coordination contracts have to be modified. Under the transfer payment contract, the system optimal policy is the unique Nash equilibrium in the decentralized supply chain. Discussion on the relationship between our coordination scheme and existing methods in the literature is also provided.", "e:keyword": ["Assembly system", "Decentralization", "Information sharing", "Supply chain coordination"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0107", "e:abstract": "Two critical decisions must be made daily when managing multiechelon repair and distribution systems for service parts: (1) allocating available repair capacity among different items and (2) allocating available inventories to field stocking locations to support service operations. In many such systems, procurement lead times for service parts are lengthy and variable, repair capacity is limited, and operational requirements change frequently--resulting in demand processes that are highly uncertain and nonstationary. As a consequence, it is common to have many items in short supply while others are abundant. In such environments, integrated real-time decision-support tools can provide significant value by reducing the impact of inventory imbalances and responding appropriately to the volatile nature of the demand processes. By \"integrated\" and \"real-time,\" we mean (respectively) tools that simultaneously consider key aspects of the current state of the operating environment in deciding what items to repair, where to ship available units, and by what mode to ship them. In this paper, we develop an integrated real-time model for making repair and inventory allocation decisions in a two-echelon reparable service parts system. We formulate the decision problem as a finite-horizon, periodic-review mathematical program, we show it can be formulated as a large-scale linear program, and we develop a practical heuristic method for solving the problem approximately. By simulating the operation of a service parts supply chain, we demonstrate the value of employing integrated decision models over using separate repair and inventory allocation rules for a range of environments where inventory imbalances exist. We also show that our heuristic approach is highly effective and that its inventory allocation subroutine, used as a stand-alone tool for making distribution decisions, outperforms a commonly used inventory allocation rule in most circumstances tested.", "e:keyword": ["Inventory", "Periodic review", "Multiechelon system", "Real-time allocation", "Limited capacity", "Reparable service parts", "Priority dispatch rule", "Emergency shipment"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0113", "e:abstract": "Radio-frequency identification (RFID) technology will reduce the costs of transactions to both customers and providers. This is a good thing, mostly. But it will be harder than people suppose, and it may have some adverse consequences. There are numerous technical and policy problems, including privacy and security issues. All these likely will be resolved, but only with time, experience, and money. Thus, it is unclear just how RFID will be used, just when various applications will become feasible. Also, as transaction costs decline, providers will be inclined to charge for things that are now free, such as public parks and congested roads. We may wait less at each transaction point, but such points may proliferate. Rather than obsess about this one technology, we should aim to think creatively about transactions and ways to improve them. Transactions affect our lives in many ways. A transaction is a process, not just an event. When people are involved, the process becomes intertwined with other intricate processes--product choice, social interaction, and the expression of power relations. RFID certainly offers the potential for improvement, but there are some lower-tech methods worth considering. This is not a systematic survey, but rather an essay, a sketch in broad strokes of an immense and varied landscape. Many topics are touched on lightly. The goal is to stimulate thought and discussion. More questions are raised than answered. In places the tone is personal, some might say eccentric.", "e:keyword": ["Transactions", "RFID", "Technology"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0120", "e:abstract": "The ISO 9000 series of quality management systems standards is widely diffused, with more than 560,000 sites certified in 152 countries as of December 2003. Anecdotal evidence suggests that global supply chains contributed to this diffusion, in the following sense. Firms in Europe were the first to seek ISO 9000 certification in large numbers. They then required their suppliers, including those abroad, to do likewise. Once the standard had thus entered other countries, it spread beyond those firms immediately exporting to Europe to be adopted by many other firms in those same countries. This paper empirically examines the validity of this view of the role of supply chains in global diffusion of ISO 9000. To do so, we decompose the statement \"supply chains contributed to the global diffusion of ISO 9000\" into a series of four requirements that must be met for the original statement to be supported. We then use firm-level data from a global survey of more than 5,000 firms in nine countries to test the hypotheses that correspond to these requirements. Our findings are consistent with the view that part of the global diffusion of ISO 9000 did move upstream in global supply chains. In short, this means that firms that export goods or services to a particular country may simultaneously be importing that country's management practices. We conclude by suggesting how these findings might form the basis for future research on the environmental management systems standard ISO 14000.", "e:keyword": ["ISO 9000", "ISO 14000", "Quality management", "Supply chains", "Diffusion", "Global", "Empirical", "Survey"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0117", "e:abstract": "Eppen (1979) showed that inventory costs in a centralized system increase with the correlation between multivariate normal product demands. Using multivariate stochastic orders, we generalize this statement to arbitrary distributions. We then describe methods to construct models with arbitrary dependence structure, using the copula of a multivariate distribution to capture the dependence between the components of a random vector. For broad classes of distributions with arbitrary marginals, we confirm that centralization or pooling of inventories is more valuable when demands are less positively dependent.", "e:keyword": ["Inventory control", "Pooling effect", "Multivariate dependence", "Copula"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0115", "e:abstract": "We show the existence of Nash equilibria in a Bertrand oligopoly price competition game using a possibly asymmetric attraction demand model with convex costs under mild assumptions. We show that the equilibrium is unique and globally stable. To our knowledge, this is the first paper to show the existence of a unique equilibrium with both nonlinear demand and nonlinear costs. In addition, we guarantee the linear convergence rate of tatnnement. We illustrate the applicability of this approach with several examples arising from operational considerations that are often ignored in the economics literature.", "e:keyword": ["Price competition", "Oligopoly", "Attraction demand model", "Nash equilibrium", "Convex costs"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0112", "e:abstract": "False failure returns are products that are returned by consumers to retailers with no functional or cosmetic defect. The cost of a false failure return includes the processing actions of testing, refurbishing (if necessary), repackaging, the loss in value during the time the product spends in the reverse supply chain (a time that can exceed several months for many firms), and the loss in revenue because the product is sold at a discounted price. This cost is significant and is incurred primarily by the manufacturer. Reducing false failure returns, however, requires effort primarily from the retailer, for example informing consumers about the exact product that best fits their needs. We address the problem of reducing false failure returns via supply chain coordination methods. Specifically, we propose a target rebate contract that pays the retailer a specific dollar amount per each unit of false failure returns below a target. This target rebate provides an incentive to the retailer to increase her effort, thus decreasing the number of false failures and (potentially) increasing net sales. We show that this contract is Pareto improving in the majority of cases. Our results also indicate that the profit improvement to both parties, and the supply chain, is substantial.", "e:keyword": ["Consumer returns", "Closed-loop supply chains", "Supply chain contracts"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0114", "e:abstract": "We analyze a serial base-stock inventory model with Poisson demand and a fill-rate constraint. Our objective is to gain insights into the linkage between the stages to facilitate optimal system design and decentralized system control. To this end, we develop a closed-form approximation for the optimal base-stock levels. The development consists of two key steps: (1) convert the service-constrained model into a backorder cost model by imputing an appropriate backorder cost rate, and then adapt the single-stage approximation developed for the latter, and (2) use a logistic distribution to approximate the lead-time demand distribution in the single-stage approximation obtained in (1) to yield closed-form expressions. We then use the closed-form expressions to conduct sensitivity analyses and establish qualitative properties on system design issues, such as optimal total system stock, stock positioning, and internal fill rates. The closed-form approximation and most of the qualitative properties apply equally to the model with a backorder cost, although some differences do exist. Other results of this study include a bottom-up recursive procedure to evaluate any given echelon base-stock policy and lower bounds on the optimal echelon base-stock levels.", "e:keyword": ["Multiechelon inventory system", "Service level", "Logistic distribution", "Closed-form approximations", "System design", "Base-stock policies"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0116", "e:abstract": "In this paper we consider a dynamic forecast-inventory model with forecast updates, based on the martingale model of forecast evolution. Two types of updates are considered, additive and multiplicative. The formulation of the model results in a dynamic program with multidimensional state space. We derive some characteristics of optimal policies and also develop a computational approach to obtain approximate solutions. The approach is based on simulation and function approximation.", "e:keyword": ["Inventory", "Demand forecasts", "Approximate solutions"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0121", "e:abstract": "Cross-selling in telephone service centers is seen by industry as a useful means to generate profits from an existing customer base. Introducing cross-sales to a service center, however, needs to be properly managed as it could degrade customer service quality. Real-time customer and system information can be used to set the appropriate control policy for cross-sales to enhance profitability without causing undesirable service degradation for calling customers. The objective of this paper is to illustrate the value of using real-time information in selecting optimal control policies for cross-sales in telephone service centers. Specifically, we develop an introductory mathematical model to incorporate the use of two types of information, system status (queuing congestion) and customer profile (likelihood of purchase) in determining the optimal control policy to maximize the expected operating profit of the system. We hope to stimulate further research in this area by providing a groundwork for further modeling. We show that using more information increases a call center's operating profit. Improvements were dramatic for environments with intermediate utilization and high customer heterogeneity.", "e:keyword": ["Call center management", "Service management", "Customer relationship management", "Cross-selling", "Threshold policies", "Optimal queue control"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0128", "e:abstract": "We consider the problem of a newsvendor that is served by multiple suppliers, where any given supplier is defined to be either perfectly reliable or unreliable. By perfectly reliable we mean a supplier that delivers an amount identically equal to the amount desired, as is the case in the most basic variant of the newsvendor problem. By unreliable, we mean a supplier that with some probability delivers an amount strictly less than the amount desired. Our results indicate the following effects of unreliability: From the perspective of the newsvendor, the aggregate quantity ordered is higher than otherwise would be ordered if the newsvendor's suppliers were completely reliable. From the perspective of end customers, however, the service level provided is lower than otherwise would be provided if the newsvendor's suppliers were completely reliable. From the perspective of the suppliers, although reliability affects how much is ordered from a selected supplier, cost generally takes precedence over reliability when it comes to selecting suppliers in the first place. Even perfect reliability is no guarantee for qualification since, in an optimal solution, a given supplier will be selected only if all less-expensive suppliers are selected, regardless of the given supplier's reliability level. Nevertheless, the relative size of a selected supplier's order depends on its reliability.", "e:keyword": ["Newsvendor", "Supplier diversification", "Random yield", "Multisupplier sourcing", "Procurement", "Disruption management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0119", "e:abstract": "Companies may choose to outsource parts, but not all, of their call-center operations. In some cases, they classify customers as high or low value, serving the former with their in-house operations and routing the latter to an outsourcer. Typically, they impose service-level constraints on the time each type of customer waits on hold. This paper considers four schemes for routing low-value calls between the client company and the outsourcer. These schemes vary in the complexity of their routing algorithms, as well as the sophistication of the telephone and information technology infrastructure they require of the two operations. For three of these schemes, this paper provides a direct characterization of system performance. For the fourth, most complex, scheme the paper provides performance bounds for the important special case in which the service requirements of high- and low-value callers are the same. These results allow the systematic comparison of the performance of the various routing schemes. The results suggest that, for clients with large outsourcing requirements, the simpler schemes that require little client-outsourcer coordination can perform very well.", "e:keyword": ["Call routing", "Outsourcing", "Priority routing", "Call-center staffing", "Bursty call arrival"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0125", "e:abstract": "In Gorman and Kanet (2005) we introduced the Author Affiliation Index (AAI) and how it could be used to rate the quality of operations management-related journals. Since then, Olson (2005) surveyed operations management professors from top U.S. universities and reported their assessment of the quality of various operations management-related journals. This paper compares the results of these two studies and concludes that patterns of research publications match the surveyed opinions from top U.S. universities, or that researchers publish in journals they rate as top. We confirm our earlier finding that the AAI is comparable to surveys for assessing journal quality.", "e:keyword": ["Operations management", "Journal quality"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0127", "e:abstract": "In this note we present algorithms that compute, exactly or approximately, time-dependent waiting time tail probabilities and the time-dependent expected waiting time in M(t)/M/s(t) queuing systems.", "e:keyword": ["Nonstationary", "Time dependent", "Multiserver queues", "Waiting time"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0118", "e:abstract": "We develop a model of a failure-prone, bufferless, paced, automatic transfer line in which material flows through a number of workstations in series, receiving continuous processing along each workstation. When a workstation fails, it stops operating, and so do all the other workstations upstream of it. The quality of the material trapped in the stopped workstations deteriorates with time. If this material remains immobilized beyond a certain critical time, its quality becomes unacceptable and it must be scrapped. We develop analytical expressions for important system performance measures for two cases. In the first case, the in-process material has no memory of the quality deterioration that it experienced during previous stoppages, whereas in the second case it has. In both cases, we assume that the workstation uptimes and downtimes follow memoryless distributions. We use the analytical expressions to numerically study the effect of system parameters on system performance. To evaluate the memoryless assumption, we compare the performance of the original model to that of a modified model in which the workstation downtimes do not follow memoryless distributions. The performance of the modified model is obtained via simulation.", "e:keyword": ["Transfer line", "Material scrapping", "Performance evaluation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0123", "e:abstract": "When two firms compete for service-sensitive demands based on their product availability, their actions will affect the future market share reallocation. This problem was first studied by Hall and Porteus (2000) using a dynamic game model. We extend their work by incorporating a general demand model, which enables us to obtain properties that reveal the dynamics of the game and the behavior of the players. In particular, we provide conditions under which the market share of a firm has a positive value and give it an upper bound. We further extend the game competition model to an infinite-horizon setting. We prove that there exists a stationary equilibrium policy and that the dynamic equilibrium policy always converges to a stationary equilibrium policy. We demonstrate that demand patterns will dictate how firms compete rationally and show the likely outcomes of the competition.", "e:keyword": ["Service-sensitive demand", "Availability competition", "Demand model", "Dynamic game", "Feedback Nash equilibrium", "Stationary policy", "Order quantity structure"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0126", "e:abstract": "Because of long lead times associated with product development and building capacity, a supplier must initiate investment in capacity when the product development effort is ongoing. Because the product is ill defined at this point in time, the buyer is unable to commit to the future terms of trade through a court-enforceable contract. Instead, to provide incentives for capacity investment, the buyer informally promises future terms of trade. The prospect of future interaction creates an incentive for the buyer to pay the supplier as promised. We characterize optimal price-only and price-and-quantity promises and compare their performance. If the production cost is low and either the capacity cost is low or the discount factor is high, then the buyer should promise to purchase a specific quantity rather than simply promise to pay a per unit price; otherwise, the buyer should simply promise to pay a specified unit price.", "e:keyword": ["Relational contracts", "Supply chain management", "Capacity investment", "Price-only contracts"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0122", "e:abstract": "We study the effects of disruption risk in a supply chain where one retailer deals with competing risky suppliers who may default during their production lead times. The suppliers, who compete for business with the retailer by setting wholesale prices, are leaders in a Stackelberg game with the retailer. The retailer, facing uncertain future demand, chooses order quantities while weighing the benefits of procuring from the cheapest supplier against the advantages of order diversification. For the model with two suppliers, we show that low supplier default correlations dampen competition among the suppliers, increasing the equilibrium wholesale prices. Therefore the retailer prefers suppliers with highly correlated default events, despite the loss of diversification benefits. In contrast, the suppliers and the channel prefer defaults that are negatively correlated. However, as the number of suppliers increases, our model predicts that the retailer may be able to take advantage of both competition and diversification.", "e:keyword": ["Resilient supply chains", "Supply risk", "Supply disruptions", "Competition", "Procurement", "Default correlation", "Equilibrium pricing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0124", "e:abstract": "New currency recirculation guidelines implemented by the Federal Reserve System (Fed) of the United States are intended to reduce the overuse of its currency processing services by depository institutions (banks). These changes are expected to have a significant impact on operating policies at those depository institutions that handle large volumes of currency. We describe two business models that capture the flow of currency between a bank and the Fed; the first model captures the current operations of most banks, while the second is expected to be adapted by many banks in response to the new guidelines. Motivated by our work with Brink's, Inc., to assess the economic impact that banks will sustain from these guidelines, we present a detailed analysis that provides managers of banks with optimal strategies to manage the flow of currency to and from the Fed for a variety of cost structures and demand patterns. Given this insight into a bank's optimal behavior, the Fed can also use our analysis to fine tune its guidelines to achieve the desired goals.", "e:keyword": ["Currency supply chain", "Federal Reserve System", "Depository institutions", "Cross-shipping", "Custodial inventory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0132", "e:abstract": "In this paper we consider workforce management in repair/maintenance environments in which repairmen are cross-trained to attend more than one type of machine. In this context, we study the machine-repairman problem with heterogeneous machines but with partially cross-trained repairmen. We introduce simple repairman-assignment rules as well as machine-priority rules that are effective in minimizing the machine downtime costs, or balancing the percentage of working machines of different types. We show that static machine priority rules are effective in minimizing systems downtime costs, while a generalized version of the longest queue policy is effective in balancing the percentage of working machines. We also introduce the concept of hidden symmetry in repair environments, and show that the well-known chain repairman skill set structure performs very well in repair environments with hidden symmetry. Finally, we provide insights into the design and control issues of repair/maintenance systems with cross-trained repairmen.", "e:keyword": ["Machine-repairman problem", "Quasi-birth-and-death process", "Cross-training", "Preemption", "Myopic approach"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0136", "e:abstract": "For many companies, inventory record inaccuracy is a major obstacle to achieving operational excellence. In this paper, we consider an inventory system in which inventory records are inaccurate. The manager makes inventory inspection and replenishment decisions at the beginning of each period. There is a cost associated with each inspection. If an inspection is performed, inventory records are aligned with physical inventory. The objective is to develop a joint inspection and replenishment policy that minimizes total costs in a finite horizon. We prove that an inspection adjusted base-stock (IABS) policy is optimal for the single-period problem. In the finite-horizon problem, we show that the IABS policy is near optimal in a numerical study. Under this policy, the manager performs an inspection if the inventory recorded is less than a threshold level, and orders up to a base-stock level that depends on the number of periods since the last inspection. The prevalent approach to deal with inventory inaccuracy in practice is to implement cycle-count programs. Based on the structure of the IABS policy, we propose a new cycle-count policy with state-dependent base-stock levels (CCABS). We show that CCABS is almost as effective as the IABS policy. In addition, we provide guidelines for practitioners to design effective cycle-count programs by conducting sensitivity analyses on the IABS policy. Finally, by comparing the costs associated with these policies and several benchmark systems, we quantify the true value of accurate inventory information, which may be provided by radio-frequency identification (RFID) systems.", "e:keyword": ["Inventory control", "Inventory inaccuracy", "Cycle-count policy", "RFID"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0138", "e:abstract": "When a make-to-order manufacturing company adopts a commit-to-delivery business mode, it commits a delivery due date for an order and is responsible for the shipping cost. Without loss of generality, we consider that transportation is done by a third-party logistics company, such as FedEx or UPS, which provides multiple shipping modes such as overnight, one-day, two-day delivery, and more. When the transportation time has to be short, clearly, shipping cost is more expensive than it could have been. How should a company schedule production for accepted orders so that the company can leave enough transportation time for orders to take slow shipping modes to reduce the shipping cost? We study this problem of integrating the production and transportation functions for a manufacturing company producing a variety of customized products in a make-to-order environment with a commit-to-delivery mode of business. Various realistic scenarios are investigated in increasing order of complexity. When partial delivery is allowed by customers, we provide both a mixed-integer programming (MIP) model and a minimum cost flow model. We show that nonpreemptive earliest due date (NEDD) production schedules are optimal when partial delivery is allowed and shipping cost is a decreasing convex function with transportation time. When partial delivery is not allowed, we develop an MIP model and prove that the problem is NP-hard. An efficient heuristic algorithm with polynomial computation time is provided for the NP-hard problem. It gives near-optimal production schedules, as shown via thousands of numerical experiments. We also provide models and analysis for other scenarios where shipping cost accounts for customer locations and quantity discounts.", "e:keyword": ["Make to order", "Scheduling", "Commit to delivery"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0141", "e:abstract": "We consider a supply chain where a contract manufacturer (CM) serves a number of original equipment manufacturers (OEMs). Investment into productive resources is made before demand realization, hence the supply chain faces the risk of under- or overinvestment. The CM and OEMs differ in their forecast accuracy and in their resource pooling capabilities, leading to a disparity in their ability to minimize costs due to demand uncertainty. We consider two scenarios in which this risk is borne by the OEM and CM, respectively. We determine which party should bear the risk so that maximum supply chain profits are achieved. We investigate the effectiveness of premium-based schemes in inducing the best party to bear the risk, and conclude that they function well despite information asymmetry when double marginalization is not very high.", "e:keyword": ["Information asymmetry", "Risk ownership", "Contract manufacturing", "Resource pooling"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0159", "e:abstract": "In a decentralized supply chain, with long-term competition between independent retailers facing random demands and buying from a common supplier, how should wholesale and retail prices be specified in an attempt to maximize supply-chain-wide profits? We show what types of coordination mechanisms allow the decentralized supply chain to generate aggregate expected profits equal to the optimal profits in a centralized system, and how the parameters of these (perfect) coordination schemes can be determined. We assume that the retailers face stochastic demand functions that may depend on all of the firms' prices as well as a measure of their service levels, e.g., the steady-state availability of the products. We systematically compare the coordination mechanisms when retailers compete only in terms of their prices, and when they engage in simultaneous price and service competition.", "e:keyword": ["Decentralized supply chains", "Coordination mechanisms", "Uncertain demands", "Infinite horizon"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0152", "e:abstract": "Decentralized organizations may incur inefficiencies because of scheduling issues associated with competition among decision makers (DMs) for limited resources. We analyze the decentralization cost (DC), i.e., the ratio between the Nash equilibrium cost and the cost attained at the centralized optimum. Solution properties of a dispatching-sequencing model are derived and subsequently used to develop bounds on the DC for an arbitrary number of jobs and DMs. A scheduling-based coordinating mechanism is then provided, ensuring that the centralized solution is obtained at equilibrium.", "e:keyword": ["Game theory", "Scheduling", "Decentralization", "Noncooperative games", "Incentives", "Contracting"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0145", "e:abstract": "The newsvendor model is designed to decide how much of a product to order when the product is to be sold over a short selling season with stochastic demand and there are no additional opportunities to replenish inventory. There are many practical situations that reasonably conform to those assumptions, but the traditional newsvendor model also assumes a fixed salvage value: all inventory left over at the end of the season is sold off at a fixed per-unit price. The fixed salvage value assumption is questionable when a clearance price is rationally chosen in response to the events observed during the selling season: a deep discount should be taken if there is plenty of inventory remaining at the end of the season, whereas a shallow discount is appropriate for a product with higher than expected demand. This paper solves for the optimal order quantity in the newsvendor model, assuming rational clearance pricing. We then study the performance of the traditional newsvendor model. The key to effective implementation of the traditional newsvendor model is choosing an appropriate fixed salvage value. (We show that an optimal order quantity cannot be generally achieved by merely enhancing the traditional newsvendor model to include a nonlinear salvage value function.) We demonstrate that several intuitive methods for estimating the salvage value can lead to an excessively large order quantity and a substantial profit loss. Even though the traditional model can result in poor performance, the model seems as if it is working correctly: the order quantity chosen is optimal given the salvage value inputted to the model, and the observed salvage value given the chosen order quantity equals the inputted one. We discuss how to estimate a salvage value that leads the traditional newsvendor model to the optimal or near-optimal order quantity. Our results highlight the importance of understanding how a model can interact with its own inputs: when inputs to a model are influenced by the decisions of the model, care is needed to appreciate how that interaction influences the decisions recommended by the model and how the model's inputs should be estimated.", "e:keyword": ["Markdown management", "Game theory", "Retailing", "Inventory", "Revenue management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0151", "e:abstract": "The allocation of inventory ownership affects the inventory availability in a supply chain, which in turn determines the supply chain performance. In this paper, we consider a supplier-retailer supply chain in which the supplier starts production well in advance of the selling season, and the retailer is offered two ordering opportunities at different points in time. An early order is allowed before the supplier's production decision, and a late order is allowed after the completion of production and after observing the demand. When the two wholesale prices change, we illustrate how the inventory decision rights and ownership are shifted and/or shared between the two firms, resulting in push, pull, or advance-purchase discount contracts. We then characterize the complete set of Pareto-dominant contracts for any given two-wholesale-price contract. We find that Pareto improvement can be achieved when inventory ownership is shifted from individual to shared and sometimes vice versa. In the latter case, push contracts not only are more likely to offer Pareto improvement, but also can achieve higher supply chain efficiency than pull contracts. We also identify conditions that enable Pareto improvement by introducing a new ordering opportunity to firms that had been bound by a single ordering opportunity without renegotiating the existing wholesale price, and we demonstrate through a numerical study that the adoption of the new ordering opportunity can significantly improve supply chain efficiency. We show that such Pareto improvement is more likely to happen when demand is more volatile.", "e:keyword": ["Pareto improvement", "Contracting", "Pricing", "Advance purchase", "Newsvendor model"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0137", "e:abstract": "We study how online business-to-business (B2B) exchanges affect buyer-supplier relationships where an exchange takes the role of a secondary market in which buyers (of the initial product) can trade excess inventory to address supply and demand imbalances. Over the last several years, B2B exchanges have attempted to provide supply for storable industrial goods with some degree of design specification (as opposed to undifferentiated commodities). Through this research, we elucidate some aspects of how speculative online exchanges with a small number of participants might behave and the impact they will have on the use of long-term contracts for supply. By endogenizing the evolution of spot prices in response to buyers' and their supplier's actions, we produce price fluctuations that exhibit significant autocorrelation in such markets. We show that participating buyers accrue network benefits as the number of participating firms increases through the inventory-pooling effects, resulting in reduced costs for them. However, a supplier acting strategically will counteract such benefits by restricting availability of goods to the spot market, sacrificing short-term spot-market revenue for long-term contract volume.", "e:keyword": ["Business-to-business exchanges", "Supply contracts", "Spot markets", "Endogenous price"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0139", "e:abstract": "With the increased emphasis on the effective management of operational issues in supply chains, the timely delivery of products has become even more important. Companies have to quote attainable delivery dates and then meet these, or face large tardiness penalties. We study systems that can be modeled by single-machine scheduling problems with due date assignment and controllable job-processing times, which are either linear or convex functions of the amount of a continuously divisible and nonrenewable resource that is allocated to the task. The due date assignment methods studied include the common due date, the slack due date, which reflects equal waiting time allowance for the jobs, and the most general method of unrestricted due dates, when each job may be assigned a different due date. For each combination of due date assignment method and processing-time function, we provide a polynomial-time algorithm to find the optimal job sequence, due date values, and resource allocations that minimize an integrated objective function, which includes the weighted number of tardy jobs, and due date assignment, makespan, and total resource consumption costs.", "e:keyword": ["Delivery time quotation", "Order sequencing", "Single-machine scheduling", "Due date assignment methods", "Controllable processing times", "Resource allocation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0196", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0197", "e:abstract": "This paper sets the stage for the special issue on the application of empirical science in operations management (OM). It highlights the contributions that empirical science can make to operations management research and practice. In particular, the role of theory building and testing as a motivation for OM empirical research is emphasized. The paper provides a brief history of the empirical tradition in OM and the scholars who were the early pioneers. The paper also gives a brief overview of the most widely used approaches to empirical research and concludes with a summary of the papers that are included in this volume.", "e:keyword": ["OM empirical research", "Theory building", "Survey research", "Case research", "Secondary data"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0168", "e:abstract": "Isuggest that the prospering fields of physics, medicine, and finance illustrate the value of a strong empirical dimension to research that is well integrated with theoretical research. I use empirical research in these fields to formulate a framework for classifying empirical research and illustrate that framework with a few selected examples in operations management. I offer some advice on data sources and approaches to conducting empirical research and suggest ways strengthening empirical research in operations management. This is obviously a partial treatment of a large subject and represents my personal point of view. This paper should encourage comments by others to further develop the topic and to offer alternative points of view.", "e:keyword": ["Empirical research", "Econometrics", "Experiments", "Case research", "Data sources"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0130", "e:abstract": "Recent operations management papers model customers as solving multiarmed bandit problems, positing that consumers use a particular heuristic when choosing among suppliers. These papers then analyze the resulting competition among suppliers and mathematically characterize the equilibrium actions. There remains a question, however, as to whether the original customer models on which the analyses are built are reasonable representations of actual consumer choice. In this paper, we empirically investigate how well these choice rules match actual performance as people solve two-armed Bernoulli bandit problems. We find that some of the most analytically tractable models perform best in tests of model fit. We also find that the expected number of consecutive trials of a given supplier is increasing in its expected quality level, with increasing differences, a result consistent with the models' predictions as well as with loyalty effects described in the popular management literature.", "e:keyword": ["Service quality", "Multiarmed bandit", "Decision-making under uncertainty", "Discrete-choice models", "Experimental tests"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0166", "e:abstract": "Classical inventory models offer a variety of insights into the optimal way to manage inventories of individual products. However, top managers and industry analysts are often concerned with the aggregate macroscopic view of a firm's inventory rather than with the inventories of individual products. Given that classical inventory models often do not account for many practical considerations that a company's management faces (e.g., competition, industry dynamics, business cycles, the financial state of the company and of the economy, etc.) and that they are derived at the product level and not the firm level, can insights from these models be used to explain the inventory dynamics of entire companies? This exploratory study aims to address this issue using empirical data. We analyze absolute and relative inventories using a quarterly data panel that contains 722 public U.S. companies for the period 1992-2002. We have chosen companies that are not widely diversified and whose business in large part relies on inventory management to concentrate on empirically testing hypotheses derived from a variety of classical inventory models (economic order quantity (EOQ), [Q, r], newsvendor, periodic review, etc.). We find empirical evidence that firms operating with more uncertain demand, longer lead times, and higher gross margins have larger inventories. Furthermore, larger companies appear to benefit from economies of scale and therefore have relatively less inventory than smaller companies. We obtain mixed evidence on the relationship between inventory levels and inventory holding costs. We also analyze the breakdown of data into eight segments--oil and gas, electronics, wholesale, retail, machinery, hardware, food, and chemicals--and find that, with a few notable exceptions, our hypotheses are supported within the segments as well. Overall, our results demonstrate that many of the predictions from classical inventory models extend beyond individual products to the aggregate firm level; hence, these models can help with high-level strategic choices in addition to tactical decisions.", "e:keyword": ["Econometrics", "Panel data", "Regression analysis", "Inventory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0129", "e:abstract": "This paper examines the inventories of publicly traded U.S. retail and wholesale companies between 1981 and 2004. First, we document that inventory holdings have been reduced. The median of wholesale inventory holding periods was reduced from 73 days to 49 days. Retail inventory did not start to decline until about 1995. Second, we document that firms with abnormally high inventories have abnormally poor long-term stock returns. Third, we illustrate these effects for the cases of Wal-Mart, 7-Eleven (Japan), and some related firms.", "e:keyword": ["Inventory", "Retail", "Wholesale", "Stock returns", "Empirical analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0149", "e:abstract": "The bullwhip effect is the phenomenon of increasing demand variability in the supply chain from downstream echelons (retail) to upstream echelons (manufacturing). The objective of this study is to document the strength of the bullwhip effect in industry-level U.S. data. In particular, we say an industry exhibits the bullwhip effect if the variance of the inflow of material to the industry (what macroeconomists often refer to as the variance of an industry's \"production\") is greater than the variance of the industry's sales. We find that wholesale industries exhibit a bullwhip effect, but retail industries generally do not exhibit the effect, nor do most manufacturing industries. Furthermore, we observe that manufacturing industries do not have substantially greater demand volatility than retail industries. Based on theoretical explanations for observing or not observing demand amplification, we are able to explain a substantial portion of the heterogeneity in the degree to which industries exhibit the bullwhip effect. In particular, the less seasonal an industry's demand, the more likely the industry amplifies volatility--highly seasonal industries tend to smooth demand volatility whereas nonseasonal industries tend to amplify.", "e:keyword": ["Bullwhip effect", "Production smoothing", "Supply chain management", "Volatility"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0134", "e:abstract": "Measuring demand uncertainty is a key activity in supply chain planning, but it is difficult when demand history is unavailable, such as for new products. One method that can be applied in such cases uses dispersion among forecasting experts as a measure of demand uncertainty. This paper provides a test for this method and presents a heteroscedastic regression model for estimating the variance of demand using dispersion among experts' forecasts and scale. We test this methodology using three data sets: demand data at item level, sales data at firm level for retailers, and sales data at firm level for manufacturers. We show that the variance of a random variable (demand and sales for our data sets) is positively correlated with both dispersion among experts' forecasts and scale: The variance increases sublinearly with dispersion and more than linearly with scale. Further, we use longitudinal data sets with sales forecasts made three to nine months before the earnings report date for retailers and manufacturers to show that the effects of dispersion and scale on variance of forecast error are consistent over time.", "e:keyword": ["Demand uncertainty", "Forecast dispersion", "Heteroscedasticity"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0156", "e:abstract": "This paper investigates the conditions under which frontline employees take initiative to improve their work systems to prevent operational failures. Drawing on the system improvement and team learning literatures, we develop a framework of frontline system improvement and test it using survey data from 37 workgroups. We find that psychological safety--the belief that one can talk about errors without risk of punishment--and problem-solving efficacy--the belief that the organization will support employees' system improvement efforts--were positively correlated with frontline system improvement (FLSI). Surprisingly, felt responsibility was negatively associated with FLSI. These findings suggest that rather than relying on hiring motivated individuals, managers need to support employees' efforts to improve their work systems by creating a work environment where it is safe to talk about operational failures and responding to employee communication about operational failures. Doing this may result in higher levels of FLSI efforts and ultimately improve work processes.", "e:keyword": ["Health care", "System improvement", "Problem solving", "Survey research"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0133", "e:abstract": "We study the impact of operational performance on profitability in the context of the U.S. domestic airline industry. In addition, we investigate the impact of focus [Skinner, W. 1974. The focused factory. Harvard Bus. Rev. 52(3) 113-121] on profitability in services. We use quarterly data on all major carriers, available since the introduction of required reporting of service indicators to the U.S. Department of Transportation. Our analysis demonstrates two main points. First, the relationship between operational performance and profitability is contingent on a company's operating model; \"focused\" airlines show a link between late arrivals and profitability while full-service airlines do not. Also, capacity utilization is a stronger driver of profitability for full-service airlines than for focused airlines. Second, focused airlines outperform the rest of the industry in terms of profitability.", "e:keyword": ["Operational performance", "Profitability", "Quality", "Operations strategy", "Focus", "Airlines"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0150", "e:abstract": "Store managers perform multiple tasks within a store, and the way in which they are evaluated and rewarded for these tasks affects their behavior. Using empirical data from multiple stores of a consumer electronics retailer, Tweeter Home Entertainment Group, we highlight the extent to which store manager incentive design impacts store manager behavior and, consequently, retail performance. More specifically, we describe the shift in store manager behavior resulting from a change in incentives, which, in part, altered the importance of sales relative to inventory shrinkage in the store manager compensation plan. Store managers, following this change, directed less attention to the prevention of inventory shrinkage and more toward sales-generating activities and made different process choices within the store. We observed increases in the level of inventory shrinkage and sales within these stores. Controlling for alternative drivers of sales and inventory shrinkage, we find this change in incentive design to be associated with a profit improvement of 4.2% of sales. This work indicates that altering how store managers are compensated impacts retail performance. Moreover, our findings underscore the importance of balancing the rewards given for different types of activities in contexts where agents face multiple competing tasks.", "e:keyword": ["Incentives", "Multitasking agent", "Retail operations", "Inventory shrinkage", "Quasi-experimental", "Store management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0135", "e:abstract": "Innovations in technology and service design have increasingly enabled firms to incorporate self-service technology to augment or substitute for \"traditional\" employee-provided service channels. Although it is clear that self-service can reduce cost, less is known about how customers utilize self-service channels in a multichannel service delivery system and the resulting impact on firm performance. An important aspect of service operations is that customers are coproducers of the service. Thus, the performance of the delivery system and customers' use of service channels can be affected by customers' own efficiency or productivity in service coproduction (customer efficiency). In this paper, we utilize prior theoretical frameworks in service operations and economics to hypothesize relationships among customer characteristics (especially coproduction efficiency), channel utilization, and firm performance. We then test these hypotheses using panel data from a large retail bank. Overall, we find that higher customer efficiency in self-service channels is associated with greater profitability and has a complex relationship with customer retention and product utilization.", "e:keyword": ["Service operations management", "Service delivery system", "Self-service technology", "Service coproduction", "Customer efficiency"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0131", "e:abstract": "Managers have long been challenged by an abundance of internal and external demands and uncertainties in their operating environments. Anecdotal evidence and a growing number of research studies have advocated process flexibility and product innovation as organization-level operating capabilities critical for responding to such demands and uncertainties, and have highlighted the need for more efficient and effective management of the firm's knowledge-based resources. Leveraging arguments from the resource-based and knowledge-based views of the firm, we introduce a second-order latent construct called operational intellectual capital, which represents the organization's operating know-how embedded in a system of complementary (i.e., covarying) knowledge-based resources. We argue that operational intellectual capital influences organization-level operating capabilities such as process flexibility and product innovation, which, in turn, influence business performance. We empirically examine these relationships using structural equation modeling on a cross-section of U.S. manufacturing survey data. Statistical results from the estimation of a coalignment model and comparisons with several other models support our operational intellectual capacity conceptualization and its impact on operating capabilities and business performance, respectively. Our research thus suggests the importance of possessing and leveraging a system of complementary knowledge-based operating resources, and addresses the need for the reformulation of operations strategy theory in terms of the emergent knowledge-based view of the firm.", "e:keyword": ["Operations strategy", "Operational intellectual capital", "Operating capabilities", "Business performance", "Structural equation modeling"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0163", "e:abstract": "As firms focus on new product, process, and service innovations, improving the performance and productivity of projects that help deliver these innovations assumes greater importance. Information technology (IT) has been an enabler of manufacturing productivity improvement, but its effect on improving the productivity of innovation-intensive operational activities has been mixed. In this paper, we explore the pathways through which IT impacts project-level performance measured in terms of speed, quality, and cost. Specifically, in this exploratory study we seek to present a theory of how the fit between enabling IT and the core characteristics of the project impacts project performance. We test our research hypotheses empirically, using a relatively large, cross-sectional sample of project data. The central contribution is the development and testing of a research model to improve our understanding of the relationship between enabling IT-project alignment, project competencies, and project performance. In doing so, our study clarifies the role of information technologies in project management, providing insights into how to integrate IT into innovation-intensive operational activities for improving project execution competence and productivity.", "e:keyword": ["Project management", "Information technology alignment", "Structure", "Uncertainty", "Volatility"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0140", "e:abstract": "This paper studies a buyback contract in the Stackelberg framework of a manufacturer (leader) selling to a price-setting newsvendor retailer (follower). Using an analytical model that focuses on a multiplicative demand form, we generalize previous results and produce new structural insights. A novel transformation technique first enables us to establish the unimodality of the profit functions for both channel partners, under relatively mild assumptions. Further analysis identifies the necessary and sufficient condition under which the optimal contract for the manufacturer (wholesale and buyback prices) is distribution free, i.e., independent of the uncertainty in customer demand. A specific instance of the above condition is also necessary and sufficient for a no-buyback contract to be optimal from the manufacturer's perspective. We then prove that the optimal performance of the decentralized channel for distribution-free buyback contracts depends only on the curvature of the deterministic demand part. In addition, some of the optimal decisions and relevant profit ratios for buyback contracts in our setting are shown to be identical to those for their deterministic price-only counterparts.", "e:keyword": ["Price-setting newsvendor", "Buyback contract", "Supply chain performance", "Demand curvature"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0176", "e:abstract": "This data set describes 38 multiechelon supply chains that have been implemented in practice. These chains exhibit special structure that can be used to inform and test analytical models. Although the data were not collected with the intention of econometric analysis, they may be useful in an empirical study. The data described in this paper are publicly available at the journal's website (http://msom.pubs.informs.org/ecompanion.html).", "e:keyword": ["Multiechelon inventory system", "Multistage supply chain application", "Inventory optimization", "Supply chain data set"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0148", "e:abstract": "This paper is concerned with the problem of determining the optimal size and composition of a permanent workforce needed to run a facility when demand is specified by a workstation group (WSG) for up to 24 hours a day, 7 days a week. For full-time employees, a solution is characterized by a bid job, which consists of a five-day-a-week schedule, a lunch break for all shifts, and a set of WSG task assignments for each of the half-hour periods in a shift. In contrast, each part-time employee may be given anywhere from one to six shifts during the week, and each shift may vary from four to eight hours in length. To facilitate supervision, all employees must be assigned to a home WSG, but when idle time exists in their schedules, they can be redeployed to other WSGs for a portion of the day. One of the complicating and unique factors addressed in this paper is the existence of nonsymmetric movement restrictions between WSGs. For example, an employee whose home base is A may be permitted to perform tasks at B, but not vice versa. Because the full problem could not be reduced to a single model, a multistage solution approach was developed. In the first stage, an extended shift-scheduling problem is solved to determine the optimal number of employees and their shifts. The results are postprocessed in subsequent stages to obtain lunch breaks, days off, and task assignments under WSG movement restrictions. In the implementation of the multistage approach, two alternatives were explored. The first was based on the idea of partitioning the WSGs into manageable clusters and then solving them in series. The second involved the direct solution of an integer programming formulation of the task assignment problem with home-base restrictions and WSG movement restrictions, but for a fixed workforce. An iterative scheme was used to adjust the size of the workforce until all constraints were satisfied and overall optimality was achieved. Testing was done with data provided by the U.S. Postal Service (USPS) mail processing and distribution center (P& DC) in Dallas. The computations showed that the second alternative always yielded the smaller workforce and was always able to find good solutions within 30 minutes.", "e:keyword": ["Staff scheduling", "Movement restrictions", "Decomposition", "Integer programming", "Workstation groups"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0153", "e:abstract": "We consider the problem of allocating demand that originates from multiple sources among multiple inventory locations. Demand from each source arrives dynamically according to an independent Poisson process. The cost of fulfilling each order depends on both the source of the order and its fulfillment location. Inventory at all locations is replenished from a shared production facility with a finite production capacity and stochastic production times. Consequently, supply lead times are load dependent and affected by congestion at the production facility. Our objective is to determine an optimal demand allocation and optimal inventory levels at each location so that the sum of transportation, inventory, and backorder costs is minimized. We formulate the problem as a nonlinear optimization problem and characterize the structure of the optimal allocation policy. We show that the optimal demand allocations are always discrete, with demand from each source always fulfilled entirely from a single inventory location. We use this discreteness property to reformulate the problems as a mixed-integer linear program and provide an exact solution procedure. We show that this discreteness property extends to systems with other forms of supply processes. However, we also show that supply systems exist for which the property does not hold. Using numerical results, we examine the impact of different parameters and provide some managerial insights.", "e:keyword": ["Production-inventory systems", "Optimal demand allocation", "Make-to-stock queues", "Facility location"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0147", "e:abstract": "Aservice encounter is an experience that extends over time. Therefore, its effective management must include the control of the timing of the delivery of each of the service's elements and the enhancement of the customer's experience between and during the delivery of the various elements. This paper provides a conceptual framework that links the duration of a service encounter to behaviors that have been shown to affect profitability. Analysis of the framework reveals a wide gap between the behavioral assumptions typically made in operations research (OR) and operations management (OM) models and the state of the art in the marketing and psychology literature. The central motivations behind this paper are (1) to help the OR and OM community bridge this gap by bringing to its attention recent findings from the behavioral literature that have implications for the design of queueing systems for service firms and (2) to identify opportunities for further research.", "e:keyword": ["Psychology of waiting", "Marketing/operations interface", "Queueing", "Behavioral operations management", "Service encounters", "Service operations management", "Services marketing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0144", "e:abstract": "The classical scheduling literature considers many problems where a given set of jobs must be processed at minimum cost, subject to various resource constraints. The literature only considers the issue of revenue generation in a very limited way, by allowing a job to remain unprocessed and its revenue contribution to be lost. By contrast, we consider three diverse practical situations where efficient scheduling affects revenue in much more general and realistic ways. First, we study two make-to-order environments where efficient scheduling increases customer goodwill, thus stimulating demand in different ways. Second, we study two make-to-stock environments where efficient scheduling creates inventory, thus also stimulating demand in different ways. Third, we study new product markets where efficient scheduling leads to a company becoming the first mover, and thus acquiring a larger market share. In each case, we provide both a computationally efficient algorithm for scheduling and a proof that a much more efficient algorithm is unlikely to exist. For both the make-to-stock and make-to-order problems, we also describe heuristic approaches that are easy to implement, and we study their average performance. The results show that substantial benefits arise from considering the implications of efficient scheduling for revenue and net profit. The practical impact of our work is to demonstrate the importance of efficient scheduling, not only in controlling cost, but also in increasing revenue and net profit.", "e:keyword": ["Manufacturing", "Scheduling", "Profit maximization", "Make-to-stock", "Make-to-order", "Algorithms", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0167", "e:abstract": "The problem of sharing manufacturing, inventory, or capacity to improve performance is applicable in many decentralized operational contexts. However, the solution of such problems commonly requires an intermediary or a broker to manage information security concerns of individual participants. Our goal is to examine use of cryptographic techniques to attain the same result without the use of a broker. To illustrate this approach, we focus on a problem faced by independent trucking companies that have separate pick-up and delivery tasks and wish to identify potential efficiency-enhancing task swaps while limiting the information they must reveal to identify these swaps. We present an algorithm that finds opportunities to swap loads without revealing any information except the loads swapped, along with proofs of the security of the protocol. We also show that it is incentive compatible for each company to correctly follow the protocol as well as provide their true data. We apply this algorithm to an empirical data set from a large transportation company and present results that suggest significant opportunities to improve efficiency through Pareto improving swaps. This paper thus uses cryptographic arguments in an operations management problem context to show how an algorithm can be proven incentive compatible as well as demonstrate the potential value of its use on an empirical data set.", "e:keyword": ["Collaboration", "Routing", "Cryptography", "Space-filling curve", "Incentive compatible", "Algorithm"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0157", "e:abstract": "We analyze the optimal design of a markdown pricing mechanism with preannounced prices. In the presence of limited supply, buyers who choose to purchase at a lower price may face a scarcity in supply. Our focus is on the structure of the optimal markdown mechanisms in the presence of rational or strategic buyers who demand multiple units. We first examine a complete information setting where the set of customer valuations is known but the seller does not know the valuation of each individual customer (i.e., cannot exercise perfect price discrimination). We then generalize our analysis to an incomplete valuation information setting where customer valuations are drawn from known distributions. For both settings, we compare the seller's profit resulting from the optimal markdown mechanism and the optimal single price. We provide a number of managerial insights into designing profitable markdown mechanisms.", "e:keyword": ["Pricing", "Markdown", "Strategic bidding", "Price discrimination"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0143", "e:abstract": "Technological advances present firms in many industries with opportunities to substantially improve their product's capabilities in short periods of time. Customers who invest in these products may, however, react adversely to rapid improvements that make their previous versions obsolete by deferring their purchase. In industrial markets, there is an emerging trend of sequentially improving products designed to be upgraded in a modular fashion. We study the impact of product architecture and introduction timing on the launch of rapidly improving products. We find that by localizing performance improvements in a sequence of upgradable modules of the product, a firm can better manage the introduction of rapidly improving products. Specifically, we show that modular upgradability can reduce the need for slowing the pace of innovation or forgoing upgrade pricing. The additional flexibility in pricing and timing makes the modular, upgradable approach preferable to an integrated architecture, even in some situations where there may be distinct performance or cost-related disadvantages to pursuing the modular architecture. We differentiate between proprietary and nonproprietary approaches to modular upgradability and consider the implications for profits. Our central contribution in this paper is the innovative integration of product architecture with pricing and timing decisions for managing the introduction of rapidly improving products.", "e:keyword": ["New product development and introduction", "Product architecture", "Modular upgradability", "Rapidly improving products"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0161", "e:abstract": "In the early 1990s, research began to show that the Japanese production theory, which espouses reduction of machine setup time as a sure way to improve production performance, may be limited. Specifically, it was found that reduction in mean setup times without any change in variance can, paradoxically, increase waiting time and work in process (WIP) in a cyclic production system. Setup time variance was demonstrated to play a central role because of the paradoxes it produced with the resulting harm to effective capacity. Subsequently, explicit formulas were derived for determining whether adding fixed forced idle time (but holding variance constant) would reduce waiting time and, if so, the optimal amount of idle time to add. However, research to date has offered little guidance to reduce setup time variance to improve waiting time. We show that a greater reduction is achievable by adding a variable idle time that is a nonincreasing function of setup time and thereby reduce the combined setup time variance. We provide explicit procedures for finding the optimal variable idle time as a function of setup time when the latter follows any finite discrete distribution. We also show how to implement our policy and show that our approach can improve waiting time even when other currently known approaches cannot.", "e:keyword": ["Cyclic production systems", "Multiproduct systems", "Setup times", "Waiting times", "Variance effects", "Stochastic inventory", "Variance reductions", "Cyclic queues"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0174", "e:abstract": "This article proposes a supporting framework for the implementation of the material control system POLCA (paired-cell overlapping loops of cards with authorization). The POLCA system is particularly appropriate for environments that involve highly variable demand and large product variety, which force small batch (or even one-of-a-kind) production. We propose a load-based version of the POLCA control system (LB-POLCA), which determines the POLCA parameters (release authorizations, allowed workloads in the loops) according to an advanced resources planning (ARP) system that adequately captures the stochastic behavior of the production system and enables fine-tuning and high-level optimization of the manufacturing lot sizes. We also discuss the implementation of an electronic LB-POLCA system in a metal shop of Spicer Off-Highway Products Division, a subsidiary of the Dana Corporation.", "e:keyword": ["Queueing analysis", "POLCA", "Advanced resources planning", "Card system", "Lead time analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0170", "e:abstract": "This paper studies a supply chain with one supplier and multiple heterogeneous retailers where base-stock policies are practiced. The supplier's performance, in terms of optimal cost, optimal stocking level, and implied service level, is investigated under different retailer replenishment sequences. We show that under many realistic demand distributions, it is optimal to sequence retailers according to ascending variability. Furthermore, if the retailers vary in magnitudes of their orders, then the optimal sequence is usually the one with descending magnitudes.", "e:keyword": ["Inventory", "Two echelons", "Operating characteristics", "Stochastic models", "Stochastic comparisons", "Sequencing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0155", "e:abstract": "We study competition between two multiproduct firms with distinct production technologies in a market where customers have heterogeneous preferences on a single taste attribute. The mass customizer (MC) has a perfectly flexible production technology and thus can offer any variety within a product space, represented by Hotelling's linear city. The mass producer (MP) has a more focused production technology and therefore offers a finite set of products in the same space. The MP can invest in more flexible technology, which reduces its cost of variety and hence allows it to offer a larger set of products; in the extreme, the MP can emulate the MC's technology and offer infinite variety. The firms simultaneously decide whether to enter the market, and the MP chooses its degree of product-mix flexibility on entry. Next, the MP designs its product line--i.e., the number and position of its products--the MC's perfectly flexible technology makes this unnecessary. Finally, both firms simultaneously set prices. We analyze the subgame-perfect Nash equilibrium in this three-stage game, allowing firm-specific fixed and variable costs that together characterize their production technology. We find that an MP facing competition from an MC offers lower product variety than an MP monopolist to reduce the intensity of price competition. We also find that the MP can survive this competition, even if it has higher fixed cost of production technology, higher marginal cost of production, or both.", "e:keyword": ["Product variety management", "Mass customization", "Operations-marketing interface", "Discrete consumer choice", "Competitive product strategy", "Pricing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0171", "e:abstract": "In some retail contexts, higher inventories not only improve service levels, but also stimulate demand by serving as a promotional tool (e.g., by increasing product visibility). Motivated by a building-products retailer's practice of stocking large quantities of products to stimulate demand, we study inventory management and pricing policies when demand is uncertain but increases with stocking quantity. We first characterize the profit-maximization policy for a stochastic inventory model with a general inventory-dependent demand distribution and given price, and show that demand stimulation (by inventories) has the effect of increasing the target service level beyond the classical newsvendor model's critical fractile ratio. To underscore the importance of considering both demand stochasticity and inventory influence, we consider two functionally oriented benchmark policies--a demand-driven policy and a critical fractile policy--that might, respectively, represent marketing and inventory managers' viewpoints. Our numerical analysis reveals that the optimal policy can generate considerably higher profits than the two complementary functional perspectives. Moreover, we prove that the optimal stocking quantity always exceeds the critical fractile solution and can even exceed the demand-driven stocking quantity. We also address the problem of jointly optimizing both stocking quantity and price for demand-stimulating products using a multiplicative model to represent the influence of price and stocking quantity on the demand distribution. For this model, we show that the pricing and stocking decisions can be determined sequentially, with the optimal policy setting higher prices and stock levels than both the functional policies.", "e:keyword": ["Retail operations", "Inventory management", "Pricing", "Newsvendor model", "Promotional inventories"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0165", "e:abstract": "Awell-known result in the Bayesian inventory management literature is: If lost sales are not observed, the Bayesian optimal inventory level is larger than the myopic inventory level (one should \"stock more\" to learn about the demand distribution). This result has been proven in other studies under the assumption that inventory is perishable, so the myopic inventory level is equal to the Bayesian optimal inventory level with observed lost sales. We break that equivalence by considering nonperishable inventory. We prove that with nonperishable inventory, the famous \"stock more\" result is often reversed to \"stock less,\" in that the Bayesian optimal inventory level with unobserved lost sales is lower than the myopic inventory level. We also prove that making lost sales unobservable increases the Bayesian optimal inventory level; in this specific sense, the famous \"stock more\" result of other studies generalizes to the case of nonperishable inventory. When the product is out of stock, a customer may accept a substitute or choose not to purchase. We incorporate learning about the probability of substitution. This reduces the Bayesian optimal inventory level in the case that lost sales are observed. Reducing the inventory level has two beneficial effects: to observe and learn more about customer substitution behavior and (for a nonperishable product) to reduce the probability of overstocking in subsequent periods. Finally, for a capacitated production-inventory system under continuous review, we derive maximum likelihood estimators (MLEs) of the demand rate and probability that customers will wait for the product. (Accepting a raincheck for delivery at some later time is a common type of substitution.) We investigate how the choice of base-stock level and production rate affect the convergence rate of these MLEs. The results reinforce those for the Bayesian, uncapacitated, periodic review system.", "e:keyword": ["Bayesian inventory management", "Unknown demand distribution", "Unobserved lost sales", "Substitution probability", "Bayesian dynamic programming", "Optimal inventory control", "Maximum likelihood estimator", "Make-to-stock queue"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0203", "e:abstract": "Inventory record inaccuracy is a significant problem for retailers using automated inventory management systems. In this paper, we consider an intelligent inventory management tool that accounts for record inaccuracy using a Bayesian belief of the physical inventory level. We assume that excess demands are lost and unobserved, in which case sales data reveal information about physical inventory levels. We show that a probability distribution on physical inventory levels is a sufficient summary of past sales and replenishment observations, and that this probability distribution can be efficiently updated in a Bayesian fashion as observations are accumulated. We also demonstrate the use of this distribution as the basis for practical replenishment and inventory audit policies and illustrate how the needed parameters can be estimated using data from a large national retailer. Our replenishment policies avoid the problem of \"freezing,\" in which a physical inventory position persists at zero while the corresponding record is positive. In addition, simulation studies show that our replenishment policies recoup much of the cost of inventory record inaccuracy, and that our audit policy significantly outperforms the popular \"zero balance walk\" audit policy.", "e:keyword": ["Retail execution", "Inventory control", "Record inaccuracy", "Inventory shrinkage", "Bayes rule"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0175", "e:abstract": "The life cycle of new products is becoming shorter and shorter in all markets. For electronic products, life cycles are measured in units of months, with 6- to 12-month life cycles being common. Given these short product life cycles, product demand is increasingly difficult to forecast. Furthermore, demand is never really stationary because the demand rate evolves over the life of the product. In this paper, we consider the problem of where in a supply chain to place strategic safety stocks to provide a high level of service to the final customer with minimum cost. We extend our model for stationary demand to the case of nonstationary demand, as might occur for products with short life cycles. We assume that we can model the supply chain as a network, that each stage in the supply chain operates with a periodic review base-stock policy, that demand is bounded, and that there is a guaranteed service time between every stage and its customers. We consider a constant service time (CST) policy for which the safety stock locations are stationary; the actual safety stock levels change as the demand process changes. We show that the optimization algorithm for the case of stationary demand extends directly to determining the safety stocks when demand is nonstationary for a CST policy. We then examine with an illustrative example how well the CST policy performs relative to a dynamic policy that dynamically modifies the service times. In addition, we report on numerical tests that demonstrate the efficacy of the proposed solution and how it would be deployed.", "e:keyword": ["Base-stock policy", "Dynamic programming application", "Multiechelon inventory system", "Nonstationary demand", "Multistage supply chain application", "Safety stock optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0169", "e:abstract": "Gallego et al. [Gallego, G., G. Iyengar, R. Phillips, A. Dubey. 2004. Managing flexible products on a network. CORC Technical Report TR-2004-01, Department of Industrial Engineering and Operations Research, Columbia University, New York.] recently proposed a choice-based deterministic linear programming model (CDLP) for network revenue management (RM) that parallels the widely used deterministic linear programming (DLP) model. While they focused on analyzing \"flexible products\"--a situation in which the provider has the flexibility of using a collection of products (e.g., different flight times and/or itineraries) to serve the same market demand (e.g., an origin-destination connection)--their approach has broader implications for understanding choice-based RM on a network. In this paper, we explore the implications in detail. Specifically, we characterize optimal offer sets (sets of available network products) by extending to the network case a notion of \"efficiency\" developed by Talluri and van Ryzin [Talluri, K. T., G. J. van Ryzin. 2004. Revenue management under a general discrete choice model of consumer behavior. Management Sci. 50 15-33.] for the single-leg, choice-based RM problem. We show that, asymptotically, as demand and capacity are scaled up, only these efficient sets are used in an optimal policy. This analysis suggests that efficiency is a potentially useful approach for identifying \"good\" offer sets on networks, as it is in the case of single-leg problems. Second, we propose a practical decomposition heuristic for converting the static CDLP solution into a dynamic control policy. The heuristic is quite similar to the familiar displacement-adjusted virtual nesting (DAVN) approximation used in traditional network RM, and it significantly improves on the performance of the static LP solution. We illustrate the heuristic on several numerical examples.", "e:keyword": ["Network revenue management", "Choice behavior", "Multinomial logit choice model", "Dynamic programming", "Linear programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0164", "e:abstract": "This study considers a supply chain that consists of n retailers, each facing a newsvendor problem, and m warehouses. The retailers are supplied with a single product via some warehouses. In these warehouses, the ordered amounts of goods of these retailers become available after some lead time. At the time that the goods arrive at the warehouses, demand realizations are known by the retailers. The retailers can increase their expected joint profits if they can coordinate their orders and make allocations after demand realization. For this setting, we consider an associated cooperative game between the retailers. We show that this associated cooperative game has a nonempty core. Finally, we introduce a noncooperative game, where the retailers decide on their order quantities individually, and show that the set of payoff vectors resulting from strong Nash equilibria corresponds to the core of the associated cooperative game.", "e:keyword": ["Supply chain management", "Newsvendor", "Warehouse", "Game theory", "Balancedness", "Strong Nash equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0160", "e:abstract": "When inventory replenishments can arrive in a different sequence than the one in which they were placed, it is important to use the shortfall distribution to set the base-stock level. Because the exact shortfall distribution is quite difficult to compute, heuristics are commonly used in its stead. Bradley and Robinson [Bradley, J. R., L. W. Robinson. 2005. Improved base-stock approximations for independent stochastic lead times with order crossover. Manufacturing Service Oper. Management 7 319-329.] developed an upper bound on the variance of the number of outstanding orders that they used within a normal approximation of the shortfall distribution. In this short note, we tighten their upper bound and use it within a beta approximation of the shortfall distribution to derive a policy whose costs average only 0.05% above that of the optimal.", "e:keyword": ["Inventory policies/management", "Base-stock policies", "Stochastic lead time", "Order crossover"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0230", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0183", "e:abstract": "We study the optimal pricing of a finite quantity of a fashion-like seasonal good in the presence of forward-looking (strategic) customers. We distinguish between two classes of pricing strategies: contingent and announced fixed-discount. In both cases, the seller acts as a Stackelberg leader announcing his pricing strategy, while consumers act as followers taking the seller's strategy as given and determining their purchasing behavior. In each case, we identify a subgame-perfect Nash equilibrium and show that given the seller's strategy, the equilibrium in the consumer subgame is unique and consists of symmetric threshold purchasing policies. For both cases, we develop a benchmark model in which customers are nonstrategic (myopic). We conduct a comprehensive numerical study to explore the impact of strategic consumer behavior on pricing policies and expected revenue performance. We show that strategic customer behavior suppresses the benefits of price segmentation, particularly under medium-to-high values of heterogeneity and modest rates of decline in valuations. However, when the level of consumer heterogeneity is small, the rate of decline is medium-to-high, and the seller can optimally choose the time of discount in advance, segmentation can be used quite effectively even with strategic consumers. We find that the seller cannot avoid the adverse impact of strategic consumer behavior even under low levels of initial inventory. We argue that while the seller expects customers to be more concerned about product availability at discount time, he cannot use high-price \"betting\" strategies as he would in the case of low inventory and myopic customers. Under certain qualifications, announced fixed-discount strategies perform essentially the same as contingent pricing policies in the case of myopic consumers. However, under strategic consumer behavior, announced pricing policies can be advantageous to the seller, compared to contingent pricing schemes. Interestingly, those cases that announced discount strategies offer a significant advantage compared to contingent pricing policies. They appear to offer only a minimal advantage in comparison to fixed-pricing policies. Finally, when the seller incorrectly assumes that strategic customers are myopic in their purchasing decisions, it can be quite costly, reaching potential revenue losses of about 20%.", "e:keyword": ["Dynamic pricing", "Game theory applications", "Marketing-operations interface", "Revenue management", "Strategic consumer behavior"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0187", "e:abstract": "Upselling is offering an additional product to a customer who just made a purchase. Most catalogers and online sellers, in addition to some traditional retailers, use upselling often to clear inventories of slow-moving items. We investigate the pricing and discounting questions for such an item, which we call the promotional product. In our model, an arriving customer may purchase this promotional product or one of the other products that the firm sells. If the customer purchases one of the other products, the promotional product is offered to the customer, possibly with a discount. While deciding whether to offer a discount and, if so, how big a discount to offer, the firm uses the information that the customer has just bought a certain product with a certain price. We investigate how discounting decisions depend on the inventory levels, time, type of pricing policy in use, and the relationship between the customers' reservation prices for the promotional product and the other products (negatively or positively correlated). In particular, we find that if the firm sets prices and discounts dynamically and the customers' reservation prices for the promotional product are negatively correlated with their reservation prices for the product they purchased, then customers are always offered a discount regardless of the inventory levels and time. On the other hand, if the customers' reservation prices for the promotional product are positively correlated with their reservation prices for the product they purchased, then the customer may or may not be offered a discount, depending on the inventory levels and time. Our numerical study shows that the benefit to the firm from using customer purchase information is high when the firm uses a static price, but chooses discounts dynamically. We also find that although dynamic discounting decisions bring modest improvements, setting the price dynamically seems to have a more significant effect on the firm's profits.", "e:keyword": ["Dynamic pricing", "Pricing of limited inventories", "Cross-selling", "Bundling"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0185", "e:abstract": "We consider a duopoly market with heterogeneous customer tastes. The firms play a two-stage game. First, each firm chooses whether to invest in mass customization, which would enable it to offer customized products that increasingly match each customer's ideal product as the chosen customization level increases. A firm that chooses not to invest in mass customization serves a standard product. Second, the firms competitively price their product lines. We characterize each firm's investment in mass customization and study its dependence on competitive position, as determined by its cost efficiency and perceived quality vis--vis its competitor. We find that the value of mass customization critically depends on the firm's competitive position. It may not be desirable even at zero cost due to its negative effect on price competition. A firm with an overall cost and quality disadvantage never unilaterally adopts mass customization. We show that allowing firms to set different prices for each product configuration leads to a broader adoption of mass customization compared to when they are restricted to uniform prices. However, a firm's chosen customization level may be higher with uniform prices. Our analysis also helps a customizing firm determine whether to target its process improvement efforts for a lower cost or a higher customization level.", "e:keyword": ["Mass customization", "Product differentiation", "Price differentiation", "Customization level", "Competition"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0179", "e:abstract": "Accurate forecasting of call arrivals is critical for staffing and scheduling of a telephone call center. We develop methods for interday and dynamic intraday forecasting of incoming call volumes. Our approach is to treat the intraday call volume profiles as a high-dimensional vector time series. We propose first to reduce the dimensionality by singular value decomposition of the matrix of historical intraday profiles and then to apply time series and regression techniques. Our approach takes into account both interday (or day-to-day) dynamics and intraday (or within-day) patterns of call arrivals. Distributional forecasts are also developed. The proposed methods are data driven, appear to be robust against model assumptions in our simulation studies, and are shown to be very competitive in out-of-sample forecast comparisons using two real data sets. Our methods are computationally fast; it is therefore feasible to use them for real-time dynamic forecasting.", "e:keyword": ["Dimension reduction", "Dynamic forecast updating", "Principal component analysis", "Penalized least squares", "Singular value decomposition", "Vector time series"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0172", "e:abstract": "This paper introduces a new method for shift scheduling in multiskill call centers. The method consists of two steps. First, staffing levels are determined, and next, in the second step, the outcomes are used as input for the scheduling problem. The scheduling problem relies on a linear programming model that is easy to implement and has short computation times, i.e., a fraction of a second. Therefore, it is useful for different purposes and it can be part of an iterative procedure: for example, one that combines shifts into rosters.", "e:keyword": ["Contact centers", "Multiskill call centers", "Shift scheduling", "Skill-based routing", "Staffing", "Workforce management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0173", "e:abstract": "We study a simple method for staffing in multiskill call centers. The method has short computation times and determines nearly optimal staffing levels. It is in both views competitive to other methods from the literature. Because of the fast and accurate performance of the method, many different scenarios can be analyzed, and our method can be used for both tactical and strategic capacity management decisions.", "e:keyword": ["Contact centers", "Multiskill call centers", "Skill-based routing", "Staffing", "Work force management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0180", "e:abstract": "This paper studies a rental firm that offers reusable products to price- and quality-of-service-sensitive customers--Netflix or Blockbuster can be thought of as the canonical example. Customers' perception of quality is determined by their likelihood of obtaining the product or service immediately upon request. We study the alternatives of offering either a subscription option that limits the number of concurrent rentals in return for a flat fee per-unit time, or a pay-per-use option with no such restriction. Customers are assumed to desire a nominal usage rate of the product, which they meet by adjusting their request rate in either option. Thus, they have a higher request rate in the subscription option. We propose a Markov chain model for customer behavior under the subscription option equivalent to the standard Poisson model under the pay-per-use option. In a large market setting, assuming exponential demand, we show that using the subscription option is more profitable for the firm. Further, via a numerical study, we show that this assumption is not essential for the result to hold. However, we show that the subscription option does not necessarily dominate the pay-per-use option in quality of service. The firm manages the trade-off between price and quality of service better in the subscription option. Moreover, we show that social welfare and the consumer surplus can also be higher in the subscription option, indicating that both the firm and the consumers can benefit from the subscription option.", "e:keyword": ["Subscription services", "Pay-per-use services", "Operational benefits", "Pricing", "Capacity sizing", "Finite customer population", "Loss system", "On-off model", "Diffusion approximation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0210", "e:abstract": "We consider a revenue management, network capacity control problem in a setting where heterogeneous customers choose among the various products offered by a firm (e.g., different flight times, fare classes, and/or routings). Customers may therefore substitute if their preferred products are not offered. These individual customer choice decisions are modeled as a very general stochastic sequence of customers, each of whom has an ordered list of preferences. Minimal assumptions are made about the statistical properties of this demand sequence. We assume that the firm controls the availability of products using a virtual nesting control strategy and would like to optimize the protection levels for its virtual classes accounting for the (potentially quite complex) choice behavior of its customers. We formulate a continuous demand and capacity approximation for this problem, which allows for the partial acceptance of requests for products. The model admits an efficient calculation of the sample path gradient of the network revenue function. This gradient is then used to construct a stochastic steepest ascent algorithm. We show the algorithm converges in probability to a stationary point of the expected revenue function under mild conditions. The algorithm is relatively efficient even on large network problems, and in our simulation experiments it produces significant revenue increases relative to traditional virtual nesting methods. On a large-scale, real-world airline example using choice behavior models fit to actual booking data, the method produced an estimated 10% improvement in revenue relative to the controls used by the airline. The examples also provide interesting insights into how protection levels should be adjusted to account for choice behavior. Overall, the results indicate that choice behavior has a significant impact on both capacity control decisions and revenue performance and that our method is a viable approach for addressing the problem.", "e:keyword": ["Choice behavior", "Revenue management", "Network capacity control", "Stochastic approximation", "Stochastic gradients"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0184", "e:abstract": "While inventory- and revenue-management problems can be represented as Markov decision process (MDP) models, in some cases the well-known dynamic-programming curse of dimensionality makes it computationally prohibitive to solve them exactly. An alternative solution, called here the control-algorithm approach, is to use a math program (MP) to approximately represent the MDP and use its optimal solution to heuristically instantiate the parameters of the decision rules of a given set of control policies. As new information is observed over time, the control algorithm can incorporate it by re-solving the MP and revising the parameters of the decision rules with the newly obtained solution. The re-solving issue arises when one reflects on the consequences of this revision: Does the performance of the control algorithm really improve by revising its decision-rule instantiation with the solution of the re-solved MP, or should an appropriate modification of the prior solution be used? This paper analyzes the control-algorithm re-solving issue for a class of finite-horizon inventory- and revenue-management problems. It establishes sufficient conditions under which re-solving does not deteriorate the performance of a control algorithm, and it applies these results to control algorithms for network revenue management and multiproduct make-to-order production with lost sales and positive lead time.", "e:keyword": ["Multiproduct inventory management", "Network revenue management", "Mathematical programming-based dynamic programming approximations", "Model predictive control", "Rollout algorithms and policies"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0182", "e:abstract": "We analyze the problem of optimal location of a set of facilities in the presence of stochastic demand and congestion. Customers travel to the closest facility to obtain service; the problem is to determine the number, locations, and capacity of the facilities. Under rather general assumptions (spatially distributed continuous demand, general arrival and service processes, and nonlinear location and capacity costs) we show that the problem can be decomposed, and construct an efficient optimization algorithm. The analysis yields several insights, including the importance of equitable facility configurations (EFCs), the behavior of optimal and near-optimal capacities, and robust class of solutions that can be constructed for this problem.", "e:keyword": ["Facility location", "Stochastic demand", "Queueing", "Service level"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0178", "e:abstract": "This paper presents two methods to solve the production smoothing problem in mixed-model just-in-time (JIT) systems with large setup and processing time variability between different models the systems produce. The problem is motivated by production planning at a leading U.S. automotive pressure hose manufacturer. One method finds all Pareto-optimal solutions that minimize total production rate variation of models and work in process (WIP), and maximize system utilization and responsiveness. These Pareto-optimal solutions are found efficiently in polynomial time with respect to total demand by an algorithm proposed in the paper. The other method relies on Daniel Webster's method of apportionment for production smoothing, which produces periodic, uniform, and reflective production sequences that can improve operations management of the JIT systems. Finally, the paper presents the results of a computational experiment with the two methods.", "e:keyword": ["Production smoothing", "Mixed-model manufacturing", "Just-in-time manufacturing", "Optimization", "Webster's method of apportionment"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0190", "e:abstract": "We investigate learning by doing in the newsvendor inventory problem. An earlier study observed that decision makers tend to anchor their orders around average demand and fail to adjust sufficiently toward the expected profit-maximizing order. Principles of behavioral theory suggest some relatively simple interventions into the decision maker's experience and feedback that might improve performance, and these guide our investigation. The results imply that the institutional organization of experience and feedback may have a significant influence on whether inventory is stocked optimally.", "e:keyword": ["Newsvendor problem", "Feedback", "Behavioral operations management", "Supply chain management", "Experimental economics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0186", "e:abstract": "This paper extends the theory of N competitive newsvendors to the case where competition occurs simultaneously in price and inventory. The basic research questions are whether the Nash equilibrium exists in this game, whether it is unique, and how the resulting inventories and prices are affected by competition. Using a novel method, we show the quasiconcavity of the competitive newsvendor's problem and establish the existence of the pure-strategy Nash equilibrium. Through a contraction mapping approach, we develop sufficient conditions for the Nash equilibrium to be unique. We then analyze the properties of the equilibrium and compare it with the optimal solution for the (noncompeting) price-sensitive newsvendor. We prove that at a symmetric equilibrium, retail prices and safety stocks strictly increase with the proportion of a newsvendor's unsatisfied customers that switch to a competitor, but strictly decrease with the intensity of price competition. Total inventories, on the other hand, increase with the intensity of price competition. Furthermore, the competitive equilibrium never has lower safety stocks and higher retail prices (a situation that definitely hurts the customers) than the solution for noncompetitive newsvendors.", "e:keyword": ["Newsvendor game", "Pricing", "Inventory", "Equilibrium analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0181", "e:abstract": "This research studies the impact of improved forecasts on the members of a two-stage supply chain. The supplier builds capacity based on original forecast information, and the manufacturer places its order after observing improved (but imperfect) demand information. We study three types of wholesale price purchasing arrangements: (1) when the wholesale price is determined to be exogenous to the supply chain; (2) when the supplier sets the wholesale price; and (3) when the manufacturer sets the wholesale price. Although improved demand information reduces the demand uncertainty that the manufacturer faces, the manufacturer is constrained by the supplier's capacity decision. In all three cases, we show that improved information can decrease the supply chain's expected profit, even as the supplier's capacity increases with improved information. Because improved demand information always increases the centralized supply chain's expected profit, we present a contract that coordinates the channel and provides flexibility in dividing systemwide profit.", "e:keyword": ["Inventory", "Forecasting", "Forecast revisions", "Decentralized supply chains"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0227", "e:abstract": "The observed behavior of customers and managers often does not fit the assumptions of theoretical models used in the operations management (OM) literature. New research in behavioral OM is emerging to bridge the gap between traditional models and these newer observational findings. This work is both deductive and inductive, and it draws on multiple reference disciplines: experimental and behavioral economics, judgment and decision making from psychology, and organizational behavior and decision analysis from management. This special issue presents seven interesting papers that reflect the wide range of current research activity in behavioral OM. These papers provide insightful and thought-provoking introduction to this new direction in OM research.", "e:keyword": ["Behavioral operations management", "Behavioral operations", "Operations management", "Experimental and behavioral economics", "Judgment and decision making", "Heuristics and biases"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0200", "e:abstract": "Many theoretical models adopt a normative approach and assume that decision makers are perfect optimizers. In contrast, this paper takes a descriptive approach and considers bounded rationality, in the sense that decision makers are prone to errors and biases. Our decision model builds on the quantal choice model: While the best decision need not always be made, better decisions are made more often. We apply this framework to the classic newsvendor model and characterize the ordering decisions made by a boundedly rational decision maker. We identify systematic biases and offer insight into when overordering and underordering may occur. We also investigate the impact of these biases on several other inventory settings that have traditionally been studied using the newsvendor model as a building block, such as supply chain contracting, the bullwhip effect, and inventory pooling. We find that incorporating decision noise and optimization error yields results that are consistent with some anomalies highlighted by recent experimental findings.", "e:keyword": ["Bounded rationality", "Newsvendor", "Logit choice", "Random utility", "Quantal response", "Supply chain", "Bullwhip effect", "Inventory", "Pooling"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0228", "e:abstract": "In the newsvendor game, the expected-profit-maximizing order quantity is higher in the demand interval when the per-unit profit margin is high and lower in the demand interval when the per-unit profit margin is low. However, laboratory experiments show a \"pull-to-center\" effect: average order quantities are too low when they should be high and vice versa. We replicate this pull-to-center effect in laboratory experiments and construct an adaptive learning model that incorporates memory, reinforcement, and probabilistic choice to explain individual decisions. The intuition underlying the model's prediction is that the most recent demand observation is more likely to have been greater than the optimal order quantity if the optimal order quantity is low, in which case a recency bias tends to pull the order quantity upward. A countervailing downward pull exists if the optimal order quantity is high. The recency effect may be augmented by a reinforcement bias, which causes subjects to focus more on the profitability of decisions they actually make and less on counterfactual payoffs that would have resulted from other order quantities. The predictions of this model track the observed data patterns across treatments. A pull-to-center pattern is also observed in designs involving doubled payoffs and reduced order frequency.", "e:keyword": ["Newsvendor problem", "Dynamic learning"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0188", "e:abstract": "Asupplier stocking goods for delivery to a retailer may face a (finite-horizon) service-level agreement (SLA). In this context, the SLA is a commitment by a supplier to achieve a minimum fill rate over a specified time horizon. This kind of SLA is an important, but understudied coordination mechanism. We focus on the impact of two contract parameters: the length of the review period and the magnitude of the bonus for meeting or exceeding the service-level target. For a supplier following a base stock (order-up-to) inventory policy, increasing the bonus increases optimal supplier stocking levels, whereas lengthening the review period may increase or decrease optimal stocking levels. We investigate these mechanisms in a controlled laboratory setting and find that longer review periods are generally more effective than shorter review periods in inducing higher stocking levels. As in several earlier laboratory studies, the explanation lies in the improved feedback reliability that longer review periods provide. The primary managerial implication of our findings is that, in practice, longer review periods may be more effective than shorter ones at inducing service improvements.", "e:keyword": ["Service-level agreements", "Behavioral operations management", "Supply chain management", "Experimental economics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0229", "e:abstract": "We study a problem of selling a fixed number of goods over a finite and known horizon. After presenting a procedure for computing optimal decision policies and some numerical results of a simple heuristic policy for the problem, we describe results from three experiments involving financially motivated subjects. The experiments reveal that decision makers employ decision policies of the same form of the optimal policy. However, they show systematic biases to demand too much when they have many units to sell and too little when they have few to sell, resulting in significant revenue losses.", "e:keyword": ["Behavioral operations", "Revenue management", "Dynamic pricing", "Decision bias", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0189", "e:abstract": "Fair process research has shown that people care not only about outcomes, but also about the process that produces these outcomes. For a decision process to be seen as fair, the people affected must have the opportunity to give input and possibly to influence the decision, and the decision process and rationale must be transparent and clear. Existing research has shown empirically that fair process enhances both employee motivation and performance in execution. However, work to date has not addressed why fair process is so often violated in practice. This paper breaks new ground by analytically examining the subtle trade-offs involved. We develop a model of fair process in a principal-agent (i.e., manager-employee) context, rooted in psychological preferences for autonomy and fairness. We show that indeed fair process will not always be used, and why the hoped-for benefits may be insufficient to convince management to use fair process.", "e:keyword": ["Fair process", "Engagement", "Transparency", "Social preference", "Agency theory", "Motivation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0177", "e:abstract": "We study a manufacturer's problem of managing his direct online sales channel together with an independently owned bricks-and-mortar retail channel, when the channels compete in service. We incorporate a detailed consumer channel choice model in which the demand faced in each channel depends on the service levels of both channels as well as the consumers' valuation of the product and shopping experience. The direct channel's service is measured by the delivery lead time for the product; the retail channel's service is measured by product availability. We identify optimal dual channel strategies that depend on the channel environment described by factors such as the cost of managing a direct channel, retailer inconvenience, and some product characteristics. We also determine when the manufacturer should establish a direct channel or a retail channel if he is already selling through one of these channels. Finally, we conduct a sequence of controlled experiments with human subjects to investigate whether our model makes reasonable predictions of human behavior. We determine that the model accurately predicts the direction of changes in the subjects' decisions, as well as their channel strategies in response to the changes in the channel environment. These observations suggest that the model can be used in designing channel strategies for an actual dual channel environment.<sup>1</sup>", "e:keyword": ["Dual channels", "Direct channel", "Service competition", "Product availability", "Supply chain contracting", "Experimental economics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0205", "e:abstract": "Human beings are critical to the functioning of the vast majority of operating systems, influencing both the way these systems work and how they perform. Yet most formal analytical models of operations assume that the people who participate in operating systems are fully rational or at least can be induced to behave rationally. Many other disciplines, including economics, finance, and marketing, have successfully incorporated departures from this rationality assumption into their models and theories. In this paper, we argue that operations management scholars should do the same. We explore the theoretical and practical implications of incorporating behavioral and cognitive factors into models of operations management and suggest fruitful avenues for research in behavioral operations.", "e:keyword": ["Behavioral operations", "Decision making", "Beer game", "System dynamics", "Cognitive biases"]}, {"@id": "http://dx.doi.org/10.1287/msom.1060.0110", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0191", "e:abstract": "Suppliers routinely sell goods to retailers on credit. Common credit terms are tantamount to a schedule of declining discounts (escalating penalties) that depend on how long the retailer takes to pay off the supplier's loan. However, issues such as which stocking policies are optimal in the presence of supplier-provided credit have been investigated only when demand is assumed deterministic. Nearly all stochastic inventory models assume either time-invariant finance charges or charges that may vary with time but not with the age of the credit. In this article we present a discrete time model of the retailer's operations with random demand, which is used to prove that the structure of the optimal policy is not affected by credit terms, although the value of the optimal policy parameter is. This is followed by a continuous time model, which leads to an algorithm for finding the optimal stock level. We also model the supplier's problem and calculate the optimal credit parameters in numerical experiments.", "e:keyword": ["Trade credit", "Finance and inventory models"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0194", "e:abstract": "This paper employs sample path arguments to derive the following convexity properties and comparative statics for an M/M/S queue with impatient customers. If the rate at which customers balk and renege is an increasing, concave function of the number of customers in the system (head count), then the head-count process and the expected rate of lost sales are decreasing and convex in the capacity (service rate or number of servers). This result applies when customers cannot observe the head count, so that the balking probability is zero and the reneging rate increases linearly with the head count. Then the optimal capacity increases with the customer arrival rate but is not monotonic in the reneging rate per customer. When capacity is expensive or the reneging rate is high, the optimal capacity decreases with any further increase in the reneging rate. Therefore, managers must understand customers' impatience to avoid building too much capacity, but customers have an incentive to conceal their impatience, to avoid a degradation in service quality. If the system manager can prevent customers from reneging during service (by requiring advance payment or training employees to establish rapport with customers), the system's convexity properties are qualitatively different, but its comparative statics remain the same. Most important, the prevention of reneging during service can substantially reduce the total expected cost of lost sales and capacity. It increases the optimal capacity (service rate or number of servers) when capacity is expensive and reduces the optimal capacity when capacity is cheap.", "e:keyword": ["Capacity planning", "Queueing systems", "Reneging", "Balking", "Unobservable queues", "Stochastic convexity", "Sample path convexity"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0192", "e:abstract": "Motivated by the ease with which online customers can bid simultaneously in multiple auctions, we analyze a system with two competing auctioneers and three types of bidders: those dedicated to either of the two auctions and those that participate simultaneously in both auctions. Bidding behavior is specified and proven to induce a Bayesian Nash equilibrium, and a closed-form expression for the expected revenue of each auctioneer is derived. For auctioneers selling a single item, partial pooling--i.e., the presence of some cross-auction bidders--is beneficial to both auctioneers as long as neither one dominates the market (e.g., possesses more than 60%-65% of the market share). For multi-item auctions, pooling is mutually beneficial only if both auctioneers have nearly identical ratios of bidders per items for sale; otherwise, only the auctioneer with the smaller ratio benefits from pooling. Pooling's impact on revenue decreases with the number of bidders, suggesting that popular auction sites need not be overly concerned with mitigating bidding across auctions.", "e:keyword": ["Auctions", "Bidding", "Pooling"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0193", "e:abstract": "Research in consumer psychology shows that customers seek reasons for service failures and that attributions of blame moderate the effects of failure on the level of customer satisfaction. This paper extends research on service operations failures by hypothesizing that attributions of blame also affect what matters to the customer during service failures. Specifically, we hypothesize that the relative weights that customers assign to key service elements in reaching an overall assessment of customer satisfaction are affected by customer attributions of blame for service failures. We use the U.S. airline industry as a quasi-experimental research setting to investigate the components of customer satisfaction for three samples of customers who experience (1) routine service, (2) flight delays of external (i.e., weather) origin, and (3) flight delays of internal origin. Although the level of customer satisfaction is lower for all service failures, we find that the key components of satisfaction differ between delayed and routine flights only when customers blame the service provider for the failure. Specifically, when delays are of external origin satisfaction is lower than for routine flights, but there is virtually no difference in the weight that customers assign to the components of customer satisfaction (including employee interactions). In contrast, when delays are of internal origin, satisfaction is lower than for either routine flights or flights delayed by external factors, and employee interactions have a significantly diminished role in customer satisfaction evaluations. Contrary to the popular view that employee interactions take on a greater role in determining customer satisfaction during service failures, we find that the opposite is true if the customer attributes blame to the service provider. Our findings highlight the important role of customer attributions during service failures and present more nuanced evidence on the role of employee-customer interactions in mitigating the effects of service failures on customer satisfaction.", "e:keyword": ["Service operations", "Customer satisfaction", "Attribution theory", "Failure recovery", "Airline industry"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0195", "e:abstract": "Generalizing earlier work on staffing and routing in telephone call centers, we consider a processing network model with large server pools and doubly stochastic input flows. In this model the processing of a job may involve several distinct operations. Alternative processing modes are also allowed. Given a finite planning horizon, attention is focused on the two-level problem of capacity choice and dynamic system control. A pointwise stationary fluid model (PSFM) is used to approximate system dynamics, which allows development of practical policies with a manageable computational burden. Earlier work in more restrictive settings suggests that our method is asymptotically optimal in a parameter regime of practical interest, but this paper contains no formal limit theory. Rather, it develops a PSFM calculus that is broadly accessible, with an emphasis on modeling and practical computation.", "e:keyword": ["Admission control", "Dynamic routing", "Doubly stochastic arrivals", "Approximation", "Pointwise stationary", "Fluid models", "Abandonments", "Stochastic networks"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0198", "e:abstract": "Moving production to low-wage countries may reduce manufacturing costs, but it increases logistics costs and is subject to foreign trade barriers, among others. This paper studies a manufacturer's multimarket facility network design problem and investigates the offshoring decision from a network capacity investment perspective. We analyze a firm that manufactures two products to serve two geographically separated markets using a common component and two localized final assemblies. The common part can be transported between the two markets that have different economic and demand characteristics. Two strategic network design questions arise naturally: (1) Should the common part be produced centrally or in two local facilities? (2) If a centralization strategy is adopted, in which market should the facility be located? We present a transportation cost threshold that captures costs, revenues, and demand risks, and below which centralization is optimal. The optimal location of commonality crucially depends on the relative magnitude of price and manufacturing cost differentials but also on demand size and uncertainty. Incorporating scale economies further enlarges the centralization's optimality region.", "e:keyword": ["Capacity investment", "Newsvendor network", "Location", "Transshipment", "Commonality"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0199", "e:abstract": "Contracting with suppliers prone to default is an increasingly common problem in some industries, particularly automotive manufacturing. We model this phenomenon as a two-period contracting game with two identical suppliers, a single buyer, deterministic demand, and uncertain production costs. The suppliers are distressed at the start of the game and do not have access to external sources of capital; hence, revenues from the buyer are crucial in determining whether default occurs. The production cost of each supplier is the sum of two stochastic components: a common term that is identical for both suppliers (representing raw materials costs, design specifications, etc.) and an idiosyncratic term that is unique to a given supplier (representing inherent firm capability). The buyer chooses a supplier and then decides on a single- or two-period contract. Comparing models with and without the possibility of default, we find that, without the possibility of supplier failure, the buyer always prefers short-term contracts over long-term contracts, whereas this preference is typically reversed in the presence of failure. Neither of these contracts coordinates the supply chain. We also consider dynamic contracts, in which the contract price is partially tied to some index representing the common component of production costs (e.g., commodity prices of raw materials such as steel or oil), allowing the buyer to shoulder some of the risk from cost uncertainty. We find that dynamic long-term contracts allow the buyer to coordinate the supply chain in the presence of default risk. We also demonstrate that our results continue to hold under a variety of alternative assumptions, including stochastic demand, allowing the buyer the option of subsidizing a bankrupt supplier via a contingent transfer payment or loan and allowing the buyer to unilaterally renegotiate contracts. We conclude that the possibility of supplier default offers a new reason to prefer long-term contracts over short-term contracts.", "e:keyword": ["Supply chain management", "Financial default", "Contracting", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0201", "e:abstract": "We consider a make-to-stock supplier that operates a production facility with limited capacity. The supplier receives orders from customers belonging to several demand classes. Some of the customer classes share advance demand information with the supplier by announcing their orders ahead of their due date. However, this advance demand information is not perfect because the customer may decide to order prior to or later than the expected due date or may decide to cancel the order altogether. Customer classes vary in their demand rates, expected due dates, cancellation probabilities, and shortage costs. The supplier must decide when to produce and, whenever an order becomes due, whether or not to satisfy it from on-hand inventory. Hence, the supplier is faced with a joint production-control and inventory-allocation problem. We formulate the problem as a Markov decision process and characterize the structure of the optimal policy. We show that the optimal production policy is a state-dependent base-stock policy with a base-stock level that is nondecreasing in the number of announced orders. We show that the optimal inventory-allocation policy is a state-dependent multilevel rationing policy, with the rationing level for each class nondecreasing in the number of announced orders (regardless of whether the class provides advance information). From numerical results, we obtain several insights into the value of advance demand information for both supplier and customers.", "e:keyword": ["Advance demand information", "Production-inventory systems", "Inventory rationing", "Make-to-stock queues", "Markov decision process"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0209", "e:abstract": "We consider a multiproduct assemble-to-order (ATO) system, in which inventory is kept only at the component level and the finished products are assembled in response to customer demands. In addition to stochastic demand for finished products, the system experiences stochastic returns of subsets of components, which can then be used to satisfy subsequent demands. The system is managed over an infinite horizon using a component-level base-stock policy. We identify several ways in which returns complicate the behavior of the system, and we demonstrate how to handle these additional complexities when calculating or approximating key order-based performance metrics, including the immediate fill rate, the fill rate within a time window, and average backorders. We also present a method for computing a near-optimal base-stock policy. We use these results to address managerial questions on both operational and product-design levels. For example, we find that tracking product-based (as opposed to component-based) return information appears to provide much less value than tracking product-based demand information, and we explore the impact of the number of products, component lead times, and different patterns of component returns (joint versus independent returns, returns of common versus dedicated components) on the value of component commonality.", "e:keyword": ["Supply chain management", "Product returns", "Assemble to order", "Reverse logistics", "Environment"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0207", "e:abstract": "Check processing institutions are being forced to downsize their workforce to cut cost and improve the efficiency of their operations as a result of continued growth of electronic payments, a consequence of the increasing popularity of debit/credit cards and use of online banking. For these institutions, these events are making more urgent the decision of how to staff a check-clearing house to trade off efficiency and the expected costs associated with the risks of delayed checks, which include fraud and float costs. In this paper, we discuss how a team of executives at a major commercial bank (CB) and Carnegie Mellon University students and faculty engaged in conducting a model-based study of the CB check-clearing operations. This project culminated in the development of a simulation optimization model to systematically analyze the nature of the highlighted risk efficiency trade-off at CB. The firm used the model recommendations to obtain operations downsizing guidelines for its senior managers during the implementation of a strategic workforce reduction program at their check-clearing house. The managerial insights from the team analysis, and the specific model-based recommendations, enabled CB executives to balance risk and efficiency while planning the reduction of their check-processing workforce.", "e:keyword": ["Check processing", "Workforce sizing", "Worker cross-training", "Cost of risk", "Simulation optimization", "Math-programming-based lower bound", "Empirical research"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0204", "e:abstract": "Motivated by many recent applications reported in the literature, we examine the impact of a second procurement opportunity on inventory management of products with short selling seasons. In our framework, the first order is placed at the start of the preseason and delivered at the start of the selling season; the second order is placed at or after the start of the selling season for subsequent delivery. Under this framework, the decision maker must make three interrelated choices: the first order quantity, when to place the second order, and the second order quantity. Our focus is on elucidating the optimal policy structure for the three interrelated decisions. By casting our models as sequential decision-making problems, we are able to reduce the optimization problems into sequential and embedded searches for the concerned decision variables that allow us to identify the conditions on the economic parameters and demand distribution to effectively facilitate the search for the optimal solutions.", "e:keyword": ["Newsvendor problem", "Short selling season", "Second order opportunity", "Inventory management", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0206", "e:abstract": "Dual sourcing and inventory are two prevalent and widely studied strategies firms use to manage yield risk. A pervasive but implicit assumption in the literature is that a firm knows its suppliers' yield distributions with certainty. This is a strong assumption in many circumstances. A firm is more likely to have a forecast of a supplier's yield distribution and to update that forecast based on its experiences with the supplier. We introduce and analyze a Bayesian model of \"supply learning\" (i.e., distribution updating) and investigate how supply learning influences both sourcing and inventory strategies in dual-sourcing and single-sourcing models, respectively. In the case of Bernoulli all-or-nothing yield distributions, we completely characterize the firm's optimal sourcing and inventory decisions for the supply-learning model. Among other results, we prove that for a given expected supplier reliability (i.e., the mean of the firm's forecast for the probability of successful delivery) an increase in the reliability forecast uncertainty increases the attractiveness of a supplier, but it reduces the firm's desire to invest in inventory to protect against future supply failures. We extend our analysis to allow for general yield distributions, multiple sourcing (i.e., more than two suppliers), and inventory carryover in the dual-sourcing model.", "e:keyword": ["Supply uncertainty", "Dual sourcing", "Inventory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0208", "e:abstract": "We address the scheduling of operations in a robotic cell that produces multiple part-types. The objective is to obtain a cyclic schedule--a sequence of robot moves and an ordering of the parts--that minimizes the long-run average time to produce a part or, equivalently, maximizes the throughput. We consider two different models that are currently used in practice. The first is a single-gripper cell with a unit-capacity output buffer at each machine. The second is a bufferless dual-gripper cell. We focus our analysis on a widely used class of cyclic solutions, referred to as CRM cycles. The main outcome of our analysis is the equivalence of the two models (i.e., the maximum throughput is the same for both models) under conditions that are common in practice. The equivalence is established in two steps: (i) identification of a subset of dominating cyclic solutions for each model, and (ii) provision of a one-to-one mapping between these dominating cycles such that corresponding cycles have the same throughput. We also analyze the computational complexity of the throughput maximization problem for the two models. The enhanced capabilities (i.e., output buffers and dual gripper) of both models were motivated by a need to improve the throughput. However, the costs of acquiring these capabilities differ significantly. Our discussions with operations managers at a Dallas-area robotic cell manufacturer revealed that the total cost of designing and programming the robot's control mechanism for a cell with output buffers is about 20% less than that for a dual-gripper cell. The equivalence of the two models is, therefore, somewhat surprising and has significant practical implications. For cells that operationalize CRM cycles, the use of output buffers instead of a dual gripper can result in considerable savings without compromising throughput.", "e:keyword": ["Robotic cells", "Cyclic production", "Dual-gripper cells", "Output buffers", "Computational complexity"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0202", "e:abstract": "We consider a component acquisition problem for a contract manufacturer who faces a one-time stochastic demand for a single product consisting of a number of components. Components can be ordered early at normal prices before the demand is revealed, or they can be replenished later at higher prices due to expediting. In addition, the customer pays a price for the final product that is nonincreasing in the delivery lead time to discourage late delivery. We propose a cost-minimization model and develop an efficient polynomial time algorithm to solve the problem.", "e:keyword": ["Assemble-to-order systems", "Component supply lead times", "Expediting"]}, {"@id": "http://dx.doi.org/10.1287/msom.1070.0211", "e:abstract": "In a recent paper we introduced the queue-and-idleness ratio (QIR) family of routing rules for many-server service systems with multiple customer classes and server pools. A newly available server serves the customer from the head of the queue of the class (from among those the server is eligible to serve) whose queue length most exceeds a specified proportion of the total queue length. Under fairly general conditions, QIR produces an important state-space collapse as the total arrival rate and the numbers of servers increase in a coordinated way. That state-space collapse was previously used to delicately balance service levels for the different customer classes. In this sequel, we show that a special version of QIR stochastically minimizes convex holding costs in a finite-horizon setting when the service rates are restricted to be pool dependent. Under additional regularity conditions, the special version of QIR reduces to a simple policy: linear costs produce a priority-type rule, in which the least-cost customers are given low priority. Strictly convex costs (plus other regularity conditions) produce a many-server analogue of the generalized-c\\mu  (Gc\\mu ) rule, under which a newly available server selects a customer from the class experiencing the greatest marginal cost at that time.", "e:keyword": ["Queues", "Many-server queues", "Heavy-traffic limits for queues", "Service systems", "Cost minimization in many-server queues", "Skill-based routing", "Generalized-c\\mu  rule", "Queue-and-idleness-ratio control"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0212", "e:abstract": "We study how rework routing together with wage and piece-rate compensation can strengthen incentives for quality. Traditionally, rework is assigned back to the agent who generates the defect (in a self-routing scheme) or to another agent dedicated to rework (in a dedicated routing scheme). In contrast, a novel cross-routing scheme allocates rework to a parallel agent performing both new jobs and rework. The agent who passes quality inspection or completes rework receives the piece rate paid per job. We compare the incentives of these rework-allocation schemes in a principal-agent model with embedded quality control and routing in a multiclass queueing network. We show that conventional self-routing of rework cannot induce first-best effort. Dedicated routing and cross-routing, however, strengthen incentives for quality by imposing an implicit punishment for quality failure. In addition, cross-routing leads to workload-allocation externalities and a prisoner's dilemma, thereby creating the greatest incentives for quality. Firm profitability depends on demand levels, revenues, and quality costs. When the number of agents increases, the incentive effect of cross-routing reduces monotonically and approaches that of dedicated routing.", "e:keyword": ["Queueing networks", "Routing", "Nash equilibrium", "Quality control", "Piece rate", "Epsilon equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0215", "e:abstract": "This paper contributes to the literature on enterprise resource planning (ERP) by pursuing two objectives. First, it identifies configurations of ERP adopters that have similar needs and develop similar competencies. Second, it tests the hypothesis that, to maximize benefits from their ERP projects, organizations should align their ERP competence-building mechanisms with the ERP needs that arise from their operational environment. The analysis of a sample of manufacturing companies that implemented ERP between 1995 and 2001 uncovers four distinct configurations representing different degrees of fit between needs and competence-building mechanisms: the frugal ERP, the extensive business process reengineering (BPR), the adaptive ERP, and the straitjacket. The results support our hypothesis and suggest that the consequences of a misfit between needs and competence-building mechanisms are more severe for companies that operate in complex and dynamic environments and have informal organizational structures than for firms with rigid structures that operate in simple and stable environments.", "e:keyword": ["Enterprise resource planning", "Operations strategy", "Information and communication technology", "Empirical research", "Cluster analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0220", "e:abstract": "We study a supply chain where an upstream supplier auctions his inventory or capacity as a bundle. The importance of this setting is twofold: From a practical point of view, there are several examples, both in manufacturing (e.g., auctioning the capacity of a wafer fabrication facility) and service industries (e.g., auctioning the sponsorship of a website), where a supplier's capacity is sold as a single piece; from a theoretical side, it highlights the information asymmetry introduced on the downstream supply chain parties when the auction result is disclosed. We formulate the problem as a two-stage supply chain comprising a single supplier and two resellers. Each reseller receives a signal of the consumer demand and bids for the capacity of the supplier. The supplier announces the winner as well as the auction price. Both resellers can get additional units in a procurement market and then engage in Cournot competition in the consumer market. We analyze the impact of the information elicited by the supplier in the early stage of the game. We characterize sufficient conditions for the existence of equilibrium behavior, derive the equilibrium bidding functions under both first- and second-price auctions, and show that the bidding functions are lower than the corresponding ones for a single-shot auction without resale. Our computational experiments indicate that both the supplier and resellers are better off running a second-price auction and that consumers benefit if the resellers have very different signals on the total demand. Overall, our results suggest that traditional auctions may have a profound impact in the context of a supply chain because of the information disclosure in the upstream stages.", "e:keyword": ["Supply chain", "Auctions", "Information asymmetry", "Demand effort", "Interdependent values"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0221", "e:abstract": "We study dynamic pricing and inventory control of substitute products for a retailer who faces a long supply lead time and a short selling season. Within a multinomial logit model of consumer choice over substitutes, we develop a stochastic dynamic programming formulation and derive the optimal dynamic pricing policy. We prove that dynamic pricing converges to static pricing as inventory levels of all variates approach the number of remaining selling periods (assuming at most one customer arrival within each period). Our extensive numerical study of the effects of time and inventory depletion on the optimal pricing reveals two fundamental underlying driving forces of the complex price behavior: the level of inventory scarcity and the quality difference among products. We also compare the performance of three restricted pricing strategies: static, unified dynamic, and mixed dynamic pricing. We find that full-scale dynamic pricing is of great value in the presence of inventory scarcity, and initial inventory decisions are quite robust in the pricing scheme employed in the selling season. Based on the above insights, we propose a computationally efficient approach to the initial inventory decision, which delivers close-to-optimal inventory levels for all testing cases.", "e:keyword": ["Dynamic pricing", "Inventory control", "Substitute products", "Multinomial logit model"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0222", "e:abstract": "When a customer orders online, an online retailer assigns the order to one or more of its warehouses and/or drop-shippers to minimize procurement and transportation costs based on the available current information. However, this assignment is necessarily myopic because it cannot account for any subsequent customer orders or future inventory replenishment. We examine the benefits of periodically reevaluating these real-time assignments. We construct near-optimal heuristics for the reassignment for a large set of customer orders to minimize the total number of shipments. Finally, we present evidence of significant saving opportunities by testing the heuristics on order data from a major online retailer.", "e:keyword": ["Online retailing", "Online order fulfillment", "Local search heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0231", "e:abstract": "In this paper, we describe data collected from five U.S. properties of a major hotel chain that can be used to benchmark the performance of choice-based revenue management (RM) algorithms. The process used to collect this data illustrates subtle complexities involved in extracting product availability information from current RM systems and sheds new light on practical issues that need to be addressed to successfully implement choice-based RM systems. The data described in this paper is publicly available at the journal's website at <ext-link ext-link-type=\"uri\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"http://msom.pubs.informs.org\">http://msom.pubs.informs.org</ext-link>.", "e:keyword": ["Empirical research", "OM-marketing interface", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0218", "e:abstract": "We consider an infinite horizon production planning problem with nonstationary, convex production and inventory costs. Backlogging is allowed, unlike as in related previous work, and inventory cost is interpreted as backlogging cost when inventory is negative. We create finite horizon truncations of the infinite horizon problem and employ classic results on convex production planning to derive a closed-form formula for the minimum forecast horizon. We show that optimal production levels are monotonically increasing in the length of horizon, leading to solution convergence and a rolling horizon procedure for delivering an infinite horizon optimal production plan. The minimum forecast horizon formula is employed to illustrate how cost parameters affect how far one must look into the future to make an infinite horizon optimal decision today.", "e:keyword": ["Minimum forecast horizons", "Convex network flow", "Production planning"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0268", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0226", "e:abstract": "This paper proposes a resource allocation and pricing mechanism for a service system that serves multiple classes of jobs within an organization. Each class of service request is subject to a class-dependent quality of service (QoS) guarantee on the expected delay bound, which may be imposed by business rules in an organization or other application-specific technical constraints. We develop an extension of a resource allocation and pricing mechanism for an M/M/1 system. In contrast to the system without the QoS guarantee, where a fixed priority scheduling policy--known as the c\\mu  rule--is optimal, we show that the system may need to adopt a more general randomized priority scheduling policy to maximize the overall system profit. We also develop a transfer pricing scheme that is both optimal and incentive compatible, allowing users to act in their self-interests while collectively achieving the system optimum. We show that the pricing scheme with the QoS guarantee depends on the scheduling policy implemented and has different characteristics from that without the QoS guarantee.", "e:keyword": ["Capacity planning and investment", "Technology management and process design", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0223", "e:abstract": "Motivated by interest in making delay announcements to arriving customers who must wait in call centers and related service systems, we study the performance of alternative real-time delay estimators based on recent customer delay experience. The main estimators considered are: (i) the delay of the last customer to enter service (LES), (ii) the delay experienced so far by the customer at the head of the line (HOL), and (iii) the delay experienced by the customer to have arrived most recently among those who have already completed service (RCS). We compare these delay-history estimators to the standard estimator based on the queue length (QL), commonly used in practice, which requires knowledge of the mean interval between successive service completions in addition to the QL. We characterize performance by the mean squared error (MSE). We do an analysis and conduct simulations for the standard GI/M/s multiserver queueing model, emphasizing the case of large s. We obtain analytical results for the conditional distribution of the delay given the observed HOL delay. An approximation to its mean value serves as a refined estimator. For all three candidate delay estimators, the MSE relative to the square of the mean is asymptotically negligible in the many-server and classical heavy-traffic (HT) limiting regimes.", "e:keyword": ["Delay estimation", "Real-time delay estimation", "Delay prediction", "Delay announcements", "Many-server queues", "Call centers", "Heavy traffic"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0225", "e:abstract": "This paper is motivated by two phenomena observed in many queueing systems in practice. The first is the partitioning of server capacity among different customers based on their service time requirements. The second is rush hour demand where a large number of customers arrive over a short period of time followed by few or no arrivals for an extended period thereafter. We study a system with multiple parallel servers and multiple customer classes. The servers can be partitioned into server groups, each dedicated to a single customer class. The system operates under a rush hour regime with a large number of customers arriving at the beginning of the rush hour period. We show that this allows us to reduce the problem to one that is deterministic and for which closed-form solutions can be obtained. We compare the performance of the system with and without server partitioning during rush hour and address three basic questions. (1) Is partitioning beneficial to the system? (2) Is it equally beneficial to all customer classes? (3) If it is implemented, what is an optimal partition? We evaluate the applicability of our results to systems where customers arrive over time using (1) deterministic fluid models and (2) simulation models for systems with stochastic interarrival times.", "e:keyword": ["Server partitioning", "Multiserver queueing systems", "Multiple demand classes", "Rush hour demand"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0233", "e:abstract": "This research empirically examines the influence of psychological safety on knowledge sharing among coworkers in manufacturing and service operations contexts. Reconciling conflicting findings in the literature, we demonstrate that whereas psychological safety is an important antecedent of knowledge sharing, the relationship between psychological safety and knowledge sharing is moderated by the level of confidence that employees have in what they know. The greater this confidence, the lesser is the importance of psychological safety in facilitating knowledge sharing. Linking this result to social network theory, we find that psychological safety increases with the frequency of communication among coworkers and that the confidence of employees in their knowledge is related to the codifiability of the knowledge involved. We further investigate direct and indirect antecedents of psychological safety. This research offers insights into actions that managers can take to enhance psychological safety and, consequently, motivate their employees to share knowledge.", "e:keyword": ["Psychological safety", "Knowledge sharing", "Operational choices", "Auxiliary network theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0232", "e:abstract": "We study the decision of a manufacturer (the buyer), expecting new sourcing opportunities in the future, in selecting between sole- and second-sourcing strategies for a noncommodity component. In a sole-sourcing strategy, the buyer commits to sourcing from a single supplier (the incumbent) over the entire horizon. In a second-sourcing strategy, the buyer keeps the option open to source from a new supplier (the entrant) in the future. Supplier costs are private information, and the incumbent's cost may change in the future because of what it has learned. The buyer is relatively sure about current demand but uncertain about future demand. A supplier has to invest in capacity to produce the inputs for the buyer. With future private cost information, the incumbent earns rent in the future, and this prospective rent influences the incumbent's decision early in the horizon. On one hand, a second-sourcing strategy allows the buyer to take advantage of alternative sourcing opportunities, lowering her future cost. This benefit to the buyer is referred to as the option value of second sourcing. On the other hand, the future supplier competition in second-sourcing hurts the incumbent's future profit. The expectation of a lower future profit in second sourcing induces the incumbent to ask for a higher price at the beginning of the horizon. This causes more initial sourcing cost for the buyer in second sourcing than in sole sourcing, and is referred to as the cost of future supplier competition of second sourcing. The overall benefit of second sourcing relative to sole sourcing is influenced by the demand distribution and capacity cost. If the demand increases over time with positive probability, the incumbent's initial capacity may not be able to cover all future demand. If the capacity is cheap, the entrant may serve as an exclusive supplier, ousting the incumbent. In this case, the option value of second sourcing is high. If the capacity is expensive, the entrant may serve as a supplementary supplier by receiving only the demand in excess of the incumbent's installed capacity. In this case, the cost of future supplier competition is low and the option value is still significant. Thus second sourcing is better than sole sourcing not only when the capacity cost is low, but also when it is high (under the condition that demand increases over time with positive probability and the entrant's cost is relatively low). For intermediate capacity cost, the cost of future supplier competition dominates the option value; hence, sole sourcing is preferred. We also find that second sourcing is more attractive when the buyer expects the future demand to be higher or more volatile. Finally, more initial incumbent capacity strengthens the incumbent's competitiveness against the entrant, reducing the cost of future supplier competition. As a result, we find that second sourcing may lead to overinvestment of the initial capacity.", "e:keyword": ["Sourcing", "Auction", "Supplier relationship", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0234", "e:abstract": "Third-party process certification programs such as the ISO 9001 and capability maturity model (CMM) have been widely adopted in recent years. In this study we employ three competing theoretical frameworks--signaling, efficiency gains, and institutional theory--to analyze the motivations for a firm to acquire quality certification and the performance implications thereafter. We test these hypotheses in the context of CMM certification based on data from the Indian offshore IT services industry between 1997 and 2002. Our results indicate that more cost-effective firms and export-oriented firms are more likely to seek out and acquire certification. In addition, CMM-certified firms show significant improvements in exports, but not on the firm's cost structure. Furthermore, our findings suggest that CMM certification helps indicate firm capabilities to potential customers and thus appear to be most consistent with signaling explanations of certification rather than the efficiency gains or institutional theories.", "e:keyword": ["Certification", "Signaling", "Outsourcing", "Quality management", "OM-information technology interface", "Technology management", "Process design", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0235", "e:abstract": "Component commonality has been widely recognized as a key factor in achieving product variety at low cost. Yet the theory on the value of component commonality is rather limited in the inventory literature. The existing results were built primarily on single-period models or periodic-review models with zero lead times. In this paper, we consider a continuous-review system with positive lead times. We find that although component commonality is in general beneficial, its value depends strongly on component costs, lead times, and dynamic allocation rules. Under certain conditions, several previous findings based on static models do not hold. In particular, component commonality does not always generate inventory benefits under certain commonly used allocation rules. We provide insight on when component commonality generates inventory benefits and when it may not. We further establish some asymptotic properties that connect component lead times and costs to the impact of component commonality. Through numerical studies, we demonstrate the value of commonality and its sensitivity to various system parameters in between the asymptotic limits. In addition, we show how to evaluate the system under a new allocation rule, a modified version of the standard first-in-first-out rule.", "e:keyword": ["Component commonality", "Dynamic model", "Lead times", "Allocation rules", "Assemble-to-order systems"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0237", "e:abstract": "This paper documents that excess inventory announcements, an indication of demand-supply mismatch, are associated with an economically and statistically significant negative stock market reaction. The results are based on a sample of 276 excess inventory announcements made during 1990-2002. Over a two-day period (the day of the announcement and the day before the announcement) the mean (median) stock market reaction ranges from -6.79% to -6.93% (-4.51% to -4.79%), depending on the benchmark used to estimate the market reaction. The percent of sample firms that experience negative market reaction ranges from 73% to 74%. When excess inventory is at the announcing firm's customers, the market reaction is more negative than when the excess inventory is at the announcing firm. The stock market reaction is less negative for excess inventory announcements made by larger firms but is more negative for firms with higher growth prospects and with higher debt-equity ratios.", "e:keyword": ["Empirical research", "Excess inventory", "Stock price performance", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0236", "e:abstract": "This paper examines the staffing, division of labor, and resulting profitability of primary care physician practices. Division of labor is viewed as a mechanism to increase the efficiency of production processes through specialization. At the same time, division of labor also introduces coordination cost as handoffs and communication needs increase. We attempt to empirically assess the net effect in primary care physician offices. We collected data from a sample of these practices and tested two hypotheses: (H1) controlling for staff size, greater delegation through the use of more staff types will decrease the throughput of visits, and (H2) controlling for staff size, income per unit time generated by the practice is decreasing in the number of staff types. We find evidence supporting both hypotheses. We conclude that many physicians are gaining little financial benefit from delegating work to support staff. This suggests that small practices with few staff may be viable alternatives to traditional practice designs.", "e:keyword": ["Healthcare management", "Service operations", "Survey research", "Econometric analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0238", "e:abstract": "In this note, we study the concurrent determination of pricing and inventory replenishment decisions for a perishable product in an infinite horizon. Demands in consecutive periods are independent and influenced by prices charged in each period. In particular, we treat price as a decision variable to maximize the total discounted profit. We analyze the optimal solution-structure of a two-period lifetime problem and from insights gained in numerical experiments, develop a base-stock/list-price heuristic policy for products with arbitrary fixed lifetimes. Experiments show this policy to be effective.", "e:keyword": ["Perishable", "Pricing and inventory control"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0239", "e:abstract": "A classic example that illustrates how observed customer behavior impacts other customers' decisions is the selection of a restaurant whose quality is uncertain. Customers often choose the busier restaurant, inferring that other customers in that restaurant know something that they do not. In an environment with random arrival and service times, customer behavior is reflected in the lengths of the queues that form at the individual servers. Therefore, queue lengths could signal two factors--potentially higher arrivals to the server or potentially slower service at the server. In this paper, we focus on both factors when customers' waiting costs are negligible. This allows us to understand how information externalities due to congestion impact customers' service choice behavior. In our model, based on private information about both the service-quality and queue-length information, customers decide which queue to join. When the service rates are the same and known, we confirm that it may be rational to ignore private information and purchase from the service provider with the longer queue when only one additional customer is present in the longer queue. We find that, due to the information externalities contained in queue lengths, there exist cycles during which one service firm is thriving whereas the other is not. Which service provider is thriving depends on luck; i.e., it is determined by the private signal of the customer arriving when both service providers are idle. These phenomena continue to hold when each service facility has multiple servers, or when a facility may go out of business when it cannot attract customers for a certain amount of time. Finally, we find that when the service rates are unknown but are negatively correlated with service values, our results are strengthened; long queues are now doubly informative. The market share of the high-quality firm is higher when there is service rate uncertainty, and it increases as the service rate decreases. When the service rates are positively correlated with unknown service values, long queues become less informative and customers might even join shorter queues.", "e:keyword": ["Customer herding", "Behavioral operations", "Information externalities", "Market dynamics", "Private signals", "Service capacity"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0242", "e:abstract": "This paper is inspired by the recurring mismatch between demand and supply in the U.S. influenza vaccine market. Economic theory predicts that an oligopolistic market with unregulated but costly entry will experience excess entry and oversupply, not the undersupply observed in the market for influenza vaccine in recent years. In this paper, we examine the interaction between yield uncertainty, a key characteristic of many production processes, including that for influenza vaccine, and firms' strategic behavior. We find that yield uncertainty can contribute to a high degree of concentration in an industry and a reduction in the industry output and the expected consumer surplus in equilibrium. We use parameter values loosely based on the U.S. influenza vaccine market to numerically illustrate the impact of yield uncertainty.", "e:keyword": ["Cournot competition", "Influenza vaccine", "Yield uncertainty", "Market structure"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0256", "e:abstract": "Product returns cost U.S. companies more than $100 billion annually. The cost and scale of returns management issues necessitate a deeper understanding of how to deal with product returns. We develop an analytical model that describes how consumer purchase and return decisions are affected by a seller's pricing and restocking fee policy. Taking into account the consumers' strategic behavior, we derive the seller's optimal policy as a function of consumer preferences, consumer uncertainty about product attributes, consumer hassle cost for returns, and the effectiveness of the seller's forward and reverse channel capability. We allow for two sources of consumer uncertainty and show how the seller may use its price and restocking fee as a means of targeting a segment of consumers who know their product consumption utilities. We find that even if it is possible to eliminate returns costlessly through the provision of information about the fit between consumer preferences and product characteristics, returns can nevertheless be part of an optimal product sales process. That is, we identify conditions under which it is (or is not) optimal to provide product fit information to consumers. We show that the marginal value of information to the seller is decreasing in the operational efficiency of the seller's forward and reverse logistics process as well as the level of product uncertainty. We identify the impact of multiple product options and sources of consumer uncertainty on the model's results. The analysis generates testable hypotheses about how consumer-level and seller-level parameters affect the return policies observed in the marketplace.", "e:keyword": ["OM-marketing interface", "Product returns", "Restocking fees", "Reverse logistics", "Demand management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0240", "e:abstract": "This paper develops a model of consumer returns policies. In our model, consumers face valuation uncertainty and realize their valuations only after purchase. There is also aggregate demand uncertainty, captured using the conventional newsvendor model. In this environment, consumers decide whether to purchase and then whether to return the product, whereas the seller sets the price, quantity, and refund amount. Using our model, we study the impact of full returns policies (e.g., using 100% money-back guarantees) and partial returns policies (e.g., when restocking fees are charged) on supply chain performance. Next, we demonstrate that consumer returns policies may distort incentives under common supply contracts (such as manufacturer buy-backs), and we propose strategies to coordinate the supply chain in the presence of consumer returns. Finally, we explore several extensions and demonstrate the robustness of our findings.", "e:keyword": ["Consumer returns", "Supply chains", "Valuation uncertainty", "Newsvendor model", "Supply contracts"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0241", "e:abstract": "We introduce and analyze a model that explicitly considers the timing effect of intertemporal pricing--the concept, found in practice, that demand during a sale is increasing in the time since the last sale. We present structural results that characterize the interaction between the decision to hold a sale and the inventory-ordering decision. We show that the optimal inventory-ordering policy is a state-dependent base-stock policy; however, the optimal pricing policy can be quite complicated due to both the value and the cost of holding inventory and delaying sales. In our computational analysis, we find that compared to a fixed-price policy, we see an average gain in profit of almost 5% from optimally varying promotion and inventory decisions accounting for intertemporal demand, and we find that this potential profit gain increases as demand variability decreases. We also develop a heuristic based on deterministic pricing and find that it performs well relative to the optimal policy.", "e:keyword": ["Inventory", "Policies", "Pricing", "Uncertainty", "Stochastic"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0243", "e:abstract": "In the small package shipping industry (as in other industries), companies try to differentiate themselves by providing high levels of customer service. This can be accomplished in several ways, including online tracking of packages, ensuring on-time delivery, and offering residential pickups. Some companies want their drivers to develop relationships with customers on a route and have the same drivers visit the same customers at roughly the same time on each day that the customers need service. These service requirements, together with traditional constraints on vehicle capacity and route length, define a variant of the classical capacitated vehicle routing problem, which we call the consistent VRP (ConVRP). In this paper, we formulate the problem as a mixed-integer program and develop an algorithm to solve the ConVRP that is based on the record-to-record travel algorithm. We compare the performance of our algorithm to the optimal mixed-integer program solutions for a set of small problems and then apply our algorithm to five simulated data sets with 1,000 customers and a real-world data set with more than 3,700 customers. We provide a technique for generating ConVRP benchmark problems from vehicle routing problem instances given in the literature and provide our solutions to these instances. The solutions produced by our algorithm on all problems do a very good job of meeting customer service objectives with routes that have a low total travel time. In the paper \"The Consistent Vehicle Routing Problem,\" published in Manufacturing & Service Operations Management, ePub ahead of print December 4, 2008, http://msom.journal.informs.org/cgi/content/abstract/msom.1080.0243v1, the authors have amended the original text published online to correct an oversight in conveying the real-world problem studied in this article.", "e:keyword": ["Vehicle routing", "Customer service", "Logistics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0244", "e:abstract": "Many service providers offer customers the choice of either waiting in a line or going offline and returning at a dynamically determined future time. The best-known example is the FASTPASS<sup></sup> system at Disneyland. To operate such a system, the service provider must make an upfront decision on how to allocate service capacity between the two lines. Then, during system operation, he must provide estimates of the waiting times for both lines to each arriving customer. The estimation of offline waiting times is complicated by the fact that some offline customers do not return for service at their appointed time. We show that when demand is large and service is fast, for any fixed-capacity allocation decision, the two-dimensional process tracking the number of customers waiting in a line and offline collapses to one dimension, and we characterize the one-dimensional limit process as a reflected diffusion with linear drift. The analytic tractability of this one-dimensional limit process allows us to solve for the capacity allocation that minimizes average cost when there are costs associated with customer abandonments and queueing. We further show that in this limit regime, a simple scheme based on Little's Law to dynamically estimate in line and offline wait times is effective.", "e:keyword": ["Service operations", "Customer abandonment", "Customer impatience", "Reneging", "Offline waiting", "Choice models", "Heavy traffic"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0245", "e:abstract": "We examine the placement of safety stocks in a supply chain for which we have an evolving demand forecast. Under assumptions about the forecasts, the demand process, and the supply chain structure, we show that safety-stock placement for such systems is effectively equivalent to the corresponding well-studied problem for systems with stationary demand bounds and base-stock policies. Hence, we can use existing algorithms to find the optimal safety stocks. We use a case study with real data to demonstrate that there are significant benefits from the inclusion of the forecast process when determining the optimal safety stocks. We also conduct a computational experiment to explore how the placement and size of the safety stocks depend on the nature of the forecast evolution process.", "e:keyword": ["Evolving forecast", "Safety stock", "Supply chain", "MRP", "Guaranteed service"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0246", "e:abstract": "In many service systems, customers are not served in the order they arrive, but according to a priority scheme that ranks them with respect to their relative \"importance.\" However, it may not be an easy task to determine the importance level of customers, especially when decisions need to be made under limited information. A typical example is from health care: When triage nurses classify patients into different priority groups, they must promptly determine each patient's criticality levels with only partial information on their conditions. We consider such a service system where customers are from one of two possible types. The service time and waiting cost for a customer depends on the customer's type. Customers' type identities are not directly available to the service provider; however, each customer provides a signal, which is an imperfect indicator of the customer's identity. The service provider uses these signals to determine priority levels for the customers with the objective of minimizing the long-run average waiting cost. In most of the paper, each customer's signal equals the probability that the customer belongs to the type that should have a higher priority and customers incur waiting costs that are linear in time. We first show that increasing the number of priority classes decreases costs, and the policy that gives the highest priority to the customer with the highest signal outperforms any finite class priority policy. We then focus on two-class priority policies and investigate how the optimal policy changes with the system load. We also investigate the properties of \"good\" signals and find that signals that are larger in convex ordering are more preferable. In a simulation study, we find that when the waiting cost functions are nondecreasing, quadratic, and convex, the policy that assigns the highest priority to the customer with the highest signal performs poorly while the two-class priority policy and an extension of the generalized c\\mu  rule perform well.", "e:keyword": ["Priority queues", "Partially observable customer types", "Service differentiation", "Triage"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0249", "e:abstract": "Managing shipping vessel profitability is a central problem in marine transportation. We consider two commonly used types of vessels--\"liners\" (ships whose routes are fixed in advance) and \"trampers\" (ships for which future route components are selected based on available shipping jobs)--and formulate a vessel profit maximization problem as a stochastic dynamic program. For liner vessels, the profit maximization reduces to the problem of minimizing refueling costs over a given route subject to random fuel prices and limited vessel fuel capacity. Under mild assumptions about the stochastic dynamics of fuel prices at different ports, we provide a characterization of the structural properties of the optimal liner refueling policies. For trampers, the vessel profit maximization combines refueling decisions and route selection, which adds a combinatorial aspect to the problem. We characterize the optimal policy in special cases where prices are constant through time and do not differ across ports and prices are constant through time and differ across ports. The structure of the optimal policy in such special cases yields insights on the complexity of the problem and also guides the construction of heuristics for the general problem setting.", "e:keyword": ["Routing", "Shipping", "Refueling", "Stochastic prices", "Maritime transportation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0250", "e:abstract": "We develop a method to estimate the capacity of agents who answer e-mail in a contact center, given aggregate historical data that have been distorted both by constraints on work availability and by internal incentives to slow down when true capacity exceeds demand. We use the capacity estimate to find a contact center's optimal daily staffing levels. The implementation results, from an actual contact center, demonstrate that the method provides accurate staffing recommendations. We also examine and test models in which agents exhibit speed-up behavior and in which capacity varies over time. Finally, we use the capacity estimates to examine the implications of solving the staffing problem with two different model formulations, the service-level constraint formulation used by the contact center and an alternate profit-maximization formulation.", "e:keyword": ["Capacity planning", "Service operations", "Empirical research", "OM-human resources interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0247", "e:abstract": "This paper studies the impact of supply reliability on a retail firm's performance under joint marketing and inventory decisions. The firm sells a product in a single selling season and can exert marketing effort to influence consumer demand. We develop a modeling framework to quantify the value of improving supply reliability and investigate how this value depends on different model parameters. Our results provide useful insights into how firms should make investment decisions on adopting new technologies to improve supply reliability. First, we establish a necessary and sufficient condition under which the maximum unit cost a firm is willing to pay to improve supply reliability increases in product price. We further show that this condition would hold in most practical situations. Thus, with some caveats, our result supports the intuition that a firm is willing to pay more to improve supply reliability for products with a higher price. Next, we show that for two products with the same price, a firm is willing to pay more to improve supply reliability for the product with a higher product cost. This implies that it is not necessarily true that emerging technologies for improving supply reliability should be first adopted for products with the highest unit contribution margin. Finally, we show that a product with a lower marketing cost function always benefits more from improved supply reliability than a product with a higher marketing cost function. This finding suggests that the priority of adopting new technologies should be given to situations where the firm can effectively induce greater demand through promotional effort.", "e:keyword": ["Supply uncertainty", "Inventory", "Marketing and operations interface", "Information technology", "Stochastic orderings"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0248", "e:abstract": "With a posterior price matching (PM) policy, a seller guarantees to reimburse the price difference to a consumer who buys a product before the seller marks it down. Such a policy has been widely adopted by retailers. We examine the impact of a posterior PM policy on consumers' purchasing behavior, a seller's pricing and inventory decisions, and their expected payoffs, assuming that the seller cannot credibly commit to a price path, but can implement a posterior PM policy. We find that the PM policy eliminates strategic consumers' waiting incentive and thus allows the seller to increase price in the regular selling season. When the fraction of strategic consumers is not too small and their valuation decline over time is neither too low nor too high, the PM policy can substantially improve the seller's profit, as well as the inventory investment. In such situations, the strategic consumers' waiting incentive and the loss if they wait are both high. However, to adopt this policy, the seller also bears the refund cost. The seller must either pay the refund that consumers will claim or forgo the salvage value of any leftover inventory. The PM policy can be detrimental when there are only a few strategic consumers or the strategic consumers' valuation decline is very low or very high. We find that the performance of this policy is insensitive to the proportion of consumers who claim the refund. From the consumers' perspective, the PM policy generally reduces consumer surplus; however, there are cases where consumer surplus can be increased, typically when the variance of the potential high-end market volume is high. As a result, a Pareto improvement on both the seller's and the consumers' payoffs is possible. Finally, we find that the ability to credibly commit to a fixed price path is not very valuable when the seller can implement price matching.", "e:keyword": ["Strategic consumers", "Inventory management", "Pricing", "Discounts", "Rational expectations equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0252", "e:abstract": "Revenue management models traditionally assume that future demand is unknown but can be described by a stochastic process or a probability distribution. Demand is, however, often difficult to characterize, especially in new or nonstationary markets. In this paper, we develop robust formulations for the capacity allocation problem in revenue management using the maximin and the minimax regret criteria under general polyhedral uncertainty sets. Our approach encompasses the following open-loop controls: partitioned booking limits, nested booking limits, displacement-adjusted virtual nesting, and fixed bid prices. In specific problem instances, we show that a booking policy of the type of displacement-adjusted virtual nesting is robust, both from maximin and minimax regret perspectives. Our numerical analysis reveals that the minimax regret controls perform very well on average, despite their worst-case focus, and outperform the traditional controls when demand is correlated or censored. In particular, on real large-scale problem sets, the minimax regret approach outperforms by up to 2% the traditional heuristics. The maximin controls are more conservative but have the merit of being associated with a minimum revenue guarantee. Our models are scalable to solve practical problems because they combine efficient (exact or heuristic) solution methods with very modest data requirements.", "e:keyword": ["Revenue management", "Yield management", "Network", "Robust optimization", "Regret"]}, {"@id": "http://dx.doi.org/10.1287/msom.1080.0251", "e:abstract": "This paper considers the coordination of pricing and scheduling decisions in a make-to-order environment. Following common industry practice, we assume knowledge of a deterministic demand function that is nonincreasing in price. We consider three alternative measures of scheduling cost: total work-in-process inventory cost of orders, total penalty for orders delivered late to customers, and total capacity usage. The objective is to maximize the total net profit, i.e., revenue less scheduling cost, resulting from the pricing and scheduling decisions. We develop computationally efficient optimal algorithms for solving the three pricing and scheduling problems. Because these problems are formally intractable, much faster algorithms are not possible. We develop a fully polynomial time approximation scheme for each problem. We also estimate the value of coordinating pricing and production scheduling decisions by comparing solutions delivered by (a) an uncoordinated approach where pricing and scheduling decisions are made independently, (b) a partially coordinated approach that uses only general information about scheduling that a marketing department typically knows, (c) a simple heuristic approach for solving the coordinated problem, and (d) our optimal algorithm for solving the coordinated problem. Our main managerial insight is that there is a significant benefit even if pricing and scheduling are only heuristically or partially coordinated. Moreover, heuristic and partial coordination are simple to achieve.", "e:keyword": ["Scheduling", "Pricing", "Optimal and approximate algorithms", "Value of coordinating decisions"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0259", "e:abstract": "In a decentralized assembly supply chain, independent suppliers produce a set of complementary components from which an assembler assembles a final product and sells it to the market. In such a channel, several competitive forces interact with one another to affect the price and quantity decisions of the firms involved. These include: (1) the direct competition each supplier faces for producing the same component, (2) the indirect competition among the suppliers producing the set of complementary components needed for assembling the final product, and (3) the vertical interaction between the assembler and the component suppliers. This paper shows that the direct competition that one supplier faces helps improve the performance of the assembler and all the other suppliers in the channel; and surprisingly, it can help improve the performance of this particular supplier facing the competition as well. Second, the assembler benefits from a merger of suppliers producing different components in the complementary set. Furthermore, the assembler prefers a merger of suppliers with less direct competition over a merger of suppliers with more direct competition.", "e:keyword": ["Supply chain management", "Noncooperative games", "Assembly systems", "Price-production decisions"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0258", "e:abstract": "A common phenomenon that occurs in any decentralized multilocation system is stock imbalance, whereby some locations have unsatisfied demands while others are overstocked. The system can be rebalanced by using a search process that is driven by either the customers or the retailers. In a customer-driven search (CDS), the customer with unmet demand may search for the product at another location and, if it is available, complete the purchase. In a retailer-driven search (RDS), the retailer with unsatisfied demand searches for product and schedules transshipment to fulfill the unmet demand at his location. Of course, the revenues generated through search in RDS need to be shared between the parties according to a transfer pricing scheme. In a setting of one manufacturer and two retailers with price-dependent and random demand, we explore the impact of the search method and the transfer price scheme used on the preferences of the manufacturer, the retailers, and the customers. With endogenous retail prices, we find that both the manufacturer and the retailers prefer RDS over CDS when they can design the transfer pricing scheme in RDS. Interestingly, neither party prefers the fixed transfer pricing scheme commonly assumed in the literature. Instead, transfer price that is proportional to the price of the retailer with either excess stock or excess demand is preferred. However, although both parties favor an RDS system when they can design the transfer pricing scheme in RDS, they may prefer RDS or CDS when the other party designs the RDS. Thus, the interests of the manufacturer and the retailers are rarely aligned. Customers benefit from a lower price in an RDS but at the expense of lower availability (as measured by the level of safety stock).", "e:keyword": ["Multilocation system", "Competition", "Search", "Inventory", "Pricing", "Information systems"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0257", "e:abstract": "We analyze a continuous review (Q, r) stochastic inventory model in which orders placed with a make-to-order manufacturer can be shipped via two alternative freight modes differing in lead time and costs. The costs of placing an order and using each freight mode consist of fixed components and hence exhibit economies of scale. We derive an optimal policy for using the two freight modes for shipping each order. This freight-mode decision is delayed until manufacturing is complete and the optimal policy uses information about the demand incurred in the meantime. Furthermore, given that the two freight modes are used optimally for shipping each order, we solve our model for reorder point and order quantity that minimizes cost. We analyze the cost savings achieved from postponing the freight-mode decision and provide analytical and numerical comparisons between the solutions to our two-freight model and single-freight models. Finally, we illustrate the properties of the solution to our model using an extensive set of numerical examples.", "e:keyword": ["Continuous review policy", "Lead times", "Freight mode selection", "Transportation economies of scale"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0285", "e:abstract": "Two single-product firms with different quality levels and fixed limited capacities engage in sequential price competition in an essentially deterministic model where customers have heterogeneous valuations for both products. We develop conditions under which the leader (she) can take strategic advantage of her limited capacity by pricing relatively low, purposefully creating shortages and leaving some leftovers for the follower (him) to feast on, avoiding direct competition. The extent to which the leader benefits in this Leftovers Equilibrium depends on operational variables such as the capacity levels of the two firms and the sequence in which customers arrive at the market. We spell out the details for three different known arrival sequences within a specific subset of plausible fixed-capacity levels. The follower's strategic shadow price can be positive even when not all his capacity is used, and the leader's can be negative when all her capacity is used. We illustrate that Leftovers Equilibria can arise when some of our assumptions are relaxed.", "e:keyword": ["Operations strategy", "Quality management", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0264", "e:abstract": "The fields of statistics and econometrics have developed powerful methods for testing the validity (specification) of a model based on its fit to underlying data. Unlike statisticians, managers are typically more interested in the performance of a decision rather than the statistical validity of the underlying model. We propose a framework and a statistical test that incorporate decision performance into a measure of statistical validity. Under general conditions on the objective function, asymptotic behavior of our test admits a sharp and simple characterization. We develop our approach in a revenue management setting and apply the test to a data set used to optimize prices for consumer loans. We show that traditional model-based goodness-of-fit tests may consistently reject simple parametric models of consumer response (e.g., the ubiquitous logit model), while at the same time these models may \"pass\" the proposed performance-based test. Such situations arise when decisions derived from a postulated (and possibly incorrect) model generate results that cannot be distinguished statistically from the best achievable performance--i.e., when demand relationships are fully known.", "e:keyword": ["Pricing", "Parametric and nonparametric estimation", "Model misspecification", "Hypothesis testing", "Goodness-of-fit test", "Asymptotic analysis", "Performance analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0262", "e:abstract": "An important challenge faced by media broadcasting companies is how to allocate limited advertising space between upfront (forward) contracts and the spot market (referred to in advertising as the scatter market) to maximize profits and meet contractual commitments. We develop stylized optimization models of airtime capacity planning and allocation across multiple clients under audience uncertainty. In a short-term profit maximizing setting, our results provide insight for capacity planning decisions upfront and during the broadcasting season. Our results suggest that broadcasting companies should prioritize upfront clients according to marginal revenue per audience unit. We find that accepted upfront market contracts can be aggregated across clients and served in proportion to the audience demanded. Closed-form solutions are obtained in a static setting. These results remain valid in a dynamic setting, when considering the opportunity to increase allocation by airing make-goods during the broadcasting season. Our structural results characterize the impact of contracting parameters, time, and audience uncertainty on profits and capacity decisions. The results hold under general audience and spot market profit models. Overall, we find that ignoring audience uncertainty can have a significant cost for media capacity planning and allocation.", "e:keyword": ["Media planning", "TV advertising", "Revenue management", "Capacity planning"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0266", "e:abstract": "We model competition between two providers who serve delay-sensitive customers. We compare a generalized delay cost structure, where a customer's delay cost depends on her service valuation, with the traditional additive delay cost structure, where the delay cost is independent of the customer's service valuation. Under the additive delay cost structure, service providers offer different prices and expected delays, but customers are indifferent between the providers. Under the generalized delay cost structure, when the providers have different capacity or operating costs, we obtain value-based market segmentation, whereby higher-value customers choose one provider and lower-value customers choose the other. We study how the delay cost parameters, the market size, and the service providers' costs affect the structure of the equilibrium.", "e:keyword": ["Delay cost structure", "Value-based market segmentation", "Service competition"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0265", "e:abstract": "A set of manufacturers outsources certain operations to a single third party following the announcement of a booking price for each available day of production. Knowing these costs, manufacturers book available production days in a first-come-first-serve order to optimize their individual cost. The cost for each manufacturer consists of booking and work-in-progress costs, as expressed by the weighted flow time. When window booking is completed, the third party identifies a schedule that minimizes the total cost incurred by all manufacturers. This coordination reduces the total cost but may result in higher costs for a subset of manufacturers. For this reason, the third party devises a savings sharing scheme with which the monetary benefit for each manufacturer is greater. In this article we present algorithms for the problem considered, as well as savings-sharing schemes that make coordination a better alternative for all parties. The highlight of our experiments is that the costs of the production chain can be reduced by an average of 32% if one-third of the members let the third party cover their increased work-in-progress cost in exchange for 38%-53% of the total savings.", "e:keyword": ["Coordination", "Cooperative games", "Incentives", "Outsourcing", "Supply chain management", "Production planning and scheduling"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0271", "e:abstract": "The Vaccine and Related Biologic Products Advisory Committee meets at least once a year to decide the composition of seasonal influenza vaccine in the United States. Past evidence suggests that the committee could use a more systematic approach to incorporate observed information and to quantify the risks associated with different options. There are two key trade-offs involved in this decision. First, if the Committee decides to retain the current vaccine composition instead of updating to a new one, there is lower uncertainty in production yields, but the current vaccine could be less effective if a new virus strain spreads. Second, if the Committee decides early with less information, then manufacturers have more production time, but the reduced information increases the risk of choosing a wrong strain. We derive an optimal dynamic policy for this decision. Because of the greater uncertainty in production yields of new vaccines, the optimal thresholds are neither symmetric between retaining and updating the composition nor monotonic over time. We apply our model to past decisions using parameter values estimated from a historical case. Our analysis shows that the dynamic optimal policy can significantly improve social welfare.", "e:keyword": ["Dynamic programming", "Health-care management", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0269", "e:abstract": "This paper considers the facility network design problem for a global firm that sells to two markets: the domestic market and a foreign market. Although the firm has to invest in capital-intensive production facilities and produce outputs in the face of demand and exchange rate uncertainties, it can postpone the transshipment, localization, and distribution of the output until after uncertainties are resolved. The key network design decisions faced by the firm are in which of the two markets to locate the capital-intensive production facilities and what the corresponding output levels should be. We provide the complete characterization of the optimal global facility network configuration for two types of global firms: a newsvendor with exogenously given selling prices and a firm with the flexibility of pricing responsively after uncertainties are resolved. Our study's focus is on the impact of exchange rate uncertainty and responsive pricing on facility network decisions. Compared with a newsvendor facing demand uncertainty only, the introduction of exchange rate uncertainty or the use of responsive pricing can increase the attractiveness of centralized production. A responsive-pricing firm's optimal network design differs from that of a newsvendor firm as follows. (1) The firm's network preference is less susceptible to the increase of localization costs. (2) The firm is less likely to switch from a network of regional production to a network of centralized production in response to the increasing size of one market. (3) Demand and exchange rate uncertainties can have opposite effects on its optimal centralized output when the nature of the random shocks they introduce to the firm's demand function is different: under the linear demand function, the optimal centralized output increases in the demand volatility when demand shock is additive, but it may decrease in the exchange rate volatility because the corresponding price shock is multiplicative. Interestingly, common to both types of firms, their network preferences, although sensitive to costs and mean demand and exchange rate, are robust to the demand and exchange rate volatilities, suggesting that transshipment-enabled output substitutability between the two markets diminishes the impact of increasing market volatilities on the network design decision.", "e:keyword": ["Global operations management", "Operations strategy", "Supply chain management", "Global facility network", "Transshipment", "Responsive pricing", "Newsvendor network"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0267", "e:abstract": "We study a newsvendor game with transshipments, in which n retailers face a stochastic demand for an identical product. Before the demand is realized, each retailer independently orders her initial inventory. After the demand is realized, the retailers select an optimal transshipment pattern and ship residual inventories to meet residual demands. Unsold inventories are salvaged at the end of the period. We compare two methods for distribution of residual profit--transshipment prices (TPs) and dual allocations (DAs)--that were previously analyzed in literature. TPs are selected before the demand is known, and DAs, which are obtained by calculating the dual prices for the transshipment problem, are calculated after observing the true demand. We first study the conditions for the existence of the Nash equilibria under DA and then compare the performance of the two methods and show that neither allocation method dominates the other. Our analysis suggests that DAs may yield higher efficiency among \"more asymmetric\" retailers, whereas TPs work better with retailers that are \"more alike,\" but the difference in profits does not seem significant. We also link expected dual prices to TPs and use those results to develop heuristics for TPs with more than two symmetric retailers. For general instances with more than two asymmetric retailers, we propose a TP agreement that uses a neutral central depot to coordinate the transshipments (TPND). Although DAs in general outperform TPND in our numerical simulations, its ease of implementation makes TPND an attractive alternative to DAs when the efficiency losses are not significant (e.g., high critical fractiles or lower demand variances).", "e:keyword": ["Inventory sharing", "Supply chain coordination", "Transshipment"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0276", "e:abstract": "We consider a discrete-time supply chain for perishable goods having separate demand streams for items of different ages. For a good that has two periods of lifetime, we build a model that generalizes and/or subsumes many of the models in the literature and study the effectiveness of two intuitive heuristic (base-stock) replenishment policies combined with different substitution rules. For each replenishment policy, we identify sufficient conditions on cost parameters for a substitution rule to be economically superior to others under our base-stock replenishment policies. Our analysis shows that the replenishment policy almost universally advocated in the perishable inventory literature may lead to pathological behavior when used with issuance rules that pool inventory (via substitution) to satisfy multiple, age-differentiated demand streams. Furthermore, we find that system behavior is typically more predictable, and potentially less costly, under a policy long-discarded in the literature that ignores the inventory of aged items, ordering a constant amount each period. Thus the benefit of pooling inventory for substitutable, perishable products depends critically on the replenishment policy used.", "e:keyword": ["Inventory management", "Perishable goods", "Substitution", "Heuristics"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0270", "e:abstract": "This paper compares two types of appointment-scheduling policies for single providers: traditional and open-access. Under traditional scheduling, each of a specified number of patients per day is booked well in advance, but may not show up for his or her appointment. Under open-access scheduling, a random number of patients call in the morning to make an appointment for that same day. Thus the number of patient arrivals will be random, for different reasons, under both policies. We find that the open-access schedule will significantly outperform the traditional schedule--in terms of a weighted average of patients' waiting time, the doctor's idle time, and the doctor's overtime--except when patient waiting time is held in little regard or when the probability of no-shows is quite small.", "e:keyword": ["Service operations", "Health-care management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0272", "e:abstract": "This paper develops a framework and proposes heuristic dynamic policies for scheduling patient appointments, taking into account the fact that patients may cancel or not show up for their appointments. In a simulation study that considers a model clinic, which is created using data obtained from an actual clinic, we find that the heuristics proposed outperform all the other benchmark policies, particularly when the patient load is high compared with the regular capacity. Supporting earlier findings in the literature, we find that the open access policy, a recently proposed popular scheduling paradigm that calls for \"meeting today's demand today,\" can be a reasonable choice when the patient load is relatively low.", "e:keyword": ["Service operations", "Health-care management", "Stochastic methods"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0297", "e:abstract": "On <related-article related-article-type=\"corrected-article\" vol=\"12\" issue=\"1\" page=\"33\" xlink:href=\"simple\">page 37 of the article \"Buy Now and Match Later: Impact of Posterior Price Matching on Profit with Strategic Consumers\" by Guoming Lai, Laurens Debo, and Katia Sycara Manufacturing & Service Operations Management, Winter 2010, Vol. 12, No. 1, 33-55)</related-article>, to the beginning of 3.4 Purchasing Behavior, the following sentence should be added: \"Similar to Png (1991), we assume that the consumers will make no purchase if the price is higher than V<sub>H</sub> and thus the seller will not set the first-period price above V<sub>H</sub> in our model. Furthermore....\" The assumption that p<sub>1</sub> \\le  V<sub>H</sub> was made, but was not stated explicitly in the model set-up section.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0275", "e:abstract": "Discrete choice models are appealing for airline revenue management (RM) because they offer a means to profitably exploit preferences for attributes such as time of day, routing, brand, and price. They are also good at modeling demand for unrestricted fare class structures, which are widespread throughout the industry. However, there is little empirical research on the practicality and effectiveness of choice-based RM models. Toward this end, we report the results of a study of choice-based RM conducted with a major U.S. airline. Our study had two main objectives: (1) to assess the extent to which choice models can be estimated well using readily available airline data, and (2) to gauge the potential impact that choice-based RM could have on a sample of test markets. We developed a maximum likelihood estimation algorithm that uses a variation of the expectation-maximization method to account for unobservable data. The procedure was applied to data for a test market from New York City to a destination in Florida. The outputs are promising in terms of the quality of the computed estimates, although a large number of departure instances may be necessary to achieve highly accurate results. These choice model estimates were then used in a simulation study to assess the revenue performance of the EMSR-b (expected marginal seat revenue, version b) capacity control policies and the current controls used by the airline relative to controls optimized to account for choice behavior. Our simulation results show 1%-5% average revenue improvements using choice-based RM. Although such simulated results must be taken with caution, overall our study suggests that choice-based revenue management is both feasible to execute and economically significant in real-world airline environments.", "e:keyword": ["Choice behavior", "Multinomial logit", "EM method", "Maximum likelihood", "Capacity control"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0273", "e:abstract": "Pipelines play a critical role in matching the supply and demand of natural gas. The pricing of their capacity is an important problem in practice for pipeline companies and the users of this capacity, which include shippers such as natural gas merchants, producers, and local distribution companies. This paper conducts a normative analysis of how pipeline capacity should be priced by each of these players. Although the trading value of this capacity should be relevant to merchants and its substitution and congestion values to shippers and pipelines, respectively, this analysis shows that all of these are equivalent values. Thus pipeline capacity should be priced at its trading value, a prediction that can be empirically investigated. This paper also conducts an empirical analysis of this prediction based on transacted prices of transport contracts for the capacity of the Tennessee Gas Pipeline, a major interstate pipeline in the United States, and finds support for it. This analysis suggests that the uncertainty in the evolution of natural gas prices is an important driver of operational performance in the pricing of pipeline capacity. The results of this paper have potential relevance for the pricing of the capacity of other commodity conversion assets.", "e:keyword": ["Pricing and revenue management", "Operations management/finance interface", "Econometric analysis", "Petroleum/natural gas industries", "Real options"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0274", "e:abstract": "We propose an extension of the competitive newsvendor model to investigate the impact of quick response under competition. For this purpose, we consider two retailers that compete in terms of inventory: customers that face a stockout at their first-choice store will look for the product at the other store. Consequently, the total demand that each retailer faces depends on the competitor's inventory level. We allow for asymmetric reordering capabilities, and we are particularly interested in the case when one of the firms has a lower ordering cost but can only produce at the beginning of the selling season, whereas the second firm has higher costs but can replenish stock in a quick response manner, taking advantage of any incremental knowledge about demand (if it is available). We visualize this problem as the competition between a traditional make-to-stock retailer that builds up inventory before the season starts versus a retailer with a responsive supply chain that can react to early demand information. We provide conditions for this game to have a unique pure-strategy subgame-perfect equilibrium, which then allows us to perform numerical comparative statics. We confirm that quick response is more beneficial when demand uncertainty is higher or exhibits a higher correlation over time. We also find that the competitive advantage from quick response is larger when facing a slow response competitor, and interestingly, asymmetric competition can be desirable to both competitors.", "e:keyword": ["Operations strategy", "Supply chain management", "Inventory competition", "Game theory", "Fast fashion"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0277", "e:abstract": "In some supply chains, materials are ordered periodically according to local information. This paper investigates how to improve the performance of such a supply chain. Specifically, we consider a serial inventory system in which each stage implements a local reorder interval policy; i.e., each stage orders up to a local base-stock level according to a fixed-interval schedule. A fixed cost is incurred for placing an order. Two improvement strategies are considered: (1) expanding the information flow by acquiring real-time demand information and (2) accelerating the material flow via flexible deliveries. The first strategy leads to a reorder interval policy with full information; the second strategy leads to a reorder point policy with local information. Both policies have been studied in the literature. Thus, to assess the benefit of these strategies, we analyze the local reorder interval policy. We develop a bottom-up recursion to evaluate the system cost and provide a method to obtain the optimal policy. A numerical study shows the following: Increasing the flexibility of deliveries lowers costs more than does expanding information flow; the fixed order costs and the system lead times are key drivers that determine the effectiveness of these improvement strategies. In addition, we find that using optimal batch sizes in the reorder point policy and demand rate to infer reorder intervals may lead to significant cost inefficiency.", "e:keyword": ["Multi-echelon", "Periodic ordering", "Policy comparison", "Value of information", "Value of flexible deliveries"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0280", "e:abstract": "The literature on many-server approximations provides significant simplifications toward the optimal capacity sizing of large-scale monopolists, but falls short of providing similar simplifications for a competitive setting in which each firm's decision is affected by its competitors' actions. In this paper, we introduce a framework that combines many-server heavy-traffic analysis with the notion of epsilon-Nash equilibrium and apply it to the study of equilibria in a market with multiple large-scale service providers that compete on both prices and response times. In an analogy to fluid and diffusion approximations for queueing systems, we introduce the notions of fluid game and diffusion game. The proposed framework allows us to provide first-order and second-order characterization results for the equilibria in these markets. We use our results to provide insights into the price and service-level choices in the market and, in particular, into the impact of market scale on the interdependence between these two strategic decisions.", "e:keyword": ["Competition", "Games", "Approximate equilibrium", "Asymptotic analysis", "Heavy traffic", "Halfin-Whitt regime", "Services"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0281", "e:abstract": "We study cross-selling operations in call centers. The following questions are addressed: How many customer-service representatives are required (staffing), and when should cross-selling opportunities be exercised (control) in a way that will maximize the expected profit of the center while maintaining a prespecified service level target? We tackle these questions by characterizing control and staffing schemes that are asymptotically optimal in the limit, as the system load grows large. Our main finding is that a threshold priority control, in which cross-selling is exercised only if the number of callers in the system is below a certain threshold, is asymptotically optimal in great generality. The asymptotic optimality of threshold priority reduces the staffing problem to a solution of a simple deterministic problem in one regime and to a simple search procedure in another. We show that our joint staffing and control scheme is nearly optimal for large systems. Furthermore, it performs extremely well, even for relatively small systems.", "e:keyword": ["Call centers", "Cross-selling", "Queueing systems", "Many-server queues", "Heavy traffic approximations", "Steady-state analysis"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0279", "e:abstract": "Surveys suggest that supply chain risk is a growing issue for executives and that supplier reliability is of particular concern. A common mitigation strategy is for the buying firm to expend effort improving the reliability of its supply base. We explore a model in which a firm can source from multiple suppliers and/or exert effort to improve supplier reliability. For both random capacity and random yield types of supply uncertainty, we propose a model of process improvement in which improvement efforts (if successful) increase supplier reliability in the sense that the delivered quantity (for any given order quantity) is stochastically larger after improvement. We characterize the optimal procurement quantities and improvement efforts and generate managerial insights. For random capacity, improvement is increasingly favored over dual sourcing as the supplier cost heterogeneity increases, but dual sourcing is favored over improvement if the supplier reliability heterogeneity is high. In the random yield model, increasing cost heterogeneity can reduce the attractiveness of improvement, and improvement can be favored over dual sourcing if the reliability heterogeneity is high. A combined strategy (improvement and dual sourcing) can provide significant value if suppliers are very unreliable and/or capacity is low relative to demand.", "e:keyword": ["Operations strategy", "Risk management", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0282", "e:abstract": "We study a profit-maximizing firm providing a service to price and delay sensitive customers. We are interested in analyzing the scale economies inherent in such a system. In particular, we study how the firm's pricing and capacity decisions change as the scale, measured by the potential market for the service, increases. These decisions turn out to depend intricately on the form of the delay costs seen by the customers; we characterize these decisions up to the dominant order in the scale for both convex and concave delay costs. We show that when serving customers on a first-come, first-served basis, if the customers' delay costs are strictly convex, the firm can increase its utilization and extract profits beyond what it can do when customers' delay costs are linear. However, with concave delay costs, the firm is forced to decrease its utilization and makes less profit than in the linear case. While studying concave delay costs, we demonstrate that these decisions depend on the scheduling policy employed as well. We show that employing the last-come, first-served rule in the concave case results in utilization and profit similar to the linear case, regardless of the actual form of the delay costs.", "e:keyword": ["Service systems", "Pricing", "Capacity planning", "Large market size", "Nonlinear delay costs", "Convex delay costs"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0283", "e:abstract": "Independent parties that produce perfectly complementary components may form alliances (or coalitions or groups) to better coordinate their pricing decisions when they sell their products to downstream buyers. This paper studies how market demand conditions (i.e., the form of the demand function, demand uncertainty, and price-sensitive demand) drive coalition formation among complementary suppliers. In a deterministic demand model, we show that for an exponential or isoelastic demand function, suppliers always prefer selling in groups; for a linear-power demand function, suppliers may all choose to sell independently in equilibrium. These results are interpreted through the pass-through rate associated with the demand function. In an uncertain demand model, we show that, in general, the introduction of a multiplicative stochastic element in demand has an insignificant impact on stable coalitions and that an endogenous retail price (i.e., demand is price sensitive) increases suppliers' incentives to form alliances relative to the case with a fixed retail price. We also consider the impact of various other factors on stable outcomes in equilibrium, e.g., sequential decision making by coalitions of different sizes, the cost effect due to alliance formation (either cost savings or additional costs), and a system without an assembler.", "e:keyword": ["Alliance formation", "Demand curvature", "Pass-through rate", "Assembly system"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0288", "e:abstract": "Dell's supply chain for desktops involves Asian vendors shipping components by sea to several U.S. plants. Although suppliers are responsible for shipping enough inventory to meet total needs across all production sites, Dell can reroute and expedite their shipments while in transit, and also transfer on-hand inventory in order to balance supply across sites. This paper describes the development, implementation, and impact of the process and optimization-based control system now used by Dell to address this supply-routing challenge for its U.S.-bound monitors. In a first phase, Dell created a new job definition focused solely on supply routing and implemented a supporting visualization tool. In a second phase, a decision support system relying on a mixed-integer programming formulation was implemented, overcoming two main challenges: (i) the estimation of shortages as a function of expected inventory, accounting for actual forecast quality; and (ii) the estimation of a meaningful shortage cost. This new methodology is estimated to have reduced Dell's inventory-repositioning costs for monitors by about 60%.", "e:keyword": ["Inventory theory and control", "Supply chain management", "Empirical research", "Logistics and transportation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0286", "e:abstract": "The literature on mass customization generally focuses on the tradeoff between higher revenues from better matching customer preferences with product specifications, and higher costs of offering a broader--possibly fully customized--product line. Less well understood is the tradeoff between the increased ability to precisely meet customer preferences and the increased leadtime from order placement to delivery often associated with customized products. In this paper, we use a locational customer choice model to formulate a firm's integrated product line design problem that involves variety, leadtime (or inventory), and pricing decisions. We propose a dynamic programming based solution procedure that amounts to solving a shortest path problem on an acyclic network, and derive some structural results on the optimal product line design. We find that unimodal preferences generally result in hybrid product lines, with standard products clustering around the mode and custom products covering the tails, in contrast with the all-custom or all-standard product lines that are optimal under uniform preferences. We also numerically examine how the firm should adjust its leadtime and variety in response to changes in parameters such as customer dispersion and operational scale. We find that the tradeoff between leadtime and variety is sometimes nonintuitive and complex.", "e:keyword": ["Product variety", "Mass customization", "Make-to-order", "Make-to-stock", "Congestion", "Leadtime", "Custom and standard products", "Product differentiation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0284", "e:abstract": "The risk of supply disruptions because of suppliers' financial problems plays a prominent role in manufacturers' risk portfolios. Even large suppliers (e.g., Delphi) could file for bankruptcy, and manufacturers' actions, such as financial subsidies to suppliers, profoundly affect suppliers' financial health. Using a dynamic, stochastic, periodic-review model of the manufacturer's joint capacity reservation and financial subsidy decisions and a general firm-value model of the supplier's financial state, this paper addresses the following questions: What is the optimal joint capacity ordering and financial subsidy policy for the manufacturer? Must subsidy and capacity ordering decisions be made jointly? How good are the recommendations from the traditional procurement models, which ignore the benefits of controlling the supplier's financial state through subsidies? The paper presents general assumptions that allow the manufacturer to make ordering decisions independent of subsidy decisions and investigates interactions between ordering and subsidy decisions when these assumptions are violated. Conditions are presented for the optimal subsidy policy to have a \"subsidize-up-to\" structure and for the optimal ordering decisions to be newsvendor fractiles.", "e:keyword": ["Supply risk", "Operations and finance interface", "Bankruptcy", "Financial defaults", "Procurement", "Supplier management", "Capacity investments", "Markov decision processes"]}, {"@id": "http://dx.doi.org/10.1287/msom.1090.0287", "e:abstract": "This paper studies a buyer's procurement strategies in a two-stage supply chain with price-sensitive demand. The buyer procures a product from a supplier and then sells to the marketplace. Market demand is stochastic and depends on the buyer's selling price. The supplier's production cost is private information, and the buyer only knows the distribution of the cost. Both the buyer and the supplier can hold inventories to improve service, and a periodic-review inventory system is considered. The buyer takes two attributes into consideration when designing the procurement mechanism: quantity attribute (i.e., the total purchase quantity) and service-level attribute (i.e., the supplier's delivery performance). We first identify the optimal procurement mechanism for the buyer, which consists of a nonlinear menu of contracts for each of the two attributes. It can be shown that the optimal mechanism induces both a lower market demand and a lower service level compared to the supply chain optimum. In view of the complexity of the optimal mechanism, we proceed to search for simpler mechanisms that perform well for the buyer. We find that the above two attributes have different implications for procurement mechanism design: The value of using complex contract terms is generally negligible for the service-level attribute, whereas it can be highly valuable for the quantity attribute. In particular, we demonstrate that a fixed service-level contract, which consists of a target service level and a price-quantity menu, yields nearly optimal profit for the buyer. Additionally, the price-quantity menu is essentially a quantity discount scheme widely observed in practice.", "e:keyword": ["Procurement", "Supply contracts", "Mechanism design", "Inventory models", "Asymmetric information"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0292", "e:abstract": "We consider the operational scheduling or dispatching problem of assigning servicemen to service requests that arrive in real time. The objective is to optimize responsiveness, i.e., to minimize waiting in excess of a promised response time. We study how responsiveness is influenced by modeling decisions and solution methods that arise when solving the dynamic problem by repeatedly solving real-time problems. Most results are derived using a set-partitioning based solution approach, which is shown to perform best among considered alternatives. Our research is based on a large-scale real-life application regarding roadside service assistance.", "e:keyword": ["Service operations", "Simulation", "Traveling repairmen"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0291", "e:abstract": "Batch (wave) release policies are prevalent in warehouses with an automated sorter, and take different forms depending on how batches released consecutively may overlap downstream in the sorter. Continuous (waveless) release constitutes an emerging alternative recently adopted by several firms. Although that new policy presents several advantages relative to waves, it requires more expensive technology and involves the possibility of congestion-induced collapse (gridlock) at the sorter. Using an extensive data set of detailed warehouse flow information from a leading U.S. online retailer, we first develop a model with validated predictive accuracy for a warehouse operating under waveless release. We then use that model to compute operational guidelines for dynamically managing the main control lever of that policy with the goal of maximizing throughput while keeping the risk of gridlock under a specified threshold. Second, we leverage that model and data set to compare the performance of wave-based and waveless policies through simulation. The best waveless policy yields larger or equal throughput than the best wave-based policy in all scenarios considered, and thus appears to merit some consideration by practitioners.", "e:keyword": ["Inventory theory and control", "Production planning and scheduling", "Dynamic programming", "Queueing theory", "Simulation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0293", "e:abstract": "In this paper, we investigate the optimality of myopic inventory replenishment policies in a periodic-review single-echelon system, with nonstationary, correlated, stochastic demand and cost, and nonincreasing stochastic prices. Using the single-unit decomposition approach, we provide certain general conditions on the demand and cost processes under which a myopic policy is optimal. Under these conditions, the optimal policy is a myopic state-dependent base-stock policy, which can be expressed in closed form as a base-probability policy. Specifically, the order associated with a given customer should be placed if and only if its arrival probability within the leadtime is higher than a threshold. Our results generalize earlier conditions for the optimality of myopic policies. Namely, we show that myopic policies can be optimal even when the demand is correlated or stochastically decreasing.", "e:keyword": ["State-dependent base-stock policies", "Single-unit decomposition approach"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0294", "e:abstract": "Previous experimental work showed that newsvendors tend to order closer to mean demand than prescribed by the normative critical fractile solution. A recently proposed explanation for this mean ordering behavior assumes that the decision maker commits random choice errors, and predicts the mean ordering pattern because there is more room to err toward mean demand than away from it. Do newsvendors exhibit mean ordering simply because they make random errors? We subject this hypothesis to an empirical test that rests on the fact that the random error explanation is insensitive to context. Our results strongly support the existence of context-sensitive decision strategies that rely directly on (biased) order-to-demand mappings, such as mean demand anchoring, demand chasing, and inventory error minimization.", "e:keyword": ["Newsvendor model", "Task context", "Heuristics", "Random choice"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0295", "e:abstract": "This paper analyzes the trade-off between (demand) substitution costs and (production) changeover costs in a discrete-time production-inventory setting using a two-product dynamic lot-sizing model with changeover, inventory carrying, and substitution costs. We first show that the problem is polynomially solvable and then develop several insights into the behavior of such systems and identify strategies for effectively managing them. A key driver for the extent of substitution is the ratio of changeover cost to the substitution cost associated with mean demand. The interaction between changeovers and substitution is most prominent when this ratio is neither too high nor too low. Furthermore, the value of this ratio also influences the length of an appropriate rolling horizon; an increase in the value of the ratio signals an increase in the length of a near-optimal rolling horizon. We identify a complementary relationship between substitution and changeover costs: When the changeover cost is large, it is better to invest in reducing the substitution cost and vice versa. As the holding cost of the substitutable product increases, substitution is (respectively, changeovers are) utilized more when the changeover (respectively, substitution) cost is large.", "e:keyword": ["Demand substitution", "Production changeovers", "Dynamic lot sizing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0301", "e:abstract": "When managing projects with considerable uncertainty, such as those arising in construction, defense, and new product development, it is customary for a manufacturer (project manager) to offer contracts under which each supplier (contractor) receives a prespecified payment when she completes her task. However, there are recent cases in which the manufacturer imposes \"delayed payment\" contracts under which each supplier is paid only when all suppliers have completed their tasks. By considering a model of one manufacturer and n \\ge 2 identical and independent suppliers with exponential completion times, we analyze the impact of both a delayed payment regime and a no-delayed-payment regime on each supplier's effort level and on the manufacturer's net profit in equilibrium. When the suppliers' work rates are unadjustable, we conjecture that the manufacturer is actually worse off under the delayed payment regime. However, when the suppliers' work rates are adjustable, we obtain a different result: the delayed payment regime is more profitable for the manufacturer either when the project revenue is sufficiently small or when the number of suppliers is sufficiently large.", "e:keyword": ["Supply chain management", "Game theory", "Product development and design"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0313", "e:abstract": "No abstract available.", "e:keyword": []}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0299", "e:abstract": "We empirically document factors that influence how local operating managers use discretion to balance the trade-off between service capacity costs and customer sensitivity to service time. Our findings, using data from one of the largest financial services providers in the United States, indicate that customer sensitivity to service time varies widely and predictably with observable market characteristics. In turn, we find evidence that local operating managers account for market-specific customer sensitivities to service times by deviating frequently and in predictable ways from the recommendations offered by a centralized capacity-planning model. Finally, we document that these discretionary capacity supply decisions exhibit a strong learning effect whereby experienced operating managers place more weight than their less-experienced counterparts on the market-specific trade-off between service capacity costs and customer sensitivity to service times. Overall, our results demonstrate both the importance of local knowledge as an input in service operations and the potential for incorporating richer data on customer behavior and preferences into service cost and productivity standard metrics.", "e:keyword": ["Service operations", "Capacity", "OM-accounting interface", "Empirical research"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0298", "e:abstract": "Product returns have become a significant feature of many manufacturing systems. Because products are returned under different operational conditions, they usually require different remanufacturing effort/costs. Motivated by a project with a major energy company that manages its inventory through options of ordering and remanufacturing returned products (cores) in various condition, in this paper, we study a single-product, periodic-review inventory system with multiple types of cores. The serviceable products used to fulfill stochastic customer demand can be either manufactured from new parts or remanufactured from the cores, and the objective is to minimize the expected total discounted cost over a finite planning horizon. We show that the optimal manufacturing-remanufacturing-disposal policy has a simple structure and can be completely characterized by a sequence of constant control parameters when manufacturing and remanufacturing leadtimes are the same. To demonstrate the value of the optimal policy, we conduct a numerical study that compares its performance with two simple heuristics, namely, pull policy without and with sorting. The results show that the reduction in system cost by using the optimal policy can be significant. When manufacturing and remanufacturing leadtimes are different, we develop a heuristic method for computing the near-optimal control policy that performs quite well as demonstrated numerically.", "e:keyword": ["Inventory system", "Product returns", "Reverse logistics", "Remanufacturing", "Optimal manufacturing and remanufacturing strategies", "Base-stock policies"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0302", "e:abstract": "We consider a nonlinear nonseparable functional approximation to the value function of a dynamic programming formulation for the network revenue management (RM) problem with customer choice. We propose a simultaneous dynamic programming approach to solve the resulting problem, which is a nonlinear optimization problem with nonlinear constraints. We show that our approximation leads to a tighter upper bound on optimal expected revenue than some known bounds in the literature. Our approach can be viewed as a variant of the classical dynamic programming decomposition widely used in the research and practice of network RM. The computational cost of this new decomposition approach is only slightly higher than the classical version. A numerical study shows that heuristic control policies from the decomposition consistently outperform policies from the classical decomposition.", "e:keyword": ["Network revenue management", "Choice behavior", "Multinomial logit choice model", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0304", "e:abstract": "Almost all research in appointment scheduling has focused on the trade-off between customer waiting times and server idle times. In this paper, we present an observation-based method for estimating the relative cost of the customer waiting time, which is a critical parameter for finding the optimal appointment schedule.", "e:keyword": ["Health-care management", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0303", "e:abstract": "We consider outsourcing in two important service settings: call center and order fulfillment operations. An important factor in both is the inherent economies of scale. Therefore, we advance a unifying model covering both applications and study the associated contracting problem under information asymmetry. At the time of contracting, the outsourcing firm, \"the originator,\" faces uncertainty regarding the demand volume but has private information about its probability distribution. The true demand is quickly observed once the service commences. The service provider invests in capacity before the start of the operation and offers a menu of contracts to screen different types of the originator. Adopting a mechanism design approach, we prove that a menu of two-part tariffs achieves the full-information solution. Hence, it is optimal among all possible contracts (in both settings) because of economies of scale and contractibility of realized demand.", "e:keyword": ["Service outsourcing", "Call centers", "Order fulfillment operations", "Economies of scale", "Information asymmetry", "Screening"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0308", "e:abstract": "We deal with the problem of a profit-maximizing vendor selling a perishable product. At the beginning of a planning cycle, the vendor determines a minimum committed order per period. During the cycle, he may also place a supplemental order in each period based on the observed demand signal in that period. Moreover, the vendor is committed to a specific service target evaluated over the planning cycle. This is a complex problem, and we, as an approximation, offer a single-period, two-stage modeling approach. Under this approach, the vendor determines a first-stage order as the minimum committed order with the possibility of supplementing it based on a demand signal observed at the second stage. The problem is to maximize his expected profit subject to a constraint on his overall service performance across all possible values of the demand signal. We characterize the optimal policy for in-stock rate and fill-rate targets, and make comparisons. Whereas in the classical newsvendor model a service target can be replaced by a single unit shortage cost, it is not so in our model. Instead, a set of unit shortage costs are imputed--one for each demand signal. The imputed shortage costs reflect trade-offs among the profits under different demand signals in meeting the service targets. We also show that under a given ordering policy, the in-stock rate is lower (higher) than the fill rate when the demand has an increasing (decreasing) hazard rate. This result suggests that the vendor can infer a fill-rate measure from the corresponding in-stock rate without the difficult task of tracking lost sales. Furthermore, we analyze how the order quantity varies according to the observed signal, which allows us to formalize the notion of a valuable demand signal.", "e:keyword": ["Newsvendor model", "In-stock probability", "Fill rate", "Demand signal", "Service constraint", "Kuhn-Tucker conditions"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0306", "e:abstract": "Consider a firm that sells products over repeated seasons, each of which includes a full-price period and a markdown period. The firm may deliberately understock products in the markdown period to induce high-value customers to purchase early at full price. Customers cannot perfectly anticipate availability. Instead, they use observed past capacities to form capacity expectations according to a heuristic smoothing rule. Based on their expectations of capacity, customers decide to buy either in the full-price period or in the markdown period. We embed this customer learning process in a dynamic program of the firm's capacity choices over time. One main result demonstrates the existence of a monotone optimal path of customers' expectations, which converges to either a rationing equilibrium or a low-price-only equilibrium. Further, there exists a critical value of capacity expectation such that the market converges to a rationing equilibrium if customers' initial expectations are less than that critical value; otherwise, a low-price-only equilibrium is the limiting outcome. These results show how firms can be stuck with unprofitable selling strategies from incumbent customer expectations. We also examine numerically how this critical value is affected by the firm's discount factor and customers' learning speed and risk aversion. Last, we show that the equilibrium under adaptive learning converges to that under rational expectations as the firm's discount factor approaches one.", "e:keyword": ["Consumer behavior", "Pricing and revenue management", "Dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0307", "e:abstract": "Trade-in programs are offered extensively in business-to-business (B2B) markets. The success of such programs depends on well-designed and executed trade-in policies as well as accurate prediction of return flow to support operational decisions. Motivated by a real problem facing a high-tech company, this paper develops methods to segment customers and forecast product returns based on return merchandise authorization information. Noisy, yet proven to be valuable, returned quantity signals are adjusted by taking product characteristics and customer heterogeneity into account, and the resulting forecast outperforms two benchmark strategies that represent the high-tech company's current practice and a widely adopted method in the literature, respectively. In addition, our methods can serve as tools for companies to uncover the root causes of return merchandise authorization discrepancy, monitor and analyze customer behavior, design segment-specific trade-in policies, and evaluate the effectiveness and efficiency of trade-in programs on a continuous basis.", "e:keyword": ["Empirical research", "Trade-in programs", "Signal-based forecast", "Count regression models", "Cluster analysis", "Customer segmentation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0309", "e:abstract": "Retail assortment planning can have a tremendous impact on a retailer's bottom-line performance. Over the past years, retailers have increasingly relied on their leading manufacturers for recommendations regarding the assortment to be offered to the consumers in a particular category, a practice often referred to as category captainship. Our research investigates the consequences of using category captains for assortment selection decisions. We develop a game-theoretic model where multiple manufacturers sell their products to consumers through a single retailer. We compare a model where the retailer selects the assortment in the category with a model where the retailer relies on a category captain for assortment decisions in return for a target category profit. We show that category captainship can, in some circumstances, benefit not only the retailer and the category captain, but also the noncaptain manufacturers. Our results have implications regarding the implementation of category captainship practices.", "e:keyword": ["Retail supply chains", "Category management", "Category captainship", "Assortment planning", "Game theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0310", "e:abstract": "How should the excess profit because of inventory pooling be shared amongst firms at different levels along the supply chain? Suppose each of several retailers observes local demand for a common item and places an order at the supplier, which is immediately filled if the supplier has the item in stock. The supplier can fill retailer orders either from their reserved inventories or from a shareable pool of inventory. Using terminology from cooperative game theory, we say that the supplier and the retailers whose orders are filled from the common pool have formed an inventory-pooling coalition and study the use of Shapley value to allocate the expected excess profit because of pooling. We find that under Shapley value allocations the retailers have incentive to join the inventory-pooling coalition, and the supplier carries the level of inventory that is optimal for the coalition. Shapley value allocations might not lie within the core of the game, but the grand coalition of all players is stable in the farsighted sense. And, although the supplier's share of the expected excess profit is largest when all the retailers participate in the inventory-pooling coalition, the allocations to the retailers may diminish as the coalition grows. Colluding against the supplier (by merging and forming larger retailers) may seem like an appealing strategy for the retailers to increase their share of the total supply chain profit, but we find that the total expected after-pooling profits of retailers may instead go down because of collusion.", "e:keyword": ["Supply chain management", "Incentives and contracting"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0312", "e:abstract": "In this paper, we study the impacts of a set of China's export-oriented tax and tariff rules on the optimal supply chain design and operations for a firm that produces its product in China and sells it in markets both inside and outside China. We develop an analytical framework to evaluate four major supply chain structures that we observed in practice. We derive the optimal supply chain decisions for each structure and investigate various business environments under which one of the structures is preferred over the others. Our analysis indicates that the ultimate purpose of a product sold in the China market (i.e., whether it will be consumed domestically or be assembled in another exported product) may have a significant impact on the structure preference. In addition, threshold values exist for several key business parameters over (or under) which certain supply chain structures are favored over others. Managerial insights based on such results are useful for multinational firms who are challenged to develop effective supply chain strategies in the region's increasingly volatile business environment.", "e:keyword": ["Tax-effective supply chain management", "International tax and tariff", "Supply chain structures"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0311", "e:abstract": "We analyze volume flexibility--the ability to produce above/below the installed capacity for a product--under endogenous pricing in a two-product setting. We discover that the value of volume flexibility is a function of demand correlation between products, an outcome that cannot be explained by classical risk-pooling arguments. Furthermore, whereas the value of product flexibility always decreases in demand correlation, we show that the value of volume flexibility can increase or decrease in demand correlation depending on whether the products are strategic complements or substitutes. We further find that volume flexibility better combats aggregate demand uncertainty for the two products, whereas product flexibility is better at mitigating individual demand uncertainty for each product. Our results thus underscore the necessity of analyzing volume flexibility with more than one product and emphasize the contrast with product flexibility. Furthermore, we highlight the possible pitfalls of combining flexibilities: we show that although adding volume flexibility to product flexibility never hurts performance, adding product flexibility to volume flexibility is not always beneficial, even when such an addition is costless.", "e:keyword": ["Capacity planning and investment", "Operations strategy", "Technology management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0316", "e:abstract": "Focusing on a seller's regret in not acting optimally, we develop a model of overbooking and fare-class allocation in the multifare, single-resource problem in revenue management. We derive optimal static overbooking levels and booking limits, in closed form, that minimize the maximum relative regret (i.e., maximize competitive ratio). We prove that the optimal booking limits are nested. Our work addresses a number of important issues. (i) We use partial information, which is critical because of the difficulty in forecasting fare-class demand. Demand and no-shows are characterized using interval uncertainty in our model. (ii) We make joint overbooking and fare-class allocation decisions. (iii) We obtain conservative but practical overbooking levels that improve the service quality without sacrificing profits. Using computational experiments, we benchmark our methods to existing ones and show that our model leads to effective, consistent, and robust decisions.", "e:keyword": ["Revenue management", "Worst-case analysis", "Regret", "Overbooking", "Fare-class allocation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0314", "e:abstract": "This paper studies the problem of purchasing and allocating copies of movies to multiple stores of a movie rental chain. A unique characteristic of this problem is the return process of rented movies. We formulate this problem for new movies as a newsvendor-like problem with multiple rental opportunities for each copy. We provide demand and return forecasts at the store-day level based on comparable movies. We estimate the parameters of various demand and return models using an iterative maximum-likelihood estimation and Bayesian estimation via Markov chain Monte Carlo simulation. Test results on data from a large movie rental firm reveal systematic underbuying of movies purchased through revenue-sharing contracts and overbuying of movies purchased through standard (nonrevenue-sharing) ones. For the movies considered, our model estimates an increase in the average profit per title for new movies by 15.5% and 2.5% for revenue sharing and standard titles, respectively. We discuss the implications of revenue sharing on the profitability of the rental firm.", "e:keyword": ["Service operations", "Supply chain management", "Inventory theory and control"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0319", "e:abstract": "This paper analyzes optimal auction design when delivery of supply is uncertain. We consider a buyer facing multiple potential suppliers, each having an associated (exogenous) reliability that quantifies its risk of supply failure. We design optimal mechanisms that depend on the buyer's level of information regarding the suppliers' cost of production and reliability. When supplier reliability is known, we find that the optimal allocation resembles the allocation under full information, but with inflated production costs. When it is unknown, the same result is true when cost and reliability of a supplier are independent. Furthermore, the buyer does not have to pay any rent for information on suppliers' reliability. Moreover, we assess the benefits of the optimal mechanism compared to traditional auctions that ignore supply risk.", "e:keyword": ["Auctions", "Supply risk", "Information asymmetry"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0315", "e:abstract": "Should a firm charge on a per-use basis or sell subscriptions when its service experiences congestion? Queueing-based models of pricing primarily focus on charging a fee per use for the service, in part because per-use pricing enables the firm to regulate congestion--raising the per-use price naturally reduces how frequently customers use a service. The firm has less control over usage with subscription pricing (by definition, with subscription pricing customers are not charged proportional to their actual usage), and this is a disadvantage when customers dislike congestion. However, we show that subscription pricing is more effective at earning revenue. Consequently, the firm may be better off with subscription pricing, even, surprisingly, when congestion is intuitively most problematic for the firm: e.g., as congestion becomes more disliked by consumers. We show that the absolute advantage of subscription pricing relative to per-use pricing can be substantial, whereas the potential advantage of per-use pricing is generally modest. Subscription pricing becomes relatively more attractive if consumers become more heterogeneous in their service rates (e.g., some know they are \"heavy\" users and others know they are \"light\" users) as long as capacity is fixed, the potential utilization is high, and the two segments have substantially different usage rates. Otherwise, heterogeneity in usage rates makes subscription pricing less attractive relative to per-use pricing. We conclude that subscription pricing can be effective even if congestion is relevant for the overall quality of a service.", "e:keyword": ["Service operations", "Operations strategy", "Pricing and revenue management", "Game theory", "Queueing theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0317", "e:abstract": "We consider a firm that produces multiple variants of a product. Products are assembled using a combination of common and dedicated components. We characterize the optimal assortment and derive the optimal inventory levels for the common and dedicated components under various bill-of-material configurations. We investigate the effect of commonality on product variety and compare its benefits under different demand characteristics. Commonality always leads to increased profits, but its effect on the level of product variety depends on the type of commonality. If all common components are used for the production of the entire set of products, then the optimal variety level increases relative to the system with no commonality. However, if the common components are used by a subset of the final products, then the optimal variety level may decrease with commonality. We find that the effects of commonality on profit and variety level are stronger under a demand model in which product demands are more variable and exhibit pairwise negative correlation relative to a model with independent demands.", "e:keyword": ["Supply chain management", "OM-marketing interface", "Product variety", "Consumer choice", "Inventory management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0318", "e:abstract": "We study a modified newsvendor model in which the newsvendor obtains a revenue from sales to end users as well as from an advertiser paying to obtain access to those end users. We study the optimal decisions for both a price-taking and a price-setting newsvendor when the advertiser has private information about its willingness to pay for advertisements. We find that the newsvendor's optimal policy excludes advertisers with low willingness to pay and distorts the price and quantity from its system-efficient level to screen the advertiser. Our analysis reveals the different roles that pricing and production quantity play as screening instruments. We perform a numerical analysis to investigate the value of information and the impact of the model parameters.", "e:keyword": ["Newsvendor model", "Pricing", "Advertising", "Mechanism design", "Value of information"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0320", "e:abstract": "Most of the literature on inventory management assumes that the demand distribution and the values of its parameters are known with certainty. In this paper, we consider a repeated newsvendor setting where this is not the case and study the problem of setting inventory targets when there is a limited amount of historical demand data. Consequently, we achieve the following objectives: (1) to quantify the inaccuracy in the inventory-target estimation as a function of the length of the historical demand data, the critical fractile, and the shape parameters of the demand distribution; and (2) to determine the inventory target that minimizes the expected cost and accounts for the uncertainty around the demand parameters estimated from limited historical data. We achieve these objectives by using the concept of expected total operating cost and representing the demand distribution with the highly flexible Johnson translation system. Our procedures require no restrictive assumptions about the first four moments of the demand random variables, and they can be easily implemented in practical settings with reduced expected total operating costs.", "e:keyword": ["Expected total operating cost", "Inventory management", "Johnson translation system", "Maximum likelihood policy", "Statistical estimation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0321", "e:abstract": "In this paper, we consider how the structures of tasks and teams interact to affect team performance. We study the effects of diversity in experience on a team's ability to respond to task changes by separately examining <i>interpersonal</i> team diversity (i.e., differences in experience across the entire team) and <i>intrapersonal</i> team diversity (i.e., whether individuals on the team are more or less specialized). We also examine whether team familiarity--team members' prior experience working with one another--helps teams to better manage challenges created by task changes and greater interpersonal team diversity. Using detailed project- and individual-level data from an Indian software services firm, we find that the interaction of task change with intrapersonal diversity is related to improved project performance, whereas the interaction of task change with interpersonal diversity is related to diminished performance. Additionally, the interaction of team familiarity with <i>inter</i>personal diversity is related to improved project performance in some cases. Our results highlight a need for more nuanced approaches to leveraging experience in team management.", "e:keyword": ["Diversity", "Knowledge work", "Project flexibility", "Task change", "Team familiarity"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0322", "e:abstract": "We study how consumers with waiting cost disutility choose between two congested services of unknown service value. Consumers observe an imperfect private signal indicating which service facility may provide better service value as well as the queue lengths at the service facilities before making their choice. If more consumers choose the same service facility because of their private information, longer queues will form at that facility and indicate higher quality. On the other hand, a long queue also implies more waiting time. We characterize the equilibrium queue-joining behavior of arriving consumers and the extent of their learning from the queue information in the presence of such positive and negative externalities. We find that when the arrival rates are <i>low</i>, utility-maximizing rational consumers herd and join the longer queue, ignoring any contrary private information. We show that even when consumers treat queues as independently evolving, herd behavior persists with consumers joining longer queues above a threshold queue difference. However, if the consumers seek to minimize ex post regret when making their decisions, herd behavior may be dampened.", "e:keyword": ["Herd behavior", "Queueing games", "Learning", "Regret", "Bounded rationality"]}, {"@id": "http://dx.doi.org/10.1287/msom.1100.0325", "e:abstract": "We conduct a field study in a U.S. health insurance firm to examine how product customization affects the firm's cost to serve customers through its call center. In our setting, the product is a complex health insurance plan. The firm incurs substantial costs in serving the customers through its call center and in adjudicating the claims using its information systems. The firm sells either standard plans or in some instances allows customer groups to customize their plans by adding and modifying certain aspects in active collaboration with the firm. Such a collaboration process is akin to the firm cocreating products with its customers. This cocreation process should increase customers' familiarity with their coverage and improve the fit with their medical needs. Better fit and familiarity in turn, reduces customers' incentives to contact the call center for clarifications regarding the firm's product coverage. In particular, we show that customers with a customized plan call 21% less frequently than customers with a standard plan. Our results account for possible self-selection of customers to customized plans. We also show no difference in the claims adjudication cost between a standard and a customized plan exists. Overall, our results suggest customized plans may be operationally cheaper to serve than standard plans. Thus, our paper provides a link between a growing business concern (customer support cost via call centers) and a prevalent business strategy (product customization via cocreation).", "e:keyword": ["Product customization", "Product cocreation", "Health insurance", "Field study", "Customer service", "Product familiarity", "Call center"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0326", "e:abstract": "We examine transshipment incentives in a decentralized supply chain where a monopolist distributes a product through independent retailers. A key insight is that the transshipment price determines whether the firms benefit from, or are hurt by, transshipment. In particular, we show that the manufacturer prefers to set the transshipment price as high as possible, whereas retailers prefer a lower transshipment price. Given the important role of the transshipment price in determining the benefits that each firm gets from transshipment, it is useful to consider transshipment in the case where retailers are under joint ownership (a \"chain store\") and the transshipment price does not play a role. This comparison yields two surprising results. First, if decentralized retailers control the transshipment price, they will choose a relatively low transshipment price as a way to mitigate the manufacturer's ability to extract profits by increasing wholesale prices; therefore, the manufacturer may prefer dealing with the chain store, which does not have a transshipment price, rather than with decentralized retailers. Similarly, the decentralized retailers can use a low transshipment price to achieve higher total profits than a chain store.", "e:keyword": ["Supply chain incentives", "Transshipment", "Decentralized retailers", "Chain store retailer"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0332", "e:abstract": "Patients' satisfaction with an appointment system when they attempt to book a nonurgent appointment is affected by their ability to book with a doctor of choice and to book an appointment at a convenient time of day. For medical conditions requiring urgent attention, patients want quick access to a familiar physician. For such instances, it is important for clinics to have open slots that allow same-day (urgent) access. A major challenge when designing outpatient appointment systems is the difficulty of matching randomly arriving patients' booking requests with physicians' available slots in a manner that maximizes patients' satisfaction as well as clinics' revenues. What makes this problem difficult is that booking preferences are not tracked, may differ from one patient to another, and may change over time. This paper describes a framework for the design of the next generation of appointment systems that dynamically learn and update patients' preferences and use this information to improve booking decisions. Analytical results leading to a partial characterization of an optimal booking policy are presented. Examples show that heuristic decision rules, based on this characterization, perform well and reveal insights about trade-offs among a variety of performance metrics important to clinic managers.", "e:keyword": ["Appointment scheduling", "Health care", "Probability", "Stochastic model applications"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0333", "e:abstract": "We investigate a firm's product line design and capacity investment problem for vertically differentiated products along design quality levels. Customers arrive according to a Poisson process and are heterogeneous in their marginal valuation of the quality level. Customers make product choices to maximize a linear utility function of price, quality level, and waiting cost. Resulting product demands are met through capacity investments in production processes, which are modeled as queuing systems. We consider two different types of production processes: product-focused, dedicated to the production of a single-product variant; and product-flexible, processing all product variants in the product line. Capacity investment and variable production costs are functions of the processed product's quality. We develop an integrated marketing-operations model that provides insights on the factors determining the right level of product variety to offer, the relative quality positioning of the products in the line, the resulting market coverage and segmentation, and the effects on production costs and congestion levels of the processes. We show that the statistical economies of scale resulting from the congestion phenomena in the production system impose limits on the optimal product variety. For product-focused processes the market size promotes a higher optimal product variety, whereas the per-unit capacity investment and customer waiting costs act as deterrents for higher product variety. For product-flexible processes optimal product variety also depends on the specific type of flexibility and the ratio of capacity investment to variable production costs.", "e:keyword": ["Capacity investments", "Flexible or dedicated facilities", "Statistical scale economies", "Product variety", "Product line design", "Vertical differentiation", "Segmentation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0335", "e:abstract": "This paper studies the role of the yield-dependent trading cost structure influencing the optimal choice of the selling price and production quantity for a firm that operates under supply uncertainty in the agricultural industry. The firm initially leases farm space, but its realized amount of fruit supply fluctuates because of weather conditions, diseases, etc. At the end of the growing season, the firm has three options: convert its crop supply to the final product, purchase additional supplies from other growers, and sell some (or all) of its crop supply in the open market without converting to the finished product. We consider the problem both from a risk-neutral and a risk-averse perspective with varying degrees of risk aversion. The paper offers three sets of contributions: (1) It shows that the use of a static cost exaggerates the initial investment in the farm space and the expected profit significantly, and the actual value gained from a secondary (emergency) option for an agricultural firm is lower under the yield-dependent cost structure. (2) It proves that although the risk-neutral firm does not benefit from fruit futures, a sufficiently risk-averse firm can benefit from the presence of a fruit futures market. The same risk-averse firm does not purchase fruit futures when it operates under static costs. Thus, fruit futures can only add value under yield-dependent trading costs. (3) Contrary to the results presented for the newsvendor problem under demand uncertainty, the firm does not always commit to a lower initial quantity (leased farm space) under risk aversion. Rather, the firm might lease a larger farm space under risk aversion.", "e:keyword": ["Supply uncertainty", "Risk aversion", "Yield-dependent trading costs", "Pricing", "Fruit futures"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0334", "e:abstract": "Platelets are short-life blood components used in hospital blood transfusion centers. Excluding time for transportation, testing, and arrangement, clinically transfusable platelets have a mere three-day life span. This paper analyzes a periodic review inventory system for such a perishable product under two replenishment modes. Regular orders are placed at the beginning of a cycle. Within the cycle, the manager has an option of placing an additional order, referred to as an expedited order, characterized by an order-up-to level policy. For this platelet inventory problem, we prove the existence and uniqueness of an optimal policy that minimizes the expected cost. We then derive the necessary and sufficient conditions for the policy, based on which an algorithm is developed. Using real-life data, we provide a numerical illustration and a sensitivity analysis followed by extensive simulation experiments to examine the dynamics of platelet inventory management. It is shown that the optimal cost is significantly affected by demand uncertainty, lead times, seasonality, and vintage of expedited orders. Given a hospital's demand profile and the cost parameters, we are able to determine whether the hospital should order daily or every other day (i.e., adopting a combined use of regular and optional expedited orders). We also show that the analytical solution is a good approximation of the optimal policy obtained from simulation experiments, where more realistic features are incorporated. This inventory research problem can be classified as a fixed-lifetime perishable problem with dual modes of replenishments.", "e:keyword": ["Perishable inventory management", "Dual sourcing", "Stochastic dynamic programming"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0337", "e:abstract": "The benefits of supplier diversification are well established for price-taking firms. In this paper, we investigate the benefits from supplier diversification for dual-sourcing duopolists. We consider a two-echelon supply chain in which suppliers sell components to buyers who produce and sell substitutable products. The suppliers' output processes are uncertain and modeled as having a proportional random yield. Buyers engage in a quantity-based Cournot competition. We find that an increase in supplier correlation leads to more correlated buyers' outputs and a decrease in their profits. In the presence of end-market competition, dual sourcing still brings value by reducing the inefficiency caused by random yield: Namely, when the suppliers' yield processes are strongly negatively correlated, dual sourcing increases the expected market output and improves the firms' profits over sole sourcing. However, unlike a monopolist firm, a duopolist does not necessarily allocate its supplier orders to minimize output variability. We generalize the main results to a two-stage order-quantity-output-quantity game and to one with asymmetric suppliers.", "e:keyword": ["Supply uncertainty", "Random yield", "Cournot competition", "Dual sourcing", "Equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0338", "e:abstract": "We consider a choice-based, network revenue management (RM) problem in a setting where heterogeneous customers consider an assortment of products offered by a firm (e.g., different flight times, fare classes, and/or routes). Individual choice decisions are modeled through an ordered list of preferences, and minimal assumptions are made about the statistical properties of this demand sequence. The firm manages the availability of products using a bid-price control strategy, and would like to optimize the control parameters. We formulate a continuous demand and capacity model for this problem that allows for the partial acceptance of requests. The model admits a simple calculation of the sample path gradient of the revenue function. This gradient is then used to construct a stochastic steepest ascent algorithm. We show that the algorithm converges (w.p.1) to a stationary point of the expected revenue function under mild conditions. The procedure is relatively efficient from a computational standpoint, and in our synthetic and real-data experiments performs comparably to or even better than other choice-based methods that are incompatible with the current infrastructure of RM systems. These features make it an interesting candidate to be pursued for real-world applications.", "e:keyword": ["Stochastic gradient methods", "Simulation-based optimization", "Choice behavior", "Network capacity control"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0345", "e:abstract": "Nonprofit organizations are a critical part of society as well as a growing sector of the economy. For funders there is an increasing and pressing need to ensure that society reaps the most social benefit for their money while also developing the nonprofit sector as a whole. By routinely scrutinizing nonprofit reports in an effort to deduce whether a nonprofit organization is efficient, funders may believe that they are, in fact, giving responsibly. However, we find that these nonprofit reports are unreliable, supporting a myriad of empirical research and revealing that report-based funding methods do not facilitate efficient allocation of funds. In response, we develop audit contracts that put funders in a position to enact change. Auditing, perhaps obviously, supports funders; however, we find that it also benefits the population of nonprofits. Moreover, auditing results in improved efficiency for the nonprofit sector overall. Indeed, our conclusions indicate that nonprofits may want to work with funders to increase the use of auditing, consequently increasing efficiency for the sector overall and impacting society as a whole.", "e:keyword": ["Public sector", "Nonprofit sector", "Audit contracts", "Adverse selection", "Principal-agent framework", "Resource allocation", "Operational transparency"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0343", "e:abstract": "In many service industries, companies compete with each other on the basis of the waiting time their customers experience, along with other strategic instruments such as the price they charge for their service. The objective of this paper is to conduct an empirical study of an important industry to measure to what extent waiting time performance impacts different firms' market shares and price decisions. We report on a large-scale empirical industrial organization study in which the demand equations for fast-food drive-thru restaurants in Cook County are estimated based on so-called structural estimation methods. Our results confirm the belief expressed by industry experts, that in the fast-food drive-thru industry customers trade off price and waiting time. More interestingly, our estimates indicate that consumers attribute a very high cost to the time they spend waiting.", "e:keyword": ["Service competition", "Structural estimation", "Operations marketing interface", "Choice models", "Queueing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0336", "e:abstract": "The effective local reuse of physical cash by depository institutions (DIs) is the primary goal of the new cash recirculation policy of the Federal Reserve System (Fed) of the United States. These guidelines, implemented since July 2007, encourage the reuse of cash by (i) penalizing a DI for the practice of <i>cross shipping</i>, the near-simultaneous deposit of used cash to--and withdrawal of fit cash from--the Fed; and (ii) offering a <i>custodial inventory</i> program that enables a DI to transfer fit cash to the Fed's books, but physically hold it within the DI's secured facility. The effective management of the inventory of cash under these new guidelines is both a challenging and important issue for DIs. We introduce two new multiperiod models--designed specifically to capture the operations of a medium-size DI--that emerge from the DI's objective to minimize the total cost incurred in managing the inventory of cash over a finite planning horizon. The Basic Model (BM) captures the DI's mode of operations if it chooses not to locally reuse cash and, instead, incur the cross-shipping penalty. Using two important structural properties, we provide a polynomial-time dynamic programming algorithm for BM. The Reuse Model (RM) represents the DI's actions when it locally recirculates cash. We first prove the hardness of RM and then develop an integer programming formulation. A comprehensive test bed--based on our interaction with a leading secure-logistics provider--helps us to develop several useful insights into the relative impacts of the DI-specific parameters and the Fed's cross-shipping fee on the effective management of cash. In particular, we show that the <i>Value of Local Reuse</i> for a DI, measured as the percentage cost saving between the optimal solutions of BM and RM, is substantial, and we analyze the forces that influence the volume of cross shipping. We also develop a rolling-horizon procedure to adapt the optimal solutions of BM and RM for obtaining near-optimal real-time solutions in the presence of a modest amount of uncertainty. Finally, we provide a comparative analysis of a DI's decisions under the Fed's mechanism and those under a socially optimal mechanism.", "e:keyword": ["Cash supply chain", "Reuse of cash", "Cross shipping", "Algorithms"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0340", "e:abstract": "We study the stochastic multiperiod inventory problem in which demand in excess of available inventory is lost and unobserved so that demand data are censored. A Bayesian scheme is employed to dynamically update the demand distribution for the problem with storable or perishable inventory and with exogenous or endogenous price. We show that the Weibull is the only newsvendor distribution for which the optimal solution can be expressed in scalable form. Moreover, for Weibull demand the cost function is not convex in general. Nevertheless, in all but the storable case, sufficient structure can be discerned so that the optimal solution can be easily computed. Specifically, for the perishable inventory case, the optimal policy can be found by solving simple recursions, whereas the perishable case with pricing requires solutions to more complex one-step look-ahead recursions. Interestingly, for the special case of exponential demand the cost function is convex, so that for the storable inventory case, the optimal policy can be found using simple one-step look-ahead recursions whereas for the perishable case the optimal policy can be expressed by exact closed-form formulas.", "e:keyword": ["Inventory", "Stochastic demand", "Lost sales", "Scalability", "Optimal policy"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0339", "e:abstract": "In this paper, we analyze a call center with impatient customers. We study how informing customers about their anticipated delays affects performance. Customers react by balking upon hearing the delay announcement and may subsequently renege, particularly if the realized waiting time exceeds the delay that has originally been announced to them. The balking and reneging from such a system are a function of the delay announcement. Modeling the call center as an <i>M</i>/<i>M</i>/<i>s</i> &plus; <i>M</i> queue with endogenized customer reactions to announcements, we analytically characterize performance measures for this model. The analysis allows us to explore the role announcing different percentiles of the waiting time distribution, i.e., <i>announcement coverage</i>, plays on subsequent performance in terms of balking and reneging. Through a numerical study, we explore when informing customers about delays is beneficial and what the optimal coverage should be in these announcements. We show how managers of a call center with delay announcements can control the trade-off between balking and reneging through their choice of announcements to be made.", "e:keyword": ["Queues", "Telephone call centers", "Customer behavior", "Impatient customers", "Balking", "State-dependent analysis", "Predicting and announcing delays"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0344", "e:abstract": "We consider the problem of pricing multiple differentiated products with the nested logit model and, as a special case, the multinomial logit model. We prove that concavity of the total profit function with respect to market share holds even when price sensitivity may vary with products. We use this result to analytically compare the optimal monopoly solution to oligopolistic equilibrium solutions. To demonstrate further applications of the concavity result, we consider several multiperiod dynamic models that incorporate the pricing of multiple products in the context of inventory control and revenue management, and establish structural results of the optimal policies.", "e:keyword": ["Multinomial logit model", "Nested logit model", "Consumer choice", "Multiproduct pricing", "Price competition with differentiated products", "Quantity competition with differentiated products"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0367", "e:abstract": "This article examines the contributions and historical context of the fire project that was undertaken in the early 1970s by the New York City-RAND Institute on behalf of the New York City Fire Department. We identify a number of technical and nontechnical factors that contributed to the high impact of this operations modeling effort. We hypothesize that these factors, though derived from the experiences in a particular public sector engagement, are applicable to other large-scale operations modeling efforts. This invited contribution is based on the author's inaugural Manufacturing and Service Operations Management (MSOM) Distinguished Fellows address that was given at the June 2011 MSOM conference at the University of Michigan.", "e:keyword": ["OM practice", "OM implementation", "Public sector applications"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0355", "e:abstract": "This paper examines the impact of group purchasing organizations (GPOs) on healthcare-product supply chains. The supply chain we study consists of a profit-maximizing manufacturer with a quantity-discount schedule that is nonincreasing in quantity and ensures nondecreasing revenue, a profit-maximizing GPO, a competitive source selling at a fixed unit price, and <i>n</i> providers (e.g., hospitals) with fixed demands for a single product. Each provider seeks to minimize its total purchasing cost (i.e., the cost of the goods plus the provider's own fixed transaction cost). Buying through the GPO offers providers possible cost reductions but may involve a membership fee. Selling through the GPO offers the manufacturer possibly higher volumes, but requires that the manufacturer pay the GPO a contract administration fee (CAF), i.e., a percentage of all revenue contracted through it. Using a game-theoretic model, we examine questions about this supply chain, including how the presence of a GPO affects the providers' total purchasing costs. We also address the controversy about whether Congress should amend the Social Security Act, which, under current law, permits CAFs. Among other things, we conclude that although CAFs affect the distribution of profits between manufacturers and GPOs, they do not affect the providers' total purchasing costs.", "e:keyword": ["Supply-chain management", "Healthcare products", "Group purchasing organizations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0346", "e:abstract": "We consider a supply chain in which a supplier sells products to multiple retailers. When orders from the retailers exceed the supplier's capacity, she must employ an allocation mechanism to balance supply and demand. In particular, we consider a commonly used allocation scheme in the automobile industry: turn-and-earn, which uses past sales to allocate capacity. In essence, retailers \"earn\" an allotment of a vehicle after they sell one. In contrast to turn-and-earn, fixed allocation ignores past sales and gives each retailer an equal share of the capacity. Earlier work has demonstrated that turn-and-earn induces more sales in a two-period setting compared to fixed allocation. The question remains unanswered whether turn-and-earn induces similar behaviors over a long horizon when retailers possess private demand information. We construct a dynamic stochastic game of order competition over an infinite horizon to track the order dynamics of the supply chain. We obtain a richer set of equilibrium behaviors than existing models predict. Instead of a symmetric allocation outcome, we observe that sales leadership may arise in equilibrium and that retailers with different past sales adopt distinct ordering strategies to respond to demand uncertainty. Transient sales dynamics suggest that sales leadership may not be persistent and can be eliminated by the occurrence of extremely low demand. This provides a theoretical explanation for several behavioral observations of some U.S. automobile dealers. In addition to the sales-inducing effect, interestingly, turn-and-earn also causes the retailers to absorb local demand variability.", "e:keyword": ["Capacity allocation", "Turn-and-earn", "Dynamic stochastic game", "Markov-perfect equilibrium"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0353", "e:abstract": "This paper studies the optimization of the (<i>S</i>, <i>T</i>) inventory policy, where <i>T</i> is the replenishment interval and <i>S</i> is the order-up-to level. First, we demonstrate that the previously established joint convexity of the long-run average cost is false. Hence, the optimization is not straightforward. We then point out that the joint convexity concept depends on whether <i>S</i> and <i>T</i> are continuous or discrete variables, and in some situations it may not even be well defined. Nonetheless, we are able to identify several useful properties of the cost function, such as submodularity and coordinatewise convexity. Based on these properties, we develop efficient algorithms to compute the optimal policy for continuous and discrete demands.", "e:keyword": ["Inventory/production", "Periodic review", "Order-up-to policy", "Convexity", "Submodularity", "Unimodality", "Brownian motion", "Gamma process", "Compound Poisson process"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0341", "e:abstract": "This paper explores the rationing of bed capacity in a cardiac intensive care unit (ICU). We find that the length of stay for patients admitted to the ICU is influenced by the occupancy level of the ICU. In particular, a patient is likely to be discharged early when the occupancy in the ICU is high. This in turn leads to an increased likelihood of the patient having to be readmitted to the ICU at a later time. Such \"bounce-backs\" have implications for the overall ICU effective capacity--an early discharge immediately frees up capacity, but at the risk of a (potentially much higher) capacity requirement when the patient needs to be readmitted. We analyze these capacity implications, shedding light on the question of whether an ICU should apply an aggressive discharge strategy or if it should follow the old quality slogan and \"do it right the first time.\" By comparing the total capacity usage for patients who were discharged early versus those who were not, we show that an aggressive discharge policy applied to patients with lower clinical severity levels frees up capacity in the ICU. However, we find that an increased number of readmissions of patients with high clinical severity levels occur when the ICU is capacity constrained, thereby effectively reducing peak bed capacity.", "e:keyword": ["Patient flow", "Health-care operations", "Capacity management", "Rework"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0349", "e:abstract": "In many call centers, agents are trained to handle all arriving calls but exhibit very different performance for the same call type, where we define performance by both the average call handling time and the call resolution probability. In this paper, we explore strategies for determining which calls should be handled by which agents, where these assignments are dynamically determined based on the specific attributes of the agents and/or the current state of the system. We test several routing strategies using data obtained from a medium-sized financial service firm's customer service call centers and present empirical performance results. These results allow us to characterize overall performance in terms of customer waiting time and overall resolution rate, identifying an efficient frontier of routing rules for this contact center.", "e:keyword": ["Contact centers", "Call resolution", "Skill-based routing", "Performance management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0351", "e:abstract": "Motivated by the mixed evidence concerning the adoption level and value of collaborative forecasting (CF) implementations in retail supply chains, in this paper, we explore the conditions under which CF offers the highest potential. We consider a two-stage supply chain with a single supplier selling its product to consumers through a single retailer. We assume that both the supplier and the retailer can improve the quality of their demand forecasts by making costly forecasting investments to gather and analyze information. First, we consider a <i>noncollaborative</i> model where the supplier and the retailer can invest in forecasting but do not share forecast information. Next, we examine a <i>collaborative forecasting</i> model where the supplier and the retailer combine their information to form a single shared demand forecast. We investigate the value of CF by comparing each party's profits in these scenarios under three contractual forms that are widely used in practice (two variations of the simple wholesale price contract as well as the buyback contract). We show that for a given set of parameters, CF may be Pareto improving for none to all three of the contractual structures, and that the Pareto regions under all three contractual structures can be expressed with a unifying expression that admits an intuitive interpretation. We observe that these regions are limited and explain how they are shaped by the contractual structure, power balance, and relative forecasting capability of the parties. To determine the specific value of collaborative forecasting as a function of different factors, we carry out a numerical analysis and observe the following. First, under noncoordinating contracts, improved information as a result of CF has the added benefit of countering the adverse effects of double marginalization in addition to reducing the cost of supply-demand mismatch. Second, one may expect the value of CF to increase with bargaining power, however this does not hold in general: The value of CF for the newsvendor first increases and then decreases in his bargaining power. Finally, whereas one may expect CF to be more valuable under coordinating contracts, rather than a simple wholesale price contract that is prone to double marginalization, the <i>magnitude of the gain</i> from CF is in many cases higher in the absence of quantity coordination.", "e:keyword": ["Supply chain management", "Collaborative forecasting", "Information sharing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0350", "e:abstract": "We study the problem of setting nurse staffing levels in hospital operating rooms when there is uncertainty about daily workload. The workload is the number of operating room hours used by a medical specialty on a given day to perform surgical procedures. Variable costs consist of wages at a regular (scheduled) rate and at an overtime rate when the realized workload exceeds the scheduled time. Using a newsvendor framework, we consider the problem of determining optimal staffing levels with different information sets available at the time of decision: no information, information on number of cases, and information on number and types of cases. We develop empirical models for the daily workload distribution in which the mean and variance change with the information available. We use these models to derive optimal staffing rules based on historical data from a U.S. teaching hospital and prospectively test the performance of these rules. Our numerical results suggest that hospitals could potentially reduce their staffing costs by up to 39%-49% by deferring staffing decisions until procedure type information is available. The results demonstrate how data availability can affect a newsvendor's performance. The systematic approach of empirical modeling presented in the paper can be applied to other newsvendor problems with heterogeneous sources of demand.", "e:keyword": ["Econometric analysis", "Health-care management", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0352", "e:abstract": "This paper studies how a firm can create and capture value by converting a waste stream into a useful and saleable by-product (i.e., implementing by-product synergy (BPS)). We show that BPS creates an operational synergy between two products that are jointly produced. In essence, BPS is a process innovation that reduces the marginal cost of the original product and/or the by-product. The firm creates value through this process innovation and can capture this value by capturing newly created market opportunities, taking market share from competitors, or licensing the innovation to its competitors. We determine the optimal operating and licensing strategies for the firm and find market conditions under which the firm would benefit most from implementing BPS. We show that the optimal operating and licensing strategies are driven by the size of the cost reduction enabled by the BPS process innovation. We also show that leveraging the synergies between the original product and by-product can lead to counterintuitive profit-maximizing operating strategies such as increasing the amount of waste generated, and strategically increasing the quantity of original product above the business as usual production volume. We present a framework for assessing the environmental impact of BPS that incorporates the impact of the optimal operating and licensing strategies.", "e:keyword": ["Operations management", "Sustainability", "Environment", "By-product synergy"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0354", "e:abstract": "A popular procurement auction format is one in which bidders compete during a live auction event but observe only the rank of their own bid and not the price bids of their competitors. We investigate the performance of auctions with rank feedback in a simple setting for which analytical benchmarks are readily available. We test these benchmarks in the laboratory by comparing the performance of auctions with rank-based feedback to auctions with full-price feedback as well as to auctions with no price feedback (sealed-bid auctions). When bidders are risk-neutral expected-profit maximizers, the buyer's expected costs should be the same under rank and full-price feedback; moreover, expected buyer costs should be the same as in a sealed-bid auction. However, when we test this theoretical equality in a controlled laboratory setting we find that, consistent with practitioners' beliefs but contrary to our model, rank feedback results in <i>lower</i> average prices than full-price feedback. We identify two behavioral reasons for the difference. The first explanation is based on the similarity of the bidders' problem in a sealed-bid first-price auction and an open-bid auction with rank feedback. The second explanation incorporates the use of jump bids motivated by bidder impatience.", "e:keyword": ["Procurement auctions", "Bidding behavior", "Laboratory experiments", "Rank", "Feedback"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0356", "e:abstract": "Attracting shoppers to stores and converting the incoming traffic into sales profitably are vital for the financial health of retailers. In this paper, we use proprietary data pertaining to an apparel retailer to study the relationship between store traffic, labor, and sales performance. We decompose sales volume into conversion rate (defined as the ratio of number of transactions to traffic) and basket value (defined as the ratio of sales volume to number of transactions) and analyze the impact of traffic on sales and its components. We find that store sales volume exhibits diminishing returns to scale with respect to traffic, and labor moderates the impact of traffic on sales. For example, we find that for values of traffic and labor corresponding to the mean, increasing average traffic per hour by one unit increases average sales volume per hour by $9.97. Further, we find that the marginal returns to traffic increases from $10.00 to $11.32 when labor increases by one standard deviation. In addition, we find that the conversion rate declines with increasing traffic and a lower conversion rate is associated with a decrease in future traffic growth. Our study underscores the importance of in-store operations in driving the financial performance of retailers.", "e:keyword": ["Store performance", "Traffic variability", "Traffic uncertainty", "Store labor management", "Retail operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0375", "e:abstract": "This essay discusses my view of the essential characteristics of interesting research in general and in operations management in particular. It is based on my Manufacturing and Service Operations Management Distinguished Fellows presentation given at the University of Michigan, June 27, 2011.", "e:keyword": ["Operations management", "Interesting", "Impact"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0361", "e:abstract": "To help a firm reduce inefficiencies associated with equipment capacity planning, we propose a <i>dual-mode equipment procurement</i> (DMEP) framework. DMEP combines dual-source (i.e., a less-expensive-but-slower base mode and a faster-but-more-expensive flexible mode) procurement with option contracts in three layers: a <i>contract negotiation</i> layer, where the firm chooses the best combination of lead time and price for each mode from the supply contract menu; a capacity <i>reservation</i> layer, where the firm reserves total equipment procurement quantities from the two supply modes before the planning horizon starts; and an <i>execution</i> layer, where the firm orders equipment from the two supply modes based on the updated demand information. We first investigate the execution layer as a dynamic dual-source capacity expansion problem with demand backlogging and demonstrate that the optimal policy lacks structure even under the simplest setting. Thus, we propose a heuristic solution for the execution-layer problem, which also serves as a building block for the other two layers. Through numerical analysis, we quantify the value of the added flexibility of DMEP for the firm. The DMEP framework has been implemented at Intel Corporation and has resulted in savings of tens of millions of dollars for one process technology.", "e:keyword": ["Capital-intensive industries", "Equipment procurement", "Capacity expansion", "Forecast revision", "Dual-mode procurement", "Dual sourcing", "Option contracts", "Heuristic", "OM practice"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0357", "e:abstract": "Cap and trade programs impose limits on industry emissions but offer individual firms the flexibility to choose among different operational levers toward compliance, including inputs, process changes, and the use of allowances to account for emissions. In this paper, we examine the relationships among (1) levers for compliance (at-source pollution prevention, end-of-pipe pollution control, and the use of allowances); (2) environmental performance; and (3) firm market performance for the context of stringent cap and trade regulation with allowance grandfathering (i.e., the allocation of allowances for free). To investigate these relationships, we use data on publicly traded utility firms operating coal-fired generating units regulated by the U.S. Acid Rain Program from three principal sources: the U.S. Energy Information Administration, the U.S. Environmental Protection Agency, and the Compustat database. Our results indicate a significant relationship between better environmental performance and lower firm market performance over at least a three-year period. From a regulatory perspective, our results show a negative association between allowance grandfathering and firm environmental performance. Overall, by explicitly considering the context of stringent regulation, we find a counter-example to the view that better environmental performance generally associates with better economic performance.", "e:keyword": ["Environmental operations", "Public policy", "Environmental compliance", "Cap and trade", "Empirical research"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0358", "e:abstract": "We study a buyer's strategic use of a dual-sourcing option when facing suppliers possessing private information about their disruption likelihood. We solve for the buyer's optimal procurement contract. We show that the optimal contract can be interpreted as the buyer choosing between diversification and competition benefits. Better information increases diversification benefits and decreases competition benefits. Therefore, with better information the buyer is more inclined to diversify. Moreover, better information may increase or decrease the value of the dual-sourcing option, depending on the buyer's unit revenue: for large revenue, the buyer uses the dual sourcing option for diversification, the benefits of which increase with information; for small revenue, the buyer uses the dual sourcing option for competition, the benefits of which decrease with information. Surprisingly, as the reliability of the entire supply base decreases, the buyer may stop diversifying under asymmetric information (to leverage competition), whereas it would never do so under symmetric information. Finally, we analyze the effect of codependence between supply disruptions. We find that lower codependence leads the buyer to rely less on competition. Because competition keeps the information costs in check, a reduction in supplier codependence increases the buyer's value of information. Therefore, strategic actions to reduce codependence between supplier disruptions should not be seen as a substitute for learning about suppliers' reliabilities.", "e:keyword": ["Auctions and mechanism design", "Incentives and contracting", "Supply chain management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0360", "e:abstract": "We examine the role of reservations in capacity-constrained services with a focus on restaurants. Although customers value reservations, restaurants typically neither charge for them nor impose penalties for failing to keep them. However, reservations impose costs on firms offering them. We offer a novel motivation for offering reservations that emphasizes the way in which reservations can alter customer behavior. We focus on a market in which demand is uncertain and the firm has limited capacity. There is a positive chance that the firm will not have enough capacity to serve all potential customers. Customers are unable to observe how many potential diners are in the market before incurring a cost to request service. Hence, if reservations are not offered, some may choose to stay home rather than risk being denied service. This lowers the firm's sales when realized demand is low. Reservations increase sales on a slow night by guaranteeing reservations holders service. However, some reservation holders may choose not to use their reservations resulting in no-shows. The firm must then trade off higher sales in a soft market with sales lost to no-shows on busy nights. We consequently evaluate various no-show mitigation strategies, all of which serve to make reservations more likely in equilibrium. Competition also makes reservations more attractive; when there are many small firms in the market, reservations are always offered.", "e:keyword": ["Service management", "Reservations", "Restaurants", "Capacity management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0359", "e:abstract": "A waste-to-energy firm that recycles organic waste with energy recovery performs two environmentally beneficial functions: it diverts waste from landfills and it produces renewable energy. At the same time, the waste-to-energy firm serves and collects revenue from two types of customers: waste generators who pay for waste disposal service and electricity consumers who buy energy. Given the process characteristics of the waste-to-energy operation, the market characteristics for waste disposal and energy, and the mechanisms regulators use to encourage production of renewable energy, we determine the profit-maximizing operating strategy of the firm. We also show how regulatory mechanisms affect the operating decisions of the waste-to-energy firm. Our analyses suggest that if the social planner's objective is to maximize landfill diversion, offering a subsidy as a per kilowatt-hour for electricity is more cost effective, whereas if the objective is to maximize renewable energy generation, giving a subsidy as a lump sum to offset capital costs is more effective. This has different regulatory implications for urban and rural settings where the environmental objectives may differ.", "e:keyword": ["Organic waste to energy", "Sustainability", "Environment", "Operating strategy", "Regulation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0362", "e:abstract": "This paper analyzes a unit-contingent power purchase agreement between an electricity distributor and a power plant. Under such a contract the distributor pays the plant a fixed price if the plant is operational and nothing if plant outage occurs. Pricing a unit-contingent contract is complicated by the fact that the plant's true status is its private information. The difference between the electricity spot price and the unit-contingent contract price provides an incentive for the plant to misreport its status and earn profit at the distributor's expense. To prevent misreporting, the distributor may inspect the plant and levy penalties if misreporting is discovered. We find that some type of misreporting under certain circumstances can benefit both the plant and the distributor, because it serves as a risk-allocation mechanism between the two parties. We show that such a risk-allocation mechanism is equivalent to using state-contingent options and prohibiting misreporting.", "e:keyword": ["Electricity industry", "Unit-contingent contract", "Spot market", "Risk allocation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0363", "e:abstract": "This paper investigates the impact of dependence among successive service times on the transient and steady-state performance of a large-scale service system. This is done by studying an infinite-server queueing model with time-varying arrival rate, exploiting a recently established heavy-traffic limit, allowing dependence among the service times. This limit shows that the number of customers in the system at any time is approximately Gaussian, where the time-varying mean is unaffected by the dependence, but the time-varying variance is affected by the dependence. As a consequence, required staffing to meet customary quality-of-service targets in a large-scale service system with finitely many servers based on a normal approximation is primarily affected by dependence among the service times through this time-varying variance. This paper develops formulas and algorithms to quantify the impact of the dependence among the service times on that variance. The approximation applies directly to infinite-server models but also indirectly to associated finite-server models, exploiting approximations based on the peakedness (the ratio of the variance to the mean in the infinite-server model). Comparisons with simulations confirm that the approximations can be useful to assess the impact of the dependence.", "e:keyword": ["Large-scale service systems", "Dependence among service times", "Stochastic models", "Infinite-server queueing models", "Peakedness", "Time-varying arrival rates"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0365", "e:abstract": "A retailer's product selection decisions are largely driven by her assumptions on how consumers make choices. We use a ranking-based consumer choice model to represent consumer preferences: every customer has a ranking of the potential products in the category and purchases his highest ranked product (if any) offered in the assortment. We consider four practically motivated special cases of this model, namely, the one-way substitution, the locational choice, the outtree, and the intree preference models, and we study the retailer's product selection problem when products have different price and cost parameters. We assume that the retailer incurs a fixed carrying cost per product offered, a goodwill penalty for each customer who does not purchase his first choice and a lost sale penalty for each customer who does not find an acceptable product to buy. For the first three models, we obtain efficient solution methods that simplify to either a shortest path method or a dynamic program. For the fourth model, we construct an effective algorithm and show numerically that, in practice, it is much faster than enumeration. We also obtain valuable insights on the structure of the optimal assortment.", "e:keyword": ["Assortment selection", "Ranking-based choice model"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0364", "e:abstract": "A decentralized system of competing retailers that order and sell the same product in a sales season is studied. When a customer demand occurs at a stocked-out retailer, that retailer requests a unit to be transshipped from another retailer who charges a transshipment price. If this request is rejected, the unsatisfied customer may go to another retailer with a customer overflow probability. Each retailer decides on the initial order quantity from a manufacturer and on the acceptance/rejection of <i>each</i> transshipment request. For two retailers, we show that retailers' optimal transshipment policies are dynamic and characterized by chronologically nonincreasing inventory holdback levels. We analytically study the sensitivity of holdback levels to explain interesting findings, such as smaller retailers and geographically distant retailers benefit more from transshipments. Numerical experiments show that retailers substantially benefit from using optimal transshipment policies compared to no sharing. The expected sales increase in all but a handful of over 3,000 problem instances. Building on the two-retailer optimal policies, we suggest an effective heuristic transshipment policy for a multiretailer system.", "e:keyword": ["Dynamic transshipment policy", "Demand overflow", "Decentralized distribution system"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0366", "e:abstract": "Recent cases of product adulteration by foreign suppliers have compelled many manufacturers to rethink approaches to deterring suppliers from cutting corners, especially when manufacturers cannot fully monitor and control the suppliers' actions. In this paper, we study three mechanisms for dealing with product adulteration problems: (a) the deferred payment mechanism--the buyer pays the supplier after the deferred payment period only if no adulteration has been discovered by the customers; (b) the inspection mechanism--the buyer pays the supplier immediately, contingent on product passing the inspection; and (c) the combined mechanism--a combination of the deferred payment and inspection mechanisms. We show that the inspection mechanism cannot completely deter the suppliers from product adulteration, whereas the deferred payment mechanism can. Surprisingly, the combined mechanism is redundant: either the inspection or the deferred payment mechanisms perform just as well. Finally, we identify four factors that determine the dominance of deferred payment mechanism over the inspection mechanism: (a) the inspection cost relative to inspection accuracy, (b) the buyer's liability for adulterated products, (c) the difference in financing rates for the buyer and the supplier relative to the defects discovery rate by customers, and (d) the difference in production costs for adulterated and unadulterated product. We find that the deferred payment mechanism is preferable to inspection if the threats of adulteration (either incentive to adulterate or the consequences) are low.", "e:keyword": ["Operations strategy", "Incentives and contracting", "Operations and finance interface", "Supply risk management", "Trade credit"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0368", "e:abstract": "Measures to extend the economic lives of products--such as remanufacturing carried out by <i>closed-loop supply chains</i>--are receiving increased attention because of various economic and regulatory factors. In this paper, we examine drivers of price differentials between new and remanufactured products using data on purchases made on eBay. Our analysis shows that seller reputation significantly explains the price differentials between new and remanufactured products. We also find that products remanufactured by original equipment manufacturers or their authorized factories are purchased at relatively higher prices than products remanufactured by third parties. However, in the presence of these reputation signals (seller reputation and remanufacturer identity), we find that stronger warranties are not significantly associated with higher prices paid for remanufactured products. Our work contributes to the closed-loop supply chain research stream in operations management by empirically examining market factors that have not been studied before.", "e:keyword": ["Environmental operations", "Empirical research", "Closed-loop supply chains", "Remanufacturing", "Seller reputation", "eBay", "OM-marketing interface", "OM-information systems interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0370", "e:abstract": "This study extends the single-period vertical price interaction in a manufacturer-retailer dyad to a multiperiod setting. A manufacturer distributes a durable product through an exclusive retailer to an exhaustible population of consumers with heterogeneous reservation prices. In each period, the manufacturer and retailer in turn set wholesale and retail prices, respectively, and customers with valuation above the retail price adopt the product at a constant (hazard) rate. We derive the open-loop, feedback, and myopic equilibria for this dynamic pricing game and compare it to the centralized solution. Although in an integrated supply chain a forward-looking dynamic pricing strategy is always desirable, we show that this is not the case in a decentralized setting, because of vertical competition. Our main result is that both supply chain entities are better off in the long run when they ignore the impact of current prices on future demand and focus on immediate-term profits. A numerical study confirms that this insight is robust under various supply- and demand-side effects. We use the channel efficiency corresponding to various pricing rules to further derive insights into decisions on decentralization and disintermediation.", "e:keyword": ["Supply chain management", "Dynamic pricing", "Differential game", "Channels of distribution", "New product diffusion", "Operations-marketing interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0376", "e:abstract": "A growing segment of the revenue management and pricing literature assumes \"strategic\" customers who are forward-looking in their pursuit of utility. Recognizing that such behavior may not be directly observable by a seller, we examine the implications of seller uncertainty over strategic customer behavior in a markdown pricing setting. We assume that some proportion of customers purchase impulsively in the first period if the price is below their willingness to pay, while other customers strategically wait for lower prices in the second period. We consider a two-period selling season in which the seller knows the aggregate demand curve but not the proportion of customers behaving strategically. We show that a robust pricing policy that requires no knowledge of the extent of strategic behavior performs remarkably well. We extend our model to a setting with stochastic demand, and show that the robust pricing policy continues to perform well, particularly as capacity is loosened or the problem is scaled up. Our results underscore the need to recognize strategic behavior, but also suggest that in many cases effective performance is possible without precise knowledge of strategic behavior.", "e:keyword": ["Consumer behavior", "Pricing and revenue management", "Retailing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0377", "e:abstract": "We consider a firm that sells two vertically (quality) differentiated products to strategically forward-looking consumers over two periods, setting the prices dynamically in each period. The consumers are heterogeneous in their evaluations of quality, and strategic in that they decide not only whether and which product variant to buy, but also when to buy, choosing the option that maximizes their utility. We derive the equilibrium of the pricing-purchasing game between the firm and the consumers. We find that the loss due to strategic customer behavior can be less with two product variants compared to the single-product benchmark, which indicates that product variety can serve as a lever when dealing with strategic customers. This benefit exists when the additional product has an inferior cost-to-quality ratio. Because of this benefit, a firm may find it attractive to sell a product variant that would be unprofitable otherwise. However, product variety can also hurt profitability due to strategic customer behavior: A product variant that would be profitable absent strategic customers can in fact be unprofitable. This can happen when customer impatience and firm costs are moderate.", "e:keyword": ["Strategic customer behavior", "Product variety", "Dynamic pricing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0378", "e:abstract": "We consider the robust design and control of call center systems with flexible interactive voice response (IVR) systems. In a typical call center equipped with an IVR system, customers are handled first by the IVR system, which is a computerized system that often enables customers to self-serve. Those customers whose service cannot be handled by the IVR system are sent to the second stage to be served by agents. To address the design of a call center with a flexible IVR system, we consider a call center whose IVR system can operate in two different service delivery modes that have statistically different outcomes. We formulate a stochastic program to determine the number of agents needed in the second stage as well as the proportion of customers that should be selected for each IVR service mode under different demand levels with the objective of minimizing the total staffing and abandonment costs. We also propose dynamic control policies to achieve the optimal proportions without the knowledge of the exact arrival rate. We show that the staffing levels found by the stochastic program combined with the proposed routing policies are asymptotically optimal in large systems. We present extensive numerical and simulation results to illustrate the effectiveness of the proposed solutions.", "e:keyword": ["Call center management", "Queueing theory", "Stochastic methods"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0382", "e:abstract": "In practice, call center service levels are reported over periods of finite length that are usually no longer than 24 hours. In such small periods the service level has a large variability. It is therefore not sufficient to base staffing decisions only on the expected service level. In this paper we consider the classical <i>M</i>/<i>M</i>/<i>s</i> queueing model that is often used in call centers. We develop accurate approximations for the service-level distribution based on extensive simulations. This distribution is used for a service-level variability-controlled staffing approach to circumvent the shortcomings of the traditional staffing based on the expected service level.", "e:keyword": ["Call centers", "Service level", "Normal distribution", "Simulations", "Staffing"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0373", "e:abstract": "Companies often face nonstationary demand due to product life cycles and seasonality, and nonstationary demand complicates supply chain managers' inventory decisions. This paper proposes a simple heuristic for determining stocking levels in a serial inventory system. Unlike the exact optimization algorithm, the heuristic generates a near-optimal solution by solving a series of independent single-stage systems. The heuristic is constructed based on three results we derive. First, we provide a new cost decomposition scheme based on echelon systems. Next, we show that the optimal base-stock level for each echelon system is bounded by those of two revised echelon systems. Last, we prove that the revised echelon systems are essentially equivalent to single-stage systems. We examine the myopic solution for these single-stage systems. In a numerical study, we find that the change of direction of the myopic solution is consistent with that of the optimal solution when system parameters vary. We then derive an analytical expression for the myopic solution and use it to gain insights into how to manage inventory. The analytical expression shows how future demand affects the current optimal local base-stock level; it also explains an observation that the safety stock at an upstream stage is often stable and may not increase when the demand variability increases over time. Finally, we discuss how the heuristic leads to a time-consistent coordination scheme that enables a decentralized supply chain to achieve the heuristic solution.", "e:keyword": ["Multiechelon", "Single-stage heuristic", "Nonstationary demand", "Myopic solution"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0379", "e:abstract": "Airline flight delays have come under increased scrutiny lately in the popular press, with the Federal Aviation Administration data revealing that airline on-time performance was at its worst level in 13 years in 2007. Flight delays have been attributed to several causes such as weather conditions, airport congestion, airspace congestion, use of smaller aircraft by airlines, etc. In this paper, we examine the impact of the scheduled block time allocated for a flight, a factor controlled by airlines, on on-time arrival performance. We analyze empirical flight data published by the Bureau of Transportation Statistics to estimate the scheduled on-time arrival probability of each commercial domestic flight flown in the United States in 2007 by a major carrier. The structural estimation approach from econometrics is then used to impute the overage to underage cost ratio of the newsvendor model for each flight. Our results show that airlines systematically \"underemphasize\" flight delays, i.e., the flight delay costs implied by the newsvendor model are less than the implied costs of early arrivals for a large fraction of flights. Our results indicate that revenue drivers (e.g., average fare) and competitive measures (e.g., market share) have a significant impact on the scheduled on-time arrival probability. We also show that the scheduled on-time arrival probability is not positively affected by the total number of passengers on the aircraft rotation who could be affected by a flight delay, or the number of incoming and outgoing connecting passengers on a flight. Operational characteristics such as the hub and spoke network structure also have a significant impact on the scheduled on-time arrival probability. Finally, full-service airlines put a higher weight on the cost of late arrivals than do low-cost carriers, and flying on the lowest fare flight on a route results in a drop in the scheduled on-time arrival probability.", "e:keyword": ["Flight delays", "Flight schedules", "Newsvendor model", "Forecasting"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0383", "e:abstract": "Commodity merchants use real option models to manage their operations. A central element of such a model is its underlying operating policy. We focus on network contracts for the transport capacity of natural gas pipelines, specific energy conversion assets. Practitioners commonly manage these contracts as portfolios of spread options. Although computationally fast, we show that this approach in general is heuristic because of the suboptimality of its operating policy. We propose a different computationally efficient real option approach based on an optimal operating policy, integrating linear optimization and Monte Carlo simulation. We use our approach to benchmark the spread option approach in a numerical study based on market data and realistic instances. We find that our approach can substantially improve on the contract valuations computed by the spread option approach, especially for contracts with flexibility in their allowed capacity usage. We also show that the optimal operating policy of contracts with this flexibility is of the greedy type, whereas the one of contracts without it in general is not. However, we observe that greedy optimization yields a near-optimal operating policy for the latter type of contracts with a substantially reduced computational effort. A version of our model was recently implemented by a major international energy trading company. Our model can also be employed to efficiently estimate the contract value sensitivities (the \"Greeks\"), which merchants use for financial hedging purposes. Potentially, our work has wider significance for the merchant management of other commodity conversion assets with payoffs determined by solving capacity constrained optimization models.", "e:keyword": ["Capacity valuation", "Commodity and energy conversion assets", "Energy-related operations", "Heuristics", "Math programming", "Monte Carlo simulation", "Operations management/finance interface", "Operations management practice", "Natural gas pipelines", "Petroleum/natural gas industries", "Real options", "Sensitivities", "Spread options"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0386", "e:abstract": "The value of seasonal energy storage depends on how the firm operates storage to capture seasonal price spreads. Energy storage operations typically face limited operational flexibility characterized by the speed of storing and releasing energy, which makes the optimal policy, in general, difficult to compute. A widely used practice-based heuristic, the rolling intrinsic (RI) policy, generally performs well compared with an optimal policy but can significantly underperform in some cases. In this paper, we aim to understand the gap between the RI policy and the optimal policy and leverage the resulting insights to improve the RI policy. A new heuristic policy, the <i>price-adjusted rolling intrinsic</i> (PARI) policy, is developed based on theoretical analysis of storage options. This heuristic adjusts certain prices before applying the RI policy to provide the RI policy with estimates of the values of various storage options. We evaluate the performance of the RI and PARI polices using actual data from the natural gas industry. Our results show that, on average, the PARI policy recovers about 67% of the value loss of the RI policy. Furthermore, when the value loss of the RI policy is larger, the PARI policy tends to recover a higher fraction of that value loss.", "e:keyword": ["Energy storage operations", "Real options", "OM practice", "Natural gas industry"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0387", "e:abstract": "We consider a newsvendor who dynamically updates her forecast of the market demand over a finite planning horizon. The forecast evolves according to the martingale model of forecast evolution (MMFE). The newsvendor can place multiple orders with increasing ordering cost over time to satisfy demand that realizes at the end of the planning horizon. In this context, we explore the trade-off between improving demand forecast and increasing ordering cost. We show that the optimal ordering policy is a state-dependent base-stock policy and analytically characterize that the base-stock level depends on the information state in a <i>linear</i> (<i>log-linear</i>) fashion for additive (multiplicative) MMFE. We also study a benchmark model where the newsvendor is restricted to order only once. By comparing the multiordering and single-ordering models, we quantify the impact of the multiordering strategy on the newsvendor's expected profit and risk exposure.", "e:keyword": ["Newsvendor", "MMFE", "Forecast evolution", "Dynamic ordering"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0397", "e:abstract": "There is now a broad consensus among healthcare professionals that the U.S. healthcare delivery system is woefully inefficient and needs to be radically redesigned. Healthcare costs have always been a driving force in policy and management, but quality has become equally important in driving decisions, particularly since emerging payment systems include metrics on clinical and operational performance. With the increasing use of information technology to capture financial, operational, and clinical data and to coordinate care across time and different venues, there is a growing demand for operations analysts to examine processes of care and provide much-needed insights on how to better utilize resources to improve outcomes while reducing costs. In this paper, I describe some of the essential features of the U.S. healthcare system and some critical issues that provide opportunities for operations researchers to make important contributions.", "e:keyword": ["Healthcare", "Information technology", "Data analysis", "Capacity planning", "Flexibility", "Coordination", "Practice variation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0384", "e:abstract": "We study the impact of physician workload on hospital reimbursement utilizing a detailed data set from the trauma department of a major urban hospital. We find that the proportion of patients assigned a \"high-severity\" status for reimbursement purposes, which maps, on average, to a 47.8% higher payment for the hospital, is substantially reduced as the workload of the discharging physician increases. This effect persists after we control for a number of systematic differences in patient characteristics, condition, and time of discharge. Furthermore, we show that it is unlikely to be caused by selection bias or endogeneity in either discharge timing or allocation of discharges to physicians. We attribute this phenomenon to a workload-induced reduction in diligence of paperwork execution. We estimate the associated monetary loss to be approximately 1.1% (95% confidence interval, 0.4%-1.9%) of the department's annual revenue.", "e:keyword": ["Empirical", "Hospital operations", "Healthcare reimbursement", "Workload management"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0388", "e:abstract": "Prostate cancer is the most common solid tumor in American men and is screened for using prostate-specific antigen (PSA) tests. We report on a nonstationary partially observable Markov decision process (POMDP) for prostate biopsy referral decisions. The core states are the patients' prostate cancer related health states, and PSA test results are the observations. Transition probabilities and rewards are inferred from the Mayo Clinic Radical Prostatectomy Registry and the medical literature. The objective of our model is to maximize expected quality-adjusted life years. We solve the POMDP model to obtain an age and belief (probability of having prostate cancer) dependent optimal biopsy referral policy. We also prove a number of structural properties including the existence of a control-limit type policy for the biopsy referral decision. Our empirical results demonstrate a nondecreasing belief threshold in age, and we provide sufficient conditions under which PSA screening should be discontinued for older patients. Finally, the benefits of screening under the optimal biopsy referral policy are estimated, and sensitivity analysis is used to prioritize the model parameters that would benefit from additional data collection.", "e:keyword": ["Partially observable Markov decision process", "PSA screening", "Biopsy", "Control-limit policy", "Stopping time problem"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0374", "e:abstract": "This research investigates the effect of process management on clinical and experiential quality. Clinical quality measures hospitals' performance on patient safety, i.e., adherence to standards, whereas experiential quality relates to patient centeredness, i.e., responsiveness to the needs and preferences of the patient. Drawing from the organizational learning literature, we argue for a trade-off between clinical and experiential quality as hospitals emphasize process management. We also study how external and internal forces, i.e., state legislation and hospital leadership, influence this relationship. A combination of primary data and secondary data collected at various time intervals is employed to test our hypotheses. Four important implications emerge from this work. First, we find that hospitals' emphasis on process management is associated with an increase in clinical quality but a decrease in experiential quality. Second, we find that state legislation initially reinforces this trade-off but, overtime, facilitates a positive impact of process management on both quality outcomes. Third, a post hoc analysis suggests that a specific type of hospital leadership, namely, patient-centered leadership, helps mitigate the negative association between process management and experiential quality. Finally, our research provides preliminary evidence regarding the relationship between clinical quality and patient satisfaction contingent on experiential quality. Implications for theory and practice are discussed.", "e:keyword": ["Healthcare operations", "Process management", "Clinical and experiential quality", "Empirical research"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0372", "e:abstract": "We consider the problem of balancing two competing objectives in the pursuit of efficient management of operating rooms in a hospital: providing surgeons with predictable, reliable access to the operating room and maintaining high utilization of capacity. The common solution to the first problem (in practice) is to grant exclusive \"block time,\" in which a portion of the week in an operating room is designated to a particular surgeon, barring other surgeons from using this room/time. As a major improvement over this existing approach, we model the possibility of \"shared\" block time, which need only satisfy capacity constraints in expectation. We reduce the computational difficulty of the resulting NP-hard block-scheduling problem by implementing a column-generation approach and demonstrate the efficacy of this technique using simulation, calibrated to a real hospital's historical data and objectives.Our simulations illustrate substantial benefits to hospitals under a variety of circumstances and demonstrate the advantages of our new approach relative to a benchmark method taken from the recent literature.", "e:keyword": ["Healthcare management", "Math programming", "Production planning and scheduling", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0380", "e:abstract": "We consider the work flow in a medical teaching facility, examining the process that involves an initial patient exam by a resident physician, a subsequent conference between the resident and the attending physician, and the attending physician's visit with the patient. We create an analytical model of a tandem queue with finite buffer space to analyze the impact of different work prioritization policies on the throughput and the flow time of patients in the facility--measures that influence both the facility's finances and patients' satisfaction. We derive throughput-optimal policies and show that these policies involve dynamic batching. This finding is interesting because our model does not include any setup times, and setup times normally imply batching; rather it is the uncertain service times and the requirement for simultaneous service in the conference step that make batching optimal. The optimal dynamic batching policy is complex, so we consider a simpler static batching policy. We show that, in systems with limited buffer space, large batches can sometimes degrade efficiency by simultaneously increasing flow time and decreasing throughput. However, in general, both flow time and throughput increase with batch size. Flow time increases at a faster rate than throughput, so hospital management may want to consider what batch size is optimal given the value it places on the two measures.", "e:keyword": ["Tandem queues", "Concurrent service", "Finite buffer", "Batching", "Flow time versus throughput"]}, {"@id": "http://dx.doi.org/10.1287/msom.1110.0371", "e:abstract": "We develop a finite-horizon discrete-time constrained Markov decision process (MDP) to model diagnostic decisions after mammography where we maximize the total expected quality-adjusted life years (QALYs) of a patient under resource constraints. We use clinical data to estimate the parameters of the MDP model and solve it as a mixed-integer program. By repeating optimization for a sequence of budget levels, we calculate incremental cost-effectiveness ratios attributable to consecutive levels of funding and compare actual clinical practice with optimal decisions. We prove that the optimal value function is concave in the allocated budget. Comparing to actual clinical practice, using optimal thresholds for decision making may result in approximately 22% cost savings without sacrificing QALYs. Our analysis indicates short-term follow-ups are the immediate target for elimination when budget becomes a concern. Policy change is more drastic in the older age group with the increasing budget, yet the gains in total expected QALYs related to larger budgets are predominantly seen in younger women along with modest gains for older women.", "e:keyword": ["Markov decision processes", "Linear programming", "Mixed-integer programming", "Constrained MDPs", "Breast cancer", "Diagnostic decisions", "Cost-effectiveness", "Medical decision making", "Mammography", "Service operations"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0385", "e:abstract": "One key driver of improvement in surgical outcomes is a surgeon's prior experience. However, research notes that not all experience provides equal value for performance. How, then, should surgeons accumulate experience to improve quality outcomes? In this paper, we investigate the differential effects of focal and related (i.e., tasks similar to, but not identical to, the focal task) experience. We open up the black box of the volume-outcome relationship by going beyond just dividing experience into focal and related categories, but also considering how subtasks and context (i.e., the organization in which the work takes place) affect performance. To understand these issues, we assemble a novel data set on 71 cardiothoracic surgeons who performed more than 6,500 procedures during a period of 10 years after the introduction of a breakthrough surgical procedure. We find that, as compared to related experience, surgeon focal experience has a greater effect on surgeon performance. We also demonstrate that subtask experience has different, nonlinear performance relationships for focal and related experience. Finally, we find that focal experience is more firm specific than related experience and that nonfirm experience reduces the learning rate for both focal and related experience. We discuss implications of our findings for healthcare delivery and operations management.", "e:keyword": ["Healthcare", "Knowledge work", "Learning", "Quality", "Specialization", "Variety"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0381", "e:abstract": "The literature on business process design has focused on issues such as bottlenecks, workflow configuration (series versus parallel), replacing an existing workflow with a shorter one, etc. One important issue that has not received adequate attention is the information-intensive nature of medical service systems. Performance of clinical workflows depends not only on how various steps are carried out but also on when certain information items are collected. We report the results of a long-term empirical study that looked at the implementation of a radiology information system (RIS) at a large regional network of radiology clinics. We find that a failure to gather necessary clinical background information in earlier steps significantly delays later steps and causes them to <i>hang over</i>, with a significant impact on the total turnaround time of diagnostic reports. We show that information systems can solve this problem by separating the task of gathering information from its usage and relocating that task upstream in the workflow. We argue that such unbundling can lead to shorter report turnaround times even if it significantly increases the utilization of the bottleneck server. These results have broader implications for the optimal design of other clinical workflows, such as the process of filling prescriptions in pharmacies or the typical surgical preanesthesia evaluation in hospitals. Finally, we explain why the impact of addressing hang-over is often nonuniform across clinical modalities, providers, and patient types.", "e:keyword": ["Medical information systems", "Clinical workflow", "Workflow design", "Hang-over", "Feedback queue"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0402", "e:abstract": "In recent years, the performance-based approach to contracting for medical services has been gaining popularity across different healthcare delivery systems, both in the United States (under the name of \"pay for performance\") and abroad (\"payment by results\" in the United Kingdom). The goal of our research is to build a unified performance-based contracting (PBC) framework that incorporates patient access-to-care requirements and that explicitly accounts for the complex outpatient care dynamics facilitated by the use of an online appointment scheduling system. We address the optimal contracting problem in a principal-agent framework where a service purchaser (the principal) minimizes her cost of purchasing the services and achieves the performance target (a waiting-time target) while taking into account the response of the provider (the agent) to the contract terms. Given the incentives offered by the contract, the provider maximizes his payoff by allocating his outpatient service capacity among three patient groups: urgent patients, dedicated advance patients, and flexible advance patients. We model the appointment dynamics as that of an <i>M/D/1</i> queue and analyze several contracting approaches under adverse selection (asymmetric information) and moral hazard (private actions) settings. Our results show that simple and popular schemes used in practice cannot implement the first-best solution and that the linear performance-based contract cannot implement the second-best solution. To overcome these limitations, we propose a threshold-penalty PBC approach and show that it coordinates the system for an arbitrary patient mix and that it achieves the second-best performance for the setting where all advance patients are dedicated.", "e:keyword": ["Healthcare", "Performance-based contracting", "Principal-agent theory", "Queueing theory"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0394", "e:abstract": "We consider an appointment-based service system (e.g., an outpatient clinic) for which appointments need to be scheduled before the service session starts. Patients with scheduled appointments may or may not show up for their appointments. The service of scheduled patients can be interrupted by emergency requests that have a higher priority. We develop a framework that can be utilized in determining the optimal appointment policies under different assumptions regarding rewards, costs, and decision variables. We propose two methods to evaluate the objective function for a given appointment schedule. We specifically consider two different formulations, both of which aim to balance the trade-off between the patient waiting times and server utilization and carry out a numerical study to provide insights into optimal policies. We find that policies that ignore interruptions perform quite badly, especially when the number of appointments to be scheduled is also a decision variable. We also find that policies that require equally spaced appointments perform reasonably well when the interruption rate is constant. However, their performance worsens significantly when the interruption rate is time dependent.", "e:keyword": ["Healthcare operations management", "Service operations", "Stochastic methods"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0403", "e:abstract": "Fee-for-service (FFS) contracts, first introduced in 2004, dramatically changed the way the pharmaceutical distribution supply chains are designed, managed, and operated. Investment buying (IB), forward buying in anticipation of drug price increases, used to be the way the distributors made most of their profits. FFS contracts limit the amount of inventory distributors can carry at any time (by imposing an inventory cap) and require inventory information sharing from the distributors to the manufacturers while compensating the distributors with a per-unit fee. In spite of its widespread popularity, the FFS model has never been rigorously analyzed or its effectiveness carefully tabulated. In this paper, we formulate the multiperiod stochastic inventory problems faced by the manufacturer and the distributor under the FFS and IB models, derive their optimal policies, and develop procedures to compute the policy parameters. We show that FFS contracts can improve the total supply chain profit--the manufacturer and distributor are now able to share a larger pie. Thus, there exists a range of the per-unit fees that leads to Pareto improvement. Simulation results show that such improvement is approximately 1.7% on average, and as much as 5.5%, and the improvement increases as the inventory cap decreases. Determining the Pareto-improving per-unit fees is a source of contention in FFS contract negotiation, and we propose a simple yet effective heuristic for computing them. Furthermore, supply chain transparency facilitated by the FFS contracts can significantly reduce the manufacturer's supply-demand mismatch costs (by approximately 3.63% on average and as much as 13.01%) and we show that the manufacturer should take advantage of this transparency especially when the inventory cap and drug price increase are high and demand variance is low. We believe that these results have the potential to improve the efficiency of pharmaceutical distribution supply chains, thus reducing the healthcare costs that are such a big burden on the U.S. economy.", "e:keyword": ["Healthcare management", "Supply chain management", "Pharmaceutical supply chains", "Contracts"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0422", "e:abstract": "This article presents three personal experiences and reflections on the process and objective of research in operations management. This invited OM Forum contribution is based on my Manufacturing and Service Operations Management Distinguished Fellow inaugural lecture given at Columbia University on June 18, 2012.", "e:keyword": ["Operations management", "Research", "Relevance", "Rewards", "Research relevance matrix"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0389", "e:abstract": "In this paper, we examine the relationship between inventory levels and one-year-ahead earnings of retailers using publicly available financial data. We use benchmarking metrics obtained from operations management literature to demonstrate an inverted-U relationship between abnormal inventory growth and one-year-ahead earnings per share for retailers. We also find that equity analysts do not fully incorporate the information contained in retailers' abnormal inventory growth in their earnings forecasts, resulting in systematic biases. Finally, we show that an investment strategy based on abnormal inventory growth yields significant abnormal stock market returns.", "e:keyword": ["Econometric analysis", "Retailing", "Operations management-accounting interface"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0390", "e:abstract": "Several design guidelines and flexibility indices have been developed in the literature to inform the design of flexible production networks. In this paper, we propose additional flexibility design guidelines for unbalanced networks, where the numbers of plants and products are not equal, by refining the well-known Chaining Guidelines. We study symmetric networks, where all plants have the same capacity and product demands are independent and identically distributed, and focus mainly on the case where each product is built at two plants. We also briefly discuss cases where (1) each product is built at three plants and (2) some products are built at only one plant. An extensive computational study suggests that our refinements work very well for finding flexible configurations with minimum shortfall in unbalanced networks.", "e:keyword": ["Process flexibility design", "Unbalanced networks", "Chaining Guidelines"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0399", "e:abstract": "Inventory pooling is at the root of many celebrated ideas in operations management. Postponement, component commonality, and resource flexibility are some examples. Motivated by our experience in the aftermarket services industry, we propose a model of inventory pooling to meet differentiated service levels for multiple customers. Our central research question is the following: What are the minimum inventory level and optimal allocation policy when a pool of inventory is used in a single period to satisfy individual service levels for multiple customers? We measure service by the probability of fulfilling a customer's entire demand immediately from stock. We characterize the optimal solution in several allocation policy classes; provide some structural results, formulas, and bounds; and also make detailed interpolicy comparisons. We show that the pooling benefit is always <i>strictly</i> positive, even when there are an arbitrary number of customers with perfectly positively correlated demands.", "e:keyword": ["Inventory pooling", "Type 1 service level", "Inventory allocation policy", "Aftermarket services", "Spare parts", "Pooling benefit", "Demand correlation"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0395", "e:abstract": "A manufacturer must choose whether to <i>delegate</i> component procurement to her tier 1 supplier or <i>control</i> it directly. Because of information asymmetry about suppliers' production costs and the use of simple quantity discount or price-only contracts, either delegation or control can yield substantially higher expected profit for the manufacturer. Delegation tends to outperform control when (1) the manufacturer is uncertain about the tier 1 supplier's cost and believes that it is likely to be high; (2) the manufacturer and the tier 1 supplier know the tier 2 supplier's cost or at least that it will be high; (3) the manufacturer has an alternative to engaging the tier 1 and tier 2 suppliers, such as in-house production; and (4) the firms use price-only contracts as opposed to quantity discount contracts. These results shed light on practices observed in the electronics industry.", "e:keyword": ["Multitier supply chain", "Delegation", "Control", "Asymmetric information", "Component procurement", "Contract design", "Price-only contracts", "Quantity discount contracts", "Robust optimization"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0398", "e:abstract": "This paper studies the preorder strategy that a seller may use to sell a perishable product in an uncertain market with heterogeneous consumers. By accepting preorders, the seller is able to obtain advance demand information for inventory planning and price discriminate the consumers. Given the preorder option, the consumers react strategically by optimizing the timing of purchase. We find that accurate demand information may improve the availability of the product, which undermines the seller's ability to charge a high preorder price. As a result, advance demand information may hurt the seller's profit due to its negative impact for the preorder season. This cautions the seller about a potential conflict between the benefits of advance demand information and price discrimination when facing strategic consumers. A common practice to contain consumers' strategic waiting is to offer price guarantees that compensate preorder consumers in case of a later price cut. Under price guarantees, the seller will reduce price in the regular season only if the preorder demand is low; however, such advance information implies weak demand in the regular season as well. This means that the seller can no longer benefit from a high demand in the regular season. Therefore, under price guarantees, more accurate advance demand information may still hurt the seller's profit due to its adverse impact for the regular season. We also investigate the seller's strategy choice in such a setting (i.e., whether the preorder option should be offered and whether it should be coupled with price guarantees) and find that the answer depends on the relative sizes of the heterogeneous consumer segments.", "e:keyword": ["Preorder", "Advance demand information", "Price discrimination", "Strategic consumer behavior", "Price guarantee"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0405", "e:abstract": "We consider different statistical models for the call arrival process in telephone call centers. We evaluate the forecasting accuracy of those models by describing results from an empirical study analyzing real-life call center data. We test forecasting accuracy using different lead times, ranging from weeks to hours in advance, to mimic real-life challenges faced by call center managers. The models considered are (i) a benchmark fixed-effects model that does not exploit any dependence structures in the data; (ii) a mixed-effects model that takes into account both interday (day-to-day) and intraday (within-day) correlations; and (iii) two new bivariate mixed-effects models, for the joint distribution of the arrival counts to two separate queues, that exploit correlations between different call types. Our study shows the importance of accounting for different correlation structures in the data.", "e:keyword": ["Forecasting", "Arrival process", "Dynamic updating", "Correlation", "Call centers"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0404", "e:abstract": "Firms that offer multiple products are often susceptible to periods of inventory mismatches where one product may face shortages while the other has excess inventories. In this paper, we study a joint implementation of price- and capacity-based substitution mechanisms to alleviate the level of such inventory disparities. We consider a firm producing substitutable products via a capacity portfolio consisting of both product-dedicated and flexible resources and characterize the structure of the optimal production and pricing decisions. We then explore how changes in various problem parameters affect the optimal policy structure. We show that the availability of a flexible resource helps maintain stable price differences across products over time even though the price of each product may fluctuate over time. This result has favorable ramifications from a marketing standpoint because it suggests that even when a firm applies a dynamic pricing strategy, it may still establish consistent price positioning among multiple products if it can employ a flexible replenishment resource. We provide numerical examples for the price stabilization effect and discuss extensions of our results to a more general multiple product setting.", "e:keyword": ["Pricing and revenue management", "Dynamic pricing", "Capacity flexibility", "Inventory control", "Substitutable products"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0400", "e:abstract": "We study the problem of a two-firm supply chain in which firms simultaneously choose a capacity before demand is realized. We focus on the role that private information about demand has on firms' ability to align their capacity decisions. When forecasts are private information, there are at most two equilibria: a complete coordination failure or a monotone equilibrium. The former equilibrium always exists, whereas the latter exists only when the marginal cost of capacity is sufficiently low. We also show that both truthful information sharing and preplay communication have an equilibrium with higher profits. We then test the model's predictions experimentally. Contrary to our theoretical predictions, we show that private demand forecasts do not have a consistently negative effect on firm profits, though capacities are more misaligned. We show that preplay communication may be more effective at increasing profits than truthful information sharing. Finally, we document that \"honesty is the best policy\" when communicating private information.", "e:keyword": ["Communication", "Coordination", "Supply chains", "Experiment"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0406", "e:abstract": "We study strategic customer behavior in a multiserver stochastic service system with a congestion-based staffing (CBS) policy. With the CBS policy, the number of working servers is dynamically adjusted according to the queue length. Besides lining up for free service, customers have the option of paying a fee and getting faster service. Customers' equilibrium behavior is studied under two information scenarios: In the no information scenario, customers only know the long-term statistics, such as the expected waiting time; in the partial information scenario, customers observe the number of working servers and understand the staffing policy upon their arrival. Unlike a queueing system with a constant staffing level, a positive externality is associated with customers' joining the CBS system. Both avoid-the-crowd and follow-the-crowd customer behaviors are possible, and multiple equilibria could exist. We develop the stationary performance measures of the system by considering the customers' strategic behavior. Numerical analysis shows that information can either hurt or improve the performance of the system, depending on the staffing and pricing policy. Another important conclusion is that the system performance is more robust to setting a relatively high than a relatively low price.", "e:keyword": ["Congestion-based staffing", "Pricing", "Delay information", "Strategic customer"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0409", "e:abstract": "Acapacity market is a business-to-business exchange in which equally capable suppliers compete with one another to satisfy generic orders from diverse buyers. The market is asymmetric because the buyers can carry inventory of the products ordered but the suppliers cannot store their capacity. In such a market, we might expect to see something like a price for capacity emerge to equilibrate demand and supply. The financial risk of participating in such a market will be driven by the volatility of the capacity price. In this paper we develop a model to explore the behavior of such a market and demonstrate, for example, that volatility of the price for capacity increases, to a point, when inflexibility of the capacity increases. We can also make statements about how the resolution of price uncertainty in the capacity market is related to the resolution of demand uncertainty faced by the buyers. Another contribution of the paper is to explain the role of market characteristics in how the market acts to minimize shortages caused by consumer demand uncertainty. We use continuous time stochastic optimal control techniques and numerical experiments to demonstrate these insights.", "e:keyword": ["Incentives and contracting", "Supply chain management", "Stochastic methods"]}, {"@id": "http://dx.doi.org/10.1287/msom.1120.0410", "e:abstract": "Consider a set of manufacturers, all of which can subcontract part of their workload to a third party. For simplicity, we assume that every manufacturer as well as the third party each possess a single production facility. Each manufacturer has to decide the amount of workload to be subcontracted so as to minimize the completion time of his in-house and subcontracted workloads. In an effort to provide good service to all, the third party gives priority to manufacturers whose subcontracted workload is small. This incentive scheme forces manufacturers to compete for position in the third-party processing sequence. We develop pure Nash equilibria schedules under three distinct protocols for production.", "e:keyword": ["Operations strategy", "Production planning and scheduling", "Supply chain management", "Incentives and contracting"]}]